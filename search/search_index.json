{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the manuals pages!","title":"index"},{"location":"Android/","text":"Android Debugging Bridge (ADB) \u00b6 ADB is a command-line tool that allows you that allows to manipulate Android devices from the PC command line. To use ADB, we need to: Download the Android Platform Tools that contains the ADB tool and extract it to a folder. On the Android device, enable the \"Developer options\" Connect the Android device to the PC via USB and select \"Transfer files\" mode.","title":"Android"},{"location":"Android/#android-debugging-bridge-adb","text":"ADB is a command-line tool that allows you that allows to manipulate Android devices from the PC command line. To use ADB, we need to: Download the Android Platform Tools that contains the ADB tool and extract it to a folder. On the Android device, enable the \"Developer options\" Connect the Android device to the PC via USB and select \"Transfer files\" mode.","title":"Android Debugging Bridge (ADB)"},{"location":"Arxiv/","text":"Submiting a paper to Arxiv \u00b6 Before submitting a preprint to arxiv, you should check whether the journal allows it, and if so, what version of the paper can be submitted to arxiv. Links: Taylor & Francis Steps on arxiv: First page: details Select arxive as the license type Second page: files Papers written in latex have to be uploaded as source files Third page: check the printed pdf check that the pdf looks good. Check for missing references ( ? ), or figures. Fourth page: Metadata keep authors in the LaTeX encoding, this is the way how arxiv wants it Uploading LaTeX source to Arxiv \u00b6 The procedure should be as follows: delete any images not used in the paper check that the pdf looks good commit and push the changes to master branch delete all journal classes, style files, etc. write a title footnote about how the article is a preprint/accepted manuscript example: This is an Accepted Manuscript of an article published by Taylor & Francis in the Journal of Intelligent Transportation Systems on 4th December 2022, available online: https://www.tandfonline.com/doi/abs/10.1080/15472450.2022.2121651 compile the paper till no errors are present pack source files into a zip archive do not include any temporary files include the compiled bibliography file ( .bbl ) create a new git branch commit to the new branch, push it to the remote Creating new version of a paper \u00b6 In the menu, click Replace . Then, the process is the same as the new submission.","title":"Arxiv"},{"location":"Arxiv/#submiting-a-paper-to-arxiv","text":"Before submitting a preprint to arxiv, you should check whether the journal allows it, and if so, what version of the paper can be submitted to arxiv. Links: Taylor & Francis Steps on arxiv: First page: details Select arxive as the license type Second page: files Papers written in latex have to be uploaded as source files Third page: check the printed pdf check that the pdf looks good. Check for missing references ( ? ), or figures. Fourth page: Metadata keep authors in the LaTeX encoding, this is the way how arxiv wants it","title":"Submiting a paper to Arxiv"},{"location":"Arxiv/#uploading-latex-source-to-arxiv","text":"The procedure should be as follows: delete any images not used in the paper check that the pdf looks good commit and push the changes to master branch delete all journal classes, style files, etc. write a title footnote about how the article is a preprint/accepted manuscript example: This is an Accepted Manuscript of an article published by Taylor & Francis in the Journal of Intelligent Transportation Systems on 4th December 2022, available online: https://www.tandfonline.com/doi/abs/10.1080/15472450.2022.2121651 compile the paper till no errors are present pack source files into a zip archive do not include any temporary files include the compiled bibliography file ( .bbl ) create a new git branch commit to the new branch, push it to the remote","title":"Uploading LaTeX source to Arxiv"},{"location":"Arxiv/#creating-new-version-of-a-paper","text":"In the menu, click Replace . Then, the process is the same as the new submission.","title":"Creating new version of a paper"},{"location":"Google%20office%20apps/","text":"Google sheets \u00b6 Data types \u00b6 This is how we can create various data types in Google Sheets: Dropdown list : Insert -> Dropdown Date and time \u00b6 To refer to the current date, we can use the TODAY() function. Conditional formatting \u00b6 To apply conditional formatting to a range of cells, select the range and go to Format -> Conditional formatting . The rules are sorted from highest to lowest priority. Rule types: Value is (lower, equal, greater) than... Date is (older, equal, newer) than... but not for complex date operations Custom formula for complex conditions =AND(E2>=TODAY(); E2<=TODAY()+90) : date in the next three months Subscript and superscript \u00b6 There is no direct way to format text as subscript or superscript. The easiest way is to copy the subscript/superscript characters from and paste them into the cell. A source for that can be the wiki page for Unicode subscript and superscript . Google docs \u00b6 Troubleshooting \u00b6 Can't open a document on Android \u00b6 This can sometime happen for large documents. The solution is to open the Google Docs application settings and clear both the cache and the app data.","title":"Google office apps"},{"location":"Google%20office%20apps/#google-sheets","text":"","title":"Google sheets"},{"location":"Google%20office%20apps/#data-types","text":"This is how we can create various data types in Google Sheets: Dropdown list : Insert -> Dropdown","title":"Data types"},{"location":"Google%20office%20apps/#date-and-time","text":"To refer to the current date, we can use the TODAY() function.","title":"Date and time"},{"location":"Google%20office%20apps/#conditional-formatting","text":"To apply conditional formatting to a range of cells, select the range and go to Format -> Conditional formatting . The rules are sorted from highest to lowest priority. Rule types: Value is (lower, equal, greater) than... Date is (older, equal, newer) than... but not for complex date operations Custom formula for complex conditions =AND(E2>=TODAY(); E2<=TODAY()+90) : date in the next three months","title":"Conditional formatting"},{"location":"Google%20office%20apps/#subscript-and-superscript","text":"There is no direct way to format text as subscript or superscript. The easiest way is to copy the subscript/superscript characters from and paste them into the cell. A source for that can be the wiki page for Unicode subscript and superscript .","title":"Subscript and superscript"},{"location":"Google%20office%20apps/#google-docs","text":"","title":"Google docs"},{"location":"Google%20office%20apps/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Google%20office%20apps/#cant-open-a-document-on-android","text":"This can sometime happen for large documents. The solution is to open the Google Docs application settings and clear both the cache and the app data.","title":"Can't open a document on Android"},{"location":"Hardware%20Manual/","text":"Diagnostic \u00b6 This chapter is about diagnosing the problem with the PC. Solutions to the problems described here are provided in the following chapters. PC is dead or shout down few seconds after start \u00b6 Unplug the power cables between the power supplz and the components and plug them again If the problem persists, check whether the source is not completely dead by connecting the green cable in the ATX 24 pin connector with anz of the black ones If the source works (power supply fan started), check the voltage on each PIN by the multimeter Corsair PIN Voltage \u00b6 Hard drive diagnostics \u00b6 When handling hard drive problems, we typically want to: identify, whether the hard drive is faulty if it is, try to backup the data buy a new hard drive clone the data from the old hard drive to the new one, or install a fresh copy of the OS, in case the old hard drive is not readable. This section is about step 1, i.e., identifying whether the hard drive is really faulty. Steps: Restart the PC, if the problem persistss, Move the hard drive to another slot, or connect it to another cable, in case of SATA drive. If none of this helps, the hard drive is faulty. It is time to proceed further and try to backup the data. Backup a hard drive from a faulty drive \u00b6 These steps assumes that the backup is run from linux, either from an installation, or from a live USB. When backing the data from a faulty hard drive, we should first check, whether the drive is accessible at all: run lsblk to list all the drives. If the drive is not listed, there is no hope install smartmontools and run smartctl -a <path-to-device> . If this command fails with an error, there is no hope If the smartctl command succeeds, it should show the SMART status of the drive, including statistics like bad sectors, worn out sectors, etc. We can proceed with the backup now. First we should decide whether to clone the whole disk, or just backup files. Clone the disk \u00b6 To clone the disk, we can use the ddrescue tool. Update GPU vBIOS \u00b6 Steps: search the manufacterer's web for the appropriate vBIOS .rom file. It needs to be a file specific for the specific model of the GPU, not just for a number or a series create a bootable DOS USB. It can be easilly created by the rufus tool, which can both download freeDOS and create a bootable USB from it. Puth the rom file to the DOS USB, together with the dost version of the update tool (most likely nvflash for DOS, do not mistake it with the Windows version) run the PC, boot DOS from USB backup the old vBIOS run the update tool using the .rom file Possible problems \u00b6 The USB is not detected in BIOS \u00b6 This problem can have two very different causes: The USB is not bootable. We could use a wrong procedure to create it, or the USB image we try to use is not actually a bootable one, but requires some hardware support to be loaded (e.g., HP PC Hardware Diagnostics). The USB is faulty. We can try to use another USB port, or if still not detected another USB stick.","title":"Hardware Manual"},{"location":"Hardware%20Manual/#diagnostic","text":"This chapter is about diagnosing the problem with the PC. Solutions to the problems described here are provided in the following chapters.","title":"Diagnostic"},{"location":"Hardware%20Manual/#pc-is-dead-or-shout-down-few-seconds-after-start","text":"Unplug the power cables between the power supplz and the components and plug them again If the problem persists, check whether the source is not completely dead by connecting the green cable in the ATX 24 pin connector with anz of the black ones If the source works (power supply fan started), check the voltage on each PIN by the multimeter","title":"PC is dead or shout down few seconds after start"},{"location":"Hardware%20Manual/#corsair-pin-voltage","text":"","title":"Corsair PIN Voltage"},{"location":"Hardware%20Manual/#hard-drive-diagnostics","text":"When handling hard drive problems, we typically want to: identify, whether the hard drive is faulty if it is, try to backup the data buy a new hard drive clone the data from the old hard drive to the new one, or install a fresh copy of the OS, in case the old hard drive is not readable. This section is about step 1, i.e., identifying whether the hard drive is really faulty. Steps: Restart the PC, if the problem persistss, Move the hard drive to another slot, or connect it to another cable, in case of SATA drive. If none of this helps, the hard drive is faulty. It is time to proceed further and try to backup the data.","title":"Hard drive diagnostics"},{"location":"Hardware%20Manual/#backup-a-hard-drive-from-a-faulty-drive","text":"These steps assumes that the backup is run from linux, either from an installation, or from a live USB. When backing the data from a faulty hard drive, we should first check, whether the drive is accessible at all: run lsblk to list all the drives. If the drive is not listed, there is no hope install smartmontools and run smartctl -a <path-to-device> . If this command fails with an error, there is no hope If the smartctl command succeeds, it should show the SMART status of the drive, including statistics like bad sectors, worn out sectors, etc. We can proceed with the backup now. First we should decide whether to clone the whole disk, or just backup files.","title":"Backup a hard drive from a faulty drive"},{"location":"Hardware%20Manual/#clone-the-disk","text":"To clone the disk, we can use the ddrescue tool.","title":"Clone the disk"},{"location":"Hardware%20Manual/#update-gpu-vbios","text":"Steps: search the manufacterer's web for the appropriate vBIOS .rom file. It needs to be a file specific for the specific model of the GPU, not just for a number or a series create a bootable DOS USB. It can be easilly created by the rufus tool, which can both download freeDOS and create a bootable USB from it. Puth the rom file to the DOS USB, together with the dost version of the update tool (most likely nvflash for DOS, do not mistake it with the Windows version) run the PC, boot DOS from USB backup the old vBIOS run the update tool using the .rom file","title":"Update GPU vBIOS"},{"location":"Hardware%20Manual/#possible-problems","text":"","title":"Possible problems"},{"location":"Hardware%20Manual/#the-usb-is-not-detected-in-bios","text":"This problem can have two very different causes: The USB is not bootable. We could use a wrong procedure to create it, or the USB image we try to use is not actually a bootable one, but requires some hardware support to be loaded (e.g., HP PC Hardware Diagnostics). The USB is faulty. We can try to use another USB port, or if still not detected another USB stick.","title":"The USB is not detected in BIOS"},{"location":"Markdown/","text":"Available elements with syntax Linter VSCode Linter specifics CommonMark specification Elements \u00b6 Links \u00b6 Links can be added to a markdown file by using the following syntax: [Link text](http://example.com) There are also simple links where the URL is used as the link text, these are called autolinks <http://example.com> If the link contains spaces, it must be enclosed in angle brackets: [Link text](<some link with spaces>) Local links \u00b6 We can also create local links to headings in the same document: [Link text](#<heading>) Here, <heading> is the heading text in lowercase, with spaces replaced by hyphens, with all special characters removed, two or more hyphens are replaced by one hyphen, and, in case of a heading with the same text, a number is added at the end. Images \u00b6 Images can be added to a markdown file by using the following syntax: ![Alt text](/path/to/image.png) Text formatting \u00b6 Only simple text formatting is supported: italic: *text* bold: **text** code: `code` strikethrough: ~~text~~ underline: __text__ superscript: ^text^ subscript: ~text~ Standardization \u00b6 The only standard for markdown is the CommonMark specification . However, this specification was created ten years after the original markdown was introduced and therefore there are many implementations of markdown does not follow the CommonMark specification. The following implementations follow the CommonMark specification: Markdown-it The following implementations do not follow the CommonMark specification: Python-Markdown Linter \u00b6 There is a linter for markdown available also as a VSCode extension . The linter can be configured: in the .markdownlint.jsonc or markdownlint.yaml file in the root of the project a file with the same name in the home directory in the settings of the VSCode extension ( markdownlint.config ) By default, the linter will check more than 50 rules. Not all of them are based on the CommonMark specification and therefore it may be useful to disable some of them. We can do this by adding the following to the configuration file: { \"MD013\": false } Handling of special characters \u00b6 To use backticks ( ` ) in code, use double backticks: ``code with `backticks` inside`` HTML Content \u00b6 If we want some object that cannot be created by markdown syntax (e.g., a table with merged cells), we can use HTML content. HTML content can be directly inserted into the markdown file, without being wrapped in special tags [ source ].","title":"Markdown"},{"location":"Markdown/#elements","text":"","title":"Elements"},{"location":"Markdown/#links","text":"Links can be added to a markdown file by using the following syntax: [Link text](http://example.com) There are also simple links where the URL is used as the link text, these are called autolinks <http://example.com> If the link contains spaces, it must be enclosed in angle brackets: [Link text](<some link with spaces>)","title":"Links"},{"location":"Markdown/#local-links","text":"We can also create local links to headings in the same document: [Link text](#<heading>) Here, <heading> is the heading text in lowercase, with spaces replaced by hyphens, with all special characters removed, two or more hyphens are replaced by one hyphen, and, in case of a heading with the same text, a number is added at the end.","title":"Local links"},{"location":"Markdown/#images","text":"Images can be added to a markdown file by using the following syntax: ![Alt text](/path/to/image.png)","title":"Images"},{"location":"Markdown/#text-formatting","text":"Only simple text formatting is supported: italic: *text* bold: **text** code: `code` strikethrough: ~~text~~ underline: __text__ superscript: ^text^ subscript: ~text~","title":"Text formatting"},{"location":"Markdown/#standardization","text":"The only standard for markdown is the CommonMark specification . However, this specification was created ten years after the original markdown was introduced and therefore there are many implementations of markdown does not follow the CommonMark specification. The following implementations follow the CommonMark specification: Markdown-it The following implementations do not follow the CommonMark specification: Python-Markdown","title":"Standardization"},{"location":"Markdown/#linter","text":"There is a linter for markdown available also as a VSCode extension . The linter can be configured: in the .markdownlint.jsonc or markdownlint.yaml file in the root of the project a file with the same name in the home directory in the settings of the VSCode extension ( markdownlint.config ) By default, the linter will check more than 50 rules. Not all of them are based on the CommonMark specification and therefore it may be useful to disable some of them. We can do this by adding the following to the configuration file: { \"MD013\": false }","title":"Linter"},{"location":"Markdown/#handling-of-special-characters","text":"To use backticks ( ` ) in code, use double backticks: ``code with `backticks` inside``","title":"Handling of special characters"},{"location":"Markdown/#html-content","text":"If we want some object that cannot be created by markdown syntax (e.g., a table with merged cells), we can use HTML content. HTML content can be directly inserted into the markdown file, without being wrapped in special tags [ source ].","title":"HTML Content"},{"location":"Network/","text":"Setup second router as a switch \u00b6 Tools \u00b6 Router 1 - master - connected to the internet Router 2 - Slave Laptop Setup Router 1 \u00b6 In the wireless security settings of the router, disable Automatic Channel selection and manually set the channel some channel. add an IP address reservation for router 2 to routers 1 setting, e.g. 192.168.0.2 Setup Router 2 \u00b6 On the laptop, log off from wifi and connect the cable to router 2 Disable the DHCP server on this router to prevent IP conflicts or network configuration issues allowing only Router 1 to manage the network. Restart the router Set the IPv4 address on the laptop manually for both laptop and gateway, the gateway has to be the fixed address of the router 2 (e.g. 192.168.0.1) Manually set the IP Address of this router to the IP reserved in step 2. Set the internet connection to static IP. Restart the router Set the IP on the laptop again (to the new address) if the network does not work Connect the two routers using a cable from any of LAN port in router 1 to any LAN port in router 2. In the wireless security settings of this router, disable Automatic Channel selection and manually set the channel to channel a different channel than the one set in step 1 Set up wireless security to be identical in router 2 as it is in router 1.","title":"Network"},{"location":"Network/#setup-second-router-as-a-switch","text":"","title":"Setup second router as a switch"},{"location":"Network/#tools","text":"Router 1 - master - connected to the internet Router 2 - Slave Laptop","title":"Tools"},{"location":"Network/#setup-router-1","text":"In the wireless security settings of the router, disable Automatic Channel selection and manually set the channel some channel. add an IP address reservation for router 2 to routers 1 setting, e.g. 192.168.0.2","title":"Setup Router 1"},{"location":"Network/#setup-router-2","text":"On the laptop, log off from wifi and connect the cable to router 2 Disable the DHCP server on this router to prevent IP conflicts or network configuration issues allowing only Router 1 to manage the network. Restart the router Set the IPv4 address on the laptop manually for both laptop and gateway, the gateway has to be the fixed address of the router 2 (e.g. 192.168.0.1) Manually set the IP Address of this router to the IP reserved in step 2. Set the internet connection to static IP. Restart the router Set the IP on the laptop again (to the new address) if the network does not work Connect the two routers using a cable from any of LAN port in router 1 to any LAN port in router 2. In the wireless security settings of this router, disable Automatic Channel selection and manually set the channel to channel a different channel than the one set in step 1 Set up wireless security to be identical in router 2 as it is in router 1.","title":"Setup Router 2"},{"location":"Security/","text":"This manual is focused on asymmetric encryption . As the principles are more or less the same for all technologies, we will focus on the technical aspects that differs. SSH \u00b6 The SSH is a protocol for secure communication between two computers. The basic usage is to connect to a remote computer: ssh <username>@<address> To close the connection, type exit to the console and press enter. If we do not want to establish a connection, but just run a single command, we can add the command at the end of the ssh command: ssh <username>@<address> <command> By default, ssh use the key authentication if configuared, and resort to password authentication if not. If we want to force password authentication, we can use the -o PreferredAuthentications=password parameter. Authentication \u00b6 There are two ways to authenticate to the server: password private key Note that the server needs to be properly configured to accept your credentials. Specifically: for password authentication, the PasswordAuthentication option in /etc/ssh/sshd_config has to be set to yes . This is often disabled for security reasons. for private key authentication, the public key has to be added to your user account on the server. Generating a key pair \u00b6 To generate a key pair, use the ssh-keygen command. Setting up the private key to be used for ssh connection \u00b6 To use a private key for ssh connection, two conditions have to be met: the private key has to have the right permissions: in Linux, the permissions have to be read/write only for the owner ( 600 ) you have to specify that the private key should be used for the connection. This can be done in multiple ways: using the -i parameter of the ssh command specifying the key in the ~/.ssh/config file using ssh agent (see the SSH Agent section below) selecting the key in an application with GUI SSH Tunneling \u00b6 An SSH tunnel can be created by the ssh command. The usuall syntax is following: ssh -L <local port>:<remote machine>:<remote port> <ssh server username>@<ssh server address> The -L argument stands for local port forwarding , i.e., we forward a local port to some remote port. Example: ssh -L 1111:localhost:5432 fiedler@its.fel.cvut.cz The local port (here 1111 ) is arbitrary, we can use any free port. The aplications using the tunnel should then be configured as: host= localhost port:= 1111 The remote machine is the address of the machine we want to access relative to the ssh server . If the ssh server is running on the computer we want to access through the tunnel, we can use localhost . Analogously, the remote port is the port we wan to use on the remote machine (here 5432 is a PostgreSQL db port). The ssh server username and ssh server address are then the username/address of the remote machine. On top of that, we need password or priveta key to validate our identity. Note that the credential here are the credential for the server, not the credentials of the service we are accessing through the ssh connection. Those credentials has to be usually supplied by the application accessing the service through the ssh tunnel. The connection can be canceled any time byping exit to the console. More information Debugging a SSH tunnel \u00b6 This guide suppose that the tunnel creation comman run without any error message. If the tunnel seems to not work, first use a command line tool to be sure: web browser for HTTP tunnels (remote port 80) psql for postgeSQL tunnels (remote port 5432) telnet for telnet tunnels (reote port 23) If the test is unsucessful, try to kill all ssh connections to the server by shutting down all applications with ssh connections tunnels, untill there will be only one connection at the server (the console). The number of ssh connections can be checked with: sudo netstat -nap | grep :22 Enabnling SSH Access on Server \u00b6 install openssh: sudo apt update sudo apt install openssh-server configure password access open /etc/ssh/sshd_config set PasswordAuothentication yes after restrat you can log in with the user and password used in Ubuntu restart the ssh server: sudo service ssh restart Enabling key authentication for a user \u00b6 The user can only use a private key for authentication if the corresponding public key is assigned to the user on server. This is done by adding the public key to the ~/.ssh/authorized_keys file. Note that the authorized_keys file has to have the right permissions , which is read/write only for the owner, and read only for the group and others ( 644 ). With the authorized_keys file, we can also restrict the user by wrapping the commands he can execute with our own script. This restriction applies to a specific key. We configure the wrapping command by adding the following to the beginning of the key line: command=\"<path to the command>\" ssh-rsa AAAA... WSL configuration \u00b6 port 22 can be used on Windows, so change the port to 2222 when loging from Windows use 127.0.0.1 as a host To change the port: on older systems: change the port in the /etc/ssh/sshd_config file restart the ssh server: sudo service ssh restart on newer systems: mkdir /etc/systemd/system/ssh.socket.d vim /etc/systemd/system/ssh.socket.d/<some name>.conf add the following content: conf [Socket] ListenStream= ListenStream=2222 systemctl daemon-reload systemctl restart ssh.socket SSH Agent \u00b6 Normally, the SSH client process runs only while the SSH session is active, then, it is terminated. That means that we have to reenter the passphrase for the private key every time we want to connect to the server. To overcome this limitation, we can use the an SSH agent program. An SSH agent is a process for storing decrypted SSH keys in memory. This means that we have to enter the passphrase only once per each OS login. The agent can be configured to automatically run on OS startup. The default SSH agent is ssh-agent , the rest of the section is dedicated to this agent. To successfully use the agent, we need to: start the agent, either manually or automatically on OS startup add the keys to the agent (only once) Starting the agent \u00b6 The starting of the agent is different for each OS: Linux : the agent can be started manually by running: bash eval `ssh-agent` We need to evaluate this command as it sets some environment variables. As the process cannot set the environment variables of the parent process due to security reasons, the ssh-agent prints the necessary commands to the console. By using eval, the ssh-agent is executed first, it prints the environment setup commands to stdout, which is captured by the eval command and executed. Windows : we need to start the OpenSSH agent service. Consult the Windows Manual for more details. Listing keys \u00b6 To list the keys stored in the agent, run: ssh-add -l Adding keys \u00b6 To add a key to the agent, run: ssh-add <path to key> Debuging \u00b6 If the agent is running and the key is listed, the first thing to try is to connect via ssh to see whether it is an agent/ssh issue or an issue of the program using the SSH (like git, IDE, file manager...) known_hosts file \u00b6 To know that a connection leads to the desired server and not to some impersonator, the server sends its public key to the client. The client then checks the public key against the list of keys previously set as valid. This list is stored in the .ssh/known_hosts file. The format of the file is: <server address> <key type> <key> Each line contains one server record. What is confusing here is that each server can have multiple records, due to: different key type (e.g., RSA, ECDSA) key for both host name and IP address (e.g., github.com and 140.82.121.3 ) It is important to delete/updete all of these recorsds in case the server change its keys. More info is at this SO answer . Screen: executing a long running process over SSH \u00b6 When the SSH connection to a server is disconnected (either manually, or by network failure or timeout), the process running in the console is canceled. To overcome this limitation, we can use the screen command, which is especially usefull for long running processes. A typical workflow can look like this: execute screen to start the screen session run the long running command disconnect connect to the server again run screen -r to recconect to the session and see the results of the command. after the command is finished, exit the screen session with exit Sometimes, the server does not detect the connection failure and do not allow you to resume the session (step 5). In this way, we need to find the screen session ID and perform a detach and atach: screen -ls read the ID from the output and exec screen -rd <ID> SCP: Copying files over SSH using \u00b6 The scp command is used to copy files over SSH. The syntax is: scp <source> <destination> The <source> and <destination> can be either local or remote. The remote files are specified using the <username>@<address>:<path> syntax. Problems \u00b6 protocol error: filename does not match request : This error is triggered if the path contains unexpected characters. Sometimes, it can be triggered even for correct path, if the local console does not match the remote console. In that case, the solution is to use the -T parameter to disable the security check. file does not exists : This can happen if the path is incorrect, but also if we use some bad quotting. If the solution is not obvious, try the most simple path possible in command line. List all active connections on a server \u00b6 To list all active connections on a server, we can use the lsof command and filter the output for the ssh connections: lsof -i -n | grep ssh Debugging \u00b6 If the server does not respond: check the ssh status with: service ssh status check the ssh port with sudo netstat -tpln If the key is not accepted: Check the log file: sudo tail -c 5000 /var/log/auth.log GNU Privacy Guard (GPG) \u00b6 GPG is a second most popular tool for encryption, after the SSH keys. Apart from encryption capabilities, it offers also a management of subkeys, and a possibility to revoke the key. It can be downloaded from the GNU-PG website . GPG comes with a build in key agent. To list the keys added to the agent, use gpg --list-keys . To import a key, call gpg --import <keyfile> . Key expiration \u00b6 There is a mechanism for key expiration in GPG. However, it is important to understand that the expiration date is mostly not a security feature! It can be useful in the following cases: you lose access to the key, and nobody else can access it as well. In that case, you cannot revoke the key, but you can just wait until the key expires. you set the expiration date for subkeys. For subkeys, the expiration date is a security feature, as it cannot be changed without the main key. To prolong the expiration date, we can use the gpg --edit-key <key-id> command. After using it: choose the key you want to edit by number now the chosen key should be marked with an asterisk * . Enter expire choose the new expiration date save the changes by save Troubleshooting \u00b6 partial length invalid for packet type 63 \u00b6 This can happen if the private key has a wrong encoding. It can be fixed by cnverting the key file to ASCII encoding. Storing passwords for the command line tools \u00b6 Sometimes, we need to store passwords for the command line tools on the local machine. For non-sensitive passwords, we can just store them in a file that is not versioned. However, for sensitive passwords and authentication tokens, we should not store them in plain text. We have two options: Use a Credential Manager, or Encrypt the file where the passwords are stored. Credential Managers \u00b6 There are plenty of credential managers available for each OS. If we want to avoid the differences between the OS, we can use the python keyring package. Python keyring \u00b6 homepage The keyring package is a Python library that provides a way to store passwords in a secure way. It uses a system-specific backend to store the passwords. To manage the passwords, we can use the keyring command provided by the package. The basic usage is: keyring set <service> <username> # set the password. After running this command, the password is prompted keyring get <service> <username> # get the password To use the keyring in a Python script, we use the keyring package: import keyring password = keyring.get_password(\"service\", \"username\") WebAuth (passkeys) \u00b6 WebAuthn is a standard for passwordless authentication using a hardware key. It is a successor of the multifactor authentication. The multifactor authentication typically consists of: a password, supplied by the user authentication by an external device, typically a smartphone Contrary to that, WebAuthn does not require a password, but only a hardware key. Typically, the key is a smart phone, tablet or a laptop. The hardware key validates the user's identity either by a PIN code or by a biometric factor. Web auth has two modes: Multi-factor mode : works like the traditional multifactor authentication: a password is supplied first and then only a confirmation (e.g., hitting enter) is required on the hardware key. Single-factor mode : only the hardware key is required, but user verification is required with the hardware key (e.g., a PIN code or a biometric). Microsoft Login \u00b6 Together with its products, Microsoft pushes a centralized login system. As the system is far from intuitive, we will cover it here. The main login page is https://login.microsoftonline.com. At this page all logged accounts can be seen. Also, we can log in here to a new account. This is very important as Microsoft login system has a design flaw that sometimes cause a redirection to an incorrect login page, if the user uses multiple accounts on the same device. When we use the login.microsoftonline.com page, we bypass this malfunctioning login system and we can choose the correct account manually.","title":"Security"},{"location":"Security/#ssh","text":"The SSH is a protocol for secure communication between two computers. The basic usage is to connect to a remote computer: ssh <username>@<address> To close the connection, type exit to the console and press enter. If we do not want to establish a connection, but just run a single command, we can add the command at the end of the ssh command: ssh <username>@<address> <command> By default, ssh use the key authentication if configuared, and resort to password authentication if not. If we want to force password authentication, we can use the -o PreferredAuthentications=password parameter.","title":"SSH"},{"location":"Security/#authentication","text":"There are two ways to authenticate to the server: password private key Note that the server needs to be properly configured to accept your credentials. Specifically: for password authentication, the PasswordAuthentication option in /etc/ssh/sshd_config has to be set to yes . This is often disabled for security reasons. for private key authentication, the public key has to be added to your user account on the server.","title":"Authentication"},{"location":"Security/#generating-a-key-pair","text":"To generate a key pair, use the ssh-keygen command.","title":"Generating a key pair"},{"location":"Security/#setting-up-the-private-key-to-be-used-for-ssh-connection","text":"To use a private key for ssh connection, two conditions have to be met: the private key has to have the right permissions: in Linux, the permissions have to be read/write only for the owner ( 600 ) you have to specify that the private key should be used for the connection. This can be done in multiple ways: using the -i parameter of the ssh command specifying the key in the ~/.ssh/config file using ssh agent (see the SSH Agent section below) selecting the key in an application with GUI","title":"Setting up the private key to be used for ssh connection"},{"location":"Security/#ssh-tunneling","text":"An SSH tunnel can be created by the ssh command. The usuall syntax is following: ssh -L <local port>:<remote machine>:<remote port> <ssh server username>@<ssh server address> The -L argument stands for local port forwarding , i.e., we forward a local port to some remote port. Example: ssh -L 1111:localhost:5432 fiedler@its.fel.cvut.cz The local port (here 1111 ) is arbitrary, we can use any free port. The aplications using the tunnel should then be configured as: host= localhost port:= 1111 The remote machine is the address of the machine we want to access relative to the ssh server . If the ssh server is running on the computer we want to access through the tunnel, we can use localhost . Analogously, the remote port is the port we wan to use on the remote machine (here 5432 is a PostgreSQL db port). The ssh server username and ssh server address are then the username/address of the remote machine. On top of that, we need password or priveta key to validate our identity. Note that the credential here are the credential for the server, not the credentials of the service we are accessing through the ssh connection. Those credentials has to be usually supplied by the application accessing the service through the ssh tunnel. The connection can be canceled any time byping exit to the console. More information","title":"SSH Tunneling"},{"location":"Security/#debugging-a-ssh-tunnel","text":"This guide suppose that the tunnel creation comman run without any error message. If the tunnel seems to not work, first use a command line tool to be sure: web browser for HTTP tunnels (remote port 80) psql for postgeSQL tunnels (remote port 5432) telnet for telnet tunnels (reote port 23) If the test is unsucessful, try to kill all ssh connections to the server by shutting down all applications with ssh connections tunnels, untill there will be only one connection at the server (the console). The number of ssh connections can be checked with: sudo netstat -nap | grep :22","title":"Debugging a SSH tunnel"},{"location":"Security/#enabnling-ssh-access-on-server","text":"install openssh: sudo apt update sudo apt install openssh-server configure password access open /etc/ssh/sshd_config set PasswordAuothentication yes after restrat you can log in with the user and password used in Ubuntu restart the ssh server: sudo service ssh restart","title":"Enabnling SSH Access on Server"},{"location":"Security/#enabling-key-authentication-for-a-user","text":"The user can only use a private key for authentication if the corresponding public key is assigned to the user on server. This is done by adding the public key to the ~/.ssh/authorized_keys file. Note that the authorized_keys file has to have the right permissions , which is read/write only for the owner, and read only for the group and others ( 644 ). With the authorized_keys file, we can also restrict the user by wrapping the commands he can execute with our own script. This restriction applies to a specific key. We configure the wrapping command by adding the following to the beginning of the key line: command=\"<path to the command>\" ssh-rsa AAAA...","title":"Enabling key authentication for a user"},{"location":"Security/#wsl-configuration","text":"port 22 can be used on Windows, so change the port to 2222 when loging from Windows use 127.0.0.1 as a host To change the port: on older systems: change the port in the /etc/ssh/sshd_config file restart the ssh server: sudo service ssh restart on newer systems: mkdir /etc/systemd/system/ssh.socket.d vim /etc/systemd/system/ssh.socket.d/<some name>.conf add the following content: conf [Socket] ListenStream= ListenStream=2222 systemctl daemon-reload systemctl restart ssh.socket","title":"WSL configuration"},{"location":"Security/#ssh-agent","text":"Normally, the SSH client process runs only while the SSH session is active, then, it is terminated. That means that we have to reenter the passphrase for the private key every time we want to connect to the server. To overcome this limitation, we can use the an SSH agent program. An SSH agent is a process for storing decrypted SSH keys in memory. This means that we have to enter the passphrase only once per each OS login. The agent can be configured to automatically run on OS startup. The default SSH agent is ssh-agent , the rest of the section is dedicated to this agent. To successfully use the agent, we need to: start the agent, either manually or automatically on OS startup add the keys to the agent (only once)","title":"SSH Agent"},{"location":"Security/#starting-the-agent","text":"The starting of the agent is different for each OS: Linux : the agent can be started manually by running: bash eval `ssh-agent` We need to evaluate this command as it sets some environment variables. As the process cannot set the environment variables of the parent process due to security reasons, the ssh-agent prints the necessary commands to the console. By using eval, the ssh-agent is executed first, it prints the environment setup commands to stdout, which is captured by the eval command and executed. Windows : we need to start the OpenSSH agent service. Consult the Windows Manual for more details.","title":"Starting the agent"},{"location":"Security/#listing-keys","text":"To list the keys stored in the agent, run: ssh-add -l","title":"Listing keys"},{"location":"Security/#adding-keys","text":"To add a key to the agent, run: ssh-add <path to key>","title":"Adding keys"},{"location":"Security/#debuging","text":"If the agent is running and the key is listed, the first thing to try is to connect via ssh to see whether it is an agent/ssh issue or an issue of the program using the SSH (like git, IDE, file manager...)","title":"Debuging"},{"location":"Security/#known_hosts-file","text":"To know that a connection leads to the desired server and not to some impersonator, the server sends its public key to the client. The client then checks the public key against the list of keys previously set as valid. This list is stored in the .ssh/known_hosts file. The format of the file is: <server address> <key type> <key> Each line contains one server record. What is confusing here is that each server can have multiple records, due to: different key type (e.g., RSA, ECDSA) key for both host name and IP address (e.g., github.com and 140.82.121.3 ) It is important to delete/updete all of these recorsds in case the server change its keys. More info is at this SO answer .","title":"known_hosts file"},{"location":"Security/#screen-executing-a-long-running-process-over-ssh","text":"When the SSH connection to a server is disconnected (either manually, or by network failure or timeout), the process running in the console is canceled. To overcome this limitation, we can use the screen command, which is especially usefull for long running processes. A typical workflow can look like this: execute screen to start the screen session run the long running command disconnect connect to the server again run screen -r to recconect to the session and see the results of the command. after the command is finished, exit the screen session with exit Sometimes, the server does not detect the connection failure and do not allow you to resume the session (step 5). In this way, we need to find the screen session ID and perform a detach and atach: screen -ls read the ID from the output and exec screen -rd <ID>","title":"Screen: executing a long running process over SSH"},{"location":"Security/#scp-copying-files-over-ssh-using","text":"The scp command is used to copy files over SSH. The syntax is: scp <source> <destination> The <source> and <destination> can be either local or remote. The remote files are specified using the <username>@<address>:<path> syntax.","title":"SCP: Copying files over SSH using"},{"location":"Security/#problems","text":"protocol error: filename does not match request : This error is triggered if the path contains unexpected characters. Sometimes, it can be triggered even for correct path, if the local console does not match the remote console. In that case, the solution is to use the -T parameter to disable the security check. file does not exists : This can happen if the path is incorrect, but also if we use some bad quotting. If the solution is not obvious, try the most simple path possible in command line.","title":"Problems"},{"location":"Security/#list-all-active-connections-on-a-server","text":"To list all active connections on a server, we can use the lsof command and filter the output for the ssh connections: lsof -i -n | grep ssh","title":"List all active connections on a server"},{"location":"Security/#debugging","text":"If the server does not respond: check the ssh status with: service ssh status check the ssh port with sudo netstat -tpln If the key is not accepted: Check the log file: sudo tail -c 5000 /var/log/auth.log","title":"Debugging"},{"location":"Security/#gnu-privacy-guard-gpg","text":"GPG is a second most popular tool for encryption, after the SSH keys. Apart from encryption capabilities, it offers also a management of subkeys, and a possibility to revoke the key. It can be downloaded from the GNU-PG website . GPG comes with a build in key agent. To list the keys added to the agent, use gpg --list-keys . To import a key, call gpg --import <keyfile> .","title":"GNU Privacy Guard (GPG)"},{"location":"Security/#key-expiration","text":"There is a mechanism for key expiration in GPG. However, it is important to understand that the expiration date is mostly not a security feature! It can be useful in the following cases: you lose access to the key, and nobody else can access it as well. In that case, you cannot revoke the key, but you can just wait until the key expires. you set the expiration date for subkeys. For subkeys, the expiration date is a security feature, as it cannot be changed without the main key. To prolong the expiration date, we can use the gpg --edit-key <key-id> command. After using it: choose the key you want to edit by number now the chosen key should be marked with an asterisk * . Enter expire choose the new expiration date save the changes by save","title":"Key expiration"},{"location":"Security/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Security/#partial-length-invalid-for-packet-type-63","text":"This can happen if the private key has a wrong encoding. It can be fixed by cnverting the key file to ASCII encoding.","title":"partial length invalid for packet type 63"},{"location":"Security/#storing-passwords-for-the-command-line-tools","text":"Sometimes, we need to store passwords for the command line tools on the local machine. For non-sensitive passwords, we can just store them in a file that is not versioned. However, for sensitive passwords and authentication tokens, we should not store them in plain text. We have two options: Use a Credential Manager, or Encrypt the file where the passwords are stored.","title":"Storing passwords for the command line tools"},{"location":"Security/#credential-managers","text":"There are plenty of credential managers available for each OS. If we want to avoid the differences between the OS, we can use the python keyring package.","title":"Credential Managers"},{"location":"Security/#python-keyring","text":"homepage The keyring package is a Python library that provides a way to store passwords in a secure way. It uses a system-specific backend to store the passwords. To manage the passwords, we can use the keyring command provided by the package. The basic usage is: keyring set <service> <username> # set the password. After running this command, the password is prompted keyring get <service> <username> # get the password To use the keyring in a Python script, we use the keyring package: import keyring password = keyring.get_password(\"service\", \"username\")","title":"Python keyring"},{"location":"Security/#webauth-passkeys","text":"WebAuthn is a standard for passwordless authentication using a hardware key. It is a successor of the multifactor authentication. The multifactor authentication typically consists of: a password, supplied by the user authentication by an external device, typically a smartphone Contrary to that, WebAuthn does not require a password, but only a hardware key. Typically, the key is a smart phone, tablet or a laptop. The hardware key validates the user's identity either by a PIN code or by a biometric factor. Web auth has two modes: Multi-factor mode : works like the traditional multifactor authentication: a password is supplied first and then only a confirmation (e.g., hitting enter) is required on the hardware key. Single-factor mode : only the hardware key is required, but user verification is required with the hardware key (e.g., a PIN code or a biometric).","title":"WebAuth (passkeys)"},{"location":"Security/#microsoft-login","text":"Together with its products, Microsoft pushes a centralized login system. As the system is far from intuitive, we will cover it here. The main login page is https://login.microsoftonline.com. At this page all logged accounts can be seen. Also, we can log in here to a new account. This is very important as Microsoft login system has a design flaw that sometimes cause a redirection to an incorrect login page, if the user uses multiple accounts on the same device. When we use the login.microsoftonline.com page, we bypass this malfunctioning login system and we can choose the correct account manually.","title":"Microsoft Login"},{"location":"Web%20Tricks/","text":"Download all photos from rajce.cz \u00b6 $(\"#photoList img\").each(function () {var a = $(this).closest(\"a\"); if (a.length){$(document.body).append(\"<img src='\" + a.attr(\"href\") + \"' \\/>\");}});$(\"body>*:not(img)\").remove();","title":"Web Tricks"},{"location":"Web%20Tricks/#download-all-photos-from-rajcecz","text":"$(\"#photoList img\").each(function () {var a = $(this).closest(\"a\"); if (a.length){$(document.body).append(\"<img src='\" + a.attr(\"href\") + \"' \\/>\");}});$(\"body>*:not(img)\").remove();","title":"Download all photos from rajce.cz"},{"location":"XML/","text":"XML is a text format for storing hierarchical data. It was the most popular format for hierarchical data before JSON became popular. Namespaces \u00b6 wiki Namespaces are used to avoid element name conflicts in XML documents. They allow you to use the same element name in the same document without causing ambiguity. Namespaces are declared using the xmlns:<prefix> attribute in the root element of the XML document: <root xmlns:ns=\"http://example.com/ns\"> <ns:element>Value</ns:element> </root> There can be multiple namespaces in a document, each namespace have a different prefix. Typically, the document also has a default namespace (without a prefix) that applies to all elements in the document that do not have a prefix. <root xmlns=\"http://example.com/ns\"> <element>Value</element> </root> Note that handling of namespace prefixes differ between the types of XML processors: DOM : use the namespace prefixes as declared in the xmlns attributes. XPath : ignores the prefixes declared in xmlns . Instead, we have to provide the binding of prefixes to namespase URIs, typically as a dictionary or a map. Check the XPath section for more details. Note that usage of namespaces is optional , you can use XML without namespaces. Also note, that to determine the namespace of an element, it is not enough to look at the element name, because the default namespace can be used. In other words, an element without a prefix can be either in the default namespace or in no namespace at all. XPath \u00b6 wiki XPath (XML Path Language) is a language for navigating XML documents. It allows you to select nodes in an XML document using a path expression. Each path consists of a series of steps separated by slashes ( / ). Each step has three parts: axis : specifies the direction to navigate in the XML document, e.g. child, parent,... node test : specifies the name of the node to select, e.g. book , author , title . predicate : specifies a condition to filter the nodes, e.g.: selecting only nodes with a specific attribute or value. Axis specifiers \u00b6 Only the most common axis specifiers are listed here. <name> , child::<name> : selects child nodes with the specified name. /<name> , descendant::<name> : selects descendant nodes with the specified name. .. , parent : selects the parent node of the current node. . , self : selects the current node. Node tests \u00b6 * : all nodes <name> : selects nodes with the specified name. @<name> : selects attributes with the specified name. *[<number>] : selects the node at the specified index (1-based). *[last()] : selects the last node in the current context. *[@<attribute>='<value>'] : selects nodes with the specified attribute value. Namespaces in XPath \u00b6 An unintuitive aspect of XPath is that it does not read the namespaces prefixes from the XML document . When using XPath in documents with namespaces we have to use the facilities of the XPath processor implementation to bind the prefixes to the namespaces. This includes the default namespace, which does not have a prefix in the XML document, but must be bound to a prefix when using XPath. The binding method depends on the implementation, it is usually done by using a dictionary or a map. Examples of for the most common Programming languages can be found in a SO post . It may seem strange that XPath ignores the prefixes declared in xmlns , yet the matching of namespace URIs to elements is done correctly. This has the following reason: Typically, the XML document is parsed by a DOM parser, which creates a tree of elements. As stated above, the DOM parser uses the prefixes declared in xmlns . Xpath then queries the already parsed document. In this parsed document, each element has the namespace URI already assigned by the DOM parser. Therefore, with the XPath, we can use a completely different prefix than the one declared in the XML document.","title":"XML"},{"location":"XML/#namespaces","text":"wiki Namespaces are used to avoid element name conflicts in XML documents. They allow you to use the same element name in the same document without causing ambiguity. Namespaces are declared using the xmlns:<prefix> attribute in the root element of the XML document: <root xmlns:ns=\"http://example.com/ns\"> <ns:element>Value</ns:element> </root> There can be multiple namespaces in a document, each namespace have a different prefix. Typically, the document also has a default namespace (without a prefix) that applies to all elements in the document that do not have a prefix. <root xmlns=\"http://example.com/ns\"> <element>Value</element> </root> Note that handling of namespace prefixes differ between the types of XML processors: DOM : use the namespace prefixes as declared in the xmlns attributes. XPath : ignores the prefixes declared in xmlns . Instead, we have to provide the binding of prefixes to namespase URIs, typically as a dictionary or a map. Check the XPath section for more details. Note that usage of namespaces is optional , you can use XML without namespaces. Also note, that to determine the namespace of an element, it is not enough to look at the element name, because the default namespace can be used. In other words, an element without a prefix can be either in the default namespace or in no namespace at all.","title":"Namespaces"},{"location":"XML/#xpath","text":"wiki XPath (XML Path Language) is a language for navigating XML documents. It allows you to select nodes in an XML document using a path expression. Each path consists of a series of steps separated by slashes ( / ). Each step has three parts: axis : specifies the direction to navigate in the XML document, e.g. child, parent,... node test : specifies the name of the node to select, e.g. book , author , title . predicate : specifies a condition to filter the nodes, e.g.: selecting only nodes with a specific attribute or value.","title":"XPath"},{"location":"XML/#axis-specifiers","text":"Only the most common axis specifiers are listed here. <name> , child::<name> : selects child nodes with the specified name. /<name> , descendant::<name> : selects descendant nodes with the specified name. .. , parent : selects the parent node of the current node. . , self : selects the current node.","title":"Axis specifiers"},{"location":"XML/#node-tests","text":"* : all nodes <name> : selects nodes with the specified name. @<name> : selects attributes with the specified name. *[<number>] : selects the node at the specified index (1-based). *[last()] : selects the last node in the current context. *[@<attribute>='<value>'] : selects nodes with the specified attribute value.","title":"Node tests"},{"location":"XML/#namespaces-in-xpath","text":"An unintuitive aspect of XPath is that it does not read the namespaces prefixes from the XML document . When using XPath in documents with namespaces we have to use the facilities of the XPath processor implementation to bind the prefixes to the namespaces. This includes the default namespace, which does not have a prefix in the XML document, but must be bound to a prefix when using XPath. The binding method depends on the implementation, it is usually done by using a dictionary or a map. Examples of for the most common Programming languages can be found in a SO post . It may seem strange that XPath ignores the prefixes declared in xmlns , yet the matching of namespace URIs to elements is done correctly. This has the following reason: Typically, the XML document is parsed by a DOM parser, which creates a tree of elements. As stated above, the DOM parser uses the prefixes declared in xmlns . Xpath then queries the already parsed document. In this parsed document, each element has the namespace URI already assigned by the DOM parser. Therefore, with the XPath, we can use a completely different prefix than the one declared in the XML document.","title":"Namespaces in XPath"},{"location":"YAML/","text":"YAML is a simple markup language that uses the syntax that can be easily read by non-programmers. Important links: YAML official website The YAML files should have a .yaml extension source . Characters with special meaning in YAML \u00b6 documentation The following characters have special meaning in YAML: {, }, [, ], &, *, #, ?, |, -, <, >, =, !, %, @, :, ` and , . In strings, these characters should be quoted. They have to be avoided in keys.","title":"YAML"},{"location":"YAML/#characters-with-special-meaning-in-yaml","text":"documentation The following characters have special meaning in YAML: {, }, [, ], &, *, #, ?, |, -, <, >, =, !, %, @, :, ` and , . In strings, these characters should be quoted. They have to be avoided in keys.","title":"Characters with special meaning in YAML"},{"location":"Zotero/","text":"Installation \u00b6 It\u2019s important to do these steps in exactly this order. Connecting to the Zotero account before setting up file sync from owncloud can result in article loss due to low space in the Zotero cloud account! Install and set up ownCloud Install Zotero Log in to Zotero account and sync data from cloud Zotero Preferences -> Sync In Data Syncing Section , connect to Zotero account after successful connection, do not leave settings or confirm the setup! in File Syncing section that just appeared: check Sync attachment files in My Library using and select WebDAV set URL: owncloud.cesnet.cz/remote.php/webdav set username: fiedlda1@fel.cvut.cz set password: <owncloud webdav password> click Verify Server leave settings and click the sync button Install BetterBibLatex plugin (from file) Set up the LaTeX \\cite Command Drag\u2019n\u2019drop \u00b6 Two things need to be set up correctly in the Preferences: Better BibTeX -> QuickCopy/drag-and-drop citations -> QuickCopy format needs to be set to LaTeX Export -> Default format needs to be Better BibTeX Citation Key Quick Copy Integration with Local Projects \u00b6 Automatic Better BibTex Export \u00b6 right-click on your library Export Library Choose Better BibLatex format and check keep updated Save the file to your desired location Integration with Overleaf \u00b6 There are two basic options: Pin the keys using better bibtex and use the standard Overleaf Zotero integration Upload the bibliography file manually to your repo Despite it may seem strange, the first option requires more maintenance, as the keys have to be pinned manually, which can be done in bulk, but new items in your library are not pinned automatically. Therefore, we will use the second option. In order to get automatic upload work, we need to set two things: BetterBibTex automatic export of the Zotero library to file (see Automatic Better BibTex Export) Automatic upload of the file to Overleaf (see next section) Automatic Upload of the Bibliography File to Overleaf \u00b6 Again, there are two options on how to sync your local reference file with Overleaf: Git and Cloud sync. Git has an advantage of keeping the history, but the auto-commit and push hook can be a distraction, and in the case of multiple repositories, it can be also very slow. The cloud sync is simpler and faster but requires manual refresh in overleaf (just like the built-in Zotero integration). Cloud Sync Direct Link Integration \u00b6 Choose a cloud storage provider that supports direct download links (GoogleDrive, DropBox, Owncloud\u2026) export your references to the folder you have in sync with cloud storage Share the reference file by link and copy a direct download link to clipboard In Overleaf, click the upload button, choose from external URL and paste the direct download link to the URL field if your library changes, select the reference file and click on the refresh button Cloud Sync Dropbox Integration \u00b6 Git \u00b6","title":"Zotero"},{"location":"Zotero/#installation","text":"It\u2019s important to do these steps in exactly this order. Connecting to the Zotero account before setting up file sync from owncloud can result in article loss due to low space in the Zotero cloud account! Install and set up ownCloud Install Zotero Log in to Zotero account and sync data from cloud Zotero Preferences -> Sync In Data Syncing Section , connect to Zotero account after successful connection, do not leave settings or confirm the setup! in File Syncing section that just appeared: check Sync attachment files in My Library using and select WebDAV set URL: owncloud.cesnet.cz/remote.php/webdav set username: fiedlda1@fel.cvut.cz set password: <owncloud webdav password> click Verify Server leave settings and click the sync button Install BetterBibLatex plugin (from file)","title":"Installation"},{"location":"Zotero/#set-up-the-latex-cite-command-dragndrop","text":"Two things need to be set up correctly in the Preferences: Better BibTeX -> QuickCopy/drag-and-drop citations -> QuickCopy format needs to be set to LaTeX Export -> Default format needs to be Better BibTeX Citation Key Quick Copy","title":"Set up the LaTeX \\cite Command Drag\u2019n\u2019drop"},{"location":"Zotero/#integration-with-local-projects","text":"","title":"Integration with Local Projects"},{"location":"Zotero/#automatic-better-bibtex-export","text":"right-click on your library Export Library Choose Better BibLatex format and check keep updated Save the file to your desired location","title":"Automatic Better BibTex Export"},{"location":"Zotero/#integration-with-overleaf","text":"There are two basic options: Pin the keys using better bibtex and use the standard Overleaf Zotero integration Upload the bibliography file manually to your repo Despite it may seem strange, the first option requires more maintenance, as the keys have to be pinned manually, which can be done in bulk, but new items in your library are not pinned automatically. Therefore, we will use the second option. In order to get automatic upload work, we need to set two things: BetterBibTex automatic export of the Zotero library to file (see Automatic Better BibTex Export) Automatic upload of the file to Overleaf (see next section)","title":"Integration with Overleaf"},{"location":"Zotero/#automatic-upload-of-the-bibliography-file-to-overleaf","text":"Again, there are two options on how to sync your local reference file with Overleaf: Git and Cloud sync. Git has an advantage of keeping the history, but the auto-commit and push hook can be a distraction, and in the case of multiple repositories, it can be also very slow. The cloud sync is simpler and faster but requires manual refresh in overleaf (just like the built-in Zotero integration).","title":"Automatic Upload of the Bibliography File to Overleaf"},{"location":"Zotero/#cloud-sync-direct-link-integration","text":"Choose a cloud storage provider that supports direct download links (GoogleDrive, DropBox, Owncloud\u2026) export your references to the folder you have in sync with cloud storage Share the reference file by link and copy a direct download link to clipboard In Overleaf, click the upload button, choose from external URL and paste the direct download link to the URL field if your library changes, select the reference file and click on the refresh button","title":"Cloud Sync Direct Link Integration"},{"location":"Zotero/#cloud-sync-dropbox-integration","text":"","title":"Cloud Sync Dropbox Integration"},{"location":"Zotero/#git","text":"","title":"Git"},{"location":"vscode/","text":"Show special characters \u00b6 To show whitespace characters, click on View > Apppearance > Render Whitespace . Line endings cannot be shown in the editor, but the status bar shows the line ending of the current file, and by clicking on it, you can change it.","title":"vscode"},{"location":"vscode/#show-special-characters","text":"To show whitespace characters, click on View > Apppearance > Render Whitespace . Line endings cannot be shown in the editor, but the status bar shows the line ending of the current file, and by clicking on it, you can change it.","title":"Show special characters"},{"location":"Audio%20%26%20Video/FFMpeg/","text":"First download binaries for Windows . Converting a Video to Gif Image \u00b6 Example: ffmpeg -ss 00:00:03 -to 00:00:06 -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=512:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" simod_showcase.gif Used options: -ss 00:00:03 : start at 3 seconds -to 00:00:06 : end at 6 seconds -r 15 : set the frame rate to 15 fps -vf \"scale=512:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" : set the size of the image to 512px width, and split the video into two streams: one for generating the palette, and one for using the palette Note that the order of the options is important. For example, if we put -ss after -i , then the video will be cut after the specified time, but the whole video will be loaded into memory before that. To change the speed, we can use an -itsscale inut option: ffmpeg -itsscale 0.2 -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=838:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -ss 00:00:00 -to 00:00:15 simod_showcase.gif Detailed description on SE A text can be then added to the video using online tools, e.g. ezgif .","title":"FFMpeg"},{"location":"Audio%20%26%20Video/FFMpeg/#converting-a-video-to-gif-image","text":"Example: ffmpeg -ss 00:00:03 -to 00:00:06 -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=512:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" simod_showcase.gif Used options: -ss 00:00:03 : start at 3 seconds -to 00:00:06 : end at 6 seconds -r 15 : set the frame rate to 15 fps -vf \"scale=512:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" : set the size of the image to 512px width, and split the video into two streams: one for generating the palette, and one for using the palette Note that the order of the options is important. For example, if we put -ss after -i , then the video will be cut after the specified time, but the whole video will be loaded into memory before that. To change the speed, we can use an -itsscale inut option: ffmpeg -itsscale 0.2 -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=838:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -ss 00:00:00 -to 00:00:15 simod_showcase.gif Detailed description on SE A text can be then added to the video using online tools, e.g. ezgif .","title":"Converting a Video to Gif Image"},{"location":"Audio%20%26%20Video/VLC/","text":"Showing subtitles under video \u00b6 This can be done using the video cropping filter ( croppadd ) check how much of the black bars are in the video, and how much of it is an empty space (use codec information tool) Options -> Advenced/All -> Filters -> croppadd a. to decrease the black bars contained in the video, set crop from top b. to decrease the black bars not contained in the video, set the padding from bottom Options -> Advenced/All -> Filters -> check the video cropping filter Change the subtitle position Restart VLC","title":"VLC"},{"location":"Audio%20%26%20Video/VLC/#showing-subtitles-under-video","text":"This can be done using the video cropping filter ( croppadd ) check how much of the black bars are in the video, and how much of it is an empty space (use codec information tool) Options -> Advenced/All -> Filters -> croppadd a. to decrease the black bars contained in the video, set crop from top b. to decrease the black bars not contained in the video, set the padding from bottom Options -> Advenced/All -> Filters -> check the video cropping filter Change the subtitle position Restart VLC","title":"Showing subtitles under video"},{"location":"Audio%20%26%20Video/yt-dlp/","text":"Introduction \u00b6 home To install, just run: pip install yt-dlp useful parameters: -x , --extract-audio : extract only the audio from the video","title":"yt-dlp"},{"location":"Audio%20%26%20Video/yt-dlp/#introduction","text":"home To install, just run: pip install yt-dlp useful parameters: -x , --extract-audio : extract only the audio from the video","title":"Introduction"},{"location":"GIS/GIS%20formats/","text":"When we do not want to store the GIS data in a database for some reason, we need to choose from one of the GIS formats. The most common GIS formats are: Shapefile , GeoJSON , and GPS Exchange Format (GPX) . There are also less common formats: Geography Markup Language (GML) Each of the formats has its adventages and disadventages, summarized in the table below: Format File type Supported geometry types Multiple geometry types in a file CRS QGIS support extension Shapefile Binary Point, Line, Polygon No Variable yes .shp, .shx, .dbf GeoJSON Text (JSON) Point, Line, Polygon Yes WGS84 [source] yes .geojson GPS Exchange Format Text (XML) Point, Line Yes WGS84 yes .gpx GML Text (XML) Point, Line, Polygon Yes Variable limited .gml Shapefile \u00b6 GeoJSON \u00b6 specification Geojson is a JSON format for encoding a variety of geographic data structures. Note that GeoJSON applications expect coordinates to be in the WGS84 coordinate reference system (CRS). If the coordinates are in a different CRS, applications may not be able to interpret the data correctly. Geography Markup Language (GML) \u00b6 specification The GML is similar to GeoJSON, but it is based on XML. The problem with GML is that it is not widely used nowadays, and it is not supported by many GIS applications.","title":"GIS formats"},{"location":"GIS/GIS%20formats/#shapefile","text":"","title":"Shapefile"},{"location":"GIS/GIS%20formats/#geojson","text":"specification Geojson is a JSON format for encoding a variety of geographic data structures. Note that GeoJSON applications expect coordinates to be in the WGS84 coordinate reference system (CRS). If the coordinates are in a different CRS, applications may not be able to interpret the data correctly.","title":"GeoJSON"},{"location":"GIS/GIS%20formats/#geography-markup-language-gml","text":"specification The GML is similar to GeoJSON, but it is based on XML. The problem with GML is that it is not widely used nowadays, and it is not supported by many GIS applications.","title":"Geography Markup Language (GML)"},{"location":"GIS/OpenStreetMap/","text":"Search element by id \u00b6 The searchbox does not work for ids. However, we can edit the URL to search for an element by id. Example: https://www.openstreetmap.org/?way=<way id>","title":"OpenStreetMap"},{"location":"GIS/OpenStreetMap/#search-element-by-id","text":"The searchbox does not work for ids. However, we can edit the URL to search for an element by id. Example: https://www.openstreetmap.org/?way=<way id>","title":"Search element by id"},{"location":"GIS/Overpass%20Manual/","text":"Sources \u00b6 wiki/Overpass QL Strucutre \u00b6 Every statement ents with ; . Sets \u00b6 Almost all statements works with sets of data. Unless specified, we work with a default set ( _ ). To wrrito to a specific set, we can use ->. operator: <statement>->.<set> writes the result of the <statement> to the . The default set can be ommited: <statement> is equal to <statement>->._ . out statement \u00b6 All queries should contain an out statement that determines the output format. out is used for data only request out geom returns data with all elements associated with their geometry. Note that while the output format can be specified, we cannot filter the output (e.g., we cannot filter the ralation members ). Area specification \u00b6 We select an area by name as: area[\"name\"=\"Letkov\"]; And then we used it as: node(area); If we need more areas in a query, we can store them in variable: area[\"name\"=\"Letkov\"]->.let; And: node(area.let); Important note: If an area needs to be used repetadly, it has to be named, as the value of area ( area._ ) is replaced by the output of any statement. Filtering \u00b6 filters are specified in brackets: rel[\"admin_level\"=8]; It is also possible to use a regex filtering, we just neeed to replace = with ~ : rel[\"admin_level\"~\".*\"]; Selecting Multiple Data Sets \u00b6 Implicitely, all filters are aplied to a default dataset called _ and also written to it. Therefore, we cannot do: rel[\"admin_level\"~\".*\"]; node; because we are basically selecting nodes from a set of relations. Solutions: 1) Union statement 2) Named sets Union Utatement \u00b6 Union statement is surounded by parantheses. We can achieve the intended behaviour by: ( rel[\"admin_level\"~\".*\"]; node; ); Select Area Boundary \u00b6 Sometimes, it is usefull to check the boundary of the named area. However, the administrative level differ for various areas cities, countries, so the admin_level relation cannot be used for that. Luckilly, there is an option to use the pivot filter. Example: area[\"name\"=\"Praha\"]; node(pivot); out geom; Discover the full name of an area \u00b6 If we want to know the full name of the area while the above query return multiple results, we can do that in openstreet map: Move the map to see the area Click the button with cusor and question mark to select the exploration tool Click inside the area Scroll down to area relations Click on the proper region The name property is what we are looking for Filter areas with duplicite names \u00b6 Sometimes, even the full name of the area is not specific enough. In that case, we can use two approaches: select the area by the area relation id specify the area by the higher level area (state, country) Select area by ID \u00b6 select the requested area copy the id and add 3 600 000 000 to it (start with 36 and fill zerose till there are 10 digits in total) replace area[\"name\"=\"<NAME>] with area(<ID>) . Note that round brackets are used instead of square brackets! Specify area with higher level area \u00b6 In this exaple, we select the Coo County, Illinois. area[name=\"Illinois\"]; rel[name=\"Cook County\"](area); map_to_area; node(pivot); out geom; more info Get historical data \u00b6 To get historical data, prepend the query with a date statement. Example: [date:\"2016-01-01T00:00:00Z\"]; area[name=\"City of New York\"]; node(pivot); out geom; Overpass in Python \u00b6 In Python, we can use the overpy to access the Overpass API. The basic usage is: import overpy api = overpy.Overpass() query = \"node(50.0878, 14.4207, 50.0888, 14.4217); out;\" result = api.query(query)","title":"Overpass Manual"},{"location":"GIS/Overpass%20Manual/#sources","text":"wiki/Overpass QL","title":"Sources"},{"location":"GIS/Overpass%20Manual/#strucutre","text":"Every statement ents with ; .","title":"Strucutre"},{"location":"GIS/Overpass%20Manual/#sets","text":"Almost all statements works with sets of data. Unless specified, we work with a default set ( _ ). To wrrito to a specific set, we can use ->. operator: <statement>->.<set> writes the result of the <statement> to the . The default set can be ommited: <statement> is equal to <statement>->._ .","title":"Sets"},{"location":"GIS/Overpass%20Manual/#out-statement","text":"All queries should contain an out statement that determines the output format. out is used for data only request out geom returns data with all elements associated with their geometry. Note that while the output format can be specified, we cannot filter the output (e.g., we cannot filter the ralation members ).","title":"out statement"},{"location":"GIS/Overpass%20Manual/#area-specification","text":"We select an area by name as: area[\"name\"=\"Letkov\"]; And then we used it as: node(area); If we need more areas in a query, we can store them in variable: area[\"name\"=\"Letkov\"]->.let; And: node(area.let); Important note: If an area needs to be used repetadly, it has to be named, as the value of area ( area._ ) is replaced by the output of any statement.","title":"Area specification"},{"location":"GIS/Overpass%20Manual/#filtering","text":"filters are specified in brackets: rel[\"admin_level\"=8]; It is also possible to use a regex filtering, we just neeed to replace = with ~ : rel[\"admin_level\"~\".*\"];","title":"Filtering"},{"location":"GIS/Overpass%20Manual/#selecting-multiple-data-sets","text":"Implicitely, all filters are aplied to a default dataset called _ and also written to it. Therefore, we cannot do: rel[\"admin_level\"~\".*\"]; node; because we are basically selecting nodes from a set of relations. Solutions: 1) Union statement 2) Named sets","title":"Selecting Multiple Data Sets"},{"location":"GIS/Overpass%20Manual/#union-utatement","text":"Union statement is surounded by parantheses. We can achieve the intended behaviour by: ( rel[\"admin_level\"~\".*\"]; node; );","title":"Union Utatement"},{"location":"GIS/Overpass%20Manual/#select-area-boundary","text":"Sometimes, it is usefull to check the boundary of the named area. However, the administrative level differ for various areas cities, countries, so the admin_level relation cannot be used for that. Luckilly, there is an option to use the pivot filter. Example: area[\"name\"=\"Praha\"]; node(pivot); out geom;","title":"Select Area Boundary"},{"location":"GIS/Overpass%20Manual/#discover-the-full-name-of-an-area","text":"If we want to know the full name of the area while the above query return multiple results, we can do that in openstreet map: Move the map to see the area Click the button with cusor and question mark to select the exploration tool Click inside the area Scroll down to area relations Click on the proper region The name property is what we are looking for","title":"Discover the full name of an area"},{"location":"GIS/Overpass%20Manual/#filter-areas-with-duplicite-names","text":"Sometimes, even the full name of the area is not specific enough. In that case, we can use two approaches: select the area by the area relation id specify the area by the higher level area (state, country)","title":"Filter areas with duplicite names"},{"location":"GIS/Overpass%20Manual/#select-area-by-id","text":"select the requested area copy the id and add 3 600 000 000 to it (start with 36 and fill zerose till there are 10 digits in total) replace area[\"name\"=\"<NAME>] with area(<ID>) . Note that round brackets are used instead of square brackets!","title":"Select area by ID"},{"location":"GIS/Overpass%20Manual/#specify-area-with-higher-level-area","text":"In this exaple, we select the Coo County, Illinois. area[name=\"Illinois\"]; rel[name=\"Cook County\"](area); map_to_area; node(pivot); out geom; more info","title":"Specify area with higher level area"},{"location":"GIS/Overpass%20Manual/#get-historical-data","text":"To get historical data, prepend the query with a date statement. Example: [date:\"2016-01-01T00:00:00Z\"]; area[name=\"City of New York\"]; node(pivot); out geom;","title":"Get historical data"},{"location":"GIS/Overpass%20Manual/#overpass-in-python","text":"In Python, we can use the overpy to access the Overpass API. The basic usage is: import overpy api = overpy.Overpass() query = \"node(50.0878, 14.4207, 50.0888, 14.4217); out;\" result = api.query(query)","title":"Overpass in Python"},{"location":"GIS/QGIS/","text":"Adding points from coordinates \u00b6 The easiest way is to use QuickWKT Count the number of features in area \u00b6 select the right layer on the top panel, third row, select the tool Select Features by area or single click select features Open the attribute table for layer. In the heder, there should be the number of selected features Layers \u00b6 Adding layers \u00b6 To add a layer, use Layer -> Add Layer . Each layer type has its own option in this menu. Temporary layer \u00b6 Temporary layers are useful for experimenting. Whether we want to add a point, line, or area, we need a layer for that even if we do not intend to save the result. The right layer for this is the temporary scratch layer we can add it by: Layer -> Create layer -> New Temporary Scratch Layer... Invalid layer \u00b6 The layer can be marked as invalid because of a missing or invalid index column. Each Postgis layer needs an id column consisting of unique integer values . Postgis layer \u00b6 Postgis layers are layers that are loaded from a Postgis database. A postgis layer can be created from: a table, a view, or a query. In all cases, the layer data has to fulfill the following requirements: the layer has to have a column with unique integer values (id) that is used as a primary key, the layer has to have a column with geometry data, and the layer definition must not modify the data in the database (due to read-only transaction mode). nextval() is therefore not allowed creating temporary tables is not allowed Debugging \u00b6 As an open-source project, QGIS has many bugs. Frequently, the program does not behave as expected, but no problem is reported. In such cases, the best way to find out what is going on is to open the log window: View -> Panels -> Log Messages .","title":"QGIS"},{"location":"GIS/QGIS/#adding-points-from-coordinates","text":"The easiest way is to use QuickWKT","title":"Adding points from coordinates"},{"location":"GIS/QGIS/#count-the-number-of-features-in-area","text":"select the right layer on the top panel, third row, select the tool Select Features by area or single click select features Open the attribute table for layer. In the heder, there should be the number of selected features","title":"Count the number of features in area"},{"location":"GIS/QGIS/#layers","text":"","title":"Layers"},{"location":"GIS/QGIS/#adding-layers","text":"To add a layer, use Layer -> Add Layer . Each layer type has its own option in this menu.","title":"Adding layers"},{"location":"GIS/QGIS/#temporary-layer","text":"Temporary layers are useful for experimenting. Whether we want to add a point, line, or area, we need a layer for that even if we do not intend to save the result. The right layer for this is the temporary scratch layer we can add it by: Layer -> Create layer -> New Temporary Scratch Layer...","title":"Temporary layer"},{"location":"GIS/QGIS/#invalid-layer","text":"The layer can be marked as invalid because of a missing or invalid index column. Each Postgis layer needs an id column consisting of unique integer values .","title":"Invalid layer"},{"location":"GIS/QGIS/#postgis-layer","text":"Postgis layers are layers that are loaded from a Postgis database. A postgis layer can be created from: a table, a view, or a query. In all cases, the layer data has to fulfill the following requirements: the layer has to have a column with unique integer values (id) that is used as a primary key, the layer has to have a column with geometry data, and the layer definition must not modify the data in the database (due to read-only transaction mode). nextval() is therefore not allowed creating temporary tables is not allowed","title":"Postgis layer"},{"location":"GIS/QGIS/#debugging","text":"As an open-source project, QGIS has many bugs. Frequently, the program does not behave as expected, but no problem is reported. In such cases, the best way to find out what is going on is to open the log window: View -> Panels -> Log Messages .","title":"Debugging"},{"location":"Images/IrfanView/","text":"Configuration \u00b6 Sort files \u00b6 By default, the files are sorted by name. For sorting by other criteria, go to Options -> Sort file list","title":"IrfanView"},{"location":"Images/IrfanView/#configuration","text":"","title":"Configuration"},{"location":"Images/IrfanView/#sort-files","text":"By default, the files are sorted by name. For sorting by other criteria, go to Options -> Sort file list","title":"Sort files"},{"location":"Images/Midjourney/","text":"quick start The basic command is /imagine . The format is: \\imagine <description> <parameters> The <description describes the content of the image in human language. The <parameters> of the comand determines what we expect from the image. /imagine parameters \u00b6 Aspect ratio \u00b6 official documentation For aspect ratio, we use --ar or --aspect parameter. The format is <width>:<height> . Both <width> and <height> must be integers. Version \u00b6 official documentation For version, we use --version parameter. Using image input \u00b6 official documentation The format for impage input prompt is: \\imagine <image url> <prompt> <params> We can also adjust the weight of the image input with --iw parameter. Choosing the right variant to vary or upscale \u00b6 After successful generation, emojis can be use to either upscale one of the four generated images, or to vary the image. The order is top-left, top-right, bottom-left, bottom-right. Showing user information \u00b6 official documentation To show user information, use the /info command in any channel (the response will be private). Showing all generated images \u00b6 All generated images can be seen at https://www.midjourney.com/imagine","title":"Midjourney"},{"location":"Images/Midjourney/#imagine-parameters","text":"","title":"/imagine parameters"},{"location":"Images/Midjourney/#aspect-ratio","text":"official documentation For aspect ratio, we use --ar or --aspect parameter. The format is <width>:<height> . Both <width> and <height> must be integers.","title":"Aspect ratio"},{"location":"Images/Midjourney/#version","text":"official documentation For version, we use --version parameter.","title":"Version"},{"location":"Images/Midjourney/#using-image-input","text":"official documentation The format for impage input prompt is: \\imagine <image url> <prompt> <params> We can also adjust the weight of the image input with --iw parameter.","title":"Using image input"},{"location":"Images/Midjourney/#choosing-the-right-variant-to-vary-or-upscale","text":"After successful generation, emojis can be use to either upscale one of the four generated images, or to vary the image. The order is top-left, top-right, bottom-left, bottom-right.","title":"Choosing the right variant to vary or upscale"},{"location":"Images/Midjourney/#showing-user-information","text":"official documentation To show user information, use the /info command in any channel (the response will be private).","title":"Showing user information"},{"location":"Images/Midjourney/#showing-all-generated-images","text":"All generated images can be seen at https://www.midjourney.com/imagine","title":"Showing all generated images"},{"location":"Images/Photopea/","text":"Fill a selection with a color \u00b6 Edit > Fill...","title":"Photopea"},{"location":"Images/Photopea/#fill-a-selection-with-a-color","text":"Edit > Fill...","title":"Fill a selection with a color"},{"location":"LaTeX/Bibliography/","text":"Bibliography \u00b6 See more on SE . For bibliography management, whole toolchain is usually needed, including: a tool that generates the bibliography file (e.g. Zotero, Mendeley, ...) a latex package that cares about the citations style (e.g. biblatex, natbib, or default style) the real bibliography processer that generates and sorts the bibliography (e.g. bibtex, biber, ...) However, not all combinations of theses tools are possible. For understanding the pipeline and the possible combinations, see the following figure: When choosing what package to use in latex, we have to take care that we: have the bibliography file in the right format ( .bib for all pipelines, but the content differs) have the style in the right format ( .bst for default or natbib, .bbx for biblatex) By default, we should use the Biblatex - Biber pipeline. However, there are some circumstances where we need to use bibtex, for example, if we need to use a style that is not available for biblatex (as there is no conversion tool ). The styles available for biblatex are listed on CTAN . Latex document configuration \u00b6 Biblatex styling \u00b6 official biblatex documentation Basic setup: \\usepackage[style=numeric]{biblatex} ... \\addbibresource{bibliography.bib} ... \\printbibliography The style parameter is optional. The styles available for biblatex are listed on CTAN . The style can be further customized by: editing the processed bibliography data ( \\DeclareSourcemap command) editing the bibliography printing using \\AtEveryBibitem command to edit the bibliography item as a whole or content of specific fields using \\DeclareFieldFormat or \\renewbibmacro command to edit the formatting of a bibliography field Customizing the bibliography printing \u00b6 The bibliography printing can be customized: using the \\AtEveryBibitem command to edit the bibliography item as a whole or content of specific fields: latex \\AtEveryBibitem{ \\clearfield{urldate} % remove the url date } Inside the \\AtEveryBibitem we typically use the following macros: \\clearfield{<field>} : remove the field from the bibliography item. Be aware that by doing this, we edit the model, i.e., the field cannot be used in subsequent commands. \\togglefalse{bbx:<field>} : disable the printing of the field. using \\DeclareFieldFormat or \\renewbibmacro command to edit the formatting of a bibliography field: latex \\DeclareFieldFormat{title}{\\textbf{#1}} % make the title bold the difference between \\DeclareFieldFormat and \\renewbibmacro is that the former is used to declare format for a bibliography field, while the latter changes an existing macro that prints the field. It is not an easy task to determine which one is applicable in a given situation, as it depends on the field we want to format, but also on the applied bibliography style. Inside the \\DeclareFieldFormat and \\renewbibmacro we typically use the following macros: \\printfield{<field>} : print the field, styled according to the bibliography style. \\thefield{<field>} : print the field value. There are some biblatex macros that can help us to controll the flow inside our customization commands. These can be used inside \\AtEveryBibitem , but also inside \\DeclareFieldFormat and \\renewbibmacro . Most useful are: \\iffieldundef{<field>}{<true>}{<false>} : if the field is undefined, execute the true branch, otherwise execute the false branch. \\ifhyperref{<true>}{<false>} : if the hyperref package is loaded, execute the true branch, otherwise execute the false branch. With this, we can compile without error even if the hyperref package is not loaded. Customizing the bibliography data \u00b6 The bibliography data can be customized using the \\DeclareSourcemap command. Example: \\DeclareSourcemap{ \\maps{ \\map{ \\step[fieldset=urldate, null] % remove the url date } } } Inside the \\map command, we can use conditional statements: \\pernottype{<type>} : execute the following steps only if the entry type is not <type> In each step, we can use multiple conditions, e.g: \\DeclareSourcemap{ \\maps{ \\map{ \\step[ fieldset=urldate, fieldvalue={2022-01-01}, null] % remove the url date if it is 2022-01-01 } } } Handle overflowing URLs in bibliography \u00b6 Sometimes, the links overflow the bibliography. To fix this, we can use the following commands: \\setcounter{biburllcpenalty}{100} \\setcounter{biburlucpenalty}{100} \\setcounter{biburlnumpenalty}{100} \\biburlnumskip=0mu plus 1mu\\relax \\biburlucskip=0mu plus 1mu\\relax \\biburllcskip=0mu plus 1mu\\relax Default and natbib styling \u00b6 Basic setup: \\bibliographystyle{plain} ... \\bibliography{bibliography} Note that we do not have to use any package to use basic cite commands. Also note, that the \\bibliographystyle command is mandatory . Finally, we do not need to specify the extension of the bibliography file. Natbib \u00b6 The bibtex bibliography management system is quite old and does not support many features. To overcome this, we can use the natbib package: \\usepackage{natbib} Commands for citing \u00b6 There are multiple commands for citing, each resulting in a different output. The two most important variants are in-text citation and parenthetical citation: In-text citation: the citation is part of the text. IEEE: this was proven by Smith et al. [1] APA: this was proven by Smith et al., 2019 Parenthetical citation: the citation is not part of the text. IEEE: this has been proven before [1] APA: this has been proven before (Smith et al., 2019) Unfortunately, the commands for these two variants are not consistent across the bibliography packages. The following table summarizes the commands for the two variants: | Package | In-text citation | Parenthetical citation | Full citation | | --- | --- | --- | --- | | Biblatex | \\textcite{<key>} | \\cite{<key>} ( \\parencite for APA) | \\fullcite{<key>} | | Natbib | \\cite{<key>} | \\citep{<key>} | \\bibentry{<key>} (requires the bibentry package) | There are more citation commands resulting in different styles for each bibliography styling package, and each of these packages can be also configurated for even more customized look. For more information, see the following links: Natbib styles Adittional details for citation (page number, chapter, ... ) \u00b6 To add additional details to the citation, we can use the optional argument of the citation command: \\cite[page~123]{key} Adding a reference to the bibliography without citing it \u00b6 For this, we use the \\nocite command. Example: \\nocite{key} Bibliography entries \u00b6 There are many types of bibliography entries, each of them with different fields. to make things even more complicated, these entries does not match the entry types in Zotero. To make it easier, many use-cases are covered in the table below: Use-case Biblatex Zotero Book @book Book Book chapter @incollection Book Section Conference paper @inproceedings Conference Paper Journal article @article Journal Article Report @report Report Thesis @thesis Thesis Web page @online Web Page Legal document @legal Unavailable. Use Report instead. Other sources: Biblatex documentation Zotero documentation Zotero legal types","title":"Bibliography"},{"location":"LaTeX/Bibliography/#bibliography","text":"See more on SE . For bibliography management, whole toolchain is usually needed, including: a tool that generates the bibliography file (e.g. Zotero, Mendeley, ...) a latex package that cares about the citations style (e.g. biblatex, natbib, or default style) the real bibliography processer that generates and sorts the bibliography (e.g. bibtex, biber, ...) However, not all combinations of theses tools are possible. For understanding the pipeline and the possible combinations, see the following figure: When choosing what package to use in latex, we have to take care that we: have the bibliography file in the right format ( .bib for all pipelines, but the content differs) have the style in the right format ( .bst for default or natbib, .bbx for biblatex) By default, we should use the Biblatex - Biber pipeline. However, there are some circumstances where we need to use bibtex, for example, if we need to use a style that is not available for biblatex (as there is no conversion tool ). The styles available for biblatex are listed on CTAN .","title":"Bibliography"},{"location":"LaTeX/Bibliography/#latex-document-configuration","text":"","title":"Latex document configuration"},{"location":"LaTeX/Bibliography/#biblatex-styling","text":"official biblatex documentation Basic setup: \\usepackage[style=numeric]{biblatex} ... \\addbibresource{bibliography.bib} ... \\printbibliography The style parameter is optional. The styles available for biblatex are listed on CTAN . The style can be further customized by: editing the processed bibliography data ( \\DeclareSourcemap command) editing the bibliography printing using \\AtEveryBibitem command to edit the bibliography item as a whole or content of specific fields using \\DeclareFieldFormat or \\renewbibmacro command to edit the formatting of a bibliography field","title":"Biblatex styling"},{"location":"LaTeX/Bibliography/#customizing-the-bibliography-printing","text":"The bibliography printing can be customized: using the \\AtEveryBibitem command to edit the bibliography item as a whole or content of specific fields: latex \\AtEveryBibitem{ \\clearfield{urldate} % remove the url date } Inside the \\AtEveryBibitem we typically use the following macros: \\clearfield{<field>} : remove the field from the bibliography item. Be aware that by doing this, we edit the model, i.e., the field cannot be used in subsequent commands. \\togglefalse{bbx:<field>} : disable the printing of the field. using \\DeclareFieldFormat or \\renewbibmacro command to edit the formatting of a bibliography field: latex \\DeclareFieldFormat{title}{\\textbf{#1}} % make the title bold the difference between \\DeclareFieldFormat and \\renewbibmacro is that the former is used to declare format for a bibliography field, while the latter changes an existing macro that prints the field. It is not an easy task to determine which one is applicable in a given situation, as it depends on the field we want to format, but also on the applied bibliography style. Inside the \\DeclareFieldFormat and \\renewbibmacro we typically use the following macros: \\printfield{<field>} : print the field, styled according to the bibliography style. \\thefield{<field>} : print the field value. There are some biblatex macros that can help us to controll the flow inside our customization commands. These can be used inside \\AtEveryBibitem , but also inside \\DeclareFieldFormat and \\renewbibmacro . Most useful are: \\iffieldundef{<field>}{<true>}{<false>} : if the field is undefined, execute the true branch, otherwise execute the false branch. \\ifhyperref{<true>}{<false>} : if the hyperref package is loaded, execute the true branch, otherwise execute the false branch. With this, we can compile without error even if the hyperref package is not loaded.","title":"Customizing the bibliography printing"},{"location":"LaTeX/Bibliography/#customizing-the-bibliography-data","text":"The bibliography data can be customized using the \\DeclareSourcemap command. Example: \\DeclareSourcemap{ \\maps{ \\map{ \\step[fieldset=urldate, null] % remove the url date } } } Inside the \\map command, we can use conditional statements: \\pernottype{<type>} : execute the following steps only if the entry type is not <type> In each step, we can use multiple conditions, e.g: \\DeclareSourcemap{ \\maps{ \\map{ \\step[ fieldset=urldate, fieldvalue={2022-01-01}, null] % remove the url date if it is 2022-01-01 } } }","title":"Customizing the bibliography data"},{"location":"LaTeX/Bibliography/#handle-overflowing-urls-in-bibliography","text":"Sometimes, the links overflow the bibliography. To fix this, we can use the following commands: \\setcounter{biburllcpenalty}{100} \\setcounter{biburlucpenalty}{100} \\setcounter{biburlnumpenalty}{100} \\biburlnumskip=0mu plus 1mu\\relax \\biburlucskip=0mu plus 1mu\\relax \\biburllcskip=0mu plus 1mu\\relax","title":"Handle overflowing URLs in bibliography"},{"location":"LaTeX/Bibliography/#default-and-natbib-styling","text":"Basic setup: \\bibliographystyle{plain} ... \\bibliography{bibliography} Note that we do not have to use any package to use basic cite commands. Also note, that the \\bibliographystyle command is mandatory . Finally, we do not need to specify the extension of the bibliography file.","title":"Default and natbib styling"},{"location":"LaTeX/Bibliography/#natbib","text":"The bibtex bibliography management system is quite old and does not support many features. To overcome this, we can use the natbib package: \\usepackage{natbib}","title":"Natbib"},{"location":"LaTeX/Bibliography/#commands-for-citing","text":"There are multiple commands for citing, each resulting in a different output. The two most important variants are in-text citation and parenthetical citation: In-text citation: the citation is part of the text. IEEE: this was proven by Smith et al. [1] APA: this was proven by Smith et al., 2019 Parenthetical citation: the citation is not part of the text. IEEE: this has been proven before [1] APA: this has been proven before (Smith et al., 2019) Unfortunately, the commands for these two variants are not consistent across the bibliography packages. The following table summarizes the commands for the two variants: | Package | In-text citation | Parenthetical citation | Full citation | | --- | --- | --- | --- | | Biblatex | \\textcite{<key>} | \\cite{<key>} ( \\parencite for APA) | \\fullcite{<key>} | | Natbib | \\cite{<key>} | \\citep{<key>} | \\bibentry{<key>} (requires the bibentry package) | There are more citation commands resulting in different styles for each bibliography styling package, and each of these packages can be also configurated for even more customized look. For more information, see the following links: Natbib styles","title":"Commands for citing"},{"location":"LaTeX/Bibliography/#adittional-details-for-citation-page-number-chapter","text":"To add additional details to the citation, we can use the optional argument of the citation command: \\cite[page~123]{key}","title":"Adittional details for citation (page number, chapter, ... )"},{"location":"LaTeX/Bibliography/#adding-a-reference-to-the-bibliography-without-citing-it","text":"For this, we use the \\nocite command. Example: \\nocite{key}","title":"Adding a reference to the bibliography without citing it"},{"location":"LaTeX/Bibliography/#bibliography-entries","text":"There are many types of bibliography entries, each of them with different fields. to make things even more complicated, these entries does not match the entry types in Zotero. To make it easier, many use-cases are covered in the table below: Use-case Biblatex Zotero Book @book Book Book chapter @incollection Book Section Conference paper @inproceedings Conference Paper Journal article @article Journal Article Report @report Report Thesis @thesis Thesis Web page @online Web Page Legal document @legal Unavailable. Use Report instead. Other sources: Biblatex documentation Zotero documentation Zotero legal types","title":"Bibliography entries"},{"location":"LaTeX/LaTeX%20workflow/","text":"So far we have three LaTeX toolchains that has proven to work well: Overleaf : Cloud tool that is stable and very good for collaboration. Texmaker + MiKTex : Traditional desktop setup. VSCode + Latex Workshop + MikTeX/Tinytex : Modern desktop setup. The main advantage is that VSCode has the best Copilot support from all the editors, which is a huge time saver. VSCode + Latex Workshop + MikTeX/Tinytex \u00b6 Installation \u00b6 The installation of VSCode and Latex Workshop (VSCode extension) is straightforward, so we cover only the installation of MikTeX/ Tinytex here. MikTeX \u00b6 The installation of MikTeX is straightforward. MikTeX installs all the required packages on the fly, so there is no need to install them manually. The only thing that we need to do manually is to install Perl which is needed for the latexmk tool. There are two Perl distributions for Windows: ActivePerl and Strawberry Perl . This LaTeX toolchain has been only tested with Strawberry Perl. The installation of Strawberry Perl is straightforward: there is an executable installer. Do not forget to add it to the PATH variable. Do not forget to restart VSCode after the installation of MikTeX and Perl. Tinytex \u00b6 Official installation guide Install Tinytex using the shell script for the respective OS. The links to the scripts are in the installation guide. Add the executable path of Tinytex to the PATH variable. Installing additional packages \u00b6 Unlike MiKTex, Tinytex does not install required packages on the fly. Instead, it only shows an error in the log. To install a missing package, run the following command: tlmgr install <package name> Latex Workshop Configuration and Usage \u00b6 wiki Syncing between PDF and source \u00b6 To jump from PDF to source, use the binding configured in: Settings > Latex Workshop > View > Pdf > Internal > SyncTeX: Keybinding . To jump from source to PDF, use the binding configured in: Keyboad Shortcuts > Latex Workshop: SynTeX from cursor . Unfortunately, mouse cannot be used here due to VSCode limitations .","title":"LaTeX workflow"},{"location":"LaTeX/LaTeX%20workflow/#vscode-latex-workshop-miktextinytex","text":"","title":"VSCode + Latex Workshop + MikTeX/Tinytex"},{"location":"LaTeX/LaTeX%20workflow/#installation","text":"The installation of VSCode and Latex Workshop (VSCode extension) is straightforward, so we cover only the installation of MikTeX/ Tinytex here.","title":"Installation"},{"location":"LaTeX/LaTeX%20workflow/#miktex","text":"The installation of MikTeX is straightforward. MikTeX installs all the required packages on the fly, so there is no need to install them manually. The only thing that we need to do manually is to install Perl which is needed for the latexmk tool. There are two Perl distributions for Windows: ActivePerl and Strawberry Perl . This LaTeX toolchain has been only tested with Strawberry Perl. The installation of Strawberry Perl is straightforward: there is an executable installer. Do not forget to add it to the PATH variable. Do not forget to restart VSCode after the installation of MikTeX and Perl.","title":"MikTeX"},{"location":"LaTeX/LaTeX%20workflow/#tinytex","text":"Official installation guide Install Tinytex using the shell script for the respective OS. The links to the scripts are in the installation guide. Add the executable path of Tinytex to the PATH variable.","title":"Tinytex"},{"location":"LaTeX/LaTeX%20workflow/#installing-additional-packages","text":"Unlike MiKTex, Tinytex does not install required packages on the fly. Instead, it only shows an error in the log. To install a missing package, run the following command: tlmgr install <package name>","title":"Installing additional packages"},{"location":"LaTeX/LaTeX%20workflow/#latex-workshop-configuration-and-usage","text":"wiki","title":"Latex Workshop Configuration and Usage"},{"location":"LaTeX/LaTeX%20workflow/#syncing-between-pdf-and-source","text":"To jump from PDF to source, use the binding configured in: Settings > Latex Workshop > View > Pdf > Internal > SyncTeX: Keybinding . To jump from source to PDF, use the binding configured in: Keyboad Shortcuts > Latex Workshop: SynTeX from cursor . Unfortunately, mouse cannot be used here due to VSCode limitations .","title":"Syncing between PDF and source"},{"location":"LaTeX/Latex%20manual/","text":"This manual covers the usage of the LaTeX language to typeset document. It is about the language itself and about ways how to achieve specific layouts/features. For typography matter, see the typography manual . Document structure \u00b6 The document structure is well documented on wikibooks . The basic structure is: \\documentclass[<options>]{<class>} ... \\begin{document} ... \\end{document} Lot of content is usually put in the preamble, i.e., between the \\documentclass and \\begin{document} commands. The preamble usual content is: loading packages with \\usepackage{<package>} providing new commands or redefining existing commands configuring packages supplying metadata for the document (title, author, etc.) Title page \u00b6 The title page typically contains the title, authors, and potentially other metadata like date or keywords. The standard way how to create the title page is to first define the metadata and then use the \\maketitle command. To print the whole title page. The metadata available for standard article class: \\title{<title>} \\author{<author>} \\date{<date>} . \\today can be used to print the current date. This is the default value if we omit the \\date command. A special non-numbered footnote can be added to most fields using the \\thanks{<text>} command. Authors specification \u00b6 By default, all authors should be filled within a single \\author command, separated by the \\and command. If we need to specify the affiliations, we can do it inside the \\author command. This way, each author have the affiliation printed right after/bellow the author name. However, if there are many authors with shared affiliations, this approach is unsustainable. Instead, we can use the authblk package which lets us specify the authors and affiliations separately and connect them to authors using footnotes. Example: \\usepackage{authblk} \\author[1]{Author 1} \\author[2]{Author 2} \\author[1]{Author 3} \\affil[1]{Affiliation 1} \\affil[2]{Affiliation 2} IEEE author list \u00b6 In IEEE publications, all authors are usually listed in the same \\author command. Also, the affiliations are inside this command, typically created using the \\thanks command. Example: \\author{David Fiedler, Jan Mrkos, Dominika \u0160\u00eddlov\u00e1, and Fabio V. Difonzo \\thanks{This work was supported by the Technology Agency of the Czech Republic within the DOPRAVA 2020+ program, project no. \\texttt{CK04000150}. Fabio V.~Difonzo gratefully thanks the INdAM-GNCS group for partial support. The access to the computational infrastructure of the OP VVV funded project \\texttt{CZ.02.1.01/0.0/0.0/16\\_019/0000765} \"Research Center for Informatics\" is also gratefully acknowledged. \\emph{(Corresponding author: David Fiedler.)}} \\thanks{D. Fiedler, J. Mrkos, and D. \u0160\u00eddlov\u00e1 are with Faculty of Electrical Engineering, CTU in Prague, Prague 121 35, Czech Republic (e-mail: david.fiedler@agents.fel.cvut.cz).} \\thanks{F. V. Difonzo is with Istituto per le Applicazioni del Calcolo \"Mauro Picone\", Consiglio Nazionale delle Ricerche, Bari 70126, Italy.}} Keywords \u00b6 Keywords are not part of the standard article class. If we need to include them when using the standard article class, we can provide the command ourselves. Example: \\providecommand{\\keywords}[1]{\\textbf{\\textit{Index terms---}} #1} ... \\keywords{keyword1, keyword2, ...} Appendices \u00b6 Appendicies are started with the command appendix . Then, each chapter started with the \\chapter command is considered an appendix. Book class structuring \u00b6 Appart from sections and chapters, the book class also supports top level parts for marking special areas of the document. The parts are: \\frontmatter : the front matter of the document, i.e., the title page, abstract, table of contents, etc. The page numbering is in roman numerals. \\mainmatter : the main matter of the document, i.e., the main content. The page numbering is in arabic numerals, and the page numbering is reset to 1. \\backmatter : the back matter of the document, i.e., the bibliography, appendices, etc. The page numbering is in arabic numerals. Chapters are not numbered. Escape characters and commands \u00b6 LaTeX uses many special characters which needs to be escaped. Unfortunatelly, there is no single escape character, instead, there are many. The following table lists the most common escape characters: Character Escape sequence [ {[} ] {]} When we need to print \\LaTeX commands, it is better to escape the whole command, not just the special characters. There are three ways how to escape the whole command: \\string<command> , \\verb|<command>| , and \\begin{verbatim}<command>\\end{verbatim} . The difference between the two are summarized in the following table: Command spaces allowed new lines allowed can be used in floats automatically changes the font \\string no no yes no \\verb yes no no yes \\verbatim yes yes no yes Special characters \u00b6 Non-breaking characters \u00b6 ~ : non-breaking space \\nobreakdash- : non-breaking dash Text and paragraph formatting \u00b6 wiki Fonts \u00b6 Determining used font \u00b6 To determine the font (size, type) used in a particular place put the following command there: \\showthe\\font Then compile the document. The font information will be printed in the log (e.g.: \\OT1/cmr/m/n/10 ). Changing font size \u00b6 The default font size is 10pt . To change the default font size, set the documentclass option 12pt or 11pt (other sizes are not avialable). See the wiki for more information. The size can be also changed for a specific part of the document. We can use some predefined sizes e.g.: Normal {\\tiny tiny} normal or we can use the \\fontsize command to set arbitrary size. Example: {\\fontsize{<size>}{<line spacing>}\\selectfont <text>} Font color \u00b6 wiki To change the font color, we need to use the xcolor package. First, we need to define the color using the \\definecolor command. Example: \\definecolor{my_color}{RGB}{255,0,0} # RGB color \\definecolor{my_color}{HTML}{FF0000} # HTML color \\definecolor{my_color}{cmyk}{0,1,1,0} # CMYK color Then, we can use various commands to change the font color: \\textcolor{<color>}{<text>} : change the color of the text {\\color{<color>}<text>} : change the color of the text. Can span multiple paragraphs. note that other commands that have color options usually depend on the xcolor package, and accept the same color definitions. Horizontal and vertical spacing \u00b6 Most of the time, the spacing should be handled by LaTeX automatically. However, there are cases when we need to adjust the spacing manually, either in a single place or globally. To adjust the spacing in a single place, we can use the following commands: \\hspace{<length>} : horizontal space \\vspace{<length>} : vertical space Note that we can use negative values for the <length> parameter, so we can use these commands to relatively positioning. Horizontal and vertical alignment \u00b6 Horizontal alignment \u00b6 wiki By default, the text is fully justified. To change the justification (alignment), to left or right, we can use either environments: flushleft for left alignment, flushright for right alignment, or commands: \\raggedright for left alignment, \\raggedleft for right alignment. Vertical alignment \u00b6 By default, the text starts from the top of the page. To align the text to the bottom of the page, we can use the vfill command. Example: Some text \\vfill Some text at the bottom Note that the vfill works only if there is something before it, it does not work if it is the first command on the page. To make it work, we can use the null command. Example: \\null \\vfill Some text at the bottom Line breaking \u00b6 Latex handle the line breaking automatically. Typically, it breaks the line at the end of the word. Sometimes, it can break long words. To prevent breaking long words , we can use the \\mbox command. Example: \\mbox{long word} To prevent breaking on spaces , we can use the non-breaking space character: ~ . Subscript and superscript \u00b6 In math mode, the subscript and superscript are created using the _ and ^ characters. In text mode, we need to use a special commands: \\textsubscript and \\textsuperscript . Example: H\\textsubscript{2}O Lists \u00b6 The list enviroments have the following syntax: \\begin{<list type>} \\item <item 1> \\item <item 2> ... \\end{<list type>} The following list types are available: itemize : bullet points enumerate : numbered list description : description list. Items can have a label, which is specified as an optional argument of the \\item command. More typoes of lists like questions or checklists can be created using external packages. Units, numbers, and currency with siunitx \u00b6 Units are usually typeset with a small space between the number and the unit. Normal space should be avoidedas it is too wide. Also, we sometimes want a special separator in large numbers. For these purposes, the best practice is to use the siunitx package. Eith siunitx , the units are typeset using the \\SI command. Example: \\SI{10}{\\meter} The numbers should be typeset using the \\num command. Example: \\num{1000000} The currecnies that have the currency sign before the number are also typeset using the \\SI command, but we need to use the optional argument for prefix: \\SI{10}[\\$]{} siunitx \\per problems \u00b6 SE thread To express a relation between two units we ofthe use the / symbol, which we can replace with the \\per command when using siunitx . Example: \\SI{10}{\\meter\\per\\second} % 10 m/s However, the output is not always as expected: \\SI{10}[\\$]{\\per\\kWH} % $10 h/kW The solution is to redefine the \\kWH command: \\AtBeginDocument{\\DeclareSIUnit{\\kWh}{kWh}} \\SI{10}[\\$]{\\per\\kWh} % $10/kWh Quotes \u00b6 The quotes are typeset using the csquotes package. The quotes are typeset using the \\enquote command. Example: \\enquote{This is a quote} Supported units \u00b6 \\s : second \\km : kilometer \\km\\per\\hour : kilometer per hour \\percent : percent Floats \u00b6 The following environments are floats: figure table algorithm Placement \u00b6 Any float takes the position as a first argument. The following positions are available: h : here t : top b : bottom p : special dedicated page per float ! : ignore nice positioning and put it according to the float specifiers The placement algorithm then iterate pages starting from the page where the float is placed. For each page, it tries to find a place for the float according to the float specifiers (in the same order as they appear in the float position argument). In case of success, the procedure stops and place the float. If the procedure fails for all pages, the float is placed at the end. Note that by specifying the float position, we only add constraints to the placement algorithm, but do not guarantee the placement. By omitting the float position, we can accually make the float appear closer to the place where it is defined. Sources: LaTeX Wikibook Overleaf Default placement \u00b6 The default placement differs between environments and also classes. For example for article class, the default placement for figure and table is tbp ( see SO ). Figures \u00b6 The float environment for figures is figure . The image itself is included using the \\includegraphics command. The mandatory argument of the \\includegraphics command is the path to the image file. This path can be relative or absolute. If the path is relative, it is relative to the location of the main .tex file. The file extension can be omitted. If the file extension is omitted, the compiler will try to find the file with various extensions. Therefore, it is only recommended to omit the file extension if there is only one file with the same name. Optional arguments of the \\includegraphics command are the following: width : the width of the image. example: \\includegraphics[width=0.5\\textwidth]{image.png} scale : the scale of the image with respect to the original size. example: \\includegraphics[scale=0.5]{image.png} Subfigures \u00b6 For more images in one float, we can use the subfigure environment from the subcaption package. The subfigure environment is used as follows: \\begin{figure}[h] \\centering \\begin{subfigure}{0.3\\textwidth} \\includegraphics[width=\\textwidth]{image1.png} \\caption{Image 1} \\label{fig:image1} \\end{subfigure} \\begin{subfigure}{0.3\\textwidth} \\includegraphics[width=\\textwidth]{image2.png} \\caption{Image 2} \\label{fig:image2} \\end{subfigure} \\caption{My figure} \\label{fig:my_figure} \\end{figure} If the figures are not of the same height, they will be aligned at the bottom. To align them at the top, we can use the T option for the subfigure environment. Example: \\begin{subfigure}[T]{0.3\\textwidth} ... \\end{subfigure} Tables \u00b6 The float environment for tables is table . However, the rows and columns are wrapped in another environment. The default inner enviroment is tabular , however, there are many other packages that extends the functionality. In practice, there are currently three inner environments to consider: tabular : the default environment that is sufficient for simple tables tabulary : the environment that allows to create columns with automatic width. If the main or only issue of the table is that it needs to fit a specific width, this is the environment to use. tblr : the tblr environment from the tabularray package is the most up to date tabular environment that support many features. Also, it splits the table presentation from the table content, which can make generating tables from code easier. The only downside is that it does not support automatic column width . Column types \u00b6 The column types are specified in the argument of the tabular or equivalent environment. The following column types are available by default: l : left aligned c : centered r : right aligned p{width} : paragraph column with specified width Other column types can be provided by the inner environment package or by the user. Simple tables with tabular environment \u00b6 The usual way to create a table in the tabular environment is: \\begin{table}[h] \\centering % center the content of the table environment \\begin{tabular}{|c|c|} ... rows and columns \\end{tabular} \\caption{My table} \\label{tab:my_table} \\end{table} Columns with automatic width: tabulary \u00b6 By default, laTeX does not support automatic width for columns, i.e., sizing the columns by their content. To enable this feature, we can use the tabulary package, which provides the tabulary environment (which is a replacement for the tabular environment). The columns with automatic width are specified by the L , C , R column types. Note that the new column types can be combined with the standard column types. In that case, the standard columns will have width according to their content, and the rest of the space will be distributed among the new column types. Complex tables with tabulararray package \u00b6 The tabulararray package provides : full control over the table, most features among all packages separation of the table content and the table presentation simpler code for basic tasks like multirows and multicolumns, wrapping text in cells, etc. Notable features present in the tabulararray package missing in other packages: footnotes in tables (otherwise, it requires a threeparttable environment wrapper) As other tabular packages, there are some incompatibilities related to the tabulararray package. So far, I observed only incompatibilities with the tabulararray talltblr environment, not with the standard tblr environment. The following table summarizes the incompatibilities I found so far: cases : Obviously, the cases environment uses table components internally. When using together with talltblr , the cases environment compilation results with the following error: latex \"\\begin{cases}\" package array empty preamble l' used . The solution is to use the use the new +cases environment provided by the tabulararray package. As a bonus, the +cases environment also fixes some visual glitches. Steps: enable the +cases environment by adding \\UseTblrLibrary{amsmath} to the preamble replace the cases environment with the +cases environment tabular : yes, the talltblr environment is incompatible with the default tabular environment. The solution is simple: replace all tabular environments with tblr environments. Styling \u00b6 Table grid (visual lines) \u00b6 Usually, tables contain some horizontal and vertical lines to separate the cells. Unfortunately, the way how to create these lines differs completely between horizontal and vertical lines. Horizontal lines \u00b6 For horizontal lines, we can: use various commands to create a line in a specific row: \\hline : creates a line in the current row specify the lines outside the data with the tabularray package: Specify the lines outside the data with the tabularray package \u00b6 The format is hline{<list of rows>} = {<line style>} . Example: \\begin{tblr}{ colspec={|c|c|c|}, hline{1,Z} = {1pt, solid}, } ... \\end{tblr} The <list of rows> can be specified as: single row: hline{1} = {1pt, solid} multiple rows: hline{1,3,5} = {1pt, solid} range of rows: hline{1-Z} = {1pt, solid} Rows are numbered from 1. The Z character is used to specify the last row. Styling the specific rows or columns \u00b6 With the tabularray package, we can style the table header differently than the rest of the table. In fact, we can style each specific row or column differently. for that, we use the row and column keys. Example \\begin{tblr}{ colspec={|c|c|c|}, row{1} = {bg=gray9, fg=white}, column{1} = {bg=gray9, fg=white}, } The syntax is the same for both keys: row{<row number>} = {<style>} . For the <row number> specification, see the grid section. The <style> specification is a list of key-value pairs. The keys are the following: font : the font style Configure the space between columns \u00b6 In most packages, the space between columns is configured using the \\tabcolsep variable. Example: \\setlength{\\tabcolsep}{10pt} However, in the tblr environment, the space between columns is configured using the leftsep and rightsep keys. Example:, \\begin{tblr} { colspec={llllrr}, leftsep=2pt, rightsep=2pt } By default, the leftsep and rightsep are set to 6pt . Define table styles with tabularray package \u00b6 The tabularray package provides a way to define table styles. First, we define the new environment with the \\NewTblrEnviron command. Later, we use the SetTblrInner and SetTblrOuter commands to define the style. Example: \\NewTblrEnviron{mytblr}{ \\SetTblrInner{rowsep=1pt} \\SetTblrOuter{hspan=minimal} } Rotated text \u00b6 To rotate some text, we can use the \\rotatebox command: \\rotatebox{90}{Rotated text} Multirows and multicolumns \u00b6 Depending on the inner environment, the multirows and multicolumns are created using different commands. tabular environment \u00b6 In the tabular environment, the multirows and multicolumns are created using the \\multicolumn and \\multirow commands. Example: \\begin{tabular}{cc} \\multicolumn{2}{c}{multi column} \\\\ \\multirow{2}{*}{multi row} & 1 \\\\ & 2 \\\\ \\end{tabular} tabulararray environment \u00b6 In the tabulararray environment, the multirows and multicolumns are created using the \\SetCell command. Example: \\begin{tblr}{cc} \\SetCell[c=2]{c} multi column & \\\\ \\SetCell[r=2]{c} multi row & 1 \\\\ & 2 \\\\ \\end{tblr} Note that for multicolumns, we need to add the column divider ( & ) after the \\SetCell command for each column that is spanned by the multicolumn . Export google sheets to latex tables \u00b6 There is ann addon called LatexKit which can be used for that. Footnotes in tables \u00b6 In tables and table captions, the \\footnote command does not work correctly. Also, it is not desirable to have the footnote at the bottom of page, instead, we want the footnote to be at the bottom of the table. To achieve this, we use a special environment: threeparttable : if we are using the tabular or tabulary environment talltblr : if we are using the tblr environment Using the threeparttable \u00b6 The threeparttable environment is used as follows: \\begin{table}[h] \\centering \\caption{My table} \\label{tab:my_table} \\begin{threeparttable} \\begin{tabular}{|c|c|} one$^a$ & two$^b$ \\\\ ... other rows and columns \\end{tabular} \\begin{tablenotes} \\item $^a$footnote 1 \\item $^b$footnote 2 \\end{tablenotes} \\end{threeparttable} \\end{table} Using the talltblr \u00b6 The talltblr environment is used as follows: \\begin{table}[h] \\centering \\caption{My table} \\label{tab:my_table} \\begin{talltblr}[ label = none, note{a} = {footnote 1}, note{b} = {footnote 2} ]{ colspec={|c|c|}, } one\\TblrNote{a} & two\\TblrNote{b} \\\\ ... other rows and columns \\end{talltblr} \\end{table} Notice the label = none option. Without it, the table numbering is raised again, resulting in the table being numbered twice. Rotated text \u00b6 To rotate some text, we can use the \\rotatebox command: \\rotatebox{90}{Rotated text} If we need to limit the width of the rotated text, we can use the \\parbox command. Example: \\rotatebox{90}{\\parbox{2cm}{Rotated text}} Algorithms \u00b6 wiki There are two types of enviroments for, depending on the purpose: algorithm : for pseudocode lstlisting : for code in a specific language Pseudocode \u00b6 The floating environment for pseudocode is algorithm . It is an equivalent of the table environment for tables. Same as with tables, there is also an inner environment for the pseudocode. The options for the inner environment are: algorithmic : basic environment for pseudocode algorithmicx : extension of the algorithmic environment, supports custom keywords algorithm2e : algpseudocodex : extension of the algorithmicx package, various improvements pseudo : a package for writing pseudocode directly, not using any special commands Their properties are summarized in the following table: | Environment | Package | Updated | Custom keywords | |--- | --- | --- | --- | | algorithmic | algorithms | 2009 | no | | algorithmicx | algorithmicx | 2005 | yes | | algorithm2e | algorithm2e | 2017 | yes | | algpseudocodex | algpseudocodex | 2023 | yes | | program | program | 2017 | no | | pseudo | pseudo | 2023 | yes | If the algorithm requires a complex description and cannot be expressed using typical pseudocode, we can resort to using human language directly in the algorithm environment. There is such example at SO Algoritmicx and Algpseudocodex \u00b6 Normal code lines are written using the \\State command: \\State $x \\gets 0$ Conditions \u00b6 Conditions are written using the \\If , \\ElseIf , and \\Else commands. Example: \\If{$x < 0$} \\State $x \\gets 0$ \\ElsIf{$x > 100$} \\State $x \\gets 100$ \\Else \\State $x \\gets x$ \\EndIf Loops \u00b6 For loop: \\For{$ i = 1, \\dots, n $} ... \\EndFor Boolean operators \u00b6 Boolean operators are not defined by default. Either use plain text, or define them (see the Additional keywords section below). Additional keywords \u00b6 We can define additional keywords using the \\algnonewcommand command. The format is \\algnonewcommand<custom command>{<keyword>} . Example: \\algnewcommand\\Not{\\textbf{not}} Empty lines \u00b6 To add an empty line, use the \\State command without any argument. If the numbering is on and we want to skip the numbering for the empty line, we can use the \\Statex command. Functions and procedures \u00b6 Functions and procedures are defined using the \\Function and \\Procedure commands. Example: \\Function{my_function}{a, b} \\State $x \\gets a + b$ \\State \\Return $x$ \\EndFunction We can call the function or procedure using the \\Call command. Example: \\Call{my_function}{1, 2} Line numbering \u00b6 To add line numbering, add an optional argument to the algorithmic environment. The argument is the number determining the frequency of the line numbering. Example: \\begin{algorithmic}[3] % number every 3rd line ... \\end{algorithmic} Centering of floats wider than text width \u00b6 If the float is wider than the text width, it is not centered, but instead it is left-aligned with the text and it overflows on the right side. To fix this, we can wrap the too-wide float content in the \\makebox command. Example: \\begin{figure}[h] \\makebox[\\textwidth]{ \\includegraphics[width=1.2\\textwidth]{my_image.png} } \\caption{My figure} \\label{fig:my_figure} \\end{figure} Boxes \u00b6 wiki Everythign in LaTeX is a box. Each character is a box, stored in a larger box for each word, and analogically for each line, paragraph, etc. Most of the time we just set properties for the boxes, but sometimes we need to create a box manually to format or position the content. The following table presents the most common box commands: Command paragraph witdth \\parbox single fixed \\pbox single flexible \\minipage multiple fixed The first two parameters are shared for these commands: pos : the position of the box, e.g., t for top, b for bottom, c for center. The position refers to the part of the box that is aligned with the surrounding text. height : the height of the box. The parbox and minipage share another two parameters that follows: contentpos : the position of the content inside the box, e.g., t for top, b for bottom, c for center. width : the width of the box. Make a box wider than the text width \u00b6 To make a box wider than the text width, we can use the \\adjustwidth command from the changepage package. Example: \\begin{adjustwidth}{-1cm}{-1cm} ... content \\end{adjustwidth} The box above will be 1cm wider on each side than the text width. Math \u00b6 wiki To use math, we need the amsmath package. The math commands only works in math mode which can be entered in one of the many math environments. Common math constructs \u00b6 The following table lists the most common math constructs: fractions : \\frac{<numerator>}{<denominator>} sum : \\sum_{<lower>}^{<upper>} binomial coefficients : \\binom{n}{k} Subscript and superscript \u00b6 Subscript and superscript are created using the _ and ^ characters. If the subscript or superscript is longer than one character, we need to wrap it in curly braces. There are aslo some special characters that result in superscript: ' : prime * : star However, these characters alone works only in normal math text. If we want to use them in a subscript or superscript, we need to use the ^ and _ characters. Example: x* % correct print a_{x*} % inccorrect print - the star is not in superscript a_{x^*} % correct print If/else variants \u00b6 For that, we use the cases environment. Example: \\begin{equation} f(x) = \\begin{cases} 0 & \\quad \\text{if } x < 0 \\\\ 1 & \\quad \\text{if } x \\geq 0 \\end{cases} \\end{equation} Equations and similar environments \u00b6 The following environments are available for equations: equation : for single equations align : for multiple equations aligned at a specific character alignat : for multiple equations aligned at multiple characters aligned : for multiple equations inside another math environment (e.g., equation ) gather : for multiple equations not aligned, each line numbered separately The new line is created using the \\\\ command. Nothe that the \\\\ command is not allowed in the equation environment . To align the equations in the align and alignat environments, we use the & character. Problem and similar environments \u00b6 wiki The environments for special math text blocks are not included in the amsmath package. We can define them manually using the \\newtheorem command. Example: \\newtheorem{problem}{Problem} \\begin{problem} This is a problem. \\end{problem} Here, the first argument of the \\newtheorem command is the name of the environment, and the second argument is the name of the environment in the output. There can be also an optional third argument, which is the name of the counter that is used for numbering the environment. For example, if we want to use another environment for propositions with the same numbering as the problems, we can use the following command: \\newtheorem{proposition}[problem]{Proposition} Some environments are already defined in the amsthm package, e.g., proof . Common environments names and their meaning \u00b6 theorem : a statement to be proved lemma : a minor theorem, with a limited applicability outside the context of the main theorem corollary : a theorem that follows directly from another theorem proposition : a less important theorem, usually used for something elementary, obvious, so that it does not require a proof premise : a statement that is assumed to be true and it represents a fact that is used in the proof of the theorem. For example: \"We only consider sets where half of the numbers are even...\" assumption : a statement that is assumed to be true and it represents a condition under which the theorem is true. For example: \"Assuming half of the numbers in the set are even...\" More can be fount on proofwiki Math fonts \u00b6 wiki the default math font is typed as common math italic. To use a different font, we need to use special commands: \\mathrm{} : for normal font in math mode, e.g., for multi-letter subscripts and superscripts \\mathbb{} : for blackboard bold font, e.g., for special sets (R, N, ...). This font requires the amsfonts package. Bold math font \u00b6 To use a bold font, we can use the \\bm command from the bm package: \\usepackage{bm} \\begin{equation} \\bm{a} = \\bm{b} + \\bm{c} \\end{equation} Correct size of braces \u00b6 To size any type of braces correctly, if the content is larger than the braces, we can use the \\left and \\right commands. Example: \\left( \\frac{a}{b} \\right) Links \u00b6 wiki For links, we need the hyperref package. Typical usage: \\url{https://www.google.com} \\href{https://www.google.com}{Google} Footnotes \u00b6 The footnote is created using the \\footnote{} command. Override footnote numbering \u00b6 To override the footnote numbering (e.g. to repeat the same number twice), we can use the \\setcounter command. Example: \\setcounter{footnote}{1} # set the footnote counter to 1 Disallow footnote splitting \u00b6 To disallow footnote splitting, we can increase the \\interfootnotelinepenalty command. Example: \\interfootnotelinepenalty=10000 Custom commands and variables \u00b6 wiki Basic syntax for defining a new command or variable is: \\newcommand{\\<command name>}[<number of arguments>]{<command definition>} The number of arguments is optional. If it is not specified, the command does not take any arguments. The command is then used as follows: \\<command name>{<argument 1>}{<argument 2>}...{<argument n>} The \\newcommand command has also its star variant \\newcommand* . The difference is that when using just \\newcommand , the command arguments can contain line breaks ( \\par commands), while when using \\newcommand* , the command arguments cannot contain line breaks. This extra restriction is useful to avoid accidental line breaks in the command arguments. As it is a common concept in LaTeX, it is called a short-form command , in contrast to the long-form command without the star. Providing default values for arguments \u00b6 Latex support default values for the first argument. The syntax is: \\newcommand{\\<command name>}[<number of arguments>][<default value>]{<command definition>} The command can than be used both with and without the optional argument: \\newcommand{\\mycommand}[2][0]{...} \\mycommand{<argument 2>} \\mycommand[<argument 1>]{<argument 2>} Note that that if we want to supply the optional argument, we use the square brackets. For two optional arguments, we have to use the twoopt package. Example: \\usepackage{twoopt} ... \\newcommandtwoopt{\\mycommand}[2][default1][default2]{...} Here again, both optioanl arguments, if supplied, must be supplied in the square brackets. Splitting the document into multiple files \u00b6 There are two ways to split the document into multiple files: \\input{file} \\include{file} The \\include is intended for chapters or other large parts of the document. It has the following properties: it starts a new page before and after the included file it does not allow nesting there is a special command \\includeonly{file1,file2,...} which allows to include only the specified files. This is useful for large documents where we want to compile only a part of the document. Without this command we would need to search for the include command and comment it out. The \\input command is intended for smaller parts of the document. Contrary to the \\include command, there is no special behavior involved. Instead, the content of the file is simply pasted at the place of the \\input command. Speedup Techniques \u00b6 The compilation of large documents can be slow. There are several techniques to speed up the compilation: split the document into multiple files and use \\includeonly to include only the relevant files precompiling the preamble using draft mode Precompiling the preamble \u00b6 The preamble is the part of the document before the \\begin{document} command. It contains the document configuration, packages, etc. Because the included packages are usually large, the compilation of the preamble can be slow. To speed up the compilation, we can precompile the preamble and use the precompiled preamble in the main document. This can be done using the mylatexformat package. The usage is as follows: At the beginning of the preamble, add the following comment: %&<format name> . This will tell the compiler to use the specified format. The <format name> can be arbitrary, but it is recommended to use the same name as the main document. To spare some preamble content from being precompiled (dynamic content), add a command \\endofdump after the content that should not be precompiled. run the following command: PowerShell pdflatex --ini -jobname=\"<format name>\" \"&pdflatex\" mylatexformat.ltx <format name>.tex Afther this, the compilation of the main document should be faster. For more information, see the package documentation or the SO question . Miscelaneous tasks \u00b6 Balancing columns in two-column documents \u00b6 To balance the columns at the end of the document, we can use the flushend package. Just add \\usepackage{flushend} to the preamble. Common problems \u00b6 Ugly font in pdf \u00b6 This can be cause by the missing vector fonts. If the vector fonts are missing, the bitmap fonts are used instead. To check if this is the cause, zoom in on the pdf. If the text is blurry, the bitmap fonts are used. To fix this, install the vector fonts. On Windows, install the cm-super package through MikTeX.","title":"Latex manual"},{"location":"LaTeX/Latex%20manual/#document-structure","text":"The document structure is well documented on wikibooks . The basic structure is: \\documentclass[<options>]{<class>} ... \\begin{document} ... \\end{document} Lot of content is usually put in the preamble, i.e., between the \\documentclass and \\begin{document} commands. The preamble usual content is: loading packages with \\usepackage{<package>} providing new commands or redefining existing commands configuring packages supplying metadata for the document (title, author, etc.)","title":"Document structure"},{"location":"LaTeX/Latex%20manual/#title-page","text":"The title page typically contains the title, authors, and potentially other metadata like date or keywords. The standard way how to create the title page is to first define the metadata and then use the \\maketitle command. To print the whole title page. The metadata available for standard article class: \\title{<title>} \\author{<author>} \\date{<date>} . \\today can be used to print the current date. This is the default value if we omit the \\date command. A special non-numbered footnote can be added to most fields using the \\thanks{<text>} command.","title":"Title page"},{"location":"LaTeX/Latex%20manual/#authors-specification","text":"By default, all authors should be filled within a single \\author command, separated by the \\and command. If we need to specify the affiliations, we can do it inside the \\author command. This way, each author have the affiliation printed right after/bellow the author name. However, if there are many authors with shared affiliations, this approach is unsustainable. Instead, we can use the authblk package which lets us specify the authors and affiliations separately and connect them to authors using footnotes. Example: \\usepackage{authblk} \\author[1]{Author 1} \\author[2]{Author 2} \\author[1]{Author 3} \\affil[1]{Affiliation 1} \\affil[2]{Affiliation 2}","title":"Authors specification"},{"location":"LaTeX/Latex%20manual/#ieee-author-list","text":"In IEEE publications, all authors are usually listed in the same \\author command. Also, the affiliations are inside this command, typically created using the \\thanks command. Example: \\author{David Fiedler, Jan Mrkos, Dominika \u0160\u00eddlov\u00e1, and Fabio V. Difonzo \\thanks{This work was supported by the Technology Agency of the Czech Republic within the DOPRAVA 2020+ program, project no. \\texttt{CK04000150}. Fabio V.~Difonzo gratefully thanks the INdAM-GNCS group for partial support. The access to the computational infrastructure of the OP VVV funded project \\texttt{CZ.02.1.01/0.0/0.0/16\\_019/0000765} \"Research Center for Informatics\" is also gratefully acknowledged. \\emph{(Corresponding author: David Fiedler.)}} \\thanks{D. Fiedler, J. Mrkos, and D. \u0160\u00eddlov\u00e1 are with Faculty of Electrical Engineering, CTU in Prague, Prague 121 35, Czech Republic (e-mail: david.fiedler@agents.fel.cvut.cz).} \\thanks{F. V. Difonzo is with Istituto per le Applicazioni del Calcolo \"Mauro Picone\", Consiglio Nazionale delle Ricerche, Bari 70126, Italy.}}","title":"IEEE author list"},{"location":"LaTeX/Latex%20manual/#keywords","text":"Keywords are not part of the standard article class. If we need to include them when using the standard article class, we can provide the command ourselves. Example: \\providecommand{\\keywords}[1]{\\textbf{\\textit{Index terms---}} #1} ... \\keywords{keyword1, keyword2, ...}","title":"Keywords"},{"location":"LaTeX/Latex%20manual/#appendices","text":"Appendicies are started with the command appendix . Then, each chapter started with the \\chapter command is considered an appendix.","title":"Appendices"},{"location":"LaTeX/Latex%20manual/#book-class-structuring","text":"Appart from sections and chapters, the book class also supports top level parts for marking special areas of the document. The parts are: \\frontmatter : the front matter of the document, i.e., the title page, abstract, table of contents, etc. The page numbering is in roman numerals. \\mainmatter : the main matter of the document, i.e., the main content. The page numbering is in arabic numerals, and the page numbering is reset to 1. \\backmatter : the back matter of the document, i.e., the bibliography, appendices, etc. The page numbering is in arabic numerals. Chapters are not numbered.","title":"Book class structuring"},{"location":"LaTeX/Latex%20manual/#escape-characters-and-commands","text":"LaTeX uses many special characters which needs to be escaped. Unfortunatelly, there is no single escape character, instead, there are many. The following table lists the most common escape characters: Character Escape sequence [ {[} ] {]} When we need to print \\LaTeX commands, it is better to escape the whole command, not just the special characters. There are three ways how to escape the whole command: \\string<command> , \\verb|<command>| , and \\begin{verbatim}<command>\\end{verbatim} . The difference between the two are summarized in the following table: Command spaces allowed new lines allowed can be used in floats automatically changes the font \\string no no yes no \\verb yes no no yes \\verbatim yes yes no yes","title":"Escape characters and commands"},{"location":"LaTeX/Latex%20manual/#special-characters","text":"","title":"Special characters"},{"location":"LaTeX/Latex%20manual/#non-breaking-characters","text":"~ : non-breaking space \\nobreakdash- : non-breaking dash","title":"Non-breaking characters"},{"location":"LaTeX/Latex%20manual/#text-and-paragraph-formatting","text":"wiki","title":"Text and paragraph formatting"},{"location":"LaTeX/Latex%20manual/#fonts","text":"","title":"Fonts"},{"location":"LaTeX/Latex%20manual/#determining-used-font","text":"To determine the font (size, type) used in a particular place put the following command there: \\showthe\\font Then compile the document. The font information will be printed in the log (e.g.: \\OT1/cmr/m/n/10 ).","title":"Determining used font"},{"location":"LaTeX/Latex%20manual/#changing-font-size","text":"The default font size is 10pt . To change the default font size, set the documentclass option 12pt or 11pt (other sizes are not avialable). See the wiki for more information. The size can be also changed for a specific part of the document. We can use some predefined sizes e.g.: Normal {\\tiny tiny} normal or we can use the \\fontsize command to set arbitrary size. Example: {\\fontsize{<size>}{<line spacing>}\\selectfont <text>}","title":"Changing font size"},{"location":"LaTeX/Latex%20manual/#font-color","text":"wiki To change the font color, we need to use the xcolor package. First, we need to define the color using the \\definecolor command. Example: \\definecolor{my_color}{RGB}{255,0,0} # RGB color \\definecolor{my_color}{HTML}{FF0000} # HTML color \\definecolor{my_color}{cmyk}{0,1,1,0} # CMYK color Then, we can use various commands to change the font color: \\textcolor{<color>}{<text>} : change the color of the text {\\color{<color>}<text>} : change the color of the text. Can span multiple paragraphs. note that other commands that have color options usually depend on the xcolor package, and accept the same color definitions.","title":"Font color"},{"location":"LaTeX/Latex%20manual/#horizontal-and-vertical-spacing","text":"Most of the time, the spacing should be handled by LaTeX automatically. However, there are cases when we need to adjust the spacing manually, either in a single place or globally. To adjust the spacing in a single place, we can use the following commands: \\hspace{<length>} : horizontal space \\vspace{<length>} : vertical space Note that we can use negative values for the <length> parameter, so we can use these commands to relatively positioning.","title":"Horizontal and vertical spacing"},{"location":"LaTeX/Latex%20manual/#horizontal-and-vertical-alignment","text":"","title":"Horizontal and vertical alignment"},{"location":"LaTeX/Latex%20manual/#horizontal-alignment","text":"wiki By default, the text is fully justified. To change the justification (alignment), to left or right, we can use either environments: flushleft for left alignment, flushright for right alignment, or commands: \\raggedright for left alignment, \\raggedleft for right alignment.","title":"Horizontal alignment"},{"location":"LaTeX/Latex%20manual/#vertical-alignment","text":"By default, the text starts from the top of the page. To align the text to the bottom of the page, we can use the vfill command. Example: Some text \\vfill Some text at the bottom Note that the vfill works only if there is something before it, it does not work if it is the first command on the page. To make it work, we can use the null command. Example: \\null \\vfill Some text at the bottom","title":"Vertical alignment"},{"location":"LaTeX/Latex%20manual/#line-breaking","text":"Latex handle the line breaking automatically. Typically, it breaks the line at the end of the word. Sometimes, it can break long words. To prevent breaking long words , we can use the \\mbox command. Example: \\mbox{long word} To prevent breaking on spaces , we can use the non-breaking space character: ~ .","title":"Line breaking"},{"location":"LaTeX/Latex%20manual/#subscript-and-superscript","text":"In math mode, the subscript and superscript are created using the _ and ^ characters. In text mode, we need to use a special commands: \\textsubscript and \\textsuperscript . Example: H\\textsubscript{2}O","title":"Subscript and superscript"},{"location":"LaTeX/Latex%20manual/#lists","text":"The list enviroments have the following syntax: \\begin{<list type>} \\item <item 1> \\item <item 2> ... \\end{<list type>} The following list types are available: itemize : bullet points enumerate : numbered list description : description list. Items can have a label, which is specified as an optional argument of the \\item command. More typoes of lists like questions or checklists can be created using external packages.","title":"Lists"},{"location":"LaTeX/Latex%20manual/#units-numbers-and-currency-with-siunitx","text":"Units are usually typeset with a small space between the number and the unit. Normal space should be avoidedas it is too wide. Also, we sometimes want a special separator in large numbers. For these purposes, the best practice is to use the siunitx package. Eith siunitx , the units are typeset using the \\SI command. Example: \\SI{10}{\\meter} The numbers should be typeset using the \\num command. Example: \\num{1000000} The currecnies that have the currency sign before the number are also typeset using the \\SI command, but we need to use the optional argument for prefix: \\SI{10}[\\$]{}","title":"Units, numbers, and currency with siunitx"},{"location":"LaTeX/Latex%20manual/#siunitx-per-problems","text":"SE thread To express a relation between two units we ofthe use the / symbol, which we can replace with the \\per command when using siunitx . Example: \\SI{10}{\\meter\\per\\second} % 10 m/s However, the output is not always as expected: \\SI{10}[\\$]{\\per\\kWH} % $10 h/kW The solution is to redefine the \\kWH command: \\AtBeginDocument{\\DeclareSIUnit{\\kWh}{kWh}} \\SI{10}[\\$]{\\per\\kWh} % $10/kWh","title":"siunitx \\per problems"},{"location":"LaTeX/Latex%20manual/#quotes","text":"The quotes are typeset using the csquotes package. The quotes are typeset using the \\enquote command. Example: \\enquote{This is a quote}","title":"Quotes"},{"location":"LaTeX/Latex%20manual/#supported-units","text":"\\s : second \\km : kilometer \\km\\per\\hour : kilometer per hour \\percent : percent","title":"Supported units"},{"location":"LaTeX/Latex%20manual/#floats","text":"The following environments are floats: figure table algorithm","title":"Floats"},{"location":"LaTeX/Latex%20manual/#placement","text":"Any float takes the position as a first argument. The following positions are available: h : here t : top b : bottom p : special dedicated page per float ! : ignore nice positioning and put it according to the float specifiers The placement algorithm then iterate pages starting from the page where the float is placed. For each page, it tries to find a place for the float according to the float specifiers (in the same order as they appear in the float position argument). In case of success, the procedure stops and place the float. If the procedure fails for all pages, the float is placed at the end. Note that by specifying the float position, we only add constraints to the placement algorithm, but do not guarantee the placement. By omitting the float position, we can accually make the float appear closer to the place where it is defined. Sources: LaTeX Wikibook Overleaf","title":"Placement"},{"location":"LaTeX/Latex%20manual/#default-placement","text":"The default placement differs between environments and also classes. For example for article class, the default placement for figure and table is tbp ( see SO ).","title":"Default placement"},{"location":"LaTeX/Latex%20manual/#figures","text":"The float environment for figures is figure . The image itself is included using the \\includegraphics command. The mandatory argument of the \\includegraphics command is the path to the image file. This path can be relative or absolute. If the path is relative, it is relative to the location of the main .tex file. The file extension can be omitted. If the file extension is omitted, the compiler will try to find the file with various extensions. Therefore, it is only recommended to omit the file extension if there is only one file with the same name. Optional arguments of the \\includegraphics command are the following: width : the width of the image. example: \\includegraphics[width=0.5\\textwidth]{image.png} scale : the scale of the image with respect to the original size. example: \\includegraphics[scale=0.5]{image.png}","title":"Figures"},{"location":"LaTeX/Latex%20manual/#subfigures","text":"For more images in one float, we can use the subfigure environment from the subcaption package. The subfigure environment is used as follows: \\begin{figure}[h] \\centering \\begin{subfigure}{0.3\\textwidth} \\includegraphics[width=\\textwidth]{image1.png} \\caption{Image 1} \\label{fig:image1} \\end{subfigure} \\begin{subfigure}{0.3\\textwidth} \\includegraphics[width=\\textwidth]{image2.png} \\caption{Image 2} \\label{fig:image2} \\end{subfigure} \\caption{My figure} \\label{fig:my_figure} \\end{figure} If the figures are not of the same height, they will be aligned at the bottom. To align them at the top, we can use the T option for the subfigure environment. Example: \\begin{subfigure}[T]{0.3\\textwidth} ... \\end{subfigure}","title":"Subfigures"},{"location":"LaTeX/Latex%20manual/#tables","text":"The float environment for tables is table . However, the rows and columns are wrapped in another environment. The default inner enviroment is tabular , however, there are many other packages that extends the functionality. In practice, there are currently three inner environments to consider: tabular : the default environment that is sufficient for simple tables tabulary : the environment that allows to create columns with automatic width. If the main or only issue of the table is that it needs to fit a specific width, this is the environment to use. tblr : the tblr environment from the tabularray package is the most up to date tabular environment that support many features. Also, it splits the table presentation from the table content, which can make generating tables from code easier. The only downside is that it does not support automatic column width .","title":"Tables"},{"location":"LaTeX/Latex%20manual/#column-types","text":"The column types are specified in the argument of the tabular or equivalent environment. The following column types are available by default: l : left aligned c : centered r : right aligned p{width} : paragraph column with specified width Other column types can be provided by the inner environment package or by the user.","title":"Column types"},{"location":"LaTeX/Latex%20manual/#simple-tables-with-tabular-environment","text":"The usual way to create a table in the tabular environment is: \\begin{table}[h] \\centering % center the content of the table environment \\begin{tabular}{|c|c|} ... rows and columns \\end{tabular} \\caption{My table} \\label{tab:my_table} \\end{table}","title":"Simple tables with tabular environment"},{"location":"LaTeX/Latex%20manual/#columns-with-automatic-width-tabulary","text":"By default, laTeX does not support automatic width for columns, i.e., sizing the columns by their content. To enable this feature, we can use the tabulary package, which provides the tabulary environment (which is a replacement for the tabular environment). The columns with automatic width are specified by the L , C , R column types. Note that the new column types can be combined with the standard column types. In that case, the standard columns will have width according to their content, and the rest of the space will be distributed among the new column types.","title":"Columns with automatic width: tabulary"},{"location":"LaTeX/Latex%20manual/#complex-tables-with-tabulararray-package","text":"The tabulararray package provides : full control over the table, most features among all packages separation of the table content and the table presentation simpler code for basic tasks like multirows and multicolumns, wrapping text in cells, etc. Notable features present in the tabulararray package missing in other packages: footnotes in tables (otherwise, it requires a threeparttable environment wrapper) As other tabular packages, there are some incompatibilities related to the tabulararray package. So far, I observed only incompatibilities with the tabulararray talltblr environment, not with the standard tblr environment. The following table summarizes the incompatibilities I found so far: cases : Obviously, the cases environment uses table components internally. When using together with talltblr , the cases environment compilation results with the following error: latex \"\\begin{cases}\" package array empty preamble l' used . The solution is to use the use the new +cases environment provided by the tabulararray package. As a bonus, the +cases environment also fixes some visual glitches. Steps: enable the +cases environment by adding \\UseTblrLibrary{amsmath} to the preamble replace the cases environment with the +cases environment tabular : yes, the talltblr environment is incompatible with the default tabular environment. The solution is simple: replace all tabular environments with tblr environments.","title":"Complex tables with tabulararray package"},{"location":"LaTeX/Latex%20manual/#styling","text":"","title":"Styling"},{"location":"LaTeX/Latex%20manual/#table-grid-visual-lines","text":"Usually, tables contain some horizontal and vertical lines to separate the cells. Unfortunately, the way how to create these lines differs completely between horizontal and vertical lines.","title":"Table grid (visual lines)"},{"location":"LaTeX/Latex%20manual/#horizontal-lines","text":"For horizontal lines, we can: use various commands to create a line in a specific row: \\hline : creates a line in the current row specify the lines outside the data with the tabularray package:","title":"Horizontal lines"},{"location":"LaTeX/Latex%20manual/#specify-the-lines-outside-the-data-with-the-tabularray-package","text":"The format is hline{<list of rows>} = {<line style>} . Example: \\begin{tblr}{ colspec={|c|c|c|}, hline{1,Z} = {1pt, solid}, } ... \\end{tblr} The <list of rows> can be specified as: single row: hline{1} = {1pt, solid} multiple rows: hline{1,3,5} = {1pt, solid} range of rows: hline{1-Z} = {1pt, solid} Rows are numbered from 1. The Z character is used to specify the last row.","title":"Specify the lines outside the data with the tabularray package"},{"location":"LaTeX/Latex%20manual/#styling-the-specific-rows-or-columns","text":"With the tabularray package, we can style the table header differently than the rest of the table. In fact, we can style each specific row or column differently. for that, we use the row and column keys. Example \\begin{tblr}{ colspec={|c|c|c|}, row{1} = {bg=gray9, fg=white}, column{1} = {bg=gray9, fg=white}, } The syntax is the same for both keys: row{<row number>} = {<style>} . For the <row number> specification, see the grid section. The <style> specification is a list of key-value pairs. The keys are the following: font : the font style","title":"Styling the specific rows or columns"},{"location":"LaTeX/Latex%20manual/#configure-the-space-between-columns","text":"In most packages, the space between columns is configured using the \\tabcolsep variable. Example: \\setlength{\\tabcolsep}{10pt} However, in the tblr environment, the space between columns is configured using the leftsep and rightsep keys. Example:, \\begin{tblr} { colspec={llllrr}, leftsep=2pt, rightsep=2pt } By default, the leftsep and rightsep are set to 6pt .","title":"Configure the space between columns"},{"location":"LaTeX/Latex%20manual/#define-table-styles-with-tabularray-package","text":"The tabularray package provides a way to define table styles. First, we define the new environment with the \\NewTblrEnviron command. Later, we use the SetTblrInner and SetTblrOuter commands to define the style. Example: \\NewTblrEnviron{mytblr}{ \\SetTblrInner{rowsep=1pt} \\SetTblrOuter{hspan=minimal} }","title":"Define table styles with tabularray package"},{"location":"LaTeX/Latex%20manual/#rotated-text","text":"To rotate some text, we can use the \\rotatebox command: \\rotatebox{90}{Rotated text}","title":"Rotated text"},{"location":"LaTeX/Latex%20manual/#multirows-and-multicolumns","text":"Depending on the inner environment, the multirows and multicolumns are created using different commands.","title":"Multirows and multicolumns"},{"location":"LaTeX/Latex%20manual/#tabular-environment","text":"In the tabular environment, the multirows and multicolumns are created using the \\multicolumn and \\multirow commands. Example: \\begin{tabular}{cc} \\multicolumn{2}{c}{multi column} \\\\ \\multirow{2}{*}{multi row} & 1 \\\\ & 2 \\\\ \\end{tabular}","title":"tabular environment"},{"location":"LaTeX/Latex%20manual/#tabulararray-environment","text":"In the tabulararray environment, the multirows and multicolumns are created using the \\SetCell command. Example: \\begin{tblr}{cc} \\SetCell[c=2]{c} multi column & \\\\ \\SetCell[r=2]{c} multi row & 1 \\\\ & 2 \\\\ \\end{tblr} Note that for multicolumns, we need to add the column divider ( & ) after the \\SetCell command for each column that is spanned by the multicolumn .","title":"tabulararray environment"},{"location":"LaTeX/Latex%20manual/#export-google-sheets-to-latex-tables","text":"There is ann addon called LatexKit which can be used for that.","title":"Export google sheets to latex tables"},{"location":"LaTeX/Latex%20manual/#footnotes-in-tables","text":"In tables and table captions, the \\footnote command does not work correctly. Also, it is not desirable to have the footnote at the bottom of page, instead, we want the footnote to be at the bottom of the table. To achieve this, we use a special environment: threeparttable : if we are using the tabular or tabulary environment talltblr : if we are using the tblr environment","title":"Footnotes in tables"},{"location":"LaTeX/Latex%20manual/#using-the-threeparttable","text":"The threeparttable environment is used as follows: \\begin{table}[h] \\centering \\caption{My table} \\label{tab:my_table} \\begin{threeparttable} \\begin{tabular}{|c|c|} one$^a$ & two$^b$ \\\\ ... other rows and columns \\end{tabular} \\begin{tablenotes} \\item $^a$footnote 1 \\item $^b$footnote 2 \\end{tablenotes} \\end{threeparttable} \\end{table}","title":"Using the threeparttable"},{"location":"LaTeX/Latex%20manual/#using-the-talltblr","text":"The talltblr environment is used as follows: \\begin{table}[h] \\centering \\caption{My table} \\label{tab:my_table} \\begin{talltblr}[ label = none, note{a} = {footnote 1}, note{b} = {footnote 2} ]{ colspec={|c|c|}, } one\\TblrNote{a} & two\\TblrNote{b} \\\\ ... other rows and columns \\end{talltblr} \\end{table} Notice the label = none option. Without it, the table numbering is raised again, resulting in the table being numbered twice.","title":"Using the talltblr"},{"location":"LaTeX/Latex%20manual/#rotated-text_1","text":"To rotate some text, we can use the \\rotatebox command: \\rotatebox{90}{Rotated text} If we need to limit the width of the rotated text, we can use the \\parbox command. Example: \\rotatebox{90}{\\parbox{2cm}{Rotated text}}","title":"Rotated text"},{"location":"LaTeX/Latex%20manual/#algorithms","text":"wiki There are two types of enviroments for, depending on the purpose: algorithm : for pseudocode lstlisting : for code in a specific language","title":"Algorithms"},{"location":"LaTeX/Latex%20manual/#pseudocode","text":"The floating environment for pseudocode is algorithm . It is an equivalent of the table environment for tables. Same as with tables, there is also an inner environment for the pseudocode. The options for the inner environment are: algorithmic : basic environment for pseudocode algorithmicx : extension of the algorithmic environment, supports custom keywords algorithm2e : algpseudocodex : extension of the algorithmicx package, various improvements pseudo : a package for writing pseudocode directly, not using any special commands Their properties are summarized in the following table: | Environment | Package | Updated | Custom keywords | |--- | --- | --- | --- | | algorithmic | algorithms | 2009 | no | | algorithmicx | algorithmicx | 2005 | yes | | algorithm2e | algorithm2e | 2017 | yes | | algpseudocodex | algpseudocodex | 2023 | yes | | program | program | 2017 | no | | pseudo | pseudo | 2023 | yes | If the algorithm requires a complex description and cannot be expressed using typical pseudocode, we can resort to using human language directly in the algorithm environment. There is such example at SO","title":"Pseudocode"},{"location":"LaTeX/Latex%20manual/#algoritmicx-and-algpseudocodex","text":"Normal code lines are written using the \\State command: \\State $x \\gets 0$","title":"Algoritmicx and Algpseudocodex"},{"location":"LaTeX/Latex%20manual/#conditions","text":"Conditions are written using the \\If , \\ElseIf , and \\Else commands. Example: \\If{$x < 0$} \\State $x \\gets 0$ \\ElsIf{$x > 100$} \\State $x \\gets 100$ \\Else \\State $x \\gets x$ \\EndIf","title":"Conditions"},{"location":"LaTeX/Latex%20manual/#loops","text":"For loop: \\For{$ i = 1, \\dots, n $} ... \\EndFor","title":"Loops"},{"location":"LaTeX/Latex%20manual/#boolean-operators","text":"Boolean operators are not defined by default. Either use plain text, or define them (see the Additional keywords section below).","title":"Boolean operators"},{"location":"LaTeX/Latex%20manual/#additional-keywords","text":"We can define additional keywords using the \\algnonewcommand command. The format is \\algnonewcommand<custom command>{<keyword>} . Example: \\algnewcommand\\Not{\\textbf{not}}","title":"Additional keywords"},{"location":"LaTeX/Latex%20manual/#empty-lines","text":"To add an empty line, use the \\State command without any argument. If the numbering is on and we want to skip the numbering for the empty line, we can use the \\Statex command.","title":"Empty lines"},{"location":"LaTeX/Latex%20manual/#functions-and-procedures","text":"Functions and procedures are defined using the \\Function and \\Procedure commands. Example: \\Function{my_function}{a, b} \\State $x \\gets a + b$ \\State \\Return $x$ \\EndFunction We can call the function or procedure using the \\Call command. Example: \\Call{my_function}{1, 2}","title":"Functions and procedures"},{"location":"LaTeX/Latex%20manual/#line-numbering","text":"To add line numbering, add an optional argument to the algorithmic environment. The argument is the number determining the frequency of the line numbering. Example: \\begin{algorithmic}[3] % number every 3rd line ... \\end{algorithmic}","title":"Line numbering"},{"location":"LaTeX/Latex%20manual/#centering-of-floats-wider-than-text-width","text":"If the float is wider than the text width, it is not centered, but instead it is left-aligned with the text and it overflows on the right side. To fix this, we can wrap the too-wide float content in the \\makebox command. Example: \\begin{figure}[h] \\makebox[\\textwidth]{ \\includegraphics[width=1.2\\textwidth]{my_image.png} } \\caption{My figure} \\label{fig:my_figure} \\end{figure}","title":"Centering of floats wider than text width"},{"location":"LaTeX/Latex%20manual/#boxes","text":"wiki Everythign in LaTeX is a box. Each character is a box, stored in a larger box for each word, and analogically for each line, paragraph, etc. Most of the time we just set properties for the boxes, but sometimes we need to create a box manually to format or position the content. The following table presents the most common box commands: Command paragraph witdth \\parbox single fixed \\pbox single flexible \\minipage multiple fixed The first two parameters are shared for these commands: pos : the position of the box, e.g., t for top, b for bottom, c for center. The position refers to the part of the box that is aligned with the surrounding text. height : the height of the box. The parbox and minipage share another two parameters that follows: contentpos : the position of the content inside the box, e.g., t for top, b for bottom, c for center. width : the width of the box.","title":"Boxes"},{"location":"LaTeX/Latex%20manual/#make-a-box-wider-than-the-text-width","text":"To make a box wider than the text width, we can use the \\adjustwidth command from the changepage package. Example: \\begin{adjustwidth}{-1cm}{-1cm} ... content \\end{adjustwidth} The box above will be 1cm wider on each side than the text width.","title":"Make a box wider than the text width"},{"location":"LaTeX/Latex%20manual/#math","text":"wiki To use math, we need the amsmath package. The math commands only works in math mode which can be entered in one of the many math environments.","title":"Math"},{"location":"LaTeX/Latex%20manual/#common-math-constructs","text":"The following table lists the most common math constructs: fractions : \\frac{<numerator>}{<denominator>} sum : \\sum_{<lower>}^{<upper>} binomial coefficients : \\binom{n}{k}","title":"Common math constructs"},{"location":"LaTeX/Latex%20manual/#subscript-and-superscript_1","text":"Subscript and superscript are created using the _ and ^ characters. If the subscript or superscript is longer than one character, we need to wrap it in curly braces. There are aslo some special characters that result in superscript: ' : prime * : star However, these characters alone works only in normal math text. If we want to use them in a subscript or superscript, we need to use the ^ and _ characters. Example: x* % correct print a_{x*} % inccorrect print - the star is not in superscript a_{x^*} % correct print","title":"Subscript and superscript"},{"location":"LaTeX/Latex%20manual/#ifelse-variants","text":"For that, we use the cases environment. Example: \\begin{equation} f(x) = \\begin{cases} 0 & \\quad \\text{if } x < 0 \\\\ 1 & \\quad \\text{if } x \\geq 0 \\end{cases} \\end{equation}","title":"If/else variants"},{"location":"LaTeX/Latex%20manual/#equations-and-similar-environments","text":"The following environments are available for equations: equation : for single equations align : for multiple equations aligned at a specific character alignat : for multiple equations aligned at multiple characters aligned : for multiple equations inside another math environment (e.g., equation ) gather : for multiple equations not aligned, each line numbered separately The new line is created using the \\\\ command. Nothe that the \\\\ command is not allowed in the equation environment . To align the equations in the align and alignat environments, we use the & character.","title":"Equations and similar environments"},{"location":"LaTeX/Latex%20manual/#problem-and-similar-environments","text":"wiki The environments for special math text blocks are not included in the amsmath package. We can define them manually using the \\newtheorem command. Example: \\newtheorem{problem}{Problem} \\begin{problem} This is a problem. \\end{problem} Here, the first argument of the \\newtheorem command is the name of the environment, and the second argument is the name of the environment in the output. There can be also an optional third argument, which is the name of the counter that is used for numbering the environment. For example, if we want to use another environment for propositions with the same numbering as the problems, we can use the following command: \\newtheorem{proposition}[problem]{Proposition} Some environments are already defined in the amsthm package, e.g., proof .","title":"Problem and similar environments"},{"location":"LaTeX/Latex%20manual/#common-environments-names-and-their-meaning","text":"theorem : a statement to be proved lemma : a minor theorem, with a limited applicability outside the context of the main theorem corollary : a theorem that follows directly from another theorem proposition : a less important theorem, usually used for something elementary, obvious, so that it does not require a proof premise : a statement that is assumed to be true and it represents a fact that is used in the proof of the theorem. For example: \"We only consider sets where half of the numbers are even...\" assumption : a statement that is assumed to be true and it represents a condition under which the theorem is true. For example: \"Assuming half of the numbers in the set are even...\" More can be fount on proofwiki","title":"Common environments names and their meaning"},{"location":"LaTeX/Latex%20manual/#math-fonts","text":"wiki the default math font is typed as common math italic. To use a different font, we need to use special commands: \\mathrm{} : for normal font in math mode, e.g., for multi-letter subscripts and superscripts \\mathbb{} : for blackboard bold font, e.g., for special sets (R, N, ...). This font requires the amsfonts package.","title":"Math fonts"},{"location":"LaTeX/Latex%20manual/#bold-math-font","text":"To use a bold font, we can use the \\bm command from the bm package: \\usepackage{bm} \\begin{equation} \\bm{a} = \\bm{b} + \\bm{c} \\end{equation}","title":"Bold math font"},{"location":"LaTeX/Latex%20manual/#correct-size-of-braces","text":"To size any type of braces correctly, if the content is larger than the braces, we can use the \\left and \\right commands. Example: \\left( \\frac{a}{b} \\right)","title":"Correct size of braces"},{"location":"LaTeX/Latex%20manual/#links","text":"wiki For links, we need the hyperref package. Typical usage: \\url{https://www.google.com} \\href{https://www.google.com}{Google}","title":"Links"},{"location":"LaTeX/Latex%20manual/#footnotes","text":"The footnote is created using the \\footnote{} command.","title":"Footnotes"},{"location":"LaTeX/Latex%20manual/#override-footnote-numbering","text":"To override the footnote numbering (e.g. to repeat the same number twice), we can use the \\setcounter command. Example: \\setcounter{footnote}{1} # set the footnote counter to 1","title":"Override footnote numbering"},{"location":"LaTeX/Latex%20manual/#disallow-footnote-splitting","text":"To disallow footnote splitting, we can increase the \\interfootnotelinepenalty command. Example: \\interfootnotelinepenalty=10000","title":"Disallow footnote splitting"},{"location":"LaTeX/Latex%20manual/#custom-commands-and-variables","text":"wiki Basic syntax for defining a new command or variable is: \\newcommand{\\<command name>}[<number of arguments>]{<command definition>} The number of arguments is optional. If it is not specified, the command does not take any arguments. The command is then used as follows: \\<command name>{<argument 1>}{<argument 2>}...{<argument n>} The \\newcommand command has also its star variant \\newcommand* . The difference is that when using just \\newcommand , the command arguments can contain line breaks ( \\par commands), while when using \\newcommand* , the command arguments cannot contain line breaks. This extra restriction is useful to avoid accidental line breaks in the command arguments. As it is a common concept in LaTeX, it is called a short-form command , in contrast to the long-form command without the star.","title":"Custom commands and variables"},{"location":"LaTeX/Latex%20manual/#providing-default-values-for-arguments","text":"Latex support default values for the first argument. The syntax is: \\newcommand{\\<command name>}[<number of arguments>][<default value>]{<command definition>} The command can than be used both with and without the optional argument: \\newcommand{\\mycommand}[2][0]{...} \\mycommand{<argument 2>} \\mycommand[<argument 1>]{<argument 2>} Note that that if we want to supply the optional argument, we use the square brackets. For two optional arguments, we have to use the twoopt package. Example: \\usepackage{twoopt} ... \\newcommandtwoopt{\\mycommand}[2][default1][default2]{...} Here again, both optioanl arguments, if supplied, must be supplied in the square brackets.","title":"Providing default values for arguments"},{"location":"LaTeX/Latex%20manual/#splitting-the-document-into-multiple-files","text":"There are two ways to split the document into multiple files: \\input{file} \\include{file} The \\include is intended for chapters or other large parts of the document. It has the following properties: it starts a new page before and after the included file it does not allow nesting there is a special command \\includeonly{file1,file2,...} which allows to include only the specified files. This is useful for large documents where we want to compile only a part of the document. Without this command we would need to search for the include command and comment it out. The \\input command is intended for smaller parts of the document. Contrary to the \\include command, there is no special behavior involved. Instead, the content of the file is simply pasted at the place of the \\input command.","title":"Splitting the document into multiple files"},{"location":"LaTeX/Latex%20manual/#speedup-techniques","text":"The compilation of large documents can be slow. There are several techniques to speed up the compilation: split the document into multiple files and use \\includeonly to include only the relevant files precompiling the preamble using draft mode","title":"Speedup Techniques"},{"location":"LaTeX/Latex%20manual/#precompiling-the-preamble","text":"The preamble is the part of the document before the \\begin{document} command. It contains the document configuration, packages, etc. Because the included packages are usually large, the compilation of the preamble can be slow. To speed up the compilation, we can precompile the preamble and use the precompiled preamble in the main document. This can be done using the mylatexformat package. The usage is as follows: At the beginning of the preamble, add the following comment: %&<format name> . This will tell the compiler to use the specified format. The <format name> can be arbitrary, but it is recommended to use the same name as the main document. To spare some preamble content from being precompiled (dynamic content), add a command \\endofdump after the content that should not be precompiled. run the following command: PowerShell pdflatex --ini -jobname=\"<format name>\" \"&pdflatex\" mylatexformat.ltx <format name>.tex Afther this, the compilation of the main document should be faster. For more information, see the package documentation or the SO question .","title":"Precompiling the preamble"},{"location":"LaTeX/Latex%20manual/#miscelaneous-tasks","text":"","title":"Miscelaneous tasks"},{"location":"LaTeX/Latex%20manual/#balancing-columns-in-two-column-documents","text":"To balance the columns at the end of the document, we can use the flushend package. Just add \\usepackage{flushend} to the preamble.","title":"Balancing columns in two-column documents"},{"location":"LaTeX/Latex%20manual/#common-problems","text":"","title":"Common problems"},{"location":"LaTeX/Latex%20manual/#ugly-font-in-pdf","text":"This can be cause by the missing vector fonts. If the vector fonts are missing, the bitmap fonts are used instead. To check if this is the cause, zoom in on the pdf. If the text is blurry, the bitmap fonts are used. To fix this, install the vector fonts. On Windows, install the cm-super package through MikTeX.","title":"Ugly font in pdf"},{"location":"Linux/Bash%20Manual/","text":"Bash Manual \u00b6 documentation wiki Bash can refer to a typical Unix shell, or just the command line interpreter for this shell or to the language used to write shell commands and scripts. In bash, commands can be separated by a newline, or a semicolon. Therefore, we can write even complicated commands to a single line in the terminal. General Remarks \u00b6 It's important to use Linux newlines, otherwise, bash scripts will fail with unexpected character error Empty constructs are not allowed, i.e, empty function or loop results in an error brackets needs spaces around them, otherwise, there will be a syntax error space around = , which is typical in other languages, is not allowed in bash bash does not support any data structures, only arrays Bash modes and environment initialization \u00b6 Bash can run in several modes. These modes have some effect, but mainly, it affects the initialization of the environment, i.e., which files are executed at the start of the shell. The modes are: login shell : mode is used when the user logs in. also used when the shell is run with the -l , --login parameter interactive shell : mode is used when the user interacts with the shell. also used when the shell is run with the -i parameter non-interactive shell : mode is used when the shell is run in a script. this shell mode is used when we run commands over ssh or wsl The execution of the initialization files is displayed in the image below: Note that the initialized environment persists in the system. In other words, the environment variables set in the files processed by the login shell are available even in the non-interactive non-login shells opened later on. However, there are still cases when the environment variables are not available because the login shell has not been run, e.g.: when the shell is run over ssh as a command: ssh user@host <command> when the shell is run from wsl as a command: wsl <command> In those cases, if we need the environment variables, we have to run the login or interactive shell explicitly, e.g.: wsl bash -lc <command> . Variables \u00b6 Variables can be defined as: var=value Note that there must not be any spaces around = . We can also declare variables with a specific type, so that no other type can be assigned to it using the declare command: declare -i var # integer declare -a var # array declare -r var # read only To access the value of a variable, we use $ : echo $var Assigning the output of a command to a variable \u00b6 The output of a command can be assigned to a variable only with the command substitution : var=<command> # wrong, the first token of the command is assigned to the variable var=$(<command>) # correct, the output of the command is assigned to the variable Example: var=$(echo $var | sed 's/old/new/') List all variables \u00b6 To list all variables, we can use the declare command: declare However, this command also lists the functions. To list only the variables, we can use: declare -p which also prints the type and attributes of the variables. Operations on variables \u00b6 There are many operations on variables, the most important are: ${#<variable>} : length of the variable ${<variable>%%<pattern>} : remove the longest suffix matching the pattern ${<variable>##<pattern>} : remove the longest prefix matching the pattern Executable execution \u00b6 Executing binaries while skipping the shell aliases \u00b6 wiki We can do this by using the command command: command <executable> <arguments> . This command will execute the executable directly, without any shell aliases. Working with I/O \u00b6 Output Forwarding \u00b6 Output forwarding is a process of redirecting the output of a command to an input of another command. The operator for that is the pipe | . The syntax is: <command 1> | <command 2> Note that the content of the pipe cannot be examined, the process on the right hand side consume it. Therefore, it is not possible to simply branch on pipe content while use it in the subsequent process. Output Redirection \u00b6 Output redirection is a process of redirecting the output (stdout, stderr,..) from the console to a file. The syntax is: <command> <operator> <file> The possible operators and their effects are listed in the table (full explanation on SO ) below: Operator Stdout Stderr Mode (in file) > file console overwrite >> file console append &> file file overwrite &>> file file append 2> console file overwrite 2>> console file append \\| tee both console overwrite \\| tee -a both console append \\|& tee both both overwrite \\|& tee -a both both append tee is actully a command, not an operator. It is used as follows: <command> | tee <file> We can also use tee to forward the output of a command to multiple commands: <command 1> | tee >(<command 2>) | <command 3> This forward <command 1> to both <command 2> and <command 3> . Use bash variables as input \u00b6 Bash variables can be used as input of a command. Syntax: <command> <<< $<variable>` Output and input redirection with sudo \u00b6 We need to consider that the sudo command is not applied to the redirected input or output. Example: sudo <command> > <file> In the above command, the sudo command is applied to the <command> , but not to the file writing, which is a bash construct. Therefore, the command will fail if the file writing requires root access. The same goes for the input redirection if we need root access for reading a file. The solution is to use tee : sudo <command> | tee <file> Command Substitution \u00b6 When we need to use an output of a command instead of a constant or variable, we have to use command substtution: `<command>` # or equivalently $(<command>) e.g.: echo resut is: `cut -d, -f 7` # or equivalently echo resut is: $(cut -d, -f 7) Conditions \u00b6 In general, condition in bash has the following syntax: if <condition> then <command> else <command> fi The condition can have several formats: plain command : the condition is true if the command returns 0 bash if grep -q \"$text\" $file then ... fi [ <condition> ] or test <condition> : The standard POSIX test construct. Now only suitable if we want to run the script outside bash. bash if [ $var = 1 ] then ... fi [[ <condition> ]] : The extended test construct. This is the recommended way of writing conditions, due to several practical features (e.g., no need to quote variables, regex support, logical operators, etc.). bash if [[ $var = 1 ]] then ... fi (( <condition> )) : The arithmetic test construct. This is used for arithmetic conditions. bash if (( $var == 1 )) then ... fi Note that if we want to use some arbitrary value (e. g. the return value of a command), or comparisons in the condition (similar to programming languages), we have to use one of the test constructs. Mind the spaces around the braces! String comparison \u00b6 Strings can be compared using the standard = operator or the == operator. If we use the [ ] construct, we have to quote the variables, otherwise, the script will fail on empty strings or strings containing spaces: if [ \"$var\" = \"string\" ] then ... fi # or equivalently if [[ $var = \"string\" ]] then ... fi Loops \u00b6 The syntax of the loop is: while <condition> do <command1> <command2> ... done Forward to loop \u00b6 We can forward an input into while loop using | as usuall. Additionally, it is possible to read from file directly by adding < to the end like this: while condition do <command1> <command2> ... done < <input> The same goes for the output, i.e., we can forward th outut of a loop with | . Strings literals \u00b6 String literals can be easily defined as: str=\"string literal\" # or equivalently str='string literal' If we use double quotes, the variables are expanded, e.g., echo \"Hello $USER\" will print Hello <username> . The problem arises when we want to use double quotes in the string literal containing variables, e.g., normal string \"quotted string\" $myvar . In this case, we have to use quite cumbersome syntax: a = \"normal string \"\\\"\"quotted string\"\\\"\" # or equivalently a = \"normal string \"'\"'\"quotted string\"'\"' Multiline string literals \u00b6 There is no dedicated syntax for multiline string literals. However, we can use the here document (HEREDOC) syntax: <target> << <delimiter> <content> <delimiter> For example, to store the command in a variable, we can use: db_sql = $(cat << SQL CREATE DATABASE test_$name OWNER $name; grant all privileges on database test_$name to $name; SQL) Note that the end <delimiter> must be at the beginning of the line, otherwise, it will not work. Functions \u00b6 Functions are defined as: function_name() { <command 1> <command 2> ... } For access the arguments of the function, we use the same syntax as for the script arguments (e.g., $1 for the first argument). We can create local variables in the function using the local keyword: function_name() { local var1=\"value\" ... } Reading from command line \u00b6 To read from command line, we can use the read command. The syntax is: read <variable> where <variable> is the name of the variable to store the input. Important parameters: -p <prompt> : prints the <prompt> before reading the input -s : do not echo the input (usefull for passwords) Bash Scripts \u00b6 Bash Script Arguments \u00b6 We refer the arguments of a bash script as $0 - the name of the script $1..$n - the arguments of the script $@ - all the arguments of the script Sometimes, it is useful to throw away processed arguments. This can be done using the shift command shift <n> , where <n> is the number of arguments to be thrown away (default is 1). The remaining arguments are then shifted to the left, i.e., $2 becomes $1 and so on. Bash Script Header Content \u00b6 Usually, here are some special lines at the beginning of the bash script. First, we can specify the interpreter to be used: #!/bin/bash Then we can set some flags for the script set -<flags> Useful flags are: e : exit on error u : error raised on undefined variables o : any command that returns a non-zero exit code will raise an error even when piped to another command detailed description gist Calling a script from another script in the same directory \u00b6 To call a script b from script a in the same directory, it is not wise to use the relative path, it is evaluated from the current directory, not from the directory of the script. To be sure that we can call script a from anywhere, we need to change the working directory to the directory of the scripts first: cd \"$(dirname \"$0\")\" ./B.sh","title":"Bash Manual"},{"location":"Linux/Bash%20Manual/#bash-manual","text":"documentation wiki Bash can refer to a typical Unix shell, or just the command line interpreter for this shell or to the language used to write shell commands and scripts. In bash, commands can be separated by a newline, or a semicolon. Therefore, we can write even complicated commands to a single line in the terminal.","title":"Bash Manual"},{"location":"Linux/Bash%20Manual/#general-remarks","text":"It's important to use Linux newlines, otherwise, bash scripts will fail with unexpected character error Empty constructs are not allowed, i.e, empty function or loop results in an error brackets needs spaces around them, otherwise, there will be a syntax error space around = , which is typical in other languages, is not allowed in bash bash does not support any data structures, only arrays","title":"General Remarks"},{"location":"Linux/Bash%20Manual/#bash-modes-and-environment-initialization","text":"Bash can run in several modes. These modes have some effect, but mainly, it affects the initialization of the environment, i.e., which files are executed at the start of the shell. The modes are: login shell : mode is used when the user logs in. also used when the shell is run with the -l , --login parameter interactive shell : mode is used when the user interacts with the shell. also used when the shell is run with the -i parameter non-interactive shell : mode is used when the shell is run in a script. this shell mode is used when we run commands over ssh or wsl The execution of the initialization files is displayed in the image below: Note that the initialized environment persists in the system. In other words, the environment variables set in the files processed by the login shell are available even in the non-interactive non-login shells opened later on. However, there are still cases when the environment variables are not available because the login shell has not been run, e.g.: when the shell is run over ssh as a command: ssh user@host <command> when the shell is run from wsl as a command: wsl <command> In those cases, if we need the environment variables, we have to run the login or interactive shell explicitly, e.g.: wsl bash -lc <command> .","title":"Bash modes and environment initialization"},{"location":"Linux/Bash%20Manual/#variables","text":"Variables can be defined as: var=value Note that there must not be any spaces around = . We can also declare variables with a specific type, so that no other type can be assigned to it using the declare command: declare -i var # integer declare -a var # array declare -r var # read only To access the value of a variable, we use $ : echo $var","title":"Variables"},{"location":"Linux/Bash%20Manual/#assigning-the-output-of-a-command-to-a-variable","text":"The output of a command can be assigned to a variable only with the command substitution : var=<command> # wrong, the first token of the command is assigned to the variable var=$(<command>) # correct, the output of the command is assigned to the variable Example: var=$(echo $var | sed 's/old/new/')","title":"Assigning the output of a command to a variable"},{"location":"Linux/Bash%20Manual/#list-all-variables","text":"To list all variables, we can use the declare command: declare However, this command also lists the functions. To list only the variables, we can use: declare -p which also prints the type and attributes of the variables.","title":"List all variables"},{"location":"Linux/Bash%20Manual/#operations-on-variables","text":"There are many operations on variables, the most important are: ${#<variable>} : length of the variable ${<variable>%%<pattern>} : remove the longest suffix matching the pattern ${<variable>##<pattern>} : remove the longest prefix matching the pattern","title":"Operations on variables"},{"location":"Linux/Bash%20Manual/#executable-execution","text":"","title":"Executable execution"},{"location":"Linux/Bash%20Manual/#executing-binaries-while-skipping-the-shell-aliases","text":"wiki We can do this by using the command command: command <executable> <arguments> . This command will execute the executable directly, without any shell aliases.","title":"Executing binaries while skipping the shell aliases"},{"location":"Linux/Bash%20Manual/#working-with-io","text":"","title":"Working with I/O"},{"location":"Linux/Bash%20Manual/#output-forwarding","text":"Output forwarding is a process of redirecting the output of a command to an input of another command. The operator for that is the pipe | . The syntax is: <command 1> | <command 2> Note that the content of the pipe cannot be examined, the process on the right hand side consume it. Therefore, it is not possible to simply branch on pipe content while use it in the subsequent process.","title":"Output Forwarding"},{"location":"Linux/Bash%20Manual/#output-redirection","text":"Output redirection is a process of redirecting the output (stdout, stderr,..) from the console to a file. The syntax is: <command> <operator> <file> The possible operators and their effects are listed in the table (full explanation on SO ) below: Operator Stdout Stderr Mode (in file) > file console overwrite >> file console append &> file file overwrite &>> file file append 2> console file overwrite 2>> console file append \\| tee both console overwrite \\| tee -a both console append \\|& tee both both overwrite \\|& tee -a both both append tee is actully a command, not an operator. It is used as follows: <command> | tee <file> We can also use tee to forward the output of a command to multiple commands: <command 1> | tee >(<command 2>) | <command 3> This forward <command 1> to both <command 2> and <command 3> .","title":"Output Redirection"},{"location":"Linux/Bash%20Manual/#use-bash-variables-as-input","text":"Bash variables can be used as input of a command. Syntax: <command> <<< $<variable>`","title":"Use bash variables as input"},{"location":"Linux/Bash%20Manual/#output-and-input-redirection-with-sudo","text":"We need to consider that the sudo command is not applied to the redirected input or output. Example: sudo <command> > <file> In the above command, the sudo command is applied to the <command> , but not to the file writing, which is a bash construct. Therefore, the command will fail if the file writing requires root access. The same goes for the input redirection if we need root access for reading a file. The solution is to use tee : sudo <command> | tee <file>","title":"Output and input redirection with sudo"},{"location":"Linux/Bash%20Manual/#command-substitution","text":"When we need to use an output of a command instead of a constant or variable, we have to use command substtution: `<command>` # or equivalently $(<command>) e.g.: echo resut is: `cut -d, -f 7` # or equivalently echo resut is: $(cut -d, -f 7)","title":"Command Substitution"},{"location":"Linux/Bash%20Manual/#conditions","text":"In general, condition in bash has the following syntax: if <condition> then <command> else <command> fi The condition can have several formats: plain command : the condition is true if the command returns 0 bash if grep -q \"$text\" $file then ... fi [ <condition> ] or test <condition> : The standard POSIX test construct. Now only suitable if we want to run the script outside bash. bash if [ $var = 1 ] then ... fi [[ <condition> ]] : The extended test construct. This is the recommended way of writing conditions, due to several practical features (e.g., no need to quote variables, regex support, logical operators, etc.). bash if [[ $var = 1 ]] then ... fi (( <condition> )) : The arithmetic test construct. This is used for arithmetic conditions. bash if (( $var == 1 )) then ... fi Note that if we want to use some arbitrary value (e. g. the return value of a command), or comparisons in the condition (similar to programming languages), we have to use one of the test constructs. Mind the spaces around the braces!","title":"Conditions"},{"location":"Linux/Bash%20Manual/#string-comparison","text":"Strings can be compared using the standard = operator or the == operator. If we use the [ ] construct, we have to quote the variables, otherwise, the script will fail on empty strings or strings containing spaces: if [ \"$var\" = \"string\" ] then ... fi # or equivalently if [[ $var = \"string\" ]] then ... fi","title":"String comparison"},{"location":"Linux/Bash%20Manual/#loops","text":"The syntax of the loop is: while <condition> do <command1> <command2> ... done","title":"Loops"},{"location":"Linux/Bash%20Manual/#forward-to-loop","text":"We can forward an input into while loop using | as usuall. Additionally, it is possible to read from file directly by adding < to the end like this: while condition do <command1> <command2> ... done < <input> The same goes for the output, i.e., we can forward th outut of a loop with | .","title":"Forward to loop"},{"location":"Linux/Bash%20Manual/#strings-literals","text":"String literals can be easily defined as: str=\"string literal\" # or equivalently str='string literal' If we use double quotes, the variables are expanded, e.g., echo \"Hello $USER\" will print Hello <username> . The problem arises when we want to use double quotes in the string literal containing variables, e.g., normal string \"quotted string\" $myvar . In this case, we have to use quite cumbersome syntax: a = \"normal string \"\\\"\"quotted string\"\\\"\" # or equivalently a = \"normal string \"'\"'\"quotted string\"'\"'","title":"Strings literals"},{"location":"Linux/Bash%20Manual/#multiline-string-literals","text":"There is no dedicated syntax for multiline string literals. However, we can use the here document (HEREDOC) syntax: <target> << <delimiter> <content> <delimiter> For example, to store the command in a variable, we can use: db_sql = $(cat << SQL CREATE DATABASE test_$name OWNER $name; grant all privileges on database test_$name to $name; SQL) Note that the end <delimiter> must be at the beginning of the line, otherwise, it will not work.","title":"Multiline string literals"},{"location":"Linux/Bash%20Manual/#functions","text":"Functions are defined as: function_name() { <command 1> <command 2> ... } For access the arguments of the function, we use the same syntax as for the script arguments (e.g., $1 for the first argument). We can create local variables in the function using the local keyword: function_name() { local var1=\"value\" ... }","title":"Functions"},{"location":"Linux/Bash%20Manual/#reading-from-command-line","text":"To read from command line, we can use the read command. The syntax is: read <variable> where <variable> is the name of the variable to store the input. Important parameters: -p <prompt> : prints the <prompt> before reading the input -s : do not echo the input (usefull for passwords)","title":"Reading from command line"},{"location":"Linux/Bash%20Manual/#bash-scripts","text":"","title":"Bash Scripts"},{"location":"Linux/Bash%20Manual/#bash-script-arguments","text":"We refer the arguments of a bash script as $0 - the name of the script $1..$n - the arguments of the script $@ - all the arguments of the script Sometimes, it is useful to throw away processed arguments. This can be done using the shift command shift <n> , where <n> is the number of arguments to be thrown away (default is 1). The remaining arguments are then shifted to the left, i.e., $2 becomes $1 and so on.","title":"Bash Script Arguments"},{"location":"Linux/Bash%20Manual/#bash-script-header-content","text":"Usually, here are some special lines at the beginning of the bash script. First, we can specify the interpreter to be used: #!/bin/bash Then we can set some flags for the script set -<flags> Useful flags are: e : exit on error u : error raised on undefined variables o : any command that returns a non-zero exit code will raise an error even when piped to another command detailed description gist","title":"Bash Script Header Content"},{"location":"Linux/Bash%20Manual/#calling-a-script-from-another-script-in-the-same-directory","text":"To call a script b from script a in the same directory, it is not wise to use the relative path, it is evaluated from the current directory, not from the directory of the script. To be sure that we can call script a from anywhere, we need to change the working directory to the directory of the scripts first: cd \"$(dirname \"$0\")\" ./B.sh","title":"Calling a script from another script in the same directory"},{"location":"Linux/Fedora/","text":"Wiki Managing packages \u00b6 Fedore uses the dnf package manager. It automatically updates the available packages, so a call like apt get update is not at all necessary. To list installed packages: dnf list installed To install a package: dnf install <package> To remove a package: dnf remove <package> To search for available packages: dnf search <package>","title":"Fedora"},{"location":"Linux/Fedora/#managing-packages","text":"Fedore uses the dnf package manager. It automatically updates the available packages, so a call like apt get update is not at all necessary. To list installed packages: dnf list installed To install a package: dnf install <package> To remove a package: dnf remove <package> To search for available packages: dnf search <package>","title":"Managing packages"},{"location":"Linux/Linux%20Manual/","text":"Usefull Comands \u00b6 Versions \u00b6 Check OS Distribution and Version \u00b6 cat /etc/*-release Checking glibc version \u00b6 ldd --version Printing used ports \u00b6 sudo lsof -i -P Exit codes \u00b6 Exit code of the last command: $? Common exit codes for C++ programs Show path to executable \u00b6 Use the which command: which <executable> . Other option is to use the command command: command -v <executable> . Unpack file \u00b6 Foe unpacking, you can use the tar -f <file> command. The most used options are: x : extract Environment Variables \u00b6 The environment variables are introduced with the export command: export <variable>=<value> without export, the variable is just a local shell variable: <variable>=<value> # local variable We will demonstrate the work with environment variables on the PATH example. If you have a program in a custom location, adding it to $PATH permanently and be able to run it under all circumstances is not an easy task on linux. Standard procedure is to add a system variable: Create a dedicated .sh file in /etc/profile.d. for your configuration (config for each app should be stored in a separate file). the file should contain: export PATH=$PATH:YOURPATH exit nano and save the file: ctrl+x and y logout and login again to load the newly added varibales for WSL close the console and reopen it, it is not necessary to restart the WSL ( click here for detailed description ) Enable Variable with sudo \u00b6 To enable the variable even if you use sudo , you need to edit sudo config using sudo visudo and: exclude PATH from variable being reset when running sudo : Defaults env_keep += \"PATH\" disable the safe path mechanism for sudo , i.e., comment the line: Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin logout and login again to load the new added varibales Enable the Variable Outside Bash \u00b6 If you need the variable outside bash, the above-mentioned approach won't work. Currently, I do not know about any general solution for variables. The solution below, unfortunately, work only for PAM shells (see this SO answer why). Add the variable to /etc/environment . Note that it is not a script file, so you can use only simple variable assignments. Enable Variable on a Device Without Root Access \u00b6 Without root access, we can only edit the user config files. put the necessary config into: ~/.bash_profile if it already exists or to ~/.profile Note that the .profile file is ignored when the .bash_profile file exists. Remove Windows $PATH from WSL \u00b6 By default, the PATH environment variable from Windows is inluded in the PATH variable in Ubuntu running in WSL. This can lead to conflicts, when the same executables are used in both systems (e.g., vcpkg, cmake). To turn of the Windows PATH inclusion: open /etc/wsl.conf add the fllowing code: [interop] appendWindowsPath = false restart WSL File System \u00b6 Unlike Windows, Linux typically doe not split the filesystem into partitions that much. Moreover, each partition path can start anywhere, instead of <drive letter>: like in Windows. Typically, the main partition is mounted at / . Standard folder structure \u00b6 The standard folder structure is: /bin core binaries, usually a limited set distributed with the OS /home/<user name> home directory for the user <user name> /opt for software packages distributed in the Windows style, i.e., as a single installation directory. /sbin same as /bin , but for system binaries, i.e., binaries for system maintenance /usr/bin and /usr/sbin For binares installed by package managers. Binaries from other sources should not be installed here, as there is a risk of overwritting them by the package manager. /usr/local/bin and /usr/local/sbin : For binaries installed by the user, i.e., not by the package manager. List files \u00b6 To list files, use ls <path> . By default, it lists the files in the current directory. The most used options are: -l : list in long format -a : list all files, including hidden ones -1 : list one file per line (instead of multiple columns) -d : list directories themselves, not their contents List the full paths \u00b6 There is no option of showing the full path. The trick is to use the -d option, as directories are listed in the long format by default. So for example, to list full paths of all subdirectories of <dir> , you can use: ls -d <dir>/* Copy files \u00b6 The cp command is used to copy files: cp <source> <destination> . The most used options are: -r , -R : copy recursively -v : verbose -f : force -p : preserve permissions and timestamps -a : same as -p -R plus some other options rsync \u00b6 For more sophisticated copying, use rsync ( documentation ): rsync <source> <destination> . Unlike the cp command, the <source> have a different meaning depending on the trailing slash: if the <source> ends with a slash, it behaves like cp : bash rsync -r /path/to/source/ /path/to/destination # the content of the source directory is copied to the /path/to/destination directory if the <source> does not end with a slash, it behaves differently: bash rsync -r /path/to/source /path/to/destination # the content of the source directory is copied to the /path/to/destination/source directory The most used options are: -r , --recursive : recurse into directories. Only files are copied without this option. -h : human readable Preserving metadata \u00b6 -l , --links : preserve links -p , --perms : preserve permissions -t , --times : preserve modification times -U , --atimes : preserve access times -N , --crtimes : preserve creation times -g , --group : preserve group -o , --owner : preserve owner --specials : preserve special files --devices : preserve device files -D : equivalent to --devices --specials -a : archive mode, equivalent to -rlptgoD Formatting the output \u00b6 --progress : show progress bar for each file transfer --info=<FLAG>[<FLAG_VALUE>] : detailed configuration of the rsync output. Here, <FLAG> is the specific output parameter we want to set: BACKUP Mention files backed up COPY : Mention files copied locally on the receiving side DEL : Mention deletions on the receiving side FLIST : Mention file-list receiving/sending (levels 1-2) MISC : Mention miscellaneous information (levels 1-2) MOUNT : Mention mounts that were found or skipped NAME : Mention 1) updated file/dir names, 2) unchanged names PROGRESS : Mention 1) per-file progress or 2) total transfer progress REMOVE : Mention files removed on the sending side SKIP : Mention files that are skipped due to options used STATS : Mention statistics at end of run (levels 1-3) SYMSAFE : Mention symlinks that are unsafe ALL : Set all --info options (e.g. all4) <FLAG_VALUE> is the value of the parameter. 0 : disable the output 1 : default value < Number > 1 > : more detailed output. The maximum level depends on the output parameter we want to set. Example: --info=progress2 will show the progress total progress, instead of the progress bar for each file transfer. Overwriting files \u00b6 --ignore-existing : ignore files that already exist on the destination. --update : ignore newer files on the destination. --checksum : use checksum to compare files, not the file size and modification time. Remove file \u00b6 The rm command is used to remove files. The most used options are: -r , -R : remove recursively To remove all files in a directory, you can use Access rights \u00b6 The Linux access rights use the same system for files and folders. The access rights are divided into three groups, from left to right: owner group other Each group has three possible access rights: r : read w : write x : execute Important aspects: to access a directory, the user has to have the x right on the directory. to access a file, the user has to have the x right on all folders in the path to the file. Compute directory size \u00b6 To compute the size of a directory, use the du command: du <path> . The most used options are: -h : human readable -s : summarize Find files \u00b6 To find files, use the find command: find <where> <options> . If the <where> is not specified, the current directory is used. If the <options> are not specified, all files matching the <where> are listed. The most used options are: -name <name pattern> : find by name. -path <path pattern> : find by path. -regex <regex pattern> : find by path specified as regex. Note that by default, the regex type is emacs-style. Therefore, we have to set the type to posix-extended to work reasonably with regexes. so in the end, we have to use: bash find -regextype posix-extended -regex '.*<regex pattern>.*' note the order of the options. Specifying the -regextype after the -regex option does not work. -printf <format> : set the output format. The most important format specifiers are: %f : file name %p : file path %TY : year %Tm : month %Td : day %TH : hour %TM : minute %TS : second modifiers can be used to format the output: %.2<format specifier> : format the output to 2 digits List disks and partitions \u00b6 To list disks and partitions, use the lsblk command. If we are not satisfied with the output, we can configure it with the -o option: lsblk -o <list of columns, separated by commas> Most important columns are: NAME : name of the disk or partition SIZE : size of the disk or partition TYPE : type of the disk or partition FSTYPE : file system type FSAVAIL : available file system space Network \u00b6 netstat \u00b6 The netstat command is the basic command to monitor the networ. It displays the TCP connections. It is available both on Linux and on Windows, although the interface differs. Important parameters: -n : do not translate IP and ports into human readable names -a : show all connections. Without this, some connections can be skipped. Bash \u00b6 See Bash Manual for detailed information about Bash scripting and commands. Root Access \u00b6 Some commands require root privilages. The basic way how to run a command as root, we can use the sudo command: sudo <command> A password of the current user is required to run the command. Also, the user has have the right to run sudo . The password is usually required only once per some time period (e.g., 15 minutes). For some operations (e.g., browsing folders requiring root access), we have to run multiple commands as root. In this case, we have to switch the shell user to root. Resources: SO Changing shell user \u00b6 To change the shell user, we can use the su command: su <username> If we omit the username, the root user is used. Note that the password of the target user is required . To change the shell user without the password of the target user, we can use the sudo command: sudo su <username> This way, the password of the current user is required. The su command can be used the same way as the sudo command, i.e., we can pass the command: su <username> -c \"<command>\" To exit the shell user, we can use the exit command. Changing shell user in a script \u00b6 In a script, we cannot change the shell user for the remaining commands in the script. If we do that, the script will execute a new shell, and the remaining commands will be executed in the old shell. To execute commands as a different user, we have several options. Every option pass the commands as a parameter to the change user command (e.g., su -c \"<command>\" ). The options are: for each command, create a new change user command create a single change user command and pass all the commands as a single parameter (e.g., with the heredoc syntax) move commands to a separate script and execute the script as a different user Managing packages \u00b6 Package management heavily depends on the distribution. Therefore, check the manual for the specific distribution: Ubuntu package management String Processing \u00b6 String filtering with grep \u00b6 The grep command is used to filter lines containing a pattern. The syntax is: grep <pattern> <file> The pattern can be a simple string, or a regex. The most used options are: -v : invert the match, i.e., print only lines not matching the pattern -e : use multiple patterns (e.g., -e <pattern 1> -e <pattern 2> ) -i : ignore case Word count with wc \u00b6 The wc command counts words, lines, and characters. What is counted is determined by the parameters: -w : words -l : lines -c : characters String mofification with sed \u00b6 Documentation sed (stream editor) is a command for string modification. It is mostly used for search and replace in string. The syntax is folowing: The syntax is: sed <options> <script> <input file> #or <input stream> | sed <options> <script> here, the parameter characteristic to sed is <script> , which a) defines the mode of operation and b) configure the operation. In the sections below, each mode of operation is described separately. Regex support in sed \u00b6 Regex can be used in sed, but the support is limited. The following is not supported: \\s Note that the <script> is processed by bash before it is passed to sed. A proper quoting is required to avoid the shell interpreting or deleting characters. Example: echo $line | sed 's/\\r//g' # removes carriage returns from the line echo $line | sed s/\\r//g # wrong, the backslash is interpreted by the shell and removed. As a result, the script is interpreted as `s/r//g` and all \"r\" characters are removed. Search and replace \u00b6 The <script> for substitution is: s/<search>/<replace>/[<occurance>] Example: s/$/,3/ This replace the end of the line with string \",3\" . Note that there is a slash at the end, despite we use the default option for occurance. Delete lines containing string \u00b6 /<pattern>/d cut \u00b6 Cut is a useful command for data with delimiters. Usage: cut -f <columns> Where columns are splited by comma, e.g., 2,4,7 . If we need to specify delimiters, we use the -d parameter: cut -d, -f 1,5 AWK \u00b6 AWK is a powerful tool for text processing. It is a programming language, so it can be used for more complex tasks than sed or cut . The basic syntax is: awk '<pattern> {<action>}' Where <pattern> is a regex and <action> is a command. The <action> is executed only if the line matches the <pattern> . Pattern \u00b6 In the awk , / is used as a delimiter of the regex pattern. Action \u00b6 The <action> can be a sequence of commands, separated by ; . We can use column values by using special column variables: $0 : the whole line $1 : the first column ... Trim string \u00b6 <command with string output> | xargs Processes \u00b6 for checking all processes , we can use htop to get a path to executable of a process by PID, we can use pwdx <PID> for checking a specific process, we can use ps to kill a process, we can use kill to kill a process by name, we can use pkill to get information about a process selected by name, we can use pgrep pkill \u00b6 The pkill command kills a process by name. The syntax is: pkill <process name> important parameters: -f : match the whole command line, not only the process name -9 : force kill Process Info \u00b6 Users \u00b6 The users are listed in /etc/passwd . The file contains one line per user, each line has the following format: <username>:<password>:<user ID>:<group ID>:<GECOS>:<home folder>:<shell> The password is typically stored in /etc/shadow and is represented by x . GECOS is some kind of a comment storing arbitrary information about the user. Adding a user \u00b6 To add a user, we can use either the useradd binary directly, or the adduser wrapper script. Here we describe the adduser script. The basic syntax is adduser <username> . Important parameters: --gecos \"<GECOS>\" : supply the content of the GECOS field. If skipped, the command will ask for the GECOS content interactively. --shell <shell> : The shell to be used by the user. If skipped, the default shell is used. This can be used to create a user without a shell by setting the shell to /usr/sbin/nologin . Note that the adduser needs to be run as root. Otherwise it will fail with bash: adduser: command not found . User Groups \u00b6 An important aspect of user management in Linux is the user groups. For example, by belonging to the sudo group, the user can execute commands with sudo . The groups are listed in /etc/group . The file contains one line per group, each line has the following format: <group name>:<password>:<group ID>:<user list> To see the groups of a user , we can use the groups command (no arguments needed). To manipulate groups and users , we need a root access. To add a user to a group, we can use the usermod command: usermod -a -G <group name> <username> To remove a user from a group , we can use the same command: usermod -G <group list> <username> where <group list> is a comma separated list of groups the user should belong to. File ownership \u00b6 Each file has a pair of owners: the user owner and the group owner. These ownerships are important, as file permissions in Linux are usually set for a triplet of: owner : the user owner group : the group owner other : all other users To change the owner of a file, we can use the chown command. The syntax is: chown <user>:<group> <file> We can skip the :<group> part, in which case the group is not changed. The chown command can also be used to change the owner of a directory. In this case, the -R parameter is used to change the owner recursively. Disable access to shell for a user \u00b6 To disable access to shell for a user, we have to configure his/her shell to /usr/sbin/nologin or similar. For new users, we can use the --shell parameter of the adduser command. For existing users, we can use the usermod command: usermod --shell /usr/sbin/nologin <username> Note that for some ssh client implementations, it is necessary to connect to a shell by default, otherwise, the connection is terminated immediately after login. In this case, the user has to connect to the server with the -N parameter, which tells the client not to execute any command after login. Services and systemd \u00b6 The systemd is a system and service manager for Linux. It is used to manage services, devices, and other aspects of the system. The main command to manage services is systemctl . The general syntax is: sudo systemctl <action> <service name> The most used actions are: start : start the service stop : stop the service restart : restart the service status : get the status of the service reload : reload the configuration of the service Get the status of a service \u00b6 To get the status of a service, we can use the status action: sudo systemctl status <service name> The statuses can be: active (running) : the service is running active (exited) : the service has finished active (waiting) : the service is not running, but it is waiting for some event TODO: add more statuses Listing services \u00b6 To list all services, we can use one of the following commands: list-units to list all units ever run on the server or list-units-files to list all units, including the ones that have never been run SSH \u00b6 Inspecting commands executed over SSH \u00b6 There are various possible ways to inspect the commands executed over SSH. Commands from interactive console should appear in history and we should see them when running history command, just like in local console. Much more complicated is to see the commands executed over SSH in a non-interactive way, e.g., when running a command over ssh ( ssh user@host <command> ). One way to do this is to use a command wrapper script for a specific ssh key: create the wrapper script: ```bash #!/usr/bin/env bash log=\"$HOME/ssh_commands.log\" Log who/when/where and the command (or \" \" if none) \u00b6 printf '%s user=%s from=%s cmd=%q\\n' \\ \"$(date '+%F %T')\" \"$USER\" \"${SSH_CONNECTION%% *}\" \\ \"${SSH_ORIGINAL_COMMAND:- }\" >> \"$log\" Run the original command or a login shell \u00b6 if [ -n \"$SSH_ORIGINAL_COMMAND\" ]; then exec bash -lc \"$SSH_ORIGINAL_COMMAND\" else exec bash -l fi 1. make the script executable: `chmod +x <path to the script>` 1. create the log file: bash touch chmod 600 1. add the script to the `authorized_keys` file: bash echo \"command= ssh-rsa AAAA...\" >> ~/.ssh/authorized_keys ``` Frequently used software \u00b6 Installing Java \u00b6 Oracle JDK \u00b6 Go to the download page, the link to the dowload page for current version of java is on the main JDK page . Click on the debian package, accept the license, and download it. If installing on system without GUI, copy now (after accepting the license) the target link and dowload the debian package with wget : wget --header \"Cookie: oraclelicense=accept-securebackup-cookie\" <COPIED LINK> . More info on SO . Install the package with sudo apt-get install <PATH TO DOWNLOADED .deb> if there is a problem with the isntallation, check the file integritiy with: sha256 <PATH TO DOWNLOADED .deb> . It should match with the checksums refered on the download page. If not cheksums do not match, go back to download step. In case there is another version of Java alreadz install, we need to overwrite it using the update-alternatives command: sudo update-alternatives --install /usr/bin/java java <PATH TO JAVA> <PRIORITY> . Example: sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk-16/bin/java 2 To check the priorities, call update-alternatives --query java . The newly installed JDK should have the highest priority. Python \u00b6 Python has to be executed with python3 by default, instead of python . GCC \u00b6 GCC is typically installed by default and itts minor versions are updated with the system updates. However, if we need a major version update, we have to install it manually as a new package: sudo apt install gcc-<version> This way, the new version is installed alongside the old version. To switch to the new version, we have to use the update-alternatives command: sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-<version> <priority> The <priority> is a number that determines the priority of the version. The version with the highest priority is used. To check the priorities, we can use the update-alternatives --query gcc command. Note that these steps only updates the C compiler . To affect the C++ compiler as well, we have to repeat the steps with the g++ command: sudo apt install g++-<version> sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-<version> <priority> Other usefull commands \u00b6 Selecting lines from file \u00b6 The head and tail commands are for that, printing the top and bottom 10 lines, respectively. Skip the header \u00b6 tail -n +2 Print lines from to \u00b6 tail -n +<from line> | head -n <number of lines> Progress bar \u00b6 the progress bar can be printed using the pv command. pv <file> | <other comands> # or <other comands> | pv | <other comands> Free disk space \u00b6 df -h piping parameters using xargs \u00b6 The xargs command transfers the output of one command into call of another command with the output of the first command as parameters of the second command. This is usefull when the second command does not accept the output of the first command as input, but accepts the output as parameters. Example: ls | xargs rm # remove all files in the current directory Upgrade \u00b6 For a system upgrade, refer to the manual for the specific distribution: - Ubuntu upgrade vim \u00b6 Vim is a console text editor. It is a modal editor, i.e., it has different modes for different operations. The most important modes are: normal mode : for navigation and file manipulation insert mode : for text editing visual mode : for text selection Normal Mode \u00b6 In normal mode, we can: navigate the file using arrow keys or hjkl (left, down, up, right) enter global commands using : (e.g., :q for quit) edit file content using special commands (e.g., dd for delete line) Global Commands \u00b6 :q : quit :w : save :wq : save and quit :q! : quit without saving File Editing Commands \u00b6 dd : delete line Insert Mode \u00b6 Insert mode is the normal text mode we know from other editors. To enter insert mode, press i . To exit insert mode, press esc . Visual Mode \u00b6 Visual mode is used for text selection. To enter visual mode, press v . To exit visual mode, press esc . Copy and paste \u00b6 Vim has its own clipboard for copy-pasting (yank, ...). However, this cannot be used to copy text outside of vim, nor to paste text from outside vim. To copy text to the system clipboard, we can: select the text using mouse or keyboard press enter to copy the text to the clipboard To paste text from the system clipboard, we press Ctrl + Shift + v .","title":"Linux Manual"},{"location":"Linux/Linux%20Manual/#usefull-comands","text":"","title":"Usefull Comands"},{"location":"Linux/Linux%20Manual/#versions","text":"","title":"Versions"},{"location":"Linux/Linux%20Manual/#check-os-distribution-and-version","text":"cat /etc/*-release","title":"Check OS Distribution and Version"},{"location":"Linux/Linux%20Manual/#checking-glibc-version","text":"ldd --version","title":"Checking glibc version"},{"location":"Linux/Linux%20Manual/#printing-used-ports","text":"sudo lsof -i -P","title":"Printing used ports"},{"location":"Linux/Linux%20Manual/#exit-codes","text":"Exit code of the last command: $? Common exit codes for C++ programs","title":"Exit codes"},{"location":"Linux/Linux%20Manual/#show-path-to-executable","text":"Use the which command: which <executable> . Other option is to use the command command: command -v <executable> .","title":"Show path to executable"},{"location":"Linux/Linux%20Manual/#unpack-file","text":"Foe unpacking, you can use the tar -f <file> command. The most used options are: x : extract","title":"Unpack file"},{"location":"Linux/Linux%20Manual/#environment-variables","text":"The environment variables are introduced with the export command: export <variable>=<value> without export, the variable is just a local shell variable: <variable>=<value> # local variable We will demonstrate the work with environment variables on the PATH example. If you have a program in a custom location, adding it to $PATH permanently and be able to run it under all circumstances is not an easy task on linux. Standard procedure is to add a system variable: Create a dedicated .sh file in /etc/profile.d. for your configuration (config for each app should be stored in a separate file). the file should contain: export PATH=$PATH:YOURPATH exit nano and save the file: ctrl+x and y logout and login again to load the newly added varibales for WSL close the console and reopen it, it is not necessary to restart the WSL ( click here for detailed description )","title":"Environment Variables"},{"location":"Linux/Linux%20Manual/#enable-variable-with-sudo","text":"To enable the variable even if you use sudo , you need to edit sudo config using sudo visudo and: exclude PATH from variable being reset when running sudo : Defaults env_keep += \"PATH\" disable the safe path mechanism for sudo , i.e., comment the line: Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin logout and login again to load the new added varibales","title":"Enable Variable with sudo"},{"location":"Linux/Linux%20Manual/#enable-the-variable-outside-bash","text":"If you need the variable outside bash, the above-mentioned approach won't work. Currently, I do not know about any general solution for variables. The solution below, unfortunately, work only for PAM shells (see this SO answer why). Add the variable to /etc/environment . Note that it is not a script file, so you can use only simple variable assignments.","title":"Enable the Variable Outside Bash"},{"location":"Linux/Linux%20Manual/#enable-variable-on-a-device-without-root-access","text":"Without root access, we can only edit the user config files. put the necessary config into: ~/.bash_profile if it already exists or to ~/.profile Note that the .profile file is ignored when the .bash_profile file exists.","title":"Enable Variable on a Device Without Root Access"},{"location":"Linux/Linux%20Manual/#remove-windows-path-from-wsl","text":"By default, the PATH environment variable from Windows is inluded in the PATH variable in Ubuntu running in WSL. This can lead to conflicts, when the same executables are used in both systems (e.g., vcpkg, cmake). To turn of the Windows PATH inclusion: open /etc/wsl.conf add the fllowing code: [interop] appendWindowsPath = false restart WSL","title":"Remove Windows $PATH from WSL"},{"location":"Linux/Linux%20Manual/#file-system","text":"Unlike Windows, Linux typically doe not split the filesystem into partitions that much. Moreover, each partition path can start anywhere, instead of <drive letter>: like in Windows. Typically, the main partition is mounted at / .","title":"File System"},{"location":"Linux/Linux%20Manual/#standard-folder-structure","text":"The standard folder structure is: /bin core binaries, usually a limited set distributed with the OS /home/<user name> home directory for the user <user name> /opt for software packages distributed in the Windows style, i.e., as a single installation directory. /sbin same as /bin , but for system binaries, i.e., binaries for system maintenance /usr/bin and /usr/sbin For binares installed by package managers. Binaries from other sources should not be installed here, as there is a risk of overwritting them by the package manager. /usr/local/bin and /usr/local/sbin : For binaries installed by the user, i.e., not by the package manager.","title":"Standard folder structure"},{"location":"Linux/Linux%20Manual/#list-files","text":"To list files, use ls <path> . By default, it lists the files in the current directory. The most used options are: -l : list in long format -a : list all files, including hidden ones -1 : list one file per line (instead of multiple columns) -d : list directories themselves, not their contents","title":"List files"},{"location":"Linux/Linux%20Manual/#list-the-full-paths","text":"There is no option of showing the full path. The trick is to use the -d option, as directories are listed in the long format by default. So for example, to list full paths of all subdirectories of <dir> , you can use: ls -d <dir>/*","title":"List the full paths"},{"location":"Linux/Linux%20Manual/#copy-files","text":"The cp command is used to copy files: cp <source> <destination> . The most used options are: -r , -R : copy recursively -v : verbose -f : force -p : preserve permissions and timestamps -a : same as -p -R plus some other options","title":"Copy files"},{"location":"Linux/Linux%20Manual/#rsync","text":"For more sophisticated copying, use rsync ( documentation ): rsync <source> <destination> . Unlike the cp command, the <source> have a different meaning depending on the trailing slash: if the <source> ends with a slash, it behaves like cp : bash rsync -r /path/to/source/ /path/to/destination # the content of the source directory is copied to the /path/to/destination directory if the <source> does not end with a slash, it behaves differently: bash rsync -r /path/to/source /path/to/destination # the content of the source directory is copied to the /path/to/destination/source directory The most used options are: -r , --recursive : recurse into directories. Only files are copied without this option. -h : human readable","title":"rsync"},{"location":"Linux/Linux%20Manual/#preserving-metadata","text":"-l , --links : preserve links -p , --perms : preserve permissions -t , --times : preserve modification times -U , --atimes : preserve access times -N , --crtimes : preserve creation times -g , --group : preserve group -o , --owner : preserve owner --specials : preserve special files --devices : preserve device files -D : equivalent to --devices --specials -a : archive mode, equivalent to -rlptgoD","title":"Preserving metadata"},{"location":"Linux/Linux%20Manual/#formatting-the-output","text":"--progress : show progress bar for each file transfer --info=<FLAG>[<FLAG_VALUE>] : detailed configuration of the rsync output. Here, <FLAG> is the specific output parameter we want to set: BACKUP Mention files backed up COPY : Mention files copied locally on the receiving side DEL : Mention deletions on the receiving side FLIST : Mention file-list receiving/sending (levels 1-2) MISC : Mention miscellaneous information (levels 1-2) MOUNT : Mention mounts that were found or skipped NAME : Mention 1) updated file/dir names, 2) unchanged names PROGRESS : Mention 1) per-file progress or 2) total transfer progress REMOVE : Mention files removed on the sending side SKIP : Mention files that are skipped due to options used STATS : Mention statistics at end of run (levels 1-3) SYMSAFE : Mention symlinks that are unsafe ALL : Set all --info options (e.g. all4) <FLAG_VALUE> is the value of the parameter. 0 : disable the output 1 : default value < Number > 1 > : more detailed output. The maximum level depends on the output parameter we want to set. Example: --info=progress2 will show the progress total progress, instead of the progress bar for each file transfer.","title":"Formatting the output"},{"location":"Linux/Linux%20Manual/#overwriting-files","text":"--ignore-existing : ignore files that already exist on the destination. --update : ignore newer files on the destination. --checksum : use checksum to compare files, not the file size and modification time.","title":"Overwriting files"},{"location":"Linux/Linux%20Manual/#remove-file","text":"The rm command is used to remove files. The most used options are: -r , -R : remove recursively To remove all files in a directory, you can use","title":"Remove file"},{"location":"Linux/Linux%20Manual/#access-rights","text":"The Linux access rights use the same system for files and folders. The access rights are divided into three groups, from left to right: owner group other Each group has three possible access rights: r : read w : write x : execute Important aspects: to access a directory, the user has to have the x right on the directory. to access a file, the user has to have the x right on all folders in the path to the file.","title":"Access rights"},{"location":"Linux/Linux%20Manual/#compute-directory-size","text":"To compute the size of a directory, use the du command: du <path> . The most used options are: -h : human readable -s : summarize","title":"Compute directory size"},{"location":"Linux/Linux%20Manual/#find-files","text":"To find files, use the find command: find <where> <options> . If the <where> is not specified, the current directory is used. If the <options> are not specified, all files matching the <where> are listed. The most used options are: -name <name pattern> : find by name. -path <path pattern> : find by path. -regex <regex pattern> : find by path specified as regex. Note that by default, the regex type is emacs-style. Therefore, we have to set the type to posix-extended to work reasonably with regexes. so in the end, we have to use: bash find -regextype posix-extended -regex '.*<regex pattern>.*' note the order of the options. Specifying the -regextype after the -regex option does not work. -printf <format> : set the output format. The most important format specifiers are: %f : file name %p : file path %TY : year %Tm : month %Td : day %TH : hour %TM : minute %TS : second modifiers can be used to format the output: %.2<format specifier> : format the output to 2 digits","title":"Find files"},{"location":"Linux/Linux%20Manual/#list-disks-and-partitions","text":"To list disks and partitions, use the lsblk command. If we are not satisfied with the output, we can configure it with the -o option: lsblk -o <list of columns, separated by commas> Most important columns are: NAME : name of the disk or partition SIZE : size of the disk or partition TYPE : type of the disk or partition FSTYPE : file system type FSAVAIL : available file system space","title":"List disks and partitions"},{"location":"Linux/Linux%20Manual/#network","text":"","title":"Network"},{"location":"Linux/Linux%20Manual/#netstat","text":"The netstat command is the basic command to monitor the networ. It displays the TCP connections. It is available both on Linux and on Windows, although the interface differs. Important parameters: -n : do not translate IP and ports into human readable names -a : show all connections. Without this, some connections can be skipped.","title":"netstat"},{"location":"Linux/Linux%20Manual/#bash","text":"See Bash Manual for detailed information about Bash scripting and commands.","title":"Bash"},{"location":"Linux/Linux%20Manual/#root-access","text":"Some commands require root privilages. The basic way how to run a command as root, we can use the sudo command: sudo <command> A password of the current user is required to run the command. Also, the user has have the right to run sudo . The password is usually required only once per some time period (e.g., 15 minutes). For some operations (e.g., browsing folders requiring root access), we have to run multiple commands as root. In this case, we have to switch the shell user to root. Resources: SO","title":"Root Access"},{"location":"Linux/Linux%20Manual/#changing-shell-user","text":"To change the shell user, we can use the su command: su <username> If we omit the username, the root user is used. Note that the password of the target user is required . To change the shell user without the password of the target user, we can use the sudo command: sudo su <username> This way, the password of the current user is required. The su command can be used the same way as the sudo command, i.e., we can pass the command: su <username> -c \"<command>\" To exit the shell user, we can use the exit command.","title":"Changing shell user"},{"location":"Linux/Linux%20Manual/#changing-shell-user-in-a-script","text":"In a script, we cannot change the shell user for the remaining commands in the script. If we do that, the script will execute a new shell, and the remaining commands will be executed in the old shell. To execute commands as a different user, we have several options. Every option pass the commands as a parameter to the change user command (e.g., su -c \"<command>\" ). The options are: for each command, create a new change user command create a single change user command and pass all the commands as a single parameter (e.g., with the heredoc syntax) move commands to a separate script and execute the script as a different user","title":"Changing shell user in a script"},{"location":"Linux/Linux%20Manual/#managing-packages","text":"Package management heavily depends on the distribution. Therefore, check the manual for the specific distribution: Ubuntu package management","title":"Managing packages"},{"location":"Linux/Linux%20Manual/#string-processing","text":"","title":"String Processing"},{"location":"Linux/Linux%20Manual/#string-filtering-with-grep","text":"The grep command is used to filter lines containing a pattern. The syntax is: grep <pattern> <file> The pattern can be a simple string, or a regex. The most used options are: -v : invert the match, i.e., print only lines not matching the pattern -e : use multiple patterns (e.g., -e <pattern 1> -e <pattern 2> ) -i : ignore case","title":"String filtering with grep"},{"location":"Linux/Linux%20Manual/#word-count-with-wc","text":"The wc command counts words, lines, and characters. What is counted is determined by the parameters: -w : words -l : lines -c : characters","title":"Word count with wc"},{"location":"Linux/Linux%20Manual/#string-mofification-with-sed","text":"Documentation sed (stream editor) is a command for string modification. It is mostly used for search and replace in string. The syntax is folowing: The syntax is: sed <options> <script> <input file> #or <input stream> | sed <options> <script> here, the parameter characteristic to sed is <script> , which a) defines the mode of operation and b) configure the operation. In the sections below, each mode of operation is described separately.","title":"String mofification with sed"},{"location":"Linux/Linux%20Manual/#regex-support-in-sed","text":"Regex can be used in sed, but the support is limited. The following is not supported: \\s Note that the <script> is processed by bash before it is passed to sed. A proper quoting is required to avoid the shell interpreting or deleting characters. Example: echo $line | sed 's/\\r//g' # removes carriage returns from the line echo $line | sed s/\\r//g # wrong, the backslash is interpreted by the shell and removed. As a result, the script is interpreted as `s/r//g` and all \"r\" characters are removed.","title":"Regex support in sed"},{"location":"Linux/Linux%20Manual/#search-and-replace","text":"The <script> for substitution is: s/<search>/<replace>/[<occurance>] Example: s/$/,3/ This replace the end of the line with string \",3\" . Note that there is a slash at the end, despite we use the default option for occurance.","title":"Search and replace"},{"location":"Linux/Linux%20Manual/#delete-lines-containing-string","text":"/<pattern>/d","title":"Delete lines containing string"},{"location":"Linux/Linux%20Manual/#cut","text":"Cut is a useful command for data with delimiters. Usage: cut -f <columns> Where columns are splited by comma, e.g., 2,4,7 . If we need to specify delimiters, we use the -d parameter: cut -d, -f 1,5","title":"cut"},{"location":"Linux/Linux%20Manual/#awk","text":"AWK is a powerful tool for text processing. It is a programming language, so it can be used for more complex tasks than sed or cut . The basic syntax is: awk '<pattern> {<action>}' Where <pattern> is a regex and <action> is a command. The <action> is executed only if the line matches the <pattern> .","title":"AWK"},{"location":"Linux/Linux%20Manual/#pattern","text":"In the awk , / is used as a delimiter of the regex pattern.","title":"Pattern"},{"location":"Linux/Linux%20Manual/#action","text":"The <action> can be a sequence of commands, separated by ; . We can use column values by using special column variables: $0 : the whole line $1 : the first column ...","title":"Action"},{"location":"Linux/Linux%20Manual/#trim-string","text":"<command with string output> | xargs","title":"Trim string"},{"location":"Linux/Linux%20Manual/#processes","text":"for checking all processes , we can use htop to get a path to executable of a process by PID, we can use pwdx <PID> for checking a specific process, we can use ps to kill a process, we can use kill to kill a process by name, we can use pkill to get information about a process selected by name, we can use pgrep","title":"Processes"},{"location":"Linux/Linux%20Manual/#pkill","text":"The pkill command kills a process by name. The syntax is: pkill <process name> important parameters: -f : match the whole command line, not only the process name -9 : force kill","title":"pkill"},{"location":"Linux/Linux%20Manual/#process-info","text":"","title":"Process Info"},{"location":"Linux/Linux%20Manual/#users","text":"The users are listed in /etc/passwd . The file contains one line per user, each line has the following format: <username>:<password>:<user ID>:<group ID>:<GECOS>:<home folder>:<shell> The password is typically stored in /etc/shadow and is represented by x . GECOS is some kind of a comment storing arbitrary information about the user.","title":"Users"},{"location":"Linux/Linux%20Manual/#adding-a-user","text":"To add a user, we can use either the useradd binary directly, or the adduser wrapper script. Here we describe the adduser script. The basic syntax is adduser <username> . Important parameters: --gecos \"<GECOS>\" : supply the content of the GECOS field. If skipped, the command will ask for the GECOS content interactively. --shell <shell> : The shell to be used by the user. If skipped, the default shell is used. This can be used to create a user without a shell by setting the shell to /usr/sbin/nologin . Note that the adduser needs to be run as root. Otherwise it will fail with bash: adduser: command not found .","title":"Adding a user"},{"location":"Linux/Linux%20Manual/#user-groups","text":"An important aspect of user management in Linux is the user groups. For example, by belonging to the sudo group, the user can execute commands with sudo . The groups are listed in /etc/group . The file contains one line per group, each line has the following format: <group name>:<password>:<group ID>:<user list> To see the groups of a user , we can use the groups command (no arguments needed). To manipulate groups and users , we need a root access. To add a user to a group, we can use the usermod command: usermod -a -G <group name> <username> To remove a user from a group , we can use the same command: usermod -G <group list> <username> where <group list> is a comma separated list of groups the user should belong to.","title":"User Groups"},{"location":"Linux/Linux%20Manual/#file-ownership","text":"Each file has a pair of owners: the user owner and the group owner. These ownerships are important, as file permissions in Linux are usually set for a triplet of: owner : the user owner group : the group owner other : all other users To change the owner of a file, we can use the chown command. The syntax is: chown <user>:<group> <file> We can skip the :<group> part, in which case the group is not changed. The chown command can also be used to change the owner of a directory. In this case, the -R parameter is used to change the owner recursively.","title":"File ownership"},{"location":"Linux/Linux%20Manual/#disable-access-to-shell-for-a-user","text":"To disable access to shell for a user, we have to configure his/her shell to /usr/sbin/nologin or similar. For new users, we can use the --shell parameter of the adduser command. For existing users, we can use the usermod command: usermod --shell /usr/sbin/nologin <username> Note that for some ssh client implementations, it is necessary to connect to a shell by default, otherwise, the connection is terminated immediately after login. In this case, the user has to connect to the server with the -N parameter, which tells the client not to execute any command after login.","title":"Disable access to shell for a user"},{"location":"Linux/Linux%20Manual/#services-and-systemd","text":"The systemd is a system and service manager for Linux. It is used to manage services, devices, and other aspects of the system. The main command to manage services is systemctl . The general syntax is: sudo systemctl <action> <service name> The most used actions are: start : start the service stop : stop the service restart : restart the service status : get the status of the service reload : reload the configuration of the service","title":"Services and systemd"},{"location":"Linux/Linux%20Manual/#get-the-status-of-a-service","text":"To get the status of a service, we can use the status action: sudo systemctl status <service name> The statuses can be: active (running) : the service is running active (exited) : the service has finished active (waiting) : the service is not running, but it is waiting for some event TODO: add more statuses","title":"Get the status of a service"},{"location":"Linux/Linux%20Manual/#listing-services","text":"To list all services, we can use one of the following commands: list-units to list all units ever run on the server or list-units-files to list all units, including the ones that have never been run","title":"Listing services"},{"location":"Linux/Linux%20Manual/#ssh","text":"","title":"SSH"},{"location":"Linux/Linux%20Manual/#inspecting-commands-executed-over-ssh","text":"There are various possible ways to inspect the commands executed over SSH. Commands from interactive console should appear in history and we should see them when running history command, just like in local console. Much more complicated is to see the commands executed over SSH in a non-interactive way, e.g., when running a command over ssh ( ssh user@host <command> ). One way to do this is to use a command wrapper script for a specific ssh key: create the wrapper script: ```bash #!/usr/bin/env bash log=\"$HOME/ssh_commands.log\"","title":"Inspecting commands executed over SSH"},{"location":"Linux/Linux%20Manual/#log-whowhenwhere-and-the-command-or-if-none","text":"printf '%s user=%s from=%s cmd=%q\\n' \\ \"$(date '+%F %T')\" \"$USER\" \"${SSH_CONNECTION%% *}\" \\ \"${SSH_ORIGINAL_COMMAND:- }\" >> \"$log\"","title":"Log who/when/where and the command (or \"\" if none)"},{"location":"Linux/Linux%20Manual/#run-the-original-command-or-a-login-shell","text":"if [ -n \"$SSH_ORIGINAL_COMMAND\" ]; then exec bash -lc \"$SSH_ORIGINAL_COMMAND\" else exec bash -l fi 1. make the script executable: `chmod +x <path to the script>` 1. create the log file: bash touch chmod 600 1. add the script to the `authorized_keys` file: bash echo \"command= ssh-rsa AAAA...\" >> ~/.ssh/authorized_keys ```","title":"Run the original command or a login shell"},{"location":"Linux/Linux%20Manual/#frequently-used-software","text":"","title":"Frequently used software"},{"location":"Linux/Linux%20Manual/#installing-java","text":"","title":"Installing Java"},{"location":"Linux/Linux%20Manual/#oracle-jdk","text":"Go to the download page, the link to the dowload page for current version of java is on the main JDK page . Click on the debian package, accept the license, and download it. If installing on system without GUI, copy now (after accepting the license) the target link and dowload the debian package with wget : wget --header \"Cookie: oraclelicense=accept-securebackup-cookie\" <COPIED LINK> . More info on SO . Install the package with sudo apt-get install <PATH TO DOWNLOADED .deb> if there is a problem with the isntallation, check the file integritiy with: sha256 <PATH TO DOWNLOADED .deb> . It should match with the checksums refered on the download page. If not cheksums do not match, go back to download step. In case there is another version of Java alreadz install, we need to overwrite it using the update-alternatives command: sudo update-alternatives --install /usr/bin/java java <PATH TO JAVA> <PRIORITY> . Example: sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk-16/bin/java 2 To check the priorities, call update-alternatives --query java . The newly installed JDK should have the highest priority.","title":"Oracle JDK"},{"location":"Linux/Linux%20Manual/#python","text":"Python has to be executed with python3 by default, instead of python .","title":"Python"},{"location":"Linux/Linux%20Manual/#gcc","text":"GCC is typically installed by default and itts minor versions are updated with the system updates. However, if we need a major version update, we have to install it manually as a new package: sudo apt install gcc-<version> This way, the new version is installed alongside the old version. To switch to the new version, we have to use the update-alternatives command: sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-<version> <priority> The <priority> is a number that determines the priority of the version. The version with the highest priority is used. To check the priorities, we can use the update-alternatives --query gcc command. Note that these steps only updates the C compiler . To affect the C++ compiler as well, we have to repeat the steps with the g++ command: sudo apt install g++-<version> sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-<version> <priority>","title":"GCC"},{"location":"Linux/Linux%20Manual/#other-usefull-commands","text":"","title":"Other usefull commands"},{"location":"Linux/Linux%20Manual/#selecting-lines-from-file","text":"The head and tail commands are for that, printing the top and bottom 10 lines, respectively.","title":"Selecting lines from file"},{"location":"Linux/Linux%20Manual/#skip-the-header","text":"tail -n +2","title":"Skip the header"},{"location":"Linux/Linux%20Manual/#print-lines-from-to","text":"tail -n +<from line> | head -n <number of lines>","title":"Print lines from to"},{"location":"Linux/Linux%20Manual/#progress-bar","text":"the progress bar can be printed using the pv command. pv <file> | <other comands> # or <other comands> | pv | <other comands>","title":"Progress bar"},{"location":"Linux/Linux%20Manual/#free-disk-space","text":"df -h","title":"Free disk space"},{"location":"Linux/Linux%20Manual/#piping-parameters-using-xargs","text":"The xargs command transfers the output of one command into call of another command with the output of the first command as parameters of the second command. This is usefull when the second command does not accept the output of the first command as input, but accepts the output as parameters. Example: ls | xargs rm # remove all files in the current directory","title":"piping parameters using xargs"},{"location":"Linux/Linux%20Manual/#upgrade","text":"For a system upgrade, refer to the manual for the specific distribution: - Ubuntu upgrade","title":"Upgrade"},{"location":"Linux/Linux%20Manual/#vim","text":"Vim is a console text editor. It is a modal editor, i.e., it has different modes for different operations. The most important modes are: normal mode : for navigation and file manipulation insert mode : for text editing visual mode : for text selection","title":"vim"},{"location":"Linux/Linux%20Manual/#normal-mode","text":"In normal mode, we can: navigate the file using arrow keys or hjkl (left, down, up, right) enter global commands using : (e.g., :q for quit) edit file content using special commands (e.g., dd for delete line)","title":"Normal Mode"},{"location":"Linux/Linux%20Manual/#global-commands","text":":q : quit :w : save :wq : save and quit :q! : quit without saving","title":"Global Commands"},{"location":"Linux/Linux%20Manual/#file-editing-commands","text":"dd : delete line","title":"File Editing Commands"},{"location":"Linux/Linux%20Manual/#insert-mode","text":"Insert mode is the normal text mode we know from other editors. To enter insert mode, press i . To exit insert mode, press esc .","title":"Insert Mode"},{"location":"Linux/Linux%20Manual/#visual-mode","text":"Visual mode is used for text selection. To enter visual mode, press v . To exit visual mode, press esc .","title":"Visual Mode"},{"location":"Linux/Linux%20Manual/#copy-and-paste","text":"Vim has its own clipboard for copy-pasting (yank, ...). However, this cannot be used to copy text outside of vim, nor to paste text from outside vim. To copy text to the system clipboard, we can: select the text using mouse or keyboard press enter to copy the text to the clipboard To paste text from the system clipboard, we press Ctrl + Shift + v .","title":"Copy and paste"},{"location":"Linux/Ubuntu/","text":"Usefull Comands \u00b6 Check Ubuntu Version \u00b6 lsb_release -a Managing packages \u00b6 To update the list of possible updates: sudo apt update To perform the update : sudo apt upgrade To list installed packages: apt list --installed We can filter the list using the grep command. To find the install location of a package: dpkg -L <package> Unfortunately, it is not possible to easily search for the user who installed the package. To search for a package: apt-cache search <package> We can limit the search to check only names of the packages using the --names-only parameter. To remove a package: sudo apt remove <package> Installing non-stable package versions \u00b6 On Linux, the stable package versions are usually outdated, sometimes years behind the current version. To install the newer version, we have usually a few options: upgrade the system : if we use an old version of the system, we can check whether the newer version is available that includes the newer package version. For more, see the Upgrade section. install from source : We can manually build the package from the source and install it. See the C++ Workflow for more. install package from an alternative repository : We can add an alternative repository to the system and install the package from there. The first two options are covered in different part of this manual. Here, we focus on the third option. To use an alternative repository, we have to a) add the repository to the system, which is a one time task, and b) install the specific package from the repository. To add a repository , we have to: add the repository to the /etc/apt/sources.list (or to a separate file in the /etc/apt/sources.list.d/ directory). Each repository should has the line that should be added to the file on its website. sudo apt update to update the list of available packages To install a package from the repository : sudo apt install -t <repository> <package> Some useful repositories \u00b6 debian backports : the repository with the newer versions of the packages for the stable Debian version Changing default package repositories \u00b6 If the downolad speed is not satisfactory, we can change the repositories. To find the fastest repository from the list of nearby repositories, run: curl -s http://mirrors.ubuntu.com/mirrors.txt | xargs -n1 -I {} sh -c 'echo `curl -r 0-10240000 -s -w %{speed_download} -o /dev/null {}/ls-lR.gz` {}' | sort -g -r The number in the leftmost column indicates the bandwidth in bytes (larger number is better). To change the repositories to the best mirror, we need to replace the mirror in etc/apt/source.list . We can do it manually, however, to prevent the mistakes, it is better to use a dedicated python script: apt-mirror-updater . Steps: install the python script: sudo pip install apt-mirror-updater backup the old file: sudo cp sources.list sources.list.bak change the mirror with the script: apt-mirror-updater -c <mirror URL> Note that the apt-mirror-updater script can also measure the bandwidth, however, the result does not seem to be reliable. Possible issues \u00b6 The repository '<repo>' no longer has a Release file \u00b6 This can happen when the repository is outdated, which can happen quickly if we use non-stable (non-LTS) versions of Ubuntu. The solution is to either: change the repository to a newer one manually or change the url of all repositories to http://old-releases.ubuntu.com/ubuntu/ and then upgrade the system to the newer version: bash sudo sed -i -e 's/archive.ubuntu.com\\|security.ubuntu.com/old-releases.ubuntu.com/g' /etc/apt/sources.list Upgrade \u00b6 For a system upgrade, follow these steps: run the update of the current version . Then optionaly backup the WSL perform the upgrade: If a) you are on a LTS version, and b) there is a new LTS version available, run sudo do-release-upgrade . Otherwise, follow the steps in the next section. Ubuntu versions Manual upgrade \u00b6 For manual upgrade, follow these steps: perform steps 1 and 2 from the normal upgrade open the /etc/update-manager/release-upgrades file and set the Prompt parameter to normal or lts (depending on the desired version) backup the /etc/apt/sources.list file change the sources to the new version: e.g., run sudo sed -i 's/<current version name>/<new version name>/g' /etc/apt/sources.list run a normal upgrade: sudo apt update && sudo apt upgrade finalizing the upgrade: sudo apt dist-upgrade WSL backup \u00b6 check the WSL distro name: wsl -l -v shutdown WSL: wsl --shutdown backup the distro: wsl --export <disto name> <backup folder path>/<backup name>.tar","title":"Ubuntu"},{"location":"Linux/Ubuntu/#usefull-comands","text":"","title":"Usefull Comands"},{"location":"Linux/Ubuntu/#check-ubuntu-version","text":"lsb_release -a","title":"Check Ubuntu Version"},{"location":"Linux/Ubuntu/#managing-packages","text":"To update the list of possible updates: sudo apt update To perform the update : sudo apt upgrade To list installed packages: apt list --installed We can filter the list using the grep command. To find the install location of a package: dpkg -L <package> Unfortunately, it is not possible to easily search for the user who installed the package. To search for a package: apt-cache search <package> We can limit the search to check only names of the packages using the --names-only parameter. To remove a package: sudo apt remove <package>","title":"Managing packages"},{"location":"Linux/Ubuntu/#installing-non-stable-package-versions","text":"On Linux, the stable package versions are usually outdated, sometimes years behind the current version. To install the newer version, we have usually a few options: upgrade the system : if we use an old version of the system, we can check whether the newer version is available that includes the newer package version. For more, see the Upgrade section. install from source : We can manually build the package from the source and install it. See the C++ Workflow for more. install package from an alternative repository : We can add an alternative repository to the system and install the package from there. The first two options are covered in different part of this manual. Here, we focus on the third option. To use an alternative repository, we have to a) add the repository to the system, which is a one time task, and b) install the specific package from the repository. To add a repository , we have to: add the repository to the /etc/apt/sources.list (or to a separate file in the /etc/apt/sources.list.d/ directory). Each repository should has the line that should be added to the file on its website. sudo apt update to update the list of available packages To install a package from the repository : sudo apt install -t <repository> <package>","title":"Installing non-stable package versions"},{"location":"Linux/Ubuntu/#some-useful-repositories","text":"debian backports : the repository with the newer versions of the packages for the stable Debian version","title":"Some useful repositories"},{"location":"Linux/Ubuntu/#changing-default-package-repositories","text":"If the downolad speed is not satisfactory, we can change the repositories. To find the fastest repository from the list of nearby repositories, run: curl -s http://mirrors.ubuntu.com/mirrors.txt | xargs -n1 -I {} sh -c 'echo `curl -r 0-10240000 -s -w %{speed_download} -o /dev/null {}/ls-lR.gz` {}' | sort -g -r The number in the leftmost column indicates the bandwidth in bytes (larger number is better). To change the repositories to the best mirror, we need to replace the mirror in etc/apt/source.list . We can do it manually, however, to prevent the mistakes, it is better to use a dedicated python script: apt-mirror-updater . Steps: install the python script: sudo pip install apt-mirror-updater backup the old file: sudo cp sources.list sources.list.bak change the mirror with the script: apt-mirror-updater -c <mirror URL> Note that the apt-mirror-updater script can also measure the bandwidth, however, the result does not seem to be reliable.","title":"Changing default package repositories"},{"location":"Linux/Ubuntu/#possible-issues","text":"","title":"Possible issues"},{"location":"Linux/Ubuntu/#the-repository-repo-no-longer-has-a-release-file","text":"This can happen when the repository is outdated, which can happen quickly if we use non-stable (non-LTS) versions of Ubuntu. The solution is to either: change the repository to a newer one manually or change the url of all repositories to http://old-releases.ubuntu.com/ubuntu/ and then upgrade the system to the newer version: bash sudo sed -i -e 's/archive.ubuntu.com\\|security.ubuntu.com/old-releases.ubuntu.com/g' /etc/apt/sources.list","title":"The repository '&lt;repo&gt;' no longer has a Release file"},{"location":"Linux/Ubuntu/#upgrade","text":"For a system upgrade, follow these steps: run the update of the current version . Then optionaly backup the WSL perform the upgrade: If a) you are on a LTS version, and b) there is a new LTS version available, run sudo do-release-upgrade . Otherwise, follow the steps in the next section. Ubuntu versions","title":"Upgrade"},{"location":"Linux/Ubuntu/#manual-upgrade","text":"For manual upgrade, follow these steps: perform steps 1 and 2 from the normal upgrade open the /etc/update-manager/release-upgrades file and set the Prompt parameter to normal or lts (depending on the desired version) backup the /etc/apt/sources.list file change the sources to the new version: e.g., run sudo sed -i 's/<current version name>/<new version name>/g' /etc/apt/sources.list run a normal upgrade: sudo apt update && sudo apt upgrade finalizing the upgrade: sudo apt dist-upgrade","title":"Manual upgrade"},{"location":"Linux/Ubuntu/#wsl-backup","text":"check the WSL distro name: wsl -l -v shutdown WSL: wsl --shutdown backup the distro: wsl --export <disto name> <backup folder path>/<backup name>.tar","title":"WSL backup"},{"location":"Programming/Common/","text":"Keymap \u00b6 Copy : Ctrl + C Cut : Ctrl + X Paste : Ctrl + V Toggle comment : Ctrl + Q Search in file : Ctrl + S Sellect all : Ctrl + A Format selection : Ctrl + F Format File : Ctrl + Shift + F Build : Ctrl + B Refactoring \u00b6 Rename : Ctrl + R Change signature : Ctrl + G Text transform : Ctrl + T + U : to upper case + T : toggle case Surround with : Ctrl + W Command Line Interface (CLI) \u00b6 This chapter should guide you on how to design CLI in a user-friendly and predictable way. Mostly, it follows the POSIX standard , with the GNU long option extensions . There are two main types of CLI arguments: Options : (e.g., --help , -h ) are used to change the behavior of the program. They are usually optional and can be in any order. Operands : (e.g., file.txt ) are the input data for the program. They are usually required and their order is important. All operands should be placed after all options. Options \u00b6 Options can be of two types: Short options : (e.g., -h ) are one character long and are prefixed with a single dash. Long options : (e.g., --help ) are multi-characcter and are prefixed with two dashes. Options can have arguments: Short options and their arguments are separated by a space or can be concatenated, e.g., -o file.txt or -ofile.txt . The first variant is strongly recommended. Long options and their arguments are separated by a space or can be concatenated with an equal sign, e.g., --output file.txt or --output=file.txt . Also, short options can be grouped, e.g., -h -v -o file.txt can be written as -hvo file.txt . Multiple values in one option or operand \u00b6 If an option or operand contains multiple values (e.g., a list of files), the values should be separated by a comma, e.g., --files file1.txt,file2.txt,file3.txt . Exceptions \u00b6 Exceptions should be used to handle erroneus situations that are expected to happen. Exceptions should not be used for: Flow control , e.g., parse float from input, catch exception and try integer, then catch exception and try string... Unexpected situations , e.g., a method should always return a positive number, but it returns a negative one. For this we should use assertions, not exceptions. There are many types of exceptions, encapsulating different types of error description data. However, to begin with, it is not important to use some specific exception type. Using some general exception class is always much better than not using exceptions at all. Tests \u00b6 There are many types of software tests: Unit tests : The most common and known ype of tests. They test individual units of code, e.g., functions, classes, or modules. Integration tests : test how different units of code work together. Functional tests : test the functionality of the software from the user's perspective. Smoke tests : Test the minimum functionality of the software. sometimes, it just builds the software. In this case, it may be called a build verification test . sometimes, it runs the software with a minimal set of inputs. This way, we can check if the runtime libraries are correctly linked and if the software can be executed. Regression tests : test if the new version of the software behaves the same as the previous one. This can mean testing the equivalence of: the output of the software, the performance of the software, the compilation time or the size of the binary,.. Test cases for each test type can be written using two main approaches: Black-box testing : the test cases are written without knowing the internal structure of the software. It does not require any knowledge of the implementation details. However, it can be less effective in finding bugs: multiple test cases can test the same functionality, and some functionalities can be left untested. White-box testing : the test cases are written with the knowledge of the internal structure of the software. It can be more effective in finding bugs, but it requires a deep understanding of the software. It can also lead to a situation where the tests are too tightly coupled with the implementation, and the tests need to be rewritten when the implementation changes. Common terms in testing: Test case : a single test that checks a single aspect of the code. Test suite : a collection of test cases. Test runner : a tool that runs the test suite and reports the results. Test fixture : a set of initial conditions for a test case. Typically, it is a function that is run before each test case and that sets up the environment for the test case. Testing private methods \u00b6 An urgent need to test privete method accompanied with a lack of knowledge of how to do it is a common problem. In almost all programming languages, the testing of private methods is obsturcted by the language itself, i.e., the test frameworks does not have a special access to private methods. In this section we disscuss the usuall solutions to this problem. These implementation is specific to a particular language, but the general ideas are the same. The possible approaches are: Do not directly test the private method : Sometimes, the private method can be tested indirectly using a public method with minimal effort. This way, we test both the private method, and its interaction with the public method. Makeing the method public : Only recommended if the method should be exposed, i.e., its functionality is not limited to the class itself. Move the method to a different class : Maybe, the method is correcly marked as private in the current context, but it can also be extracted to its own class, where it will become the main method of the class. This applies to methods that can be used in other contexts, or for methods contained in large classes. Mark the method as internal and make it public : This is a strategy that can be always applied with minimum effort. Various ways how to signalize that the method is intended for internal use are: Naming convention : The method name can start with an underscore, e.g., _my_method . Documentation : The comments can contain a warning that the method is intended for internal use. Namespace : The method can be placed in a namespace that signals that it is intended for internal use, e.g., internal::my_method . Special access : We can use special language-dependant tools that can provide a special access to private methods: in C++ the friend keyword can be used to grant access to a class to another class. In Java, the @VisibleForTesting annotation can be used to mark a method as visible for testing. In Python, the __test__ attribute can be used to mark a method as visible for testing. Finding Duplicates \u00b6 For finding duplicates, there are two possible approaches: Using hash sets : iteratively checking if the current element is in the set of already seen elements and adding it to the set if not. Sorting : sorting the collection and then for each element checking if the current element is the same as the previous one. Comparison: | Approach | Time complexity (worst case asymptothic)| Time complexity (average expected) | Space complexity | allocation complexity | | --- | --- | --- | --- | --- | | Sets | O(log n) (both contains and add) | O(1) (both contains and add) | O(n) | O(1) | | Sorting | O(n log n) (sorting) | O(n log n) (sorting) + O(n) (duplicates check) | 0 or O(n) if we need to left the source collection unsorted | 0 or O(1) in case of new collection | JetBrains Products \u00b6 Configuration \u00b6 Compact tabs \u00b6 Settings -> Appearance & Behavior -> New UI and select Compact mode Layouts \u00b6 Layout is not synced between products and computers by default. To store and sync the layout: Arrange the IDE as you want Window -> Layouts -> Save current layout as new... On other computers, Window -> Layouts -> <Layout Name> -> Restore Advanced Configuration - Registry \u00b6 Sometimes, we need to edit advenced settings like with id cidr.debugger.gdb.usePythonToLoadData . This registry can be accessed by Help -> Find Action... -> type Registry -> find the desired setting. Troubleshooting \u00b6 Update is paused and does not resume even after closing the IDE \u00b6 Probably, the IDE installation directory is blocked by some other process. Try to close the WSL. If it does not help, find out which process is blocking the directory (On Windows: C:\\Users\\<User>\\AppData\\Local\\Programs\\<IDE name> ) Output console inserts new line when \\r is used \u00b6 This is unfortunately a default behavior of the IDE output console. To mitigate this, we have to emulate the console. To do this, open the run configuration and check the Emulate terminal in output console option. Visual Studio Code \u00b6 Configuration \u00b6 In VS Code, the configuration is stored in the settings.json . There is a system file, and also a workspace file, with a higher priority. Typically, the settings are configured using UI, which can be accessed by clicking on the gear icon in the bottom left corner of the window and selecting Settings . Some settings can only be configured by editing the settings.json file. If it is the case, there is typically a button in the UI that opens the file in the editor. There is a lot of options, and the UI is very basic. Therefore, it is usually best to use the search box to find the setting you are looking for. Basic Configuration \u00b6 Enable Multi-row tabs: workbench.editor.wrapTabs Forward and backward navigation \u00b6 To display forward and backward navigation: Right click on the top toolbar Select Command Center Set a visual guidline at 120 characters \u00b6 To set a visual guidline at 120 characters, we can use the editor.rulers setting: open the settings and search for rulers . click on Edit in settings.json add the following: json \"editor.rulers\": [ 120 ] Language specific settings \u00b6 Almost all settings can be set specifically to some language. To do that: in settings, next to the filter box, click on the filter icon and select language select the language find the setting either manually or by adding more filters change the setting To be sure that the setting is applied only to the selected language, look at the panel under the search box. Instead of User , Workspace , there should be User[<language>] , Workspace[<language>] . Language specific settings can be also configured in the settings.json file. Just modify the language section (object) of the settings.json file. Example: \"editor.formatOnSave\": false, \"editor.defaultFormatter\": \"esbenp.prettier-vscode\", \"[python]\": { \"editor.formatOnSave\": true } Here we configure the editor to format the code on save only for Python. Code Style \u00b6 Code stylke can be only configured in the settings.json file, there is no GUI for it. Typically, we apply the settings to only one language by adding them to the language section of the settings.json file. The typical settings are: editor.insertSpaces : if true, the editor will insert spaces instead of tabs Code Formatting \u00b6 Formatting in VS Code is defined only for some languages. For others, it has to be installed as an extension. Note that if the formatting is not defined for a language, selection formatting do nothing . To check the availability, try to format the whole document, then, an error box should appear. Tasks \u00b6 Documentation When coding, running various command line tools (e.g., pytest , npm , cargo , etc.) is often required. To avoid repeating typing the same commands over and over again in console, we can store them as tasks. Tasks for a project are stored in the .vscode/tasks.json file. For some languages, tasks are automatically suggested by the IDE. If the suggestions are accepted, the corresponding tasks are automatically added to the .vscode/tasks.json file. We can also create tasks manually by adding task objects to the tasks array in the .vscode/tasks.json file. The important properties are: label : the name of the task displayed in the IDE type : the type of the task, it can be: shell : run a shell command process : run a process options : three properties can be set here: cwd : the working directory for the task env : the environment variables for the task shell : the shell to use for the task command : the command or process to run args : the arguments of the command or process (array of strings). Extra quoting is not needed (unlike in Visual Studio). problemMatcher : the problem matcher to use for the task (array of strings). If not set, the IDE may ask the user to set it when the task is run. Releasing the software \u00b6 When releasing the software, you should follow the steps for each particular language and distribution channel. However, there are some common steps that should be done for each release which are described in this section, namely: Versioning : update the version number in the distribution files (e.g., setup.py , pom.xml , package.json , etc.). Changelog : update the changelog file with the new version and the changes. License : check if the license is present in all files and if it is up-to-date. Licenses \u00b6 Licensing has two parts: add the LICENSE file to the root of the project if not already present and add the license to the top of each source file. The first part is easy, just copy the license text to an empty file named LICENSE at the root of the project. To choose a license for your project, you can use the Choose a License website. The second part can be automated using the licenseheaders fork of the original licenseheaders project. A typical usage is: licenseheaders -t mit -o \"Czech Technical University in Prague\" -cy -n ShoDi -u \"https://github.com/aicenter/ShoDi\" -d C:\\Workspaces\\AIC\\shortest-distances\\ where: -t mit specifies the template to use (MIT license in this case), -o \"Czech Technical University in Prague\" specifies the organization name, -cy specifies to replace the years in the existing headers with the current year, -n ShoDi specifies the project name, -u \"https://github.com/aicenter/ShoDi\" specifies the project URL, -d C:\\Workspaces\\AIC\\shortest-distances\\ specifies the directory to process.","title":"Common"},{"location":"Programming/Common/#keymap","text":"Copy : Ctrl + C Cut : Ctrl + X Paste : Ctrl + V Toggle comment : Ctrl + Q Search in file : Ctrl + S Sellect all : Ctrl + A Format selection : Ctrl + F Format File : Ctrl + Shift + F Build : Ctrl + B","title":"Keymap"},{"location":"Programming/Common/#refactoring","text":"Rename : Ctrl + R Change signature : Ctrl + G Text transform : Ctrl + T + U : to upper case + T : toggle case Surround with : Ctrl + W","title":"Refactoring"},{"location":"Programming/Common/#command-line-interface-cli","text":"This chapter should guide you on how to design CLI in a user-friendly and predictable way. Mostly, it follows the POSIX standard , with the GNU long option extensions . There are two main types of CLI arguments: Options : (e.g., --help , -h ) are used to change the behavior of the program. They are usually optional and can be in any order. Operands : (e.g., file.txt ) are the input data for the program. They are usually required and their order is important. All operands should be placed after all options.","title":"Command Line Interface (CLI)"},{"location":"Programming/Common/#options","text":"Options can be of two types: Short options : (e.g., -h ) are one character long and are prefixed with a single dash. Long options : (e.g., --help ) are multi-characcter and are prefixed with two dashes. Options can have arguments: Short options and their arguments are separated by a space or can be concatenated, e.g., -o file.txt or -ofile.txt . The first variant is strongly recommended. Long options and their arguments are separated by a space or can be concatenated with an equal sign, e.g., --output file.txt or --output=file.txt . Also, short options can be grouped, e.g., -h -v -o file.txt can be written as -hvo file.txt .","title":"Options"},{"location":"Programming/Common/#multiple-values-in-one-option-or-operand","text":"If an option or operand contains multiple values (e.g., a list of files), the values should be separated by a comma, e.g., --files file1.txt,file2.txt,file3.txt .","title":"Multiple values in one option or operand"},{"location":"Programming/Common/#exceptions","text":"Exceptions should be used to handle erroneus situations that are expected to happen. Exceptions should not be used for: Flow control , e.g., parse float from input, catch exception and try integer, then catch exception and try string... Unexpected situations , e.g., a method should always return a positive number, but it returns a negative one. For this we should use assertions, not exceptions. There are many types of exceptions, encapsulating different types of error description data. However, to begin with, it is not important to use some specific exception type. Using some general exception class is always much better than not using exceptions at all.","title":"Exceptions"},{"location":"Programming/Common/#tests","text":"There are many types of software tests: Unit tests : The most common and known ype of tests. They test individual units of code, e.g., functions, classes, or modules. Integration tests : test how different units of code work together. Functional tests : test the functionality of the software from the user's perspective. Smoke tests : Test the minimum functionality of the software. sometimes, it just builds the software. In this case, it may be called a build verification test . sometimes, it runs the software with a minimal set of inputs. This way, we can check if the runtime libraries are correctly linked and if the software can be executed. Regression tests : test if the new version of the software behaves the same as the previous one. This can mean testing the equivalence of: the output of the software, the performance of the software, the compilation time or the size of the binary,.. Test cases for each test type can be written using two main approaches: Black-box testing : the test cases are written without knowing the internal structure of the software. It does not require any knowledge of the implementation details. However, it can be less effective in finding bugs: multiple test cases can test the same functionality, and some functionalities can be left untested. White-box testing : the test cases are written with the knowledge of the internal structure of the software. It can be more effective in finding bugs, but it requires a deep understanding of the software. It can also lead to a situation where the tests are too tightly coupled with the implementation, and the tests need to be rewritten when the implementation changes. Common terms in testing: Test case : a single test that checks a single aspect of the code. Test suite : a collection of test cases. Test runner : a tool that runs the test suite and reports the results. Test fixture : a set of initial conditions for a test case. Typically, it is a function that is run before each test case and that sets up the environment for the test case.","title":"Tests"},{"location":"Programming/Common/#testing-private-methods","text":"An urgent need to test privete method accompanied with a lack of knowledge of how to do it is a common problem. In almost all programming languages, the testing of private methods is obsturcted by the language itself, i.e., the test frameworks does not have a special access to private methods. In this section we disscuss the usuall solutions to this problem. These implementation is specific to a particular language, but the general ideas are the same. The possible approaches are: Do not directly test the private method : Sometimes, the private method can be tested indirectly using a public method with minimal effort. This way, we test both the private method, and its interaction with the public method. Makeing the method public : Only recommended if the method should be exposed, i.e., its functionality is not limited to the class itself. Move the method to a different class : Maybe, the method is correcly marked as private in the current context, but it can also be extracted to its own class, where it will become the main method of the class. This applies to methods that can be used in other contexts, or for methods contained in large classes. Mark the method as internal and make it public : This is a strategy that can be always applied with minimum effort. Various ways how to signalize that the method is intended for internal use are: Naming convention : The method name can start with an underscore, e.g., _my_method . Documentation : The comments can contain a warning that the method is intended for internal use. Namespace : The method can be placed in a namespace that signals that it is intended for internal use, e.g., internal::my_method . Special access : We can use special language-dependant tools that can provide a special access to private methods: in C++ the friend keyword can be used to grant access to a class to another class. In Java, the @VisibleForTesting annotation can be used to mark a method as visible for testing. In Python, the __test__ attribute can be used to mark a method as visible for testing.","title":"Testing private methods"},{"location":"Programming/Common/#finding-duplicates","text":"For finding duplicates, there are two possible approaches: Using hash sets : iteratively checking if the current element is in the set of already seen elements and adding it to the set if not. Sorting : sorting the collection and then for each element checking if the current element is the same as the previous one. Comparison: | Approach | Time complexity (worst case asymptothic)| Time complexity (average expected) | Space complexity | allocation complexity | | --- | --- | --- | --- | --- | | Sets | O(log n) (both contains and add) | O(1) (both contains and add) | O(n) | O(1) | | Sorting | O(n log n) (sorting) | O(n log n) (sorting) + O(n) (duplicates check) | 0 or O(n) if we need to left the source collection unsorted | 0 or O(1) in case of new collection |","title":"Finding Duplicates"},{"location":"Programming/Common/#jetbrains-products","text":"","title":"JetBrains Products"},{"location":"Programming/Common/#configuration","text":"","title":"Configuration"},{"location":"Programming/Common/#compact-tabs","text":"Settings -> Appearance & Behavior -> New UI and select Compact mode","title":"Compact tabs"},{"location":"Programming/Common/#layouts","text":"Layout is not synced between products and computers by default. To store and sync the layout: Arrange the IDE as you want Window -> Layouts -> Save current layout as new... On other computers, Window -> Layouts -> <Layout Name> -> Restore","title":"Layouts"},{"location":"Programming/Common/#advanced-configuration-registry","text":"Sometimes, we need to edit advenced settings like with id cidr.debugger.gdb.usePythonToLoadData . This registry can be accessed by Help -> Find Action... -> type Registry -> find the desired setting.","title":"Advanced Configuration - Registry"},{"location":"Programming/Common/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Programming/Common/#update-is-paused-and-does-not-resume-even-after-closing-the-ide","text":"Probably, the IDE installation directory is blocked by some other process. Try to close the WSL. If it does not help, find out which process is blocking the directory (On Windows: C:\\Users\\<User>\\AppData\\Local\\Programs\\<IDE name> )","title":"Update is paused and does not resume even after closing the IDE"},{"location":"Programming/Common/#output-console-inserts-new-line-when-r-is-used","text":"This is unfortunately a default behavior of the IDE output console. To mitigate this, we have to emulate the console. To do this, open the run configuration and check the Emulate terminal in output console option.","title":"Output console inserts new line when \\r is used"},{"location":"Programming/Common/#visual-studio-code","text":"","title":"Visual Studio Code"},{"location":"Programming/Common/#configuration_1","text":"In VS Code, the configuration is stored in the settings.json . There is a system file, and also a workspace file, with a higher priority. Typically, the settings are configured using UI, which can be accessed by clicking on the gear icon in the bottom left corner of the window and selecting Settings . Some settings can only be configured by editing the settings.json file. If it is the case, there is typically a button in the UI that opens the file in the editor. There is a lot of options, and the UI is very basic. Therefore, it is usually best to use the search box to find the setting you are looking for.","title":"Configuration"},{"location":"Programming/Common/#basic-configuration","text":"Enable Multi-row tabs: workbench.editor.wrapTabs","title":"Basic Configuration"},{"location":"Programming/Common/#forward-and-backward-navigation","text":"To display forward and backward navigation: Right click on the top toolbar Select Command Center","title":"Forward and backward navigation"},{"location":"Programming/Common/#set-a-visual-guidline-at-120-characters","text":"To set a visual guidline at 120 characters, we can use the editor.rulers setting: open the settings and search for rulers . click on Edit in settings.json add the following: json \"editor.rulers\": [ 120 ]","title":"Set a visual guidline at 120 characters"},{"location":"Programming/Common/#language-specific-settings","text":"Almost all settings can be set specifically to some language. To do that: in settings, next to the filter box, click on the filter icon and select language select the language find the setting either manually or by adding more filters change the setting To be sure that the setting is applied only to the selected language, look at the panel under the search box. Instead of User , Workspace , there should be User[<language>] , Workspace[<language>] . Language specific settings can be also configured in the settings.json file. Just modify the language section (object) of the settings.json file. Example: \"editor.formatOnSave\": false, \"editor.defaultFormatter\": \"esbenp.prettier-vscode\", \"[python]\": { \"editor.formatOnSave\": true } Here we configure the editor to format the code on save only for Python.","title":"Language specific settings"},{"location":"Programming/Common/#code-style","text":"Code stylke can be only configured in the settings.json file, there is no GUI for it. Typically, we apply the settings to only one language by adding them to the language section of the settings.json file. The typical settings are: editor.insertSpaces : if true, the editor will insert spaces instead of tabs","title":"Code Style"},{"location":"Programming/Common/#code-formatting","text":"Formatting in VS Code is defined only for some languages. For others, it has to be installed as an extension. Note that if the formatting is not defined for a language, selection formatting do nothing . To check the availability, try to format the whole document, then, an error box should appear.","title":"Code Formatting"},{"location":"Programming/Common/#tasks","text":"Documentation When coding, running various command line tools (e.g., pytest , npm , cargo , etc.) is often required. To avoid repeating typing the same commands over and over again in console, we can store them as tasks. Tasks for a project are stored in the .vscode/tasks.json file. For some languages, tasks are automatically suggested by the IDE. If the suggestions are accepted, the corresponding tasks are automatically added to the .vscode/tasks.json file. We can also create tasks manually by adding task objects to the tasks array in the .vscode/tasks.json file. The important properties are: label : the name of the task displayed in the IDE type : the type of the task, it can be: shell : run a shell command process : run a process options : three properties can be set here: cwd : the working directory for the task env : the environment variables for the task shell : the shell to use for the task command : the command or process to run args : the arguments of the command or process (array of strings). Extra quoting is not needed (unlike in Visual Studio). problemMatcher : the problem matcher to use for the task (array of strings). If not set, the IDE may ask the user to set it when the task is run.","title":"Tasks"},{"location":"Programming/Common/#releasing-the-software","text":"When releasing the software, you should follow the steps for each particular language and distribution channel. However, there are some common steps that should be done for each release which are described in this section, namely: Versioning : update the version number in the distribution files (e.g., setup.py , pom.xml , package.json , etc.). Changelog : update the changelog file with the new version and the changes. License : check if the license is present in all files and if it is up-to-date.","title":"Releasing the software"},{"location":"Programming/Common/#licenses","text":"Licensing has two parts: add the LICENSE file to the root of the project if not already present and add the license to the top of each source file. The first part is easy, just copy the license text to an empty file named LICENSE at the root of the project. To choose a license for your project, you can use the Choose a License website. The second part can be automated using the licenseheaders fork of the original licenseheaders project. A typical usage is: licenseheaders -t mit -o \"Czech Technical University in Prague\" -cy -n ShoDi -u \"https://github.com/aicenter/ShoDi\" -d C:\\Workspaces\\AIC\\shortest-distances\\ where: -t mit specifies the template to use (MIT license in this case), -o \"Czech Technical University in Prague\" specifies the organization name, -cy specifies to replace the years in the existing headers with the current year, -n ShoDi specifies the project name, -u \"https://github.com/aicenter/ShoDi\" specifies the project URL, -d C:\\Workspaces\\AIC\\shortest-distances\\ specifies the directory to process.","title":"Licenses"},{"location":"Programming/Conda%20and%20Mamba/","text":"Introduction \u00b6 Conda and Mamba are package managers that shares the same history, principles, and partially a CLI syntax. They are: multi-platform language-agnostic non-destructive (unlike linux system package managers, or Windows installers) Mamba differs from Conda in that it is faster and more memory-efficient, which can be crucial for large projects. Installation and Initialization \u00b6 Windows \u00b6 Download and Install Miniforge On windows, the best way to install Conda and Mamba is to install Miniforge. It is basivally an installation of Conda and Mamba together with some initialization scripts set up so that it uses the conda-forge repository instead of the default conda repository. Open the Miniforge Terminal from the start menu and run conda init : After the installation, we can ony use conda and mamba commands using a special terminal shortcut installed to start menu. We can enable the use of conda and mamba commands in PowerShell profile by running the conda init in this terminal. Open new terminal Window with PowerShell and run conda auto_activate false This will disable the automatic activation of the conda environment when the terminal is opened. Auto activation in PowerShell is dangerous, as this puts many conda executables early in the PATH, which can override the system executables (e.g., git, cmake) Linux \u00b6 On linux, we have several options to install Conda and Mamba: install Miniforge using the instructions on the website install mamba using the system package manager. Note that this typically installs micromamba, so: only mamba executables are available, no conda default environment location have to be set up using environment variables, as the installation defaults to /envs . Set up the environment root directory \u00b6 Sometimes, we may want to change the default environment root directory. For this, we change the following environment variables: CONDA_ROOT_PREFIX : for conda MAMBA_ROOT_PREFIX : for mamba MICROMAMBA_ROOT_PREFIX : for micromamba Environments \u00b6 Conda and Mamba use environments similar to virtual environments of pip. By default, these package managers use the base environment, which is pre-created. However, unlike with pip, mamba discurages the use of the default (base) environment . Typically, commands can be run with the -n <environment name> parameter to specify the environment. If no environment is specified, the active environment is used. To create a new environment , we use the create command: <conda/mamba> create -n <environment name> <package names> To remove an environment , we use the remove command: <conda/mamba> remove -n <environment name> --all Packages \u00b6 Packages in Conda and Mamba are similar to packages in pip, but they are not limited to Python. To list the packages in an environment , we use the list command: <conda/mamba> list -n <environment name> To install a package into an environment , we use the install command: <conda/mamba> install -n <environment name> <package name> If the package is not available in the conda-forge but in in PyPI, we can install it from PyPI using the pip command like we used to do normally. If we are in a conda/mamba environment, the conda/mamba pip will be used instead of the system pip. Usually, it is the best to install all the dependencies first to minimize the number of packages that need to be installed from PyPI instead of conda-forge. Other commands \u00b6 Run a single command \u00b6 To run a single command in a conda/mamba environment we use the run command: <conda/mamba> run <command> <command arguments> Some important parameters: --no-capture-output : By default, the output is buffered. This parameter disables the buffering and prints the output to the console immediately. Mamba \u00b6 Home User guide Conda \u00b6 Wiki","title":"Conda and Mamba"},{"location":"Programming/Conda%20and%20Mamba/#introduction","text":"Conda and Mamba are package managers that shares the same history, principles, and partially a CLI syntax. They are: multi-platform language-agnostic non-destructive (unlike linux system package managers, or Windows installers) Mamba differs from Conda in that it is faster and more memory-efficient, which can be crucial for large projects.","title":"Introduction"},{"location":"Programming/Conda%20and%20Mamba/#installation-and-initialization","text":"","title":"Installation and Initialization"},{"location":"Programming/Conda%20and%20Mamba/#windows","text":"Download and Install Miniforge On windows, the best way to install Conda and Mamba is to install Miniforge. It is basivally an installation of Conda and Mamba together with some initialization scripts set up so that it uses the conda-forge repository instead of the default conda repository. Open the Miniforge Terminal from the start menu and run conda init : After the installation, we can ony use conda and mamba commands using a special terminal shortcut installed to start menu. We can enable the use of conda and mamba commands in PowerShell profile by running the conda init in this terminal. Open new terminal Window with PowerShell and run conda auto_activate false This will disable the automatic activation of the conda environment when the terminal is opened. Auto activation in PowerShell is dangerous, as this puts many conda executables early in the PATH, which can override the system executables (e.g., git, cmake)","title":"Windows"},{"location":"Programming/Conda%20and%20Mamba/#linux","text":"On linux, we have several options to install Conda and Mamba: install Miniforge using the instructions on the website install mamba using the system package manager. Note that this typically installs micromamba, so: only mamba executables are available, no conda default environment location have to be set up using environment variables, as the installation defaults to /envs .","title":"Linux"},{"location":"Programming/Conda%20and%20Mamba/#set-up-the-environment-root-directory","text":"Sometimes, we may want to change the default environment root directory. For this, we change the following environment variables: CONDA_ROOT_PREFIX : for conda MAMBA_ROOT_PREFIX : for mamba MICROMAMBA_ROOT_PREFIX : for micromamba","title":"Set up the environment root directory"},{"location":"Programming/Conda%20and%20Mamba/#environments","text":"Conda and Mamba use environments similar to virtual environments of pip. By default, these package managers use the base environment, which is pre-created. However, unlike with pip, mamba discurages the use of the default (base) environment . Typically, commands can be run with the -n <environment name> parameter to specify the environment. If no environment is specified, the active environment is used. To create a new environment , we use the create command: <conda/mamba> create -n <environment name> <package names> To remove an environment , we use the remove command: <conda/mamba> remove -n <environment name> --all","title":"Environments"},{"location":"Programming/Conda%20and%20Mamba/#packages","text":"Packages in Conda and Mamba are similar to packages in pip, but they are not limited to Python. To list the packages in an environment , we use the list command: <conda/mamba> list -n <environment name> To install a package into an environment , we use the install command: <conda/mamba> install -n <environment name> <package name> If the package is not available in the conda-forge but in in PyPI, we can install it from PyPI using the pip command like we used to do normally. If we are in a conda/mamba environment, the conda/mamba pip will be used instead of the system pip. Usually, it is the best to install all the dependencies first to minimize the number of packages that need to be installed from PyPI instead of conda-forge.","title":"Packages"},{"location":"Programming/Conda%20and%20Mamba/#other-commands","text":"","title":"Other commands"},{"location":"Programming/Conda%20and%20Mamba/#run-a-single-command","text":"To run a single command in a conda/mamba environment we use the run command: <conda/mamba> run <command> <command arguments> Some important parameters: --no-capture-output : By default, the output is buffered. This parameter disables the buffering and prints the output to the console immediately.","title":"Run a single command"},{"location":"Programming/Conda%20and%20Mamba/#mamba","text":"Home User guide","title":"Mamba"},{"location":"Programming/Conda%20and%20Mamba/#conda","text":"Wiki","title":"Conda"},{"location":"Programming/Git/","text":"Installation \u00b6 Windows \u00b6 Git can be bundeled within other software, but it is best to install it separately ( Git for Windows ), because the installer contains a lot of important configuration options, that can have very inconvenient defaults. Important options, that defaults to inpractical values: default text editor : defaults to vim default ssh client : defaults to bundled openssh (part of git bash) context menu integration : defaults to \"Open in Git GUI\" and \"Open in Git Bash\" GitExtensions \u00b6 GitExtensions is a GUI for Git that can make some operations easier, as diffs and branch structure are more clear when using a GUI. The tool is intuitive it is only important to install git first . Limitations \u00b6 Unfortunatelly, GitExtensions does not support wordwrap in the diff view. There is an issue for that (closed, but never resolved). Basics \u00b6 The structure of a single git repository is displayed in the following picture: Explanation of the picture: The working tree or working directory is the directory where the files are stored, typically the root directory of the project where the .git directory is located. The index or staging area is a cache of all changes that are marked ( staged ) for the next commit. To stage a change, we need to call git add command. Only files in the index are committed when we call git commit . The HEAD is a pointer to the last commit in the current branch. The following scheme shows the operations that can be performed between the working tree, index, HEAD and the remote repository (blue arrows represent the typical workflow): Configuration \u00b6 For configuration, we can use the git config command. There are three levels of configuration: system : the configuration is applied to all users on the system. This configuration is set during the installation of git. global : the configuration is applied to all repositories of the current user. This configuration is set by the --global parameter. local : the configuration is applied only to the current repository. This configuration is set by the --local parameter. To list the configuration, use the -l / --list parameter of the git config command. To list the configuration for a specific level, use the --system , --global , --local parameters. To see the default value of a configuration, search in the git config documentation . The git local configuration is stored in the .gitconfig file in the user's home directory. It can be edited by editing the file directly, or by calling git config command. To display the active configuration in the command line, call: git config --list We can also show whether the configuration results from the system, user or local configuration file: git config --list --show-origin Basic Tasks \u00b6 Rewrite remote with local changes without merging \u00b6 git push -f Go back to branch \u00b6 git revert --no-commit 0766c053..HEAD Untrack files, but not delete them locally: \u00b6 git rm --cached <FILEPATH> usefull params: -r : recursive - deletes content of directories -n dry run Branches \u00b6 Branches are a way to manage parallel development. Each branch has a name and its own history. Typically (but not necessarily) a local branch tracks a remote branch with the same name. To list the branches , we call git branch . To switch to a branch , we call git checkout <branch name> . For that, the branch must exist locally. If it does not, we need to create it first: new branch from scratch: git branch <branch name> new branch from a remote branch: git checkout --track origin/<REMOTE_BRANCH_NAME> for this, the local repository needs to know about the remote branch. We may need to first git fetch to update the remote branches. Renaming a branch \u00b6 When renaming a branch we need to: rename the branch locally: git branch -m <old name> <new name> delete the old branch on the remote and push the new branch: git push origin :<old name> <new name> on all machines, change the remote branch to the new one on any other machine, rename the branch locally Not that if the branch is protected or default, we cannot delete it directly. In that can, we need to remove the protection first, usually using the web interface of the particular remote. Remote Repositories \u00b6 Git Basics manual Normally, we have a single remote repository, which is typically called origin . In this repository, we share the code with other developers. In this case we don't have to care about the remote repository handling, because the remote is automatically set to the repository we cloned from. However, sometimes we need to work with multiple remote repositories. In this case, we have to add remotes manually and also specify the remote repository when we push or pull. To add a remote , we call: git remote add <remote name> <URL> Where the remote and <URL> is is the link we use to clone the repository. To list remotes , we call: git remote -v Then we need to specify the remote when we want to use a command that interacts with the remote repository and we want to use a remote other than the default one. For example, to push to a remote repository, instead of just git push , we call: git push <remote name> <branch name> Wildcards \u00b6 Can be used in gitignore and also in some git commands. All described in the Manual . Usefull wildcards: **/ in any directory /** everything in a directory Reverting \u00b6 When we want to revert something using git there are multiple options depending on the situation. The commands are: git checkout for overwriting local files with version from a specified tree (also for switching branches) and git reset for reverting commits and effectively rewriting the history. git read-tree : similar to checkout, but does not require a cleanup before The following table shows the differences between the commands: Command revrerts commits overwrite local changes delete files committed after <commit> git checkout <commit> no yes no git read-tree -m -u <commit> no yes yes git reset --soft <commit> yes no no git reset --hard <commit> yes yes yes To decide between the commands, the first consideration should be whether we want to preserve the history or not: we want to reach a specific state in the history and commit the changes as a new commit -> use git checkout or git read-tree we want to reach a specific state in the history and discard all changes after that point -> use git reset Keep the history \u00b6 If we want to keep the history, there are still two options: we want to revert the whole working tree and also delete all files that were committed after the specified commit -> use git read-tree -m -u <commit> we want to revert the whole working tree, but keep all files that were committed after the specified commit -> use git checkout <commit> we want to revert only some files -> use git checkout <commit> <filepath> Using git checkout \u00b6 To reset an individual file, call: git checkout <commit> <filepath> , to reset all files, call: git checkout <commit> . . If the <commit> parameter is ommited, the local files will be overwritten by the HEAD. Drop the history \u00b6 Dropping the history can be useful in many cases. For example, we may commit some changes to the master, but then we realize that they belong to a branch. A simple solution is to create a branch, and then reset the master to the previous commit. Note that if the wrong history was already pushed to the remote, we need to fix the history on the remote as well. This is done by force pushing: git push -f Removing specific files from history \u00b6 Removing files from history can be done using multiple tools: bfg repo cleaner is the simplest tool. It can only select files by filename or size, but that is sufficient in most cases. git filter-repo is a more sophisticated successor to BFG. It can do almost anything possible. Nevertheless, it is less intuitive to operate, and it is harder to analyze its result. The filter-branch command is the original tool for filtering git history. It is slow and problematic, so it should be used only if the above two tools are not available. No matter of the used tool, before you begin: commit and push from all machines, backup the repository Similarly, at the end: It is important not to merge (branch or conflitcs) that originated before the cleanup on other machines. Otherwise, the deleted file can be reintroduced to the history. Pull on other machines. Add the file to gitignore so that it wont be commited again BFG \u00b6 With BFG, only a file with specific filename can be deleted . It is not possible to use exact file path. To remove file by its name: remove the file locally clone the whole repo again with the --mirror option on the mirrored repo, run the cleaner: bfg --delete-files <FILENAME> run the git commands that appears at the end of the bfg output run git push Git filter-repo \u00b6 The git filter-repo can be installed using pip: pip install git-filter-repo . To remove file by its path: run the command with the --dry-run parameter: git filter-repo --invert-paths --force --dry-run --path <PATH TO THE FILE TO BE REMOVED> inspect the changes in .git/filter-repo directory: Compare the files in KDiff3 To skip the lines starting with original-oid : go to the file selection dialog click Configure to the line-matching preprocessor command, add: sed 's/original-oid .*//' click OK run the command without the --dry-run parameter force push the comman to the remote git push origin --force --all If the push is rejected by the remote hook, the master branch is probably protected. It has to be unprotected first in the repository config. Appart from exact file path, we can also use: glob patterns: --path-glob <GLOB PATTERN> regex patterns: --path-regex <REGEX PATTERN> git filter-repo manual More information on github filter-branch \u00b6 git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch <FILE>' --prune-empty --tag-name-filter cat -- --all Merging \u00b6 Syntax: git merge <params> <commit> Merge can be aborted at any time by calling git merge --abort . This resets the state of the repository. Resolving Conflicts \u00b6 Sometimes, the same file is changed on the same line in both branches we are merging. This is called a conflict. In this case, the conflicting content is marked with conflict markers: <<<<<<< HEAD This is the content of the file in the current branch ======= This is the content of the file in the branch we are merging >>>>>>> branch-a We can resolve the conflict manually by editing the file, but more often, we want to use a merge tool. To do that, we can call: git mergetool The mergetool should be first configured, as the default one (vimdiff) is not very user friendly. To configure the mergetool, we edit the git configuration. The appropriate section is mergetool . It has a lot of options, but for typical mergetool, is enough to set the name, as the mergetool command for that specific merge tool is preconfigured in recent versions of git. Example of the .gitconfig file configured for KDiff3: ... [mergetool \"kdiff3\"] ... More information on git mergetool Debugging mergetool \u00b6 Sometimes, this error can occur after calling git mergetool : git-mergetool--lib \"command not found\" This means that the mergetool is misconfigured. Inspect the mergetool section in the .gitconfig file to find the error. Merging Moved Files \u00b6 Sometimes, it's necessary to tweak the command to help it locate the moved files. What can help: -X rename-threshold=25 : Changing the threshold is important for languages that changes the file content when moving it with automatic refactoring (especially important for Java files, which usually have tons of imports) -X ignore-space-change Revert merge \u00b6 git revert -m 1 Note that when the merge is reverted, the changes cannot be merged again, because they predates the revert! Update \u00b6 On windows, run: git update-git-for-windows Pull Requests \u00b6 Pull requests are a way to propose changes to the repository. The principle is as follows: Create a branch for the changes Commit the changes to the branch and push it to the remote Create a pull request on the remote, that suggest to merge the branch into the master branch There are two possible scenarios: We have the permission to create a branch in the repository: in this case, we can create the branch directly on the remote We do not have the permission to create a branch in the repository: in this case, we have to: Fork the repository Clone the fork Create the branch locally in the fork Commit and push the changes to the fork Create a pull request from the fork to the original repository Update pull request \u00b6 If there are changes requested by the reviewer or we just forgot to add something, we can update the pull request by pushing the updates to the PR branch. The pull request will be automatically updated. Showing the state in the command line \u00b6 Mostly, we use GUI tools to show the state of the repository. However, sometimes we may need to show the state in the command line. The following commands can be useful: git status : shows the state of the repository git log : shows the history of the repository git diff : shows the changes between the working tree and the index git log \u00b6 The git log shows the history of the repository. It is the analogue of the main view in GitExtensions. The most useful parameters are: --oneline : shows the history in a compact form --graph : shows the history as a graph --all : shows the history of all branches, not only the current one --decorate : shows the names of the branches and tags, not only the commit hashes GitHub \u00b6 Creating a GitHub Release \u00b6 To create a release: In the repository, under the Releases heading, click Create a new release Click on the Choose a tag dropdown Select an existing tag or create a new one by filling the text field with the tag name and clicking Create new tag button Fill the Release title and Description fields Click Publish release GitHub CLI \u00b6 Github has a CLI tool that can be used to interact with the repository. The tool can be installed from the GitHub CLI page . The main command is gh . Typically, we need to authenticate the tool first by calling gh auth login . There are two options for authentication: browser authentication (default) token authentication: suitable for automation only the old token type can be used, the fine-grained tokens are not supported yet Token Authentication \u00b6 To authenticate using a token, we first need to create a token . The token settings are in Profile Settings -> Developer settings -> Personal access tokens . Thereare two types of tokens: classic tokens: legacy access tokens fine-grained tokens: new access tokens with more granular permissions Unless we need some specific permissions available only in the classic tokens, we should use the fine-grained tokens. This is the only option if we want to limit the scope of the token only to certain repositories. To know what specific permissions are needed for a particular command, we can call the command with the --help parameter read the documentation of the command run the command and read the error message ask Then we can authenticate using the token . We can either: suply the token to the gh auth login command: PowerShell gh auth login -h github.com -p ssh --skip-ssh-key --with-token <TOKEN> or set the environment variable GH_TOKEN to the token value and call commands without authentication. Managing realeases \u00b6 Required permissions: Read and Write access to code To list releases, we can call the gh release view command. To create a release, we can the gh release create command: gh release create <TAG> --repo <REPO> --title <TITLE> --generate-notes Here, the <REPO> is the full url of the repository, e.g. git@github.com:F-I-D-O/Future-Config.git To delete a release, we can call the gh release delete command: gh release delete <TAG> --repo <REPO> --cleanup-tag -y Synchronizing a fork with the original repository \u00b6 Required permissions: Read and Write access to code Read and Write access to workflows To synchronize a fork with the original repository, we call: gh repo sync <repo path> -b <branch name> Here, the <repo path> is the path on the github.com domain, e.g. F-I-D-O/Future-Config . We can skip the -b <branch name> parameter if we want to synchronize the default branch. Repository Migration \u00b6 Gitlab to GitHub migration tool Git Large File Storage \u00b6 Homepage GitHub documentation Git Large File Storage is a system for storing and versioning large files in git repositories. One of the main use cases is to store experiment code and data in one place. Note that with publicaly available repository hosting services like GitHub, the git LFS has severe limitations preventing almost any practical use. (see limitations ) First, install Git LFS in the system: Run the installer downloaded from the homepage run git lfs install to initialize Git LFS Then to use it in the repository : define for which files Git LFS should be used. For example, by extension: by running git lfs track \"*.png\" directly in the .gitattributes file: add the following line: *.png filter=lfs diff=lfs merge=lfs -text commit all work except the large files, including the changed .gitattributes file where the LFS filters are defined check that the LFS status of the files is correct by running git lfs ls-files (optional) And that's it, now it should work. The only problematic part may be a slower push/pull performance. Note that this simple procedure only affects new files . To affect existing files, we have to distinguish two cases: files added in the incorrect (text) mode that are not in the history yet (added, but not commited). These can be fixed by running add with the --renormalize parameter: bash git add . --renormalize files added before the LFS initialization (already in the history). These can be fixed by running with the migrate subcommand. The migrate subcommand \u00b6 documentation The migrate subcommand of the git lfs command was introduced to modify history. It has three subcommands: import : converts files in history to the large file mode export : converts files in history to the text mode info : shows the information about the files tracked by LFS bash git lfs migrate import --fixup migrate import \u00b6 This subcommand is used to convert files in history to the large file mode. There are two possible cases: we need to convert files that were committed before we start working with the LFS. We need to select such files manually, for example by specifying extensions: bash git lfs migrate import --include \"*.png,*.jpg,*.jpeg,*.gif,*.bmp,*.tiff,*.ico,*.webp\" we need to convert files that were committed after we start working with the LFS and the correct .gitattributes file was already in the history. This is much less likely, but it can happen. In this case, we can run the command with the --fixup parameter: bash git lfs migrate import --fixup Git LFS Limitations \u00b6 When working with git LFS, we need to consider some limitations: it slows down the push/pull operations we need to push early, not right before leaving the office the speed can be affected even without any new large files added There are further limitations depending on the remote repository hosting service: GitHub: maximum file size of 2GB forks cannot introduce any new large files Maximum bandwidth per month : 10GB Troubleshooting \u00b6 GUI tools show no history, git log results in errors \u00b6 This may be due to broken refs in .git/refs/ . Possible errors: your current branch appears to be broken -> the checked out branch is broken fatal: bad object: <ref> -> the ref is broken To fix the issue, we can copy the refs from the .git/logs/refs/ : find the broken file in .git/refs/ find the corresponding file in .git/logs/refs/ copy the hash from the last line, second column and write it to the broken file ssh key passphrase is required, despite the ssh key agent is running \u00b6 This can have multiple reasons. To narrow down the issue, firtst check the ssh command separately. If the ssh does not require a passphrase, but the git does, check whether git uses the same ssh: open git bash run where ssh The first (and only) path should be the same as the one that you are using with the agent, most likely the OpenSSH from Windows. If the git bash bundeled OpenSSH command is first, it means an incorrect git installation.","title":"Git"},{"location":"Programming/Git/#installation","text":"","title":"Installation"},{"location":"Programming/Git/#windows","text":"Git can be bundeled within other software, but it is best to install it separately ( Git for Windows ), because the installer contains a lot of important configuration options, that can have very inconvenient defaults. Important options, that defaults to inpractical values: default text editor : defaults to vim default ssh client : defaults to bundled openssh (part of git bash) context menu integration : defaults to \"Open in Git GUI\" and \"Open in Git Bash\"","title":"Windows"},{"location":"Programming/Git/#gitextensions","text":"GitExtensions is a GUI for Git that can make some operations easier, as diffs and branch structure are more clear when using a GUI. The tool is intuitive it is only important to install git first .","title":"GitExtensions"},{"location":"Programming/Git/#limitations","text":"Unfortunatelly, GitExtensions does not support wordwrap in the diff view. There is an issue for that (closed, but never resolved).","title":"Limitations"},{"location":"Programming/Git/#basics","text":"The structure of a single git repository is displayed in the following picture: Explanation of the picture: The working tree or working directory is the directory where the files are stored, typically the root directory of the project where the .git directory is located. The index or staging area is a cache of all changes that are marked ( staged ) for the next commit. To stage a change, we need to call git add command. Only files in the index are committed when we call git commit . The HEAD is a pointer to the last commit in the current branch. The following scheme shows the operations that can be performed between the working tree, index, HEAD and the remote repository (blue arrows represent the typical workflow):","title":"Basics"},{"location":"Programming/Git/#configuration","text":"For configuration, we can use the git config command. There are three levels of configuration: system : the configuration is applied to all users on the system. This configuration is set during the installation of git. global : the configuration is applied to all repositories of the current user. This configuration is set by the --global parameter. local : the configuration is applied only to the current repository. This configuration is set by the --local parameter. To list the configuration, use the -l / --list parameter of the git config command. To list the configuration for a specific level, use the --system , --global , --local parameters. To see the default value of a configuration, search in the git config documentation . The git local configuration is stored in the .gitconfig file in the user's home directory. It can be edited by editing the file directly, or by calling git config command. To display the active configuration in the command line, call: git config --list We can also show whether the configuration results from the system, user or local configuration file: git config --list --show-origin","title":"Configuration"},{"location":"Programming/Git/#basic-tasks","text":"","title":"Basic Tasks"},{"location":"Programming/Git/#rewrite-remote-with-local-changes-without-merging","text":"git push -f","title":"Rewrite remote with local changes without merging"},{"location":"Programming/Git/#go-back-to-branch","text":"git revert --no-commit 0766c053..HEAD","title":"Go back to branch"},{"location":"Programming/Git/#untrack-files-but-not-delete-them-locally","text":"git rm --cached <FILEPATH> usefull params: -r : recursive - deletes content of directories -n dry run","title":"Untrack files, but not delete them locally:"},{"location":"Programming/Git/#branches","text":"Branches are a way to manage parallel development. Each branch has a name and its own history. Typically (but not necessarily) a local branch tracks a remote branch with the same name. To list the branches , we call git branch . To switch to a branch , we call git checkout <branch name> . For that, the branch must exist locally. If it does not, we need to create it first: new branch from scratch: git branch <branch name> new branch from a remote branch: git checkout --track origin/<REMOTE_BRANCH_NAME> for this, the local repository needs to know about the remote branch. We may need to first git fetch to update the remote branches.","title":"Branches"},{"location":"Programming/Git/#renaming-a-branch","text":"When renaming a branch we need to: rename the branch locally: git branch -m <old name> <new name> delete the old branch on the remote and push the new branch: git push origin :<old name> <new name> on all machines, change the remote branch to the new one on any other machine, rename the branch locally Not that if the branch is protected or default, we cannot delete it directly. In that can, we need to remove the protection first, usually using the web interface of the particular remote.","title":"Renaming a branch"},{"location":"Programming/Git/#remote-repositories","text":"Git Basics manual Normally, we have a single remote repository, which is typically called origin . In this repository, we share the code with other developers. In this case we don't have to care about the remote repository handling, because the remote is automatically set to the repository we cloned from. However, sometimes we need to work with multiple remote repositories. In this case, we have to add remotes manually and also specify the remote repository when we push or pull. To add a remote , we call: git remote add <remote name> <URL> Where the remote and <URL> is is the link we use to clone the repository. To list remotes , we call: git remote -v Then we need to specify the remote when we want to use a command that interacts with the remote repository and we want to use a remote other than the default one. For example, to push to a remote repository, instead of just git push , we call: git push <remote name> <branch name>","title":"Remote Repositories"},{"location":"Programming/Git/#wildcards","text":"Can be used in gitignore and also in some git commands. All described in the Manual . Usefull wildcards: **/ in any directory /** everything in a directory","title":"Wildcards"},{"location":"Programming/Git/#reverting","text":"When we want to revert something using git there are multiple options depending on the situation. The commands are: git checkout for overwriting local files with version from a specified tree (also for switching branches) and git reset for reverting commits and effectively rewriting the history. git read-tree : similar to checkout, but does not require a cleanup before The following table shows the differences between the commands: Command revrerts commits overwrite local changes delete files committed after <commit> git checkout <commit> no yes no git read-tree -m -u <commit> no yes yes git reset --soft <commit> yes no no git reset --hard <commit> yes yes yes To decide between the commands, the first consideration should be whether we want to preserve the history or not: we want to reach a specific state in the history and commit the changes as a new commit -> use git checkout or git read-tree we want to reach a specific state in the history and discard all changes after that point -> use git reset","title":"Reverting"},{"location":"Programming/Git/#keep-the-history","text":"If we want to keep the history, there are still two options: we want to revert the whole working tree and also delete all files that were committed after the specified commit -> use git read-tree -m -u <commit> we want to revert the whole working tree, but keep all files that were committed after the specified commit -> use git checkout <commit> we want to revert only some files -> use git checkout <commit> <filepath>","title":"Keep the history"},{"location":"Programming/Git/#using-git-checkout","text":"To reset an individual file, call: git checkout <commit> <filepath> , to reset all files, call: git checkout <commit> . . If the <commit> parameter is ommited, the local files will be overwritten by the HEAD.","title":"Using git checkout"},{"location":"Programming/Git/#drop-the-history","text":"Dropping the history can be useful in many cases. For example, we may commit some changes to the master, but then we realize that they belong to a branch. A simple solution is to create a branch, and then reset the master to the previous commit. Note that if the wrong history was already pushed to the remote, we need to fix the history on the remote as well. This is done by force pushing: git push -f","title":"Drop the history"},{"location":"Programming/Git/#removing-specific-files-from-history","text":"Removing files from history can be done using multiple tools: bfg repo cleaner is the simplest tool. It can only select files by filename or size, but that is sufficient in most cases. git filter-repo is a more sophisticated successor to BFG. It can do almost anything possible. Nevertheless, it is less intuitive to operate, and it is harder to analyze its result. The filter-branch command is the original tool for filtering git history. It is slow and problematic, so it should be used only if the above two tools are not available. No matter of the used tool, before you begin: commit and push from all machines, backup the repository Similarly, at the end: It is important not to merge (branch or conflitcs) that originated before the cleanup on other machines. Otherwise, the deleted file can be reintroduced to the history. Pull on other machines. Add the file to gitignore so that it wont be commited again","title":"Removing specific files from history"},{"location":"Programming/Git/#bfg","text":"With BFG, only a file with specific filename can be deleted . It is not possible to use exact file path. To remove file by its name: remove the file locally clone the whole repo again with the --mirror option on the mirrored repo, run the cleaner: bfg --delete-files <FILENAME> run the git commands that appears at the end of the bfg output run git push","title":"BFG"},{"location":"Programming/Git/#git-filter-repo","text":"The git filter-repo can be installed using pip: pip install git-filter-repo . To remove file by its path: run the command with the --dry-run parameter: git filter-repo --invert-paths --force --dry-run --path <PATH TO THE FILE TO BE REMOVED> inspect the changes in .git/filter-repo directory: Compare the files in KDiff3 To skip the lines starting with original-oid : go to the file selection dialog click Configure to the line-matching preprocessor command, add: sed 's/original-oid .*//' click OK run the command without the --dry-run parameter force push the comman to the remote git push origin --force --all If the push is rejected by the remote hook, the master branch is probably protected. It has to be unprotected first in the repository config. Appart from exact file path, we can also use: glob patterns: --path-glob <GLOB PATTERN> regex patterns: --path-regex <REGEX PATTERN> git filter-repo manual More information on github","title":"Git filter-repo"},{"location":"Programming/Git/#filter-branch","text":"git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch <FILE>' --prune-empty --tag-name-filter cat -- --all","title":"filter-branch"},{"location":"Programming/Git/#merging","text":"Syntax: git merge <params> <commit> Merge can be aborted at any time by calling git merge --abort . This resets the state of the repository.","title":"Merging"},{"location":"Programming/Git/#resolving-conflicts","text":"Sometimes, the same file is changed on the same line in both branches we are merging. This is called a conflict. In this case, the conflicting content is marked with conflict markers: <<<<<<< HEAD This is the content of the file in the current branch ======= This is the content of the file in the branch we are merging >>>>>>> branch-a We can resolve the conflict manually by editing the file, but more often, we want to use a merge tool. To do that, we can call: git mergetool The mergetool should be first configured, as the default one (vimdiff) is not very user friendly. To configure the mergetool, we edit the git configuration. The appropriate section is mergetool . It has a lot of options, but for typical mergetool, is enough to set the name, as the mergetool command for that specific merge tool is preconfigured in recent versions of git. Example of the .gitconfig file configured for KDiff3: ... [mergetool \"kdiff3\"] ... More information on git mergetool","title":"Resolving Conflicts"},{"location":"Programming/Git/#debugging-mergetool","text":"Sometimes, this error can occur after calling git mergetool : git-mergetool--lib \"command not found\" This means that the mergetool is misconfigured. Inspect the mergetool section in the .gitconfig file to find the error.","title":"Debugging mergetool"},{"location":"Programming/Git/#merging-moved-files","text":"Sometimes, it's necessary to tweak the command to help it locate the moved files. What can help: -X rename-threshold=25 : Changing the threshold is important for languages that changes the file content when moving it with automatic refactoring (especially important for Java files, which usually have tons of imports) -X ignore-space-change","title":"Merging Moved Files"},{"location":"Programming/Git/#revert-merge","text":"git revert -m 1 Note that when the merge is reverted, the changes cannot be merged again, because they predates the revert!","title":"Revert merge"},{"location":"Programming/Git/#update","text":"On windows, run: git update-git-for-windows","title":"Update"},{"location":"Programming/Git/#pull-requests","text":"Pull requests are a way to propose changes to the repository. The principle is as follows: Create a branch for the changes Commit the changes to the branch and push it to the remote Create a pull request on the remote, that suggest to merge the branch into the master branch There are two possible scenarios: We have the permission to create a branch in the repository: in this case, we can create the branch directly on the remote We do not have the permission to create a branch in the repository: in this case, we have to: Fork the repository Clone the fork Create the branch locally in the fork Commit and push the changes to the fork Create a pull request from the fork to the original repository","title":"Pull Requests"},{"location":"Programming/Git/#update-pull-request","text":"If there are changes requested by the reviewer or we just forgot to add something, we can update the pull request by pushing the updates to the PR branch. The pull request will be automatically updated.","title":"Update pull request"},{"location":"Programming/Git/#showing-the-state-in-the-command-line","text":"Mostly, we use GUI tools to show the state of the repository. However, sometimes we may need to show the state in the command line. The following commands can be useful: git status : shows the state of the repository git log : shows the history of the repository git diff : shows the changes between the working tree and the index","title":"Showing the state in the command line"},{"location":"Programming/Git/#git-log","text":"The git log shows the history of the repository. It is the analogue of the main view in GitExtensions. The most useful parameters are: --oneline : shows the history in a compact form --graph : shows the history as a graph --all : shows the history of all branches, not only the current one --decorate : shows the names of the branches and tags, not only the commit hashes","title":"git log"},{"location":"Programming/Git/#github","text":"","title":"GitHub"},{"location":"Programming/Git/#creating-a-github-release","text":"To create a release: In the repository, under the Releases heading, click Create a new release Click on the Choose a tag dropdown Select an existing tag or create a new one by filling the text field with the tag name and clicking Create new tag button Fill the Release title and Description fields Click Publish release","title":"Creating a GitHub Release"},{"location":"Programming/Git/#github-cli","text":"Github has a CLI tool that can be used to interact with the repository. The tool can be installed from the GitHub CLI page . The main command is gh . Typically, we need to authenticate the tool first by calling gh auth login . There are two options for authentication: browser authentication (default) token authentication: suitable for automation only the old token type can be used, the fine-grained tokens are not supported yet","title":"GitHub CLI"},{"location":"Programming/Git/#token-authentication","text":"To authenticate using a token, we first need to create a token . The token settings are in Profile Settings -> Developer settings -> Personal access tokens . Thereare two types of tokens: classic tokens: legacy access tokens fine-grained tokens: new access tokens with more granular permissions Unless we need some specific permissions available only in the classic tokens, we should use the fine-grained tokens. This is the only option if we want to limit the scope of the token only to certain repositories. To know what specific permissions are needed for a particular command, we can call the command with the --help parameter read the documentation of the command run the command and read the error message ask Then we can authenticate using the token . We can either: suply the token to the gh auth login command: PowerShell gh auth login -h github.com -p ssh --skip-ssh-key --with-token <TOKEN> or set the environment variable GH_TOKEN to the token value and call commands without authentication.","title":"Token Authentication"},{"location":"Programming/Git/#managing-realeases","text":"Required permissions: Read and Write access to code To list releases, we can call the gh release view command. To create a release, we can the gh release create command: gh release create <TAG> --repo <REPO> --title <TITLE> --generate-notes Here, the <REPO> is the full url of the repository, e.g. git@github.com:F-I-D-O/Future-Config.git To delete a release, we can call the gh release delete command: gh release delete <TAG> --repo <REPO> --cleanup-tag -y","title":"Managing realeases"},{"location":"Programming/Git/#synchronizing-a-fork-with-the-original-repository","text":"Required permissions: Read and Write access to code Read and Write access to workflows To synchronize a fork with the original repository, we call: gh repo sync <repo path> -b <branch name> Here, the <repo path> is the path on the github.com domain, e.g. F-I-D-O/Future-Config . We can skip the -b <branch name> parameter if we want to synchronize the default branch.","title":"Synchronizing a fork with the original repository"},{"location":"Programming/Git/#repository-migration","text":"Gitlab to GitHub migration tool","title":"Repository Migration"},{"location":"Programming/Git/#git-large-file-storage","text":"Homepage GitHub documentation Git Large File Storage is a system for storing and versioning large files in git repositories. One of the main use cases is to store experiment code and data in one place. Note that with publicaly available repository hosting services like GitHub, the git LFS has severe limitations preventing almost any practical use. (see limitations ) First, install Git LFS in the system: Run the installer downloaded from the homepage run git lfs install to initialize Git LFS Then to use it in the repository : define for which files Git LFS should be used. For example, by extension: by running git lfs track \"*.png\" directly in the .gitattributes file: add the following line: *.png filter=lfs diff=lfs merge=lfs -text commit all work except the large files, including the changed .gitattributes file where the LFS filters are defined check that the LFS status of the files is correct by running git lfs ls-files (optional) And that's it, now it should work. The only problematic part may be a slower push/pull performance. Note that this simple procedure only affects new files . To affect existing files, we have to distinguish two cases: files added in the incorrect (text) mode that are not in the history yet (added, but not commited). These can be fixed by running add with the --renormalize parameter: bash git add . --renormalize files added before the LFS initialization (already in the history). These can be fixed by running with the migrate subcommand.","title":"Git Large File Storage"},{"location":"Programming/Git/#the-migrate-subcommand","text":"documentation The migrate subcommand of the git lfs command was introduced to modify history. It has three subcommands: import : converts files in history to the large file mode export : converts files in history to the text mode info : shows the information about the files tracked by LFS bash git lfs migrate import --fixup","title":"The migrate subcommand"},{"location":"Programming/Git/#migrate-import","text":"This subcommand is used to convert files in history to the large file mode. There are two possible cases: we need to convert files that were committed before we start working with the LFS. We need to select such files manually, for example by specifying extensions: bash git lfs migrate import --include \"*.png,*.jpg,*.jpeg,*.gif,*.bmp,*.tiff,*.ico,*.webp\" we need to convert files that were committed after we start working with the LFS and the correct .gitattributes file was already in the history. This is much less likely, but it can happen. In this case, we can run the command with the --fixup parameter: bash git lfs migrate import --fixup","title":"migrate import"},{"location":"Programming/Git/#git-lfs-limitations","text":"When working with git LFS, we need to consider some limitations: it slows down the push/pull operations we need to push early, not right before leaving the office the speed can be affected even without any new large files added There are further limitations depending on the remote repository hosting service: GitHub: maximum file size of 2GB forks cannot introduce any new large files Maximum bandwidth per month : 10GB","title":"Git LFS Limitations"},{"location":"Programming/Git/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Programming/Git/#gui-tools-show-no-history-git-log-results-in-errors","text":"This may be due to broken refs in .git/refs/ . Possible errors: your current branch appears to be broken -> the checked out branch is broken fatal: bad object: <ref> -> the ref is broken To fix the issue, we can copy the refs from the .git/logs/refs/ : find the broken file in .git/refs/ find the corresponding file in .git/logs/refs/ copy the hash from the last line, second column and write it to the broken file","title":"GUI tools show no history, git log results in errors"},{"location":"Programming/Git/#ssh-key-passphrase-is-required-despite-the-ssh-key-agent-is-running","text":"This can have multiple reasons. To narrow down the issue, firtst check the ssh command separately. If the ssh does not require a passphrase, but the git does, check whether git uses the same ssh: open git bash run where ssh The first (and only) path should be the same as the one that you are using with the agent, most likely the OpenSSH from Windows. If the git bash bundeled OpenSSH command is first, it means an incorrect git installation.","title":"ssh key passphrase is required, despite the ssh key agent is running"},{"location":"Programming/Gurobi/","text":"Introduction \u00b6 homepage Manual Official Tutorials Running Gurobi CLI \u00b6 There are two gurobi command line tools: gurobi_cl : for solving the optimization problems passed as a file gurobi : for running the interactive shell (ipython-like) Parallel Execution \u00b6 The gurobi solver solves a problem in parallel by default, trying multiple solution methods at the same time (see the official description ). It is also possible to run multiple problems in parallel ( source ), but each problem should be run in its own gurobi environment. Also, each environment should be configured to use only a single thread (e.g., in C++: env.set(GRB_IntParam_Threads, 1); ). The problem with this approach is that the CPU is usually not the bottleneck of the computation, the bottleneck is the memory ( source ). Therefore, solving multiple problems in parallel does not guarantee any speed up, it could be actually slower. The performance could be most likely improved when running the problems in parallel on multiple machines (not multiple cores of the same machine). Some advised to use MPI for that. Model Parameters \u00b6 Reference In some (but not many) cases, we can improve the solution time by tuning the model parameters. The most important parameters are discussed here, for others, see the reference. Cuts \u00b6 Reference Cuts parameter affects the cutting plane generation. Possible values: -1 : automatic cuts (default) 0 : no cuts are generated 1 : moderate cut generation 2 : aggressive cut generation 3 : very aggressive cut generation Thgis parameter can be overridden for a specific cut type through other parameters (e.g., CliqueCuts parameter). PreCrush \u00b6 Reference PreCrush parameter affects the presolve behavior (see Presolve ). 0 : presolve creates models as reduced as possible (default) 1 : presolve preserve mapping between original and reduced model constraints. This setting is only useful if we want to add user-defined cuts. Presolve \u00b6 Reference Explanation The presolve heuristic works the following way: A smaller model is created from the original model together with a mapping of the original variables to the smaller model variables. this model is infeasible if and only if the original model is infeasible if it has an optimal solution, it is the same as the optimal solution of the original model if the mapping is followed, and the solution values are the same The smaller model is solved. The solution of the smaller model is mapped back to the original model. Possible parameters values: -1 : automatic presolve (default) 0 : presolve is disabled 1 : conservative presolve 2 : aggressive presolve Undersanding Gurobi Logs \u00b6 Typically, the Gurobi Solver logs in the following order: Header message logging start message Output log path, if configured Gurobi version CPU info Model info Solution process info (depends on the model type, and configuration) Solution info Solution Process Info \u00b6 The solution process info depends on the model type, and configuration. For a typical MIP model, the process is: Heuristic solution Presolve LP relaxation log Integer solution log LP Log \u00b6 Gurobi uses the Barrier method for solving LPs. The log in order contains (in order): Root barrier log... Ordering Barrier statistics Barrier iteration log The barrier iteration log has the following columns: Iter : iteration number Objective : Primal : Objective value of the primal Dual : Objective value of the dual Residual : Primal : Scaled measure of the primal feasibility error Dual : Scaled measure of the dual feasibility error Compl : Scaled complementarity measure Time : Cumulative time Under default setup, Gurobi typically stops iterating when values are as follows: both residuals are less than 1e-8 the Compl value is less than 1e-10 Gurobi in Python \u00b6 Official Tutorial Advanced Tutorial with dictionaries Tupledict \u00b6 Official Documentation Tupledict is a dictionary that maps tuples to values, typically variables or constraints. Typically, when we add multiple variables or constraints at once to the model, the returned object is a tupledict with indices matching the first arguments of the addVars or addConstrs methods. To further modify the tupledict, we can use the array operators: vdict = model.addVars(3, 2, name='v') vdict[3, 0] = 1 # new variable outside of the index range above Variables \u00b6 The variables are added using the addVar method. The method has six arguments (all optional). Typically, we use the following three arguments: vtype : the type of the variable, one of the following: GRB.CONTINUOUS : continuous variable (default) GRB.BINARY : binary variable GRB.INTEGER : integer variable GRB.SEMICONT : GRB.SEMIINT : obj : the objective function coefficient of the variable, default is 0 name : the name of the variable, default is \"\" Other than that, the arguments are: - lb : the lower bound of the variable, default is 0 - ub : the upper bound of the variable, default is GRB.INFINITY - column : initial coefficients for the column (default is None) The addVar method returns the created variable, representing it as a Var class instance. Add multiple variables at once \u00b6 We can add multiple variables at once using the addVars method. The signature is the kind of similar to the addVar method, the biggest difference is that we have to supply an extra indices parameter that represents the index of the resulting variable collection. The indices parameter is a variable length positional argument . This means that we can enter any number of index arguments. The addVars method then creates a collection of variables that has the same number of dimensions as the number of index arguments, and each dimension will be indexed according to the corresponding index argument. Each index may be specified as: A single value : in that case, the number marks the length of the dimension. A list of values : in that case, the list marks the values of the dimension. A list of tuples : in that case, each tuple specifies a single value for each dimension. This is useful when the index is sparse (contrary to the cartesian product of all indices). Another special argument is the name argument. It is specified as constant, and the indices in the name are generated from the index arguments automatically. Other addVars arguments can be set up in two ways: Using a scalar value: result in a constant value for all variables Using a collection proportional to the index: list of values for one-dimensional index dict with tuples as keys for multi-dimensional index The - The addVars method returns a tupledict object, which is a dictionary that maps the indices to the variables. Constraints \u00b6 The constraints are added using the addConstr method. The method has two arguments: The constraint expression , and The name of the constraint The name of the constraint is straightforward, it can be any ASCII string without spaces (so that we can export the model to LP format). The constraint expression is, however, a complex topic, as: There are multiple types of constraints (e.g., linear, quadratic, indicator, integer, etc.), and Each type can be written in a variety of ways. Here, we demonstrate various ways how to write a linear constraint . Other types of constraints are listed in the TempConstr reference (a class representation of any constraint expression). The linear constraint is in the form of <Expression A> <Operator> <Expression B> , where: <Expression A> and <Expression B> can be both constants or linear expressions and , <Operator> is one of the following: == , <= , >= Add multiple constraints at once \u00b6 We can add multiple constraints at once using the addConstrs method. It has two arguments: The generator , and The name of the constraint The name of the constraint is automatically generated similarly to the addVars method. The generator is a Python generator function that produces a Gurobi constraint expression. Linear Expressions \u00b6 The linear expression is represented by the LinExpr class. There are several ways how to build a linear expression, here sorted from slowest to fastest : Using natural syntax: Python expr = x + 2*y + 3*z this works because of operator overloading on the gurobi variable class using the quicksum Gurobi function Python expr = quicksum([1*x, 2*y, 3*z]) using the add method: Python expr = LinExpr() expr.add(1, x) expr.add(2, y) expr.add(3, z) using the addTerms method of the LinExpr class: Python expr = LinExpr() expr.addTerms([1, 2, 3], [x, y, z]) using the LinExpr constructor : Python expr = LinExpr([1, 2, 3], [x, y, z]) Creating a linear expression from a tupledict \u00b6 Instead of creating linear expression from a list of variables and coefficients, we can use dedicated methods of the tupledict class that can be used to build a linear expression from a single variable type: sum() for the sum of the variables. prod(coeffs) for the product of the variables and coefficients, stored in a dictionary with the same indices as the tupledict. For all these methods, we can use a pattern arguments. This argument limits the indices of the variables that are used in the expression. For example, in the expression bellow, we sum only the first row var_dict = model.addVars(3, 2, name='v') expr = var_dict.sum(0, '*') # sum of the first row: v[0, 0] + v[0, 1]","title":"Gurobi"},{"location":"Programming/Gurobi/#introduction","text":"homepage Manual Official Tutorials","title":"Introduction"},{"location":"Programming/Gurobi/#running-gurobi-cli","text":"There are two gurobi command line tools: gurobi_cl : for solving the optimization problems passed as a file gurobi : for running the interactive shell (ipython-like)","title":"Running Gurobi CLI"},{"location":"Programming/Gurobi/#parallel-execution","text":"The gurobi solver solves a problem in parallel by default, trying multiple solution methods at the same time (see the official description ). It is also possible to run multiple problems in parallel ( source ), but each problem should be run in its own gurobi environment. Also, each environment should be configured to use only a single thread (e.g., in C++: env.set(GRB_IntParam_Threads, 1); ). The problem with this approach is that the CPU is usually not the bottleneck of the computation, the bottleneck is the memory ( source ). Therefore, solving multiple problems in parallel does not guarantee any speed up, it could be actually slower. The performance could be most likely improved when running the problems in parallel on multiple machines (not multiple cores of the same machine). Some advised to use MPI for that.","title":"Parallel Execution"},{"location":"Programming/Gurobi/#model-parameters","text":"Reference In some (but not many) cases, we can improve the solution time by tuning the model parameters. The most important parameters are discussed here, for others, see the reference.","title":"Model Parameters"},{"location":"Programming/Gurobi/#cuts","text":"Reference Cuts parameter affects the cutting plane generation. Possible values: -1 : automatic cuts (default) 0 : no cuts are generated 1 : moderate cut generation 2 : aggressive cut generation 3 : very aggressive cut generation Thgis parameter can be overridden for a specific cut type through other parameters (e.g., CliqueCuts parameter).","title":"Cuts"},{"location":"Programming/Gurobi/#precrush","text":"Reference PreCrush parameter affects the presolve behavior (see Presolve ). 0 : presolve creates models as reduced as possible (default) 1 : presolve preserve mapping between original and reduced model constraints. This setting is only useful if we want to add user-defined cuts.","title":"PreCrush"},{"location":"Programming/Gurobi/#presolve","text":"Reference Explanation The presolve heuristic works the following way: A smaller model is created from the original model together with a mapping of the original variables to the smaller model variables. this model is infeasible if and only if the original model is infeasible if it has an optimal solution, it is the same as the optimal solution of the original model if the mapping is followed, and the solution values are the same The smaller model is solved. The solution of the smaller model is mapped back to the original model. Possible parameters values: -1 : automatic presolve (default) 0 : presolve is disabled 1 : conservative presolve 2 : aggressive presolve","title":"Presolve"},{"location":"Programming/Gurobi/#undersanding-gurobi-logs","text":"Typically, the Gurobi Solver logs in the following order: Header message logging start message Output log path, if configured Gurobi version CPU info Model info Solution process info (depends on the model type, and configuration) Solution info","title":"Undersanding Gurobi Logs"},{"location":"Programming/Gurobi/#solution-process-info","text":"The solution process info depends on the model type, and configuration. For a typical MIP model, the process is: Heuristic solution Presolve LP relaxation log Integer solution log","title":"Solution Process Info"},{"location":"Programming/Gurobi/#lp-log","text":"Gurobi uses the Barrier method for solving LPs. The log in order contains (in order): Root barrier log... Ordering Barrier statistics Barrier iteration log The barrier iteration log has the following columns: Iter : iteration number Objective : Primal : Objective value of the primal Dual : Objective value of the dual Residual : Primal : Scaled measure of the primal feasibility error Dual : Scaled measure of the dual feasibility error Compl : Scaled complementarity measure Time : Cumulative time Under default setup, Gurobi typically stops iterating when values are as follows: both residuals are less than 1e-8 the Compl value is less than 1e-10","title":"LP Log"},{"location":"Programming/Gurobi/#gurobi-in-python","text":"Official Tutorial Advanced Tutorial with dictionaries","title":"Gurobi in Python"},{"location":"Programming/Gurobi/#tupledict","text":"Official Documentation Tupledict is a dictionary that maps tuples to values, typically variables or constraints. Typically, when we add multiple variables or constraints at once to the model, the returned object is a tupledict with indices matching the first arguments of the addVars or addConstrs methods. To further modify the tupledict, we can use the array operators: vdict = model.addVars(3, 2, name='v') vdict[3, 0] = 1 # new variable outside of the index range above","title":"Tupledict"},{"location":"Programming/Gurobi/#variables","text":"The variables are added using the addVar method. The method has six arguments (all optional). Typically, we use the following three arguments: vtype : the type of the variable, one of the following: GRB.CONTINUOUS : continuous variable (default) GRB.BINARY : binary variable GRB.INTEGER : integer variable GRB.SEMICONT : GRB.SEMIINT : obj : the objective function coefficient of the variable, default is 0 name : the name of the variable, default is \"\" Other than that, the arguments are: - lb : the lower bound of the variable, default is 0 - ub : the upper bound of the variable, default is GRB.INFINITY - column : initial coefficients for the column (default is None) The addVar method returns the created variable, representing it as a Var class instance.","title":"Variables"},{"location":"Programming/Gurobi/#add-multiple-variables-at-once","text":"We can add multiple variables at once using the addVars method. The signature is the kind of similar to the addVar method, the biggest difference is that we have to supply an extra indices parameter that represents the index of the resulting variable collection. The indices parameter is a variable length positional argument . This means that we can enter any number of index arguments. The addVars method then creates a collection of variables that has the same number of dimensions as the number of index arguments, and each dimension will be indexed according to the corresponding index argument. Each index may be specified as: A single value : in that case, the number marks the length of the dimension. A list of values : in that case, the list marks the values of the dimension. A list of tuples : in that case, each tuple specifies a single value for each dimension. This is useful when the index is sparse (contrary to the cartesian product of all indices). Another special argument is the name argument. It is specified as constant, and the indices in the name are generated from the index arguments automatically. Other addVars arguments can be set up in two ways: Using a scalar value: result in a constant value for all variables Using a collection proportional to the index: list of values for one-dimensional index dict with tuples as keys for multi-dimensional index The - The addVars method returns a tupledict object, which is a dictionary that maps the indices to the variables.","title":"Add multiple variables at once"},{"location":"Programming/Gurobi/#constraints","text":"The constraints are added using the addConstr method. The method has two arguments: The constraint expression , and The name of the constraint The name of the constraint is straightforward, it can be any ASCII string without spaces (so that we can export the model to LP format). The constraint expression is, however, a complex topic, as: There are multiple types of constraints (e.g., linear, quadratic, indicator, integer, etc.), and Each type can be written in a variety of ways. Here, we demonstrate various ways how to write a linear constraint . Other types of constraints are listed in the TempConstr reference (a class representation of any constraint expression). The linear constraint is in the form of <Expression A> <Operator> <Expression B> , where: <Expression A> and <Expression B> can be both constants or linear expressions and , <Operator> is one of the following: == , <= , >=","title":"Constraints"},{"location":"Programming/Gurobi/#add-multiple-constraints-at-once","text":"We can add multiple constraints at once using the addConstrs method. It has two arguments: The generator , and The name of the constraint The name of the constraint is automatically generated similarly to the addVars method. The generator is a Python generator function that produces a Gurobi constraint expression.","title":"Add multiple constraints at once"},{"location":"Programming/Gurobi/#linear-expressions","text":"The linear expression is represented by the LinExpr class. There are several ways how to build a linear expression, here sorted from slowest to fastest : Using natural syntax: Python expr = x + 2*y + 3*z this works because of operator overloading on the gurobi variable class using the quicksum Gurobi function Python expr = quicksum([1*x, 2*y, 3*z]) using the add method: Python expr = LinExpr() expr.add(1, x) expr.add(2, y) expr.add(3, z) using the addTerms method of the LinExpr class: Python expr = LinExpr() expr.addTerms([1, 2, 3], [x, y, z]) using the LinExpr constructor : Python expr = LinExpr([1, 2, 3], [x, y, z])","title":"Linear Expressions"},{"location":"Programming/Gurobi/#creating-a-linear-expression-from-a-tupledict","text":"Instead of creating linear expression from a list of variables and coefficients, we can use dedicated methods of the tupledict class that can be used to build a linear expression from a single variable type: sum() for the sum of the variables. prod(coeffs) for the product of the variables and coefficients, stored in a dictionary with the same indices as the tupledict. For all these methods, we can use a pattern arguments. This argument limits the indices of the variables that are used in the expression. For example, in the expression bellow, we sum only the first row var_dict = model.addVars(3, 2, name='v') expr = var_dict.sum(0, '*') # sum of the first row: v[0, 0] + v[0, 1]","title":"Creating a linear expression from a tupledict"},{"location":"Programming/HDF/","text":"Hierarchical Data Format (HDF) is a binary data format designed to store large amounts of data. It is widely used and it has support for many programming languages. It is a self-contained format: all metadata needed to read the data is embedded in the file. HDF in C++ \u00b6 HDF in Python \u00b6 HDF in Java \u00b6 In Java, there are many options to work with HDF files. We list some of them in the following table: Name Type Description last update HDF5 wrapper The official wrapper for the HDF5 library. It is a jar file provided by the main installer of HDF5 library. (in the lib folder). The documentation for Java is missing. 2023-10-27 sis-jhdf5 wrapper A wrapper for the HDF5 library developed at ETH Zurich. 2022-07-28 netCDF-Java library for multiple file formats A library that provides an interface to read and write netCDF, HDF5, GRIB, BUFR, and other file formats. daily, last release 2022-07-05 HDFQL query language access to HDF5 A library that provides a SQL-like access to HDF5 files. It has wrappers for Java, Python, C#, Fortran, R. Also, it contains a command-line tool to query HDF5 files. 2023-09-04 JHDF pure Java implementation A pure Java implementation of the HDF5 file format. Currently, it only supports reading HDF5 files. Also, the file size is limited to 32-bit integer size (an array of about 4 billion members) weekly, last release 2023-05-10","title":"HDF"},{"location":"Programming/HDF/#hdf-in-c","text":"","title":"HDF in C++"},{"location":"Programming/HDF/#hdf-in-python","text":"","title":"HDF in Python"},{"location":"Programming/HDF/#hdf-in-java","text":"In Java, there are many options to work with HDF files. We list some of them in the following table: Name Type Description last update HDF5 wrapper The official wrapper for the HDF5 library. It is a jar file provided by the main installer of HDF5 library. (in the lib folder). The documentation for Java is missing. 2023-10-27 sis-jhdf5 wrapper A wrapper for the HDF5 library developed at ETH Zurich. 2022-07-28 netCDF-Java library for multiple file formats A library that provides an interface to read and write netCDF, HDF5, GRIB, BUFR, and other file formats. daily, last release 2022-07-05 HDFQL query language access to HDF5 A library that provides a SQL-like access to HDF5 files. It has wrappers for Java, Python, C#, Fortran, R. Also, it contains a command-line tool to query HDF5 files. 2023-09-04 JHDF pure Java implementation A pure Java implementation of the HDF5 file format. Currently, it only supports reading HDF5 files. Also, the file size is limited to 32-bit integer size (an array of about 4 billion members) weekly, last release 2023-05-10","title":"HDF in Java"},{"location":"Programming/JSON/","text":"Jackson \u00b6 Usefull Annotations \u00b6 @JsonIncludeProperties : Ignore all properties except listed @JsonProperty(\"my_name\") : Custom name of the JSON key @JsonIgnore : Ignore the json property below Wrapping the Obejct in Another JSON Object \u00b6 To do that, use these annotations above the class. @JsonTypeName(value = \"action\") @JsonTypeInfo(include=As.WRAPPER_OBJECT, use=Id.NAME) If you do not care about the name, you can skip the @JsonTypeName annotation. Written with StackEdit .","title":"JSON"},{"location":"Programming/JSON/#jackson","text":"","title":"Jackson"},{"location":"Programming/JSON/#usefull-annotations","text":"@JsonIncludeProperties : Ignore all properties except listed @JsonProperty(\"my_name\") : Custom name of the JSON key @JsonIgnore : Ignore the json property below","title":"Usefull Annotations"},{"location":"Programming/JSON/#wrapping-the-obejct-in-another-json-object","text":"To do that, use these annotations above the class. @JsonTypeName(value = \"action\") @JsonTypeInfo(include=As.WRAPPER_OBJECT, use=Id.NAME) If you do not care about the name, you can skip the @JsonTypeName annotation. Written with StackEdit .","title":"Wrapping the Obejct in Another JSON Object"},{"location":"Programming/Native%20Libraries/","text":"Native libraries are an important part of programming in most languages. They allow us to run the code without the need for an interpreter or virtual machine. Native libraries can be written in any language that can be compiled to machine code. This includes C, C++, Rust, and many others. There are two ways how to link a native library to your code: statically and dynamically: Static libraries are linked at compile time. This means that the library code is included in the final executable. This makes the executable larger, but it also means that the executable is self-contained and does not require the library to be installed on the target system. Dynamic libraries are usually linked at runtime. This means that the library code is not included in the final executable. Instead, the executable loads the library code from a separate file when it is run. This makes the executable smaller, but it also means that the library must be installed on the target system. Each operating system has its own way of loading and using native libraries. Also, The following table shows the extension of the library file for each operating system: Operating System Static Library Extension Dynamic (shared) Library Extension Windows .lib .dll Linux .a .so Deciding between static and dynamic libraries \u00b6 When deciding between static and dynamic libraries, there are a few things to consider: Property Static Libraries Dynamic Libraries Memory footprint Each executable includes a copy of all used library code All executables share the same copy of the library code Modularity None We can extend the functionality of the program by adding new shared libraries Licensing Some licenses (e.g. GPL) require that the source code of the library is made available if the library is linked statically The library can be linked dynamically without having to make the source code available Symbol export All symbols are exported by default Only symbols marked for export are exported Linking of transitive dependencies All transitive dependencies must be linked explicitly Transitive dependencies are loaded automatically at runtime Historically, there were other factors that are not relevant anymore: Size : The historical argument was that dynamic libraries can save disk space because only one copy of the library is needed in the system. However, a) disk space is cheap now and b) dynamic libraries are now distributed with the application on many platforms. Maintenance : The historical argument was that dynamic libraries are easier to maintain because they can be updated without recompiling the application and distributing a new version. However, this is now not true because on many platforms the dynamic libraries are distributed with the application. Portability : The historical argument was that static libraries are more portable because they do not depend on the presence of the library on the target system. However, we can distribute dynamic libraries with the application to mitigate this issue of dynamic libraries. So finally, how to decide between static and dynamic libraries? We create a library that will be used by many applications, and it is large -> dynamic libraries. We use a GPL library and we do not want to make the source code available -> dynamic libraries. Users are expected to only use a fraction of the library -> static libraries. Otherwise -> it does not matter. Runing a program depending on a native library \u00b6 When running a program that depends on a native library, the operating system must be able to find the library. This search has a special logic and, most importantly, there is no guarantee that the library found is the one that the program was compiled and linked against, or that the library will be found at all. A typical problem scenario: The program is compiled and linked against a new version of the library. An older version of the library is installed in a location that is searched before the location of the new version. The library API has changed between the two versions. The program crashes at runtime The search order differs between operating systems. Here, we present only the short (incomplete) order. See official documentation ( Windows ) for full version. On Windows: The directory containing the executable The system directory and the Windows directory The CWD (folder where the program is run) The directories listed in the PATH environment variable in the order they are listed Object Libraries \u00b6 CMake description of object libraries In some computer languages, compilation units are first compiled into object files, and then these object files are linked into a library. This is the case for C and C++. These languages can use these objects as libraries and integrate them into other libraries or executables. Unlike other libraries, object libraries are not linked to the binaries, but directly included. They are similar to source files, just pre-compiled. Static libraries are then archives of object files that are linked.","title":"Native Libraries"},{"location":"Programming/Native%20Libraries/#deciding-between-static-and-dynamic-libraries","text":"When deciding between static and dynamic libraries, there are a few things to consider: Property Static Libraries Dynamic Libraries Memory footprint Each executable includes a copy of all used library code All executables share the same copy of the library code Modularity None We can extend the functionality of the program by adding new shared libraries Licensing Some licenses (e.g. GPL) require that the source code of the library is made available if the library is linked statically The library can be linked dynamically without having to make the source code available Symbol export All symbols are exported by default Only symbols marked for export are exported Linking of transitive dependencies All transitive dependencies must be linked explicitly Transitive dependencies are loaded automatically at runtime Historically, there were other factors that are not relevant anymore: Size : The historical argument was that dynamic libraries can save disk space because only one copy of the library is needed in the system. However, a) disk space is cheap now and b) dynamic libraries are now distributed with the application on many platforms. Maintenance : The historical argument was that dynamic libraries are easier to maintain because they can be updated without recompiling the application and distributing a new version. However, this is now not true because on many platforms the dynamic libraries are distributed with the application. Portability : The historical argument was that static libraries are more portable because they do not depend on the presence of the library on the target system. However, we can distribute dynamic libraries with the application to mitigate this issue of dynamic libraries. So finally, how to decide between static and dynamic libraries? We create a library that will be used by many applications, and it is large -> dynamic libraries. We use a GPL library and we do not want to make the source code available -> dynamic libraries. Users are expected to only use a fraction of the library -> static libraries. Otherwise -> it does not matter.","title":"Deciding between static and dynamic libraries"},{"location":"Programming/Native%20Libraries/#runing-a-program-depending-on-a-native-library","text":"When running a program that depends on a native library, the operating system must be able to find the library. This search has a special logic and, most importantly, there is no guarantee that the library found is the one that the program was compiled and linked against, or that the library will be found at all. A typical problem scenario: The program is compiled and linked against a new version of the library. An older version of the library is installed in a location that is searched before the location of the new version. The library API has changed between the two versions. The program crashes at runtime The search order differs between operating systems. Here, we present only the short (incomplete) order. See official documentation ( Windows ) for full version. On Windows: The directory containing the executable The system directory and the Windows directory The CWD (folder where the program is run) The directories listed in the PATH environment variable in the order they are listed","title":"Runing a program depending on a native library"},{"location":"Programming/Native%20Libraries/#object-libraries","text":"CMake description of object libraries In some computer languages, compilation units are first compiled into object files, and then these object files are linked into a library. This is the case for C and C++. These languages can use these objects as libraries and integrate them into other libraries or executables. Unlike other libraries, object libraries are not linked to the binaries, but directly included. They are similar to source files, just pre-compiled. Static libraries are then archives of object files that are linked.","title":"Object Libraries"},{"location":"Programming/Neural%20Networks/","text":"Introduction \u00b6","title":"Neural Networks"},{"location":"Programming/Neural%20Networks/#introduction","text":"","title":"Introduction"},{"location":"Programming/Perl/","text":"Introduction \u00b6 home wikipedia Perl is an interpreted language. Installation \u00b6 Windows: Strawberry Perl Installing dependencies \u00b6 The dependencies are installed using cpan <module> command. The full path to the module is required, for example to install pgTAP module, use: cpanm TAP::Parser::SourceHandler::pgTAP To browse the list of available modules, we can use the web interface .","title":"Perl"},{"location":"Programming/Perl/#introduction","text":"home wikipedia Perl is an interpreted language.","title":"Introduction"},{"location":"Programming/Perl/#installation","text":"Windows: Strawberry Perl","title":"Installation"},{"location":"Programming/Perl/#installing-dependencies","text":"The dependencies are installed using cpan <module> command. The full path to the module is required, for example to install pgTAP module, use: cpanm TAP::Parser::SourceHandler::pgTAP To browse the list of available modules, we can use the web interface .","title":"Installing dependencies"},{"location":"Programming/Regex/","text":"Symbol meaning \u00b6 . any character [xyz] one of these characters [c-j] any character between c and j . We can combine this and the previou syntax, e.g.: [az0-4jsd] . Note that the minus sign is interpreted as a range only if it is between two characters. [^c-g] ^ means negation: anything except the following set of characters. Note that the negation( ^ ) sign needs to be the first character in the bracket. \\ : escape character. | means OR. It has the lowest precedence, so it is evaluated last. ? lazy quantifier. It will try to match as few characters as possible (i.e., previous pattern will try to match only till the next patern matches). ?R recursive pattern. Quantifiers \u00b6 * : zero or more + : one or more ? zero or one {<from>, <to>} between from and to times. If to is omitted, it means infinity. If from is omitted, it means zero. If there is only one number, it means exact count. If both are omitted, it means one. Anchors \u00b6 ^x must start with x x$ must end with x Groups and Lookarounds \u00b6 () capture group. We can refer to it later, either in the regex, or in the result of the match, depending on the programming language. The nubering starts from 1, the 0 group is usually the whole match. in the regex we refer to group using \\1 , \\2 , etc. (?:) non-caputing group. It is useful when we want to use the quantifiers on a group, but we don't want to capture it. (?=) positive lookahead. It will try to match the pattern, but it will not consume it. (?!) negative lookahead. It is useful when we want to match a pattern, but we don't want to consume it. (?<=) positive lookbehind. Same as positive lookahead, but it looks behind. (?<!) negative lookbehind. Same as negative lookahead, but it looks behind. Capture the same group multiple times \u00b6 Sometimes, it would be useful to capture the same pattern multiple times in a separate groups dynamically, without specifying the number of repetitions. Unfortunately, this is not possible with just regex. Instead we need to specify the pattern and use the programming language to capture the group multiple times (e.g., in Python, we can use re.findall() ). Principles \u00b6 Non-capturing groups \u00b6 Non-capturing groups are groups that helps to specify the match but they are not captured. They are useful when we want to use the group content to specify the match, but we don't want to capture/consume the group. Some of them can be replaced, but usually with a more complicated regex. All of the non-capturing groups start with (? and end with ) . The ? is followed by a character that specifies the type of the group. The most common are: ?: non-capturing group ?= positive lookahead ?! negative lookahead ?<= positive lookbehind ?<! negative lookbehind The actual content of the group is specified between the group type specifier (e.g., ?= ) and the closing bracket ( ) ). Example: (?=d)a This regex will match a only if it is followed by d . The d will not be consumed. Note that some regex engines don't support variable length lookbehind . To overcome this, we can use the following tricks: use multiple lookbehinds with fixed length construct a more complicated regex that will match the same thing place a marker with one regex replace and then use the lookbehind to match the marker Examples \u00b6 Any whitespace \u00b6 /[\\x{00a0}\\s]/u Non-breaking space \u00b6 ((?!&nbsp;)[^\\s\\x{00a0}]) Transform Google sheet to latex table \u00b6 naj\u00edt ([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n nahradit \\1 & \\2 & \\3 & \\4 \\\\\\\\\\r\\n CSV to Latex \u00b6 Search: ([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,\\r\\n]*)\\n Replace \\1 & \\2 & \\3 & \\4 & \\5 & \\6 & \\7 \\\\\\\\\\r\\n Name regex \u00b6 ([A\u00c1BC\u010cDEFGHIJKLMNOPQR\u0158S\u0160TU\u00daVWXYZ\u017d]{1}[a\u00e1bc\u010dd\u010fe\u00e9\u011bfghchi\u00edjklmn\u0148o\u00f3pqr\u0159s\u0161t\u0165u\u00fa\u016fvwxy\u00fdz\u017ew]+ *){2,3} Archive \u00b6 vasdomovnik \u00b6 naj\u00edt [0-9]+[ ]*([^\\r\\n]*)[\\r\\n]+ nahradit '\\1' , sloupce na pole php \u00b6 naj\u00edt ([^\\r\\n]*)([\\r\\n])+ nahradit '\\1' => ''\\2, Pogamut - jen logy od jednoho bota \u00b6 \\(TeamCTF[^2][^\\r\\n]*\\r\\n nahradit za pr\u00e1zdno","title":"Regex"},{"location":"Programming/Regex/#symbol-meaning","text":". any character [xyz] one of these characters [c-j] any character between c and j . We can combine this and the previou syntax, e.g.: [az0-4jsd] . Note that the minus sign is interpreted as a range only if it is between two characters. [^c-g] ^ means negation: anything except the following set of characters. Note that the negation( ^ ) sign needs to be the first character in the bracket. \\ : escape character. | means OR. It has the lowest precedence, so it is evaluated last. ? lazy quantifier. It will try to match as few characters as possible (i.e., previous pattern will try to match only till the next patern matches). ?R recursive pattern.","title":"Symbol meaning"},{"location":"Programming/Regex/#quantifiers","text":"* : zero or more + : one or more ? zero or one {<from>, <to>} between from and to times. If to is omitted, it means infinity. If from is omitted, it means zero. If there is only one number, it means exact count. If both are omitted, it means one.","title":"Quantifiers"},{"location":"Programming/Regex/#anchors","text":"^x must start with x x$ must end with x","title":"Anchors"},{"location":"Programming/Regex/#groups-and-lookarounds","text":"() capture group. We can refer to it later, either in the regex, or in the result of the match, depending on the programming language. The nubering starts from 1, the 0 group is usually the whole match. in the regex we refer to group using \\1 , \\2 , etc. (?:) non-caputing group. It is useful when we want to use the quantifiers on a group, but we don't want to capture it. (?=) positive lookahead. It will try to match the pattern, but it will not consume it. (?!) negative lookahead. It is useful when we want to match a pattern, but we don't want to consume it. (?<=) positive lookbehind. Same as positive lookahead, but it looks behind. (?<!) negative lookbehind. Same as negative lookahead, but it looks behind.","title":"Groups and Lookarounds"},{"location":"Programming/Regex/#capture-the-same-group-multiple-times","text":"Sometimes, it would be useful to capture the same pattern multiple times in a separate groups dynamically, without specifying the number of repetitions. Unfortunately, this is not possible with just regex. Instead we need to specify the pattern and use the programming language to capture the group multiple times (e.g., in Python, we can use re.findall() ).","title":"Capture the same group multiple times"},{"location":"Programming/Regex/#principles","text":"","title":"Principles"},{"location":"Programming/Regex/#non-capturing-groups","text":"Non-capturing groups are groups that helps to specify the match but they are not captured. They are useful when we want to use the group content to specify the match, but we don't want to capture/consume the group. Some of them can be replaced, but usually with a more complicated regex. All of the non-capturing groups start with (? and end with ) . The ? is followed by a character that specifies the type of the group. The most common are: ?: non-capturing group ?= positive lookahead ?! negative lookahead ?<= positive lookbehind ?<! negative lookbehind The actual content of the group is specified between the group type specifier (e.g., ?= ) and the closing bracket ( ) ). Example: (?=d)a This regex will match a only if it is followed by d . The d will not be consumed. Note that some regex engines don't support variable length lookbehind . To overcome this, we can use the following tricks: use multiple lookbehinds with fixed length construct a more complicated regex that will match the same thing place a marker with one regex replace and then use the lookbehind to match the marker","title":"Non-capturing groups"},{"location":"Programming/Regex/#examples","text":"","title":"Examples"},{"location":"Programming/Regex/#any-whitespace","text":"/[\\x{00a0}\\s]/u","title":"Any whitespace"},{"location":"Programming/Regex/#non-breaking-space","text":"((?!&nbsp;)[^\\s\\x{00a0}])","title":"Non-breaking space"},{"location":"Programming/Regex/#transform-google-sheet-to-latex-table","text":"naj\u00edt ([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n nahradit \\1 & \\2 & \\3 & \\4 \\\\\\\\\\r\\n","title":"Transform Google sheet to latex table"},{"location":"Programming/Regex/#csv-to-latex","text":"Search: ([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,\\r\\n]*)\\n Replace \\1 & \\2 & \\3 & \\4 & \\5 & \\6 & \\7 \\\\\\\\\\r\\n","title":"CSV to Latex"},{"location":"Programming/Regex/#name-regex","text":"([A\u00c1BC\u010cDEFGHIJKLMNOPQR\u0158S\u0160TU\u00daVWXYZ\u017d]{1}[a\u00e1bc\u010dd\u010fe\u00e9\u011bfghchi\u00edjklmn\u0148o\u00f3pqr\u0159s\u0161t\u0165u\u00fa\u016fvwxy\u00fdz\u017ew]+ *){2,3}","title":"Name regex"},{"location":"Programming/Regex/#archive","text":"","title":"Archive"},{"location":"Programming/Regex/#vasdomovnik","text":"naj\u00edt [0-9]+[ ]*([^\\r\\n]*)[\\r\\n]+ nahradit '\\1' ,","title":"vasdomovnik"},{"location":"Programming/Regex/#sloupce-na-pole-php","text":"naj\u00edt ([^\\r\\n]*)([\\r\\n])+ nahradit '\\1' => ''\\2,","title":"sloupce na pole php"},{"location":"Programming/Regex/#pogamut-jen-logy-od-jednoho-bota","text":"\\(TeamCTF[^2][^\\r\\n]*\\r\\n nahradit za pr\u00e1zdno","title":"Pogamut - jen logy od jednoho bota"},{"location":"Programming/Ruby%20Workflow/","text":"Classical workflow is to use: official Ruby distribution Bundler to manage dependencies Installation \u00b6 Windows \u00b6 Download and run Ruby installer use the versin with DevKit at the end, a command prompt will open, confitm the prompt with Enter Project setup \u00b6 The project configuration is stored in the Gemfile file. It contains a list of dependencies, which are installed using the bundle command. Typically, the file contains the following lines: source \"https://rubygems.org\" # the source of the gems gem \"jekyll\" # the gem to install Gems \u00b6 Gems are packages for Ruby. They can be installed using the gem command, but moslty, they are installed as dependencies using the bundle command. The gem specification in the Gemfile contains the following parameters (split by spaces): gem : the name of the gem (required) version : the version of the gem group : the group of the gem. It can be used run a command only for a specific group. For example, bundle install --without development will not install gems from the development group. The group parameter is a new syntax, the old syntax is to use the group command: group :development do gem \"jekyll\" end","title":"Ruby Workflow"},{"location":"Programming/Ruby%20Workflow/#installation","text":"","title":"Installation"},{"location":"Programming/Ruby%20Workflow/#windows","text":"Download and run Ruby installer use the versin with DevKit at the end, a command prompt will open, confitm the prompt with Enter","title":"Windows"},{"location":"Programming/Ruby%20Workflow/#project-setup","text":"The project configuration is stored in the Gemfile file. It contains a list of dependencies, which are installed using the bundle command. Typically, the file contains the following lines: source \"https://rubygems.org\" # the source of the gems gem \"jekyll\" # the gem to install","title":"Project setup"},{"location":"Programming/Ruby%20Workflow/#gems","text":"Gems are packages for Ruby. They can be installed using the gem command, but moslty, they are installed as dependencies using the bundle command. The gem specification in the Gemfile contains the following parameters (split by spaces): gem : the name of the gem (required) version : the version of the gem group : the group of the gem. It can be used run a command only for a specific group. For example, bundle install --without development will not install gems from the development group. The group parameter is a new syntax, the old syntax is to use the group command: group :development do gem \"jekyll\" end","title":"Gems"},{"location":"Programming/C%2B%2B/C%20Manual/","text":"Standard Library \u00b6 Standards \u00b6 Unlike C++ standards that are released every 3 years, C standard releases are not regular, and they are less frequent. Moreover, the support of the new features is only poorly documented. The C compiler support on cppreference only covers the C99 and C23 standards. Therefore, it is better to consult the documentation of each individual compiler: GCC: C99 Status C11 Status The standards are: C89: The original standard, sometimes referred to as ANSI C. C95: wchars, alternative logical operators C99: long long and other new types, variable-length arrays, removed several dangerous features from C89 like implicit int or implicit function declaration. C11: improved unicode support, cross-platform multithreading, atomic types C17: only address defects from C11 C23: auto , new string functions, typeof standardized String functions \u00b6 Copying strings \u00b6 For copying strings, there are the following functions: strcpy : strcpy(<destination>, <source>) copies the string source to destination . strncpy : strncpy(<destination>, <source>, <count>) copies at most count characters from source to destination . note that this function was never intended to be used for copying strings as we know them today, but rather for copying old fixed-length strings. It is unsafe even for the C standards and should not ever be used. ( source ) Additionally, the optional part of the C11 standard introduced more secure versions of the string functions, with the suffix _s : strncpy_s However, the _s functions should be avoided as they are not supported by some major compilers ( e.g. GCC ). Some rational why these functions are problematic can be found in: the document Field Experience With Annex K . this Codidact answer This reddit post This StackOverflow answer","title":"C Manual"},{"location":"Programming/C%2B%2B/C%20Manual/#standard-library","text":"","title":"Standard Library"},{"location":"Programming/C%2B%2B/C%20Manual/#standards","text":"Unlike C++ standards that are released every 3 years, C standard releases are not regular, and they are less frequent. Moreover, the support of the new features is only poorly documented. The C compiler support on cppreference only covers the C99 and C23 standards. Therefore, it is better to consult the documentation of each individual compiler: GCC: C99 Status C11 Status The standards are: C89: The original standard, sometimes referred to as ANSI C. C95: wchars, alternative logical operators C99: long long and other new types, variable-length arrays, removed several dangerous features from C89 like implicit int or implicit function declaration. C11: improved unicode support, cross-platform multithreading, atomic types C17: only address defects from C11 C23: auto , new string functions, typeof standardized","title":"Standards"},{"location":"Programming/C%2B%2B/C%20Manual/#string-functions","text":"","title":"String functions"},{"location":"Programming/C%2B%2B/C%20Manual/#copying-strings","text":"For copying strings, there are the following functions: strcpy : strcpy(<destination>, <source>) copies the string source to destination . strncpy : strncpy(<destination>, <source>, <count>) copies at most count characters from source to destination . note that this function was never intended to be used for copying strings as we know them today, but rather for copying old fixed-length strings. It is unsafe even for the C standards and should not ever be used. ( source ) Additionally, the optional part of the C11 standard introduced more secure versions of the string functions, with the suffix _s : strncpy_s However, the _s functions should be avoided as they are not supported by some major compilers ( e.g. GCC ). Some rational why these functions are problematic can be found in: the document Field Experience With Annex K . this Codidact answer This reddit post This StackOverflow answer","title":"Copying strings"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/","text":"Compiler Errors \u00b6 General rules \u00b6 Make the code you want to compile reachable. In C++, only reachable methods/classes are compiled! Solve errors that stop the compilation first. Warnings can stay in logs even if solved until the main error is gone and the build is finished Be aware that the cause of the error can be on a different line than the one in the error log! If the source of the compilation bug cannot be found \u00b6 read the error examples below check that the code follow the my guidelines and C++ Core Guidelines . read the cpp reference for the parts of the problematic code check the const correctness . It is a causes a lot of problems. try a different compiler, the error message can be more informative try to isolate the case in some small example copy the project and remove the stuff until the problem is gone Practical static assertions \u00b6 Test for concept satisfaction: static_assert(My_concept<My_class>); Useful Predicates \u00b6 std::is_same_as for checking the type equality. Determining type at compile time \u00b6 Sometimes, it is practical to know the exact type during compile time. There is no direct way for that, but we can trick the compiler to print the type in an error message: template <typename...> struct Get_type; class Something {}; Get_type<<TYPE TO GET>> my_type{}; This should print an error message similar to: error: variable \u2018Get_type<<TYPE TO GET RESOLVED>> my_type\u2019 has initializer but incomplete type . Errors with a missing copy constructor \u00b6 e.g. error C2280: 'Solution<N>::Solution(const Solution<N> &)': attempting to reference a deleted function These can be split into two groups: We want to copy the object, but the copy constructor is missing We do not want to copy the object, but the copy constructor is still called The firs case can be resolved easily. Below, we discuss the second case. Copy constructor is called against the will \u00b6 There are two possible reasons for this: We perform an operation that requires a copy without realizing it. In this case, check the scenarios that can cause the copy in the C++ Manual Especially take care in case of STL collections, as those can display the error in the place of template instantiation, not in the place of the copy (see the Collection Manual ). We perform an operation that requires a move, but the move constructor is not available. In this case, change the code so that the move constructor is defined. To check if the class is move-constructible: cpp static_assert(std::is_move_constructible<Solution<Cordeau_node>>::value); Possible reasons, why the move constructor is not available: Move constructor is not implicitly declared due to a broken rule of five, i.e., one of the other constructors/assignments/destructors is defined Implicitly declared move constructor is deleted. Possible reasons: the class have a member that cannot be moved from const members Errors with missing move constructor \u00b6 First, we should check whether the object's type T is movable using static_assert(std::is_move_constructible_v<T>) If the static assertion is false: Check whether all base classes has move constructors available the std::is_move_constructible_v<T> cannot be used for that, as the move operations can be protected. Instead, look for the presence of the move constructors in base classes (any base class should have them declared, as the implicit declaration of copy/move/destruction does not work with virtual classes) Check whether all class members are move constructible using the std::is_move_constructible_v<T> concept. Do not forget the const qualifiers! Multiply Defined Symbols \u00b6 e.g. name already used for a template in the current scope. The source of the duplicate should be in the compiler output. Usually, this can be solved by using namespaces. Conversion is inaccessible \u00b6 This mostly happens if we forgot to add the public keyword when inheriting from a base class, resulting in the (default) private inheritance. Cannot convert from 'initializer list' to... \u00b6 This happans when there is no function signature matching the arguments. Sometimes, the specific argument can be found by using constructor instead of initializer list. Otherwise, check the arguments one by one. For each argument: check if the type matches check if the value type matches, i.e. value/ref/pointer check if the const matches check if the problem is not in wrong template argument deduction. The automatic deduction can use value type instead of reference type... Returning address of local variable or temporary \u00b6 This happens when we assign a lambda to std::function and the labda return by reference and does not have an explicit return type. The solution is to add an explicit return type. Cannot resolve symbol \u00b6 or alternatively: 'identifier' was unexpected here; expected 'type specifier' . It simply means that the type cannot be reolved from the code location. Possible reasons: Circular dinclude Linker Errors \u00b6 Undefined \u00b6 Something like unresolved external symbol... . For symbols that should come from your code: check that all used files are listed in CMakeLists.txt ( add_executable command\u2026) check that all templates are defined in header files check that all functions are defined correctly (even unsigned vs unsigned int can make problems...) For symbols that should come from a library: check that all necessary libraries are linked in CMakeLists.txt check that all libraries configured for linking in CMakeLists.txt are available on the system if the compiler environment may have changed since the last build (e.g., system update, compiler update, or when running the build on remote server), try to erase the build directory and configure the project again If none of the above works, check this list: https://stackoverflow.com/questions/12573816/what-is-an-undefined-reference-unresolved-external-symbol-error-and-how-do-i-fix? Multiply Defined \u00b6 Check the recent includes, are all of them correct? Check the multiply defined symbol. If it is a function defined in a header file, it has to be static or inline. there can be a bug even in an external library! https://stackoverflow.com/questions/30180145/general-techniques-for-debugging-the-multiple-definition-of-error LINK : fatal error LNK1181: cannot open input file \u00b6 The error LNK1181 means that the linker cannot find the specified library file. To daiagnose this, look at how the file is specified in the target's vxproj file: Open the target's vxproj file in a text editor Find the Link element Check the AdditionalDependencies attribute of the Link element. The path to the library file should be there. With exception of the standard libraries, the path should be either absolute (for installed libraries, like vcpkg libraries) or relative to the project directory (for libraries that are distributed with the project). Two cases can happen: The path looks correct: double check that hte library file is present on the specified path The path is incorrect, e.g., the library is specified only by its name. This means that the vxproj file is not generated correctly, i.e., the problem is in the configuration phase. note that CMake does not check the validity of the targets specified in the target_link_libraries command! Runtime errors \u00b6 First, identify the exception context . To do that, look at the line where the exception is thrown. If the throwing line is not on the call stack, it is possible that the debugger does not break on the particular exception type. to change that go to the Exception Settings and check the exception type there. If the cause of the exception is not clear from the context, it may be usefull to check the exception details . First, look at the exception message. The easiest way is to catch the exception in the code and print the message. In Google Test, there is a catch-all handler, just run the test without the --gtest_break_on_failure flag. If the message is not enough, look at the exception content in the debugger. Unfortunately, it is not possible to inspect unhandled exception object easily. To do so, add the following watch: (<exception type>*) <exception address> where <exception type> is the type of the exception (e.g. std::exception ) and <exception address> is the address from the exception dialog. Finally, if the cause of the exception is still unclear, look at the exception type, and proceed to the respective section below, if there is one. Error codes \u00b6 Error codes are the only thing visible when the exception is not caught and no debugger is attached. Unfortunately, the error codes are platform-specific, and mostly undocumented even for operating system facilities and standard libraries. Windows Error Codes \u00b6 On windows, many error codes can be emitted by the system or standard libraries: 0x0 to 0x3e7f : Win32 error codes : Errors emitted by Windows high-level functionalities 0xC0000000 to 0xCFFFFFFF : NT status codes : Standardized 32-bit error codes used in Windows kernel, drivers, and protocols. Notable examples: 0x00000003 : STATUS_BREAKPOINT ( {EXCEPTION} Breakpoint A breakpoint has been reached. ) 0xC0000005 : Access violation ( The instruction at 0x%08lx referenced memory at 0x%08lx. The memory could not be %s. ) 0xC0000142 : STATUS_DLL_INIT_FAILED: Initialization of a dynamic link library failed. The process is terminating abnormally. This means that all linked libraries are available at runtime, but some of them failed to initialize, which may be due to a missing dependency (between the linked library and another library not linked to the executable) 0xC0000374 : Heap corruption ( A heap has been corrupted. ) Missing dll \u00b6 When a dll file is missing, we can temprorarily fix the problem by copying the dll file to the same directory as the executable. However, it is best to investigate the cause, which we discuss in this section. The most common cause is that we do not set the target to copy the required runtime dependencies to the output directory. This is necessary even when building with CMake: CMake does not copy the runtime dependencies automatically! To configure CMake to copy the runtime dependencies, see the CMake Manual . If this command is present and copies some runtime dependencies to the output directory, but not all of them, we need to investigate further: determine why the missing library is required (if we do not know immediately...) fix the dependency so it is recognized by CMake if the library is required by our targets, we have some linking misconfiguration -> we have to fix it if the library is required by another library (transitive dependency), the bug is probably there. We cannot fix it, but we can report it and manually link the missing library to the library that requires it in our project (see the CMake Manual for details). Showing the runtime dependency tree \u00b6 First, we should check the CMake dependencies. We can see them using the --graphviz argument of the cmake command: cmake --graphviz=dependencies.dot In the dependencies.dot file, we can see the dependency tree of visible cmake dependencies, i.e., the libraries that: are linked in cmake using the target_link_libraries command (in contrast to command line linking, etc), and are linked publicly (i.e., with the PUBLIC or INTERFACE keyword, not PRIVATE ) If the missing library is not in this dependency tree, we should inspect the executable for the dependencies. We can do that using the Dependencies tool . Download and extract the tool Run the DependenciesGui.exe Drag and drop the executable to the tool Memory Errors \u00b6 These exception are raised when an unallocated memory is accesed. The following signalize a memory error: Read Access Violation HEAP CORRUPTION DETECTED First, most of the memory errors can be caught by various assertions and guards in the debug mode. If possible, try to run the program in the debugg mode, even if it takes a long time, because this way, you can catch the problem when it happens, before the memory is corrupted. If that does not help, read the following sections. Other reassons are also discussed here Accessing null pointer \u00b6 A frequent cause of memory errors is accessing a null pointer object's data. In this case, the cause of the problem can be quickly determined in the debugger. Just follow the lifetime of the pointer and find the momemnt when it becom null. Read Access Violation Caused by a Demaged vtable \u00b6 In case of some previous memory mismanagement, the heap can be demaged, possibly resulting in a corrupted virtual table for objects on the heap. To check whether the virtual table is corrupted, add the following watch to the debugger: <var name>.__vfptr Where <var name> is the name of the object you want to inspect. To resolve this problem, see debugging memory errors. Using Application Verifier to find causes of memory related errors. \u00b6 A lot of memory errors can be caught just by running the program in the debugger. The STL containers, for example, containst various assertions that break the code on wrong memory access related to these collections. To add even more assert guards (e.g., for dynamic arrays), we can use the Application Verifier which is installed as a part of Windows SDK (which is typically installed together with Visual Studio). To debug the application with the Application verifier enabled: Open AV right click -> add executable and select the executable to test select the appropriete test suite (the basic one is enouh for the memory testing) click save close AV run the executable in the debugger, find the problem, fix it open AV delete the exectable from the list Using Address Sanitizer \u00b6 A linux memory tool called address sanitizer can be used to debug memory related errors. To use it from the Visual Studio: Check that the libasan lib is installed on WSL In Cmake Settings for the debug configuration, check Enable AddressSanitizer build the project run The program should now break on the first problem. The details are displayed in the output window More at Microsoft learn Using Valgrind to debug memory errors \u00b6 Valgrind is a toolbox for debugging C/C++ code. The most famous tool is calle Memcheck and is intended for memory error detection. Basic usage: valgrind --leak-check=yes <program> <program arguments> The explanation of the error messages can be found on the Valgrind website The most common errors and tips: Conditional jump or move depends on uninitialised value : triggers on the first usage (not copy) of the uninitialized data note that the uninitialized variable can look normal (e.g. if it is a number), just the value is random. Invalid read of size ... : can happen to both stack and heap memory the content can still be in memory, it just means that the memory has been freed/invalidated. There are some expected messeges not to be worried about: Warning: set address range perms: large range Logical Errors \u00b6 C++ Specific Numerical Errors \u00b6 First possible error is overflow . C++ does not handle or report overflow! The behaviour is undefined. Second potential danger is the unsigned integer overflow . In case the result below zero is stored in unsigned integer, the number is wrapped around, resulting in large positive integer. Another thing is that when signed and unsigned integers are used in one operation, the signed integer is implicitely converted to unsigned integer before the operation! This is called promotion and it also works for other types (see in a table on SO ). In general to prevent the overflow: check each operation for the potential overflow, inluding the unsigned integer overflow with negative numbers if the overflow can happen, cast the types before any arithmetic operation to prevent the overflow also, one have to use the right type in templates like std::max Static code analysis \u00b6 Static code analysis is a process of checking the code without running it. The basic static analysis is available in IDEs, or as a part of the compiler. However, there are also more advanced tools available that can check the code for various problems, including: memory leaks and memory errors undefined behaviour dead code code style Cppcheck \u00b6 Cppcheck is a GUI tool that can be used to check the code for various problems. It is available for Windows and Linux. Testing - Google Test \u00b6 Google test is a c++ testing framework that can be used to write unit tests. As it is not officially shipped with vcpkg, we have to use FetchContent to download it. The basic usage is as follows: Add the googletest to the project using FetchContent Add the test target using add_executable Link the test target with the gtest and gtest_main targets Optionally, if we want the tests to be run automatically by the ctest , we add the gtest_discover_tests command to the CMakeLists.txt . Debugging tests \u00b6 For test debugging, some google test options may be usefull: --gtest_break_on_failure : breaks the test on a failed test or failed assertion. --gtest_catch_exceptions=0 : stops google test from catching exceptions, hence the program crashes on an unhandled exception. Without this, we cannot use breakpoints on unhandled exceptions. Note that this way, we will not see the exception message, as C++ default exception handler does not print the message. --gtest_filter=Some_test_prefix* : filter tests by name. Astrix ( * ) can be used as a wildcard for both prefix and suffix. The pattern is <test_suite_name>.<test_name> . Visual Studio Errors \u00b6 False errors appears in a file / in many files \u00b6 close visual studio delete the out folder open the visual studio again Refactoring options not present \u00b6 If the refactoring options like change signature are not present, try to send the Resharper bug reports :), or create a new project. IntalliSense false errors \u00b6 Sometimes, the errors disappears by deleting the .suo file located in <project dir>/.vs/<project name>/<VS version> Using the debugger \u00b6 Clion \u00b6 Unlike in Visual Studio, the Clion debugger does not break be default. To break on exceptions or breakpoints, we need to use the debug button instead of the run button. To debug multiple targets at once: Open the Run/Debug Configurations dialog Add a new configuration of type Compound Add the configurations you want to run together using the + button Debug the compound configuration Visual Studio \u00b6 Improve debugger experience with natvis \u00b6 Natvis is a visualization system that enables to enhance the presentation of variables in debugger locals or watch windows. It is a XML file, where we can define visualization rules for any type. Structure: <Type Name=\"My_class\"> ... visualization rules </Type> The name must be fully qualified, i.e., we have to include namespaces. The big advantage is that natvis files can be changed while the debugger is running and the presentation of the type in locals/watch window is changed immediatly after saving the natvis file. Natvis expressions \u00b6 The expression in natvis are sorounded by {} . If we want curly braces in the text, we can double them {{...}} . Unfortunatelly, function calles cannot be used in natvis expressions . Natvis Errors \u00b6 Natvis errors can be displayed in the output window if turned on in settins: Debug -> Options -> Debugging -> Output Window . Existing visualisations \u00b6 Existing natvis files are stored in <VS Installation Folder>\\Common7\\Packages\\Debugger\\Visualizers folder. The STL visualizations are in stl.natvis Debugging polymorphic classes \u00b6 Unfortunately, the debugger does not show the actual type of the object when the object is cast to a base class. To see the object content including the members of the derived class, we have to cast the object to the derived class in the watch window: ((Derived_class*) object) for pointers ((Derived_class&) object) for references Address breakpoints \u00b6 Address breakpoints can be used to watch a change of a variable or in general, a change of any memory location. To set an address breakpoint, we nned to first find the address of the variable. To do that, we can: use the & operator on the variable in the watch window use the & operator on the variable in the immediate window The address should have a format 0x0000000000000000 . Profiling \u00b6 There are multiple profiler options for C++ , but not all the tools are easy to use or maintained. CPU Profiling \u00b6 Visual Studio \u00b6 In visual studio: Run the program and either wait for breakpoint hit or manually pause the execution In Diagnostic Tools tab, hit the Enable CPU Profiling button Unpause the execution Pause the execution again, the profiler results covering the period between resume and pause should be available CLion \u00b6 documentation TL;DR: The profiler does not work in WSL. CLion profiler is based on the perf tool and therefore it is available only on Linux (and WSL). Currently I have a problem with the profiler: after clicking the buytton to stop profiling and show the results, the profiler window just shows a message No profiler data . Clion issue . Old CLion WSL issue \u00b6 However, in WSL, currently the profiler does not work (the profiler immediatelly terminates with an unknown error). Fortunatelly, there is a workaround (described in another issue ): - open the Clion registry: Help -> Find Action -> Registry - disable the wsl.use.remote.agent.for.launch.processes option VTune \u00b6 VTune can be run ftom the Visual Studio only for VS solution projects. In case of CMake projects, we need to run the VTune GUI and configure the debugging there. Memory Profiling \u00b6 For memory profiling to work, two things needs to be taken care of: if the application allocates a lot memory inside parallel region, disable paralelization for profiling. Otherwise, there can be too many allocation events for the profiler to handle if you use a custom memory allocator, disable it and use a standard allocator for memory profiling Memory Profiling in Visual Studio \u00b6 To profile memory in Visul Studio Set a breakpoint before the region of interest Run the app and wait for the hit In Diagnostic Tools tab -> Summary , click Enable heap profiling for next session Restart the app and wait for the hit. Take memory snapshot Add a breakpoint to the end of the region of interest Wait for the hit, take snapshot and check both snapshots Showing size, alignment, and memory layout of structures \u00b6 In both CLion and Visual Studio, the size and alignment of the structure is displayed in the tooltip when hovering over the structure name, or member name. Additionally, Visual Studio shows the memory layout if we click the appropriate button in the tooltip. Visual Studio tutorial Showing Linked Libraries \u00b6 To show dynamically linked libraries, we can use the Process Explorer tool. Find the process of the running executable View -> Lower Pane View -> DLLs If we need to see all linked libraries, and maybe also the specific objects, it is best to inspect the compiler logs when running the build using the verbose mode.","title":"C++ Debugging and Profiling"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#compiler-errors","text":"","title":"Compiler Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#general-rules","text":"Make the code you want to compile reachable. In C++, only reachable methods/classes are compiled! Solve errors that stop the compilation first. Warnings can stay in logs even if solved until the main error is gone and the build is finished Be aware that the cause of the error can be on a different line than the one in the error log!","title":"General rules"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#if-the-source-of-the-compilation-bug-cannot-be-found","text":"read the error examples below check that the code follow the my guidelines and C++ Core Guidelines . read the cpp reference for the parts of the problematic code check the const correctness . It is a causes a lot of problems. try a different compiler, the error message can be more informative try to isolate the case in some small example copy the project and remove the stuff until the problem is gone","title":"If the source of the compilation bug cannot be found"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#practical-static-assertions","text":"Test for concept satisfaction: static_assert(My_concept<My_class>);","title":"Practical static assertions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#useful-predicates","text":"std::is_same_as for checking the type equality.","title":"Useful Predicates"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#determining-type-at-compile-time","text":"Sometimes, it is practical to know the exact type during compile time. There is no direct way for that, but we can trick the compiler to print the type in an error message: template <typename...> struct Get_type; class Something {}; Get_type<<TYPE TO GET>> my_type{}; This should print an error message similar to: error: variable \u2018Get_type<<TYPE TO GET RESOLVED>> my_type\u2019 has initializer but incomplete type .","title":"Determining type at compile time"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#errors-with-a-missing-copy-constructor","text":"e.g. error C2280: 'Solution<N>::Solution(const Solution<N> &)': attempting to reference a deleted function These can be split into two groups: We want to copy the object, but the copy constructor is missing We do not want to copy the object, but the copy constructor is still called The firs case can be resolved easily. Below, we discuss the second case.","title":"Errors with a missing copy constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#copy-constructor-is-called-against-the-will","text":"There are two possible reasons for this: We perform an operation that requires a copy without realizing it. In this case, check the scenarios that can cause the copy in the C++ Manual Especially take care in case of STL collections, as those can display the error in the place of template instantiation, not in the place of the copy (see the Collection Manual ). We perform an operation that requires a move, but the move constructor is not available. In this case, change the code so that the move constructor is defined. To check if the class is move-constructible: cpp static_assert(std::is_move_constructible<Solution<Cordeau_node>>::value); Possible reasons, why the move constructor is not available: Move constructor is not implicitly declared due to a broken rule of five, i.e., one of the other constructors/assignments/destructors is defined Implicitly declared move constructor is deleted. Possible reasons: the class have a member that cannot be moved from const members","title":"Copy constructor is called against the will"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#errors-with-missing-move-constructor","text":"First, we should check whether the object's type T is movable using static_assert(std::is_move_constructible_v<T>) If the static assertion is false: Check whether all base classes has move constructors available the std::is_move_constructible_v<T> cannot be used for that, as the move operations can be protected. Instead, look for the presence of the move constructors in base classes (any base class should have them declared, as the implicit declaration of copy/move/destruction does not work with virtual classes) Check whether all class members are move constructible using the std::is_move_constructible_v<T> concept. Do not forget the const qualifiers!","title":"Errors with missing move constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#multiply-defined-symbols","text":"e.g. name already used for a template in the current scope. The source of the duplicate should be in the compiler output. Usually, this can be solved by using namespaces.","title":"Multiply Defined Symbols"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#conversion-is-inaccessible","text":"This mostly happens if we forgot to add the public keyword when inheriting from a base class, resulting in the (default) private inheritance.","title":"Conversion is inaccessible"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#cannot-convert-from-initializer-list-to","text":"This happans when there is no function signature matching the arguments. Sometimes, the specific argument can be found by using constructor instead of initializer list. Otherwise, check the arguments one by one. For each argument: check if the type matches check if the value type matches, i.e. value/ref/pointer check if the const matches check if the problem is not in wrong template argument deduction. The automatic deduction can use value type instead of reference type...","title":"Cannot convert from 'initializer list' to..."},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#returning-address-of-local-variable-or-temporary","text":"This happens when we assign a lambda to std::function and the labda return by reference and does not have an explicit return type. The solution is to add an explicit return type.","title":"Returning address of local variable or temporary"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#cannot-resolve-symbol","text":"or alternatively: 'identifier' was unexpected here; expected 'type specifier' . It simply means that the type cannot be reolved from the code location. Possible reasons: Circular dinclude","title":"Cannot resolve symbol"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#linker-errors","text":"","title":"Linker Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#undefined","text":"Something like unresolved external symbol... . For symbols that should come from your code: check that all used files are listed in CMakeLists.txt ( add_executable command\u2026) check that all templates are defined in header files check that all functions are defined correctly (even unsigned vs unsigned int can make problems...) For symbols that should come from a library: check that all necessary libraries are linked in CMakeLists.txt check that all libraries configured for linking in CMakeLists.txt are available on the system if the compiler environment may have changed since the last build (e.g., system update, compiler update, or when running the build on remote server), try to erase the build directory and configure the project again If none of the above works, check this list: https://stackoverflow.com/questions/12573816/what-is-an-undefined-reference-unresolved-external-symbol-error-and-how-do-i-fix?","title":"Undefined"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#multiply-defined","text":"Check the recent includes, are all of them correct? Check the multiply defined symbol. If it is a function defined in a header file, it has to be static or inline. there can be a bug even in an external library! https://stackoverflow.com/questions/30180145/general-techniques-for-debugging-the-multiple-definition-of-error","title":"Multiply Defined"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#link-fatal-error-lnk1181-cannot-open-input-file","text":"The error LNK1181 means that the linker cannot find the specified library file. To daiagnose this, look at how the file is specified in the target's vxproj file: Open the target's vxproj file in a text editor Find the Link element Check the AdditionalDependencies attribute of the Link element. The path to the library file should be there. With exception of the standard libraries, the path should be either absolute (for installed libraries, like vcpkg libraries) or relative to the project directory (for libraries that are distributed with the project). Two cases can happen: The path looks correct: double check that hte library file is present on the specified path The path is incorrect, e.g., the library is specified only by its name. This means that the vxproj file is not generated correctly, i.e., the problem is in the configuration phase. note that CMake does not check the validity of the targets specified in the target_link_libraries command!","title":"LINK : fatal error LNK1181: cannot open input file"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#runtime-errors","text":"First, identify the exception context . To do that, look at the line where the exception is thrown. If the throwing line is not on the call stack, it is possible that the debugger does not break on the particular exception type. to change that go to the Exception Settings and check the exception type there. If the cause of the exception is not clear from the context, it may be usefull to check the exception details . First, look at the exception message. The easiest way is to catch the exception in the code and print the message. In Google Test, there is a catch-all handler, just run the test without the --gtest_break_on_failure flag. If the message is not enough, look at the exception content in the debugger. Unfortunately, it is not possible to inspect unhandled exception object easily. To do so, add the following watch: (<exception type>*) <exception address> where <exception type> is the type of the exception (e.g. std::exception ) and <exception address> is the address from the exception dialog. Finally, if the cause of the exception is still unclear, look at the exception type, and proceed to the respective section below, if there is one.","title":"Runtime errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#error-codes","text":"Error codes are the only thing visible when the exception is not caught and no debugger is attached. Unfortunately, the error codes are platform-specific, and mostly undocumented even for operating system facilities and standard libraries.","title":"Error codes"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#windows-error-codes","text":"On windows, many error codes can be emitted by the system or standard libraries: 0x0 to 0x3e7f : Win32 error codes : Errors emitted by Windows high-level functionalities 0xC0000000 to 0xCFFFFFFF : NT status codes : Standardized 32-bit error codes used in Windows kernel, drivers, and protocols. Notable examples: 0x00000003 : STATUS_BREAKPOINT ( {EXCEPTION} Breakpoint A breakpoint has been reached. ) 0xC0000005 : Access violation ( The instruction at 0x%08lx referenced memory at 0x%08lx. The memory could not be %s. ) 0xC0000142 : STATUS_DLL_INIT_FAILED: Initialization of a dynamic link library failed. The process is terminating abnormally. This means that all linked libraries are available at runtime, but some of them failed to initialize, which may be due to a missing dependency (between the linked library and another library not linked to the executable) 0xC0000374 : Heap corruption ( A heap has been corrupted. )","title":"Windows Error Codes"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#missing-dll","text":"When a dll file is missing, we can temprorarily fix the problem by copying the dll file to the same directory as the executable. However, it is best to investigate the cause, which we discuss in this section. The most common cause is that we do not set the target to copy the required runtime dependencies to the output directory. This is necessary even when building with CMake: CMake does not copy the runtime dependencies automatically! To configure CMake to copy the runtime dependencies, see the CMake Manual . If this command is present and copies some runtime dependencies to the output directory, but not all of them, we need to investigate further: determine why the missing library is required (if we do not know immediately...) fix the dependency so it is recognized by CMake if the library is required by our targets, we have some linking misconfiguration -> we have to fix it if the library is required by another library (transitive dependency), the bug is probably there. We cannot fix it, but we can report it and manually link the missing library to the library that requires it in our project (see the CMake Manual for details).","title":"Missing dll"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#showing-the-runtime-dependency-tree","text":"First, we should check the CMake dependencies. We can see them using the --graphviz argument of the cmake command: cmake --graphviz=dependencies.dot In the dependencies.dot file, we can see the dependency tree of visible cmake dependencies, i.e., the libraries that: are linked in cmake using the target_link_libraries command (in contrast to command line linking, etc), and are linked publicly (i.e., with the PUBLIC or INTERFACE keyword, not PRIVATE ) If the missing library is not in this dependency tree, we should inspect the executable for the dependencies. We can do that using the Dependencies tool . Download and extract the tool Run the DependenciesGui.exe Drag and drop the executable to the tool","title":"Showing the runtime dependency tree"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#memory-errors","text":"These exception are raised when an unallocated memory is accesed. The following signalize a memory error: Read Access Violation HEAP CORRUPTION DETECTED First, most of the memory errors can be caught by various assertions and guards in the debug mode. If possible, try to run the program in the debugg mode, even if it takes a long time, because this way, you can catch the problem when it happens, before the memory is corrupted. If that does not help, read the following sections. Other reassons are also discussed here","title":"Memory Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#accessing-null-pointer","text":"A frequent cause of memory errors is accessing a null pointer object's data. In this case, the cause of the problem can be quickly determined in the debugger. Just follow the lifetime of the pointer and find the momemnt when it becom null.","title":"Accessing null pointer"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#read-access-violation-caused-by-a-demaged-vtable","text":"In case of some previous memory mismanagement, the heap can be demaged, possibly resulting in a corrupted virtual table for objects on the heap. To check whether the virtual table is corrupted, add the following watch to the debugger: <var name>.__vfptr Where <var name> is the name of the object you want to inspect. To resolve this problem, see debugging memory errors.","title":"Read Access Violation Caused by a Demaged vtable"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#using-application-verifier-to-find-causes-of-memory-related-errors","text":"A lot of memory errors can be caught just by running the program in the debugger. The STL containers, for example, containst various assertions that break the code on wrong memory access related to these collections. To add even more assert guards (e.g., for dynamic arrays), we can use the Application Verifier which is installed as a part of Windows SDK (which is typically installed together with Visual Studio). To debug the application with the Application verifier enabled: Open AV right click -> add executable and select the executable to test select the appropriete test suite (the basic one is enouh for the memory testing) click save close AV run the executable in the debugger, find the problem, fix it open AV delete the exectable from the list","title":"Using Application Verifier to find causes of memory related errors."},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#using-address-sanitizer","text":"A linux memory tool called address sanitizer can be used to debug memory related errors. To use it from the Visual Studio: Check that the libasan lib is installed on WSL In Cmake Settings for the debug configuration, check Enable AddressSanitizer build the project run The program should now break on the first problem. The details are displayed in the output window More at Microsoft learn","title":"Using Address Sanitizer"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#using-valgrind-to-debug-memory-errors","text":"Valgrind is a toolbox for debugging C/C++ code. The most famous tool is calle Memcheck and is intended for memory error detection. Basic usage: valgrind --leak-check=yes <program> <program arguments> The explanation of the error messages can be found on the Valgrind website The most common errors and tips: Conditional jump or move depends on uninitialised value : triggers on the first usage (not copy) of the uninitialized data note that the uninitialized variable can look normal (e.g. if it is a number), just the value is random. Invalid read of size ... : can happen to both stack and heap memory the content can still be in memory, it just means that the memory has been freed/invalidated. There are some expected messeges not to be worried about: Warning: set address range perms: large range","title":"Using Valgrind to debug memory errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#logical-errors","text":"","title":"Logical Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#c-specific-numerical-errors","text":"First possible error is overflow . C++ does not handle or report overflow! The behaviour is undefined. Second potential danger is the unsigned integer overflow . In case the result below zero is stored in unsigned integer, the number is wrapped around, resulting in large positive integer. Another thing is that when signed and unsigned integers are used in one operation, the signed integer is implicitely converted to unsigned integer before the operation! This is called promotion and it also works for other types (see in a table on SO ). In general to prevent the overflow: check each operation for the potential overflow, inluding the unsigned integer overflow with negative numbers if the overflow can happen, cast the types before any arithmetic operation to prevent the overflow also, one have to use the right type in templates like std::max","title":"C++ Specific Numerical Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#static-code-analysis","text":"Static code analysis is a process of checking the code without running it. The basic static analysis is available in IDEs, or as a part of the compiler. However, there are also more advanced tools available that can check the code for various problems, including: memory leaks and memory errors undefined behaviour dead code code style","title":"Static code analysis"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#cppcheck","text":"Cppcheck is a GUI tool that can be used to check the code for various problems. It is available for Windows and Linux.","title":"Cppcheck"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#testing-google-test","text":"Google test is a c++ testing framework that can be used to write unit tests. As it is not officially shipped with vcpkg, we have to use FetchContent to download it. The basic usage is as follows: Add the googletest to the project using FetchContent Add the test target using add_executable Link the test target with the gtest and gtest_main targets Optionally, if we want the tests to be run automatically by the ctest , we add the gtest_discover_tests command to the CMakeLists.txt .","title":"Testing - Google Test"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#debugging-tests","text":"For test debugging, some google test options may be usefull: --gtest_break_on_failure : breaks the test on a failed test or failed assertion. --gtest_catch_exceptions=0 : stops google test from catching exceptions, hence the program crashes on an unhandled exception. Without this, we cannot use breakpoints on unhandled exceptions. Note that this way, we will not see the exception message, as C++ default exception handler does not print the message. --gtest_filter=Some_test_prefix* : filter tests by name. Astrix ( * ) can be used as a wildcard for both prefix and suffix. The pattern is <test_suite_name>.<test_name> .","title":"Debugging tests"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#visual-studio-errors","text":"","title":"Visual Studio Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#false-errors-appears-in-a-file-in-many-files","text":"close visual studio delete the out folder open the visual studio again","title":"False errors appears in a file / in many files"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#refactoring-options-not-present","text":"If the refactoring options like change signature are not present, try to send the Resharper bug reports :), or create a new project.","title":"Refactoring options not present"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#intallisense-false-errors","text":"Sometimes, the errors disappears by deleting the .suo file located in <project dir>/.vs/<project name>/<VS version>","title":"IntalliSense false errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#using-the-debugger","text":"","title":"Using the debugger"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#clion","text":"Unlike in Visual Studio, the Clion debugger does not break be default. To break on exceptions or breakpoints, we need to use the debug button instead of the run button. To debug multiple targets at once: Open the Run/Debug Configurations dialog Add a new configuration of type Compound Add the configurations you want to run together using the + button Debug the compound configuration","title":"Clion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#visual-studio","text":"","title":"Visual Studio"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#improve-debugger-experience-with-natvis","text":"Natvis is a visualization system that enables to enhance the presentation of variables in debugger locals or watch windows. It is a XML file, where we can define visualization rules for any type. Structure: <Type Name=\"My_class\"> ... visualization rules </Type> The name must be fully qualified, i.e., we have to include namespaces. The big advantage is that natvis files can be changed while the debugger is running and the presentation of the type in locals/watch window is changed immediatly after saving the natvis file.","title":"Improve debugger experience with natvis"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#natvis-expressions","text":"The expression in natvis are sorounded by {} . If we want curly braces in the text, we can double them {{...}} . Unfortunatelly, function calles cannot be used in natvis expressions .","title":"Natvis expressions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#natvis-errors","text":"Natvis errors can be displayed in the output window if turned on in settins: Debug -> Options -> Debugging -> Output Window .","title":"Natvis Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#existing-visualisations","text":"Existing natvis files are stored in <VS Installation Folder>\\Common7\\Packages\\Debugger\\Visualizers folder. The STL visualizations are in stl.natvis","title":"Existing visualisations"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#debugging-polymorphic-classes","text":"Unfortunately, the debugger does not show the actual type of the object when the object is cast to a base class. To see the object content including the members of the derived class, we have to cast the object to the derived class in the watch window: ((Derived_class*) object) for pointers ((Derived_class&) object) for references","title":"Debugging polymorphic classes"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#address-breakpoints","text":"Address breakpoints can be used to watch a change of a variable or in general, a change of any memory location. To set an address breakpoint, we nned to first find the address of the variable. To do that, we can: use the & operator on the variable in the watch window use the & operator on the variable in the immediate window The address should have a format 0x0000000000000000 .","title":"Address breakpoints"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#profiling","text":"There are multiple profiler options for C++ , but not all the tools are easy to use or maintained.","title":"Profiling"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#cpu-profiling","text":"","title":"CPU Profiling"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#visual-studio_1","text":"In visual studio: Run the program and either wait for breakpoint hit or manually pause the execution In Diagnostic Tools tab, hit the Enable CPU Profiling button Unpause the execution Pause the execution again, the profiler results covering the period between resume and pause should be available","title":"Visual Studio"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#clion_1","text":"documentation TL;DR: The profiler does not work in WSL. CLion profiler is based on the perf tool and therefore it is available only on Linux (and WSL). Currently I have a problem with the profiler: after clicking the buytton to stop profiling and show the results, the profiler window just shows a message No profiler data . Clion issue .","title":"CLion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#old-clion-wsl-issue","text":"However, in WSL, currently the profiler does not work (the profiler immediatelly terminates with an unknown error). Fortunatelly, there is a workaround (described in another issue ): - open the Clion registry: Help -> Find Action -> Registry - disable the wsl.use.remote.agent.for.launch.processes option","title":"Old CLion WSL issue"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#vtune","text":"VTune can be run ftom the Visual Studio only for VS solution projects. In case of CMake projects, we need to run the VTune GUI and configure the debugging there.","title":"VTune"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#memory-profiling","text":"For memory profiling to work, two things needs to be taken care of: if the application allocates a lot memory inside parallel region, disable paralelization for profiling. Otherwise, there can be too many allocation events for the profiler to handle if you use a custom memory allocator, disable it and use a standard allocator for memory profiling","title":"Memory Profiling"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#memory-profiling-in-visual-studio","text":"To profile memory in Visul Studio Set a breakpoint before the region of interest Run the app and wait for the hit In Diagnostic Tools tab -> Summary , click Enable heap profiling for next session Restart the app and wait for the hit. Take memory snapshot Add a breakpoint to the end of the region of interest Wait for the hit, take snapshot and check both snapshots","title":"Memory Profiling in Visual Studio"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#showing-size-alignment-and-memory-layout-of-structures","text":"In both CLion and Visual Studio, the size and alignment of the structure is displayed in the tooltip when hovering over the structure name, or member name. Additionally, Visual Studio shows the memory layout if we click the appropriate button in the tooltip. Visual Studio tutorial","title":"Showing size, alignment, and memory layout of structures"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#showing-linked-libraries","text":"To show dynamically linked libraries, we can use the Process Explorer tool. Find the process of the running executable View -> Lower Pane View -> DLLs If we need to see all linked libraries, and maybe also the specific objects, it is best to inspect the compiler logs when running the build using the verbose mode.","title":"Showing Linked Libraries"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/","text":"Source and Header Files \u00b6 In C++, there are: header files, that contain declarations of functions, classes, variables, etc. and, for templated code, the code itself. they have the .h or .hpp extension source files, that contain the definitions of the functions, classes, variables, etc. they have the .cpp extension Each source file added to the target (see CMake Manual for adding source files to the target in CMake projects) is compiled into an object file - a translation unit . The header files are not compiled, but insted, they serve as a promise that there will be some translation unit that will contain the code that will be linked to the final executable. The above mechanism provides a flexible and practical interface, but a special care must be taken to avoid mistakes, that typically result in a liker error. Most important here is the One Definition Rule (ODR) : each entity (e.g., class, function, variable, etc.) must be defined exactly once in the whole program. If something is defined in multiple translation units, it will result in a multiple definition error. If we forgot to define some entity, or we do not add the source file with the definition to the target, it will result in an undefined reference error. Typically, each header has a corresponding source file, that contains the definition of all entities declared in the header. However, there is no requirement to have a single source file for each header, we can separate the code into multiple source files. Note that all entities in the translation units added to the target are compiled, even if they are not used . This is in contrast with the statements inside functions, which are not compiled if they are not used. Storage Duration \u00b6 cppreference The storage duration of a variable is the period for which the storage is guaranteed to be available. There are the following storage durations : static storage duration : these variables are initialized at the start of the program before the main function is called. They are destroyed at the end of the program, after the main function returns. thread storage duration : these variables are initialized at the start of the thread before the thread function is called. They are destroyed at the end of the thread, after the thread function returns. automatic storage duration : these variables are initialized at the point of their declaration and destroyed at the end of the block they are declared in. dynamic storage duration : these variables are initialized by some memory management mechanism, and they are destroyed by an analogous mechanism. To determine the scope of a variable, we can use the following rules : Variables within the namespace scope (including the undeclared global namespace) have static storage duration unless they are declared with the thread_local specifier, in which case they have thread storage duration. block scope variables have automatic storage duration unless they are declared with the static specifier, in which case they have static storage duration. block static variables, unlike namespace static variables, are not initialized at the start of the program, but at the first time the line with the declaration is executed. parameters have automatic storage duration. objects created with mechanics like new / delete , malloc / free , new[] / delete[] , or std::make_unique , std::make_shared , etc. have dynamic storage duration. An important lesson from these rules is that the global variables have static storage duration even without the static specifier. The reason for using the static specifier for globals is to control the linkage of the variable. Type System \u00b6 cppreference Type is a property of each: object reference function expression Complete and Incomplete Types \u00b6 In many context, we have to supply a type with a requirement of being a complete type. So what types are incomplete? The void type is always incomplete Any structure without definition (e.g. using struct structure *ps; , without defining structure .) An array without dimensions is an incomplete type: int a[]; is incomplete, while int a[5]; is complete. An array of incomplete elements is incomplete. A type trait that can be used to determine whether a type is complete is described here . Aggregate types \u00b6 Aggregate types are: array types class types that fullfill the following conditions no private or protected members no constructores declared (including inherited constructors) no private or protected base classes no virtual member functions The elements of the aggregate types can and are ment to be constructed using the aggregate initialization (see the local variable initialization section). Type Conversion \u00b6 cppreference: implicit conversion In some context, an implicit type conversion is aplied. This happens if we use a value of one type in a context that expects a different type. The conversion is applied automatically by the compiler, but it can be also applied explicitly using the static_cast operator. In some cases where the conversion is potentially dangerous, the static_cast is the only way to prevent compiler warnings. Numeric Conversion \u00b6 There are two basic types of numeric conversion: standard implicit conversion that can be of many types: this conversion is applied if we use an expression of type T in a context that expects a type U . Example: ```cpp void print_int(int a){ std::cout << a << std::endl; } int main(){ short a = 5; print_int(a); // a is implicitly converted to int } ``` usual arithmetic conversion which is applied when we use two different types in an arithmetic binary operation. Example: cpp int main(){ short a = 5; int b = 2; int c = a + b; // a is converted to int } Implicit Numeric Conversion \u00b6 Integral Promotion \u00b6 Integral promotion is a coversion of an integer type to a larger integer type. The promotion should be safe in a sense that it never changes the value. Important promotions are: bool is promoted to int : false -> 0 , true -> 1 Integral Conversion \u00b6 Unlike integral promotion, integral conversion coverts to a smaller type, so the value can be changed. The conversion is safe only if the value is in the range of the target type. Important conversions are: Usual Arithmetic Conversion \u00b6 cppreference This conversion is applied when we use two different types in an arithmetic binary operation. The purpose of this conversion is convert both operands to the same type before the operation is applied. The result of the conversion is then the type of the operands. The conversion has the following steps steps: lvalue to rvalue conversion of both operands special step for enum types special step for floating point types conversion of both operands to the common type The last step: the conversion of both operands to the common type is performed using the following rules: If both operands have the same type, no conversion is performed. If both operands have signed integer types or both have unsigned integer types, the operand with the type of lesser integer conversion rank (size) is converted to the type of the operand with greater rank. otherwise, we have a mix of signed and unsigned types. The following rules are applied: If the unsigned type has conversion rank greater or equal to the rank of the signed type, then the unsigned type is used. Otherwise, if the signed type can represent all values of the unsigned type, then the signed type is used. Otherwise, both operands are converted to the unsigned type corresponding to the signed type (same rank). Here especially the rule 3.1 leads to many unexpected results and hard to find bugs. Example: int main(){ unsigned int a = 10; int b = -1; auto c = b - a; // c is unsigned and the value is 4294967285 } To avoid this problem, always use the static_cast operator if dealing with mixed signed/unsigned types . Show the Type at Runtime \u00b6 It may be useful to show the type of a variable at runtime: for debugging purposes for logging to compare the types of two variables Note however, that in C++, there is no reflection support. Therefore, we cannot retrieve the name of the type at runtime in a reliable way . Instead, the name retrieved by the methods described below can depend on the compiler and the compiler settings. Resolved complicated types \u00b6 Sometimes, it is useful to print the type, so that we can see the real type of some complicated template code. For that, the following template can be used: #include <string_view> template <typename T> constexpr auto type_name() { std::string_view name, prefix, suffix; #ifdef __clang__ name = __PRETTY_FUNCTION__; prefix = \"auto type_name() [T = \"; suffix = \"]\"; #elif defined(__GNUC__) name = __PRETTY_FUNCTION__; prefix = \"constexpr auto type_name() [with T = \"; suffix = \"]\"; #elif defined(_MSC_VER) name = __FUNCSIG__; prefix = \"auto __cdecl type_name<\"; suffix = \">(void)\"; #endif name.remove_prefix(prefix.size()); name.remove_suffix(suffix.size()); return name; } Usage: std::cout << type_name<std::remove_pointer_t<typename std::vector<std::string>::iterator::value_type>>() << std::endl; // Prints: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > Source on SO Show the user-provided types (std::type_info) \u00b6 If we want to show the type of a variable provided by the user (e.g., by a function accepting std::any ), we can use the typeid operator which returns a std::type_info object. Built-in and STL Types \u00b6 Arithmetic Types \u00b6 cppreference Integers \u00b6 Integer types varies in the sign and size. Unfortunatelly, the minimum sizes guaranteed by the standard are not usable, because the real size is different and it differs even between platforms . Especially the long type. To use an integer with a specific size, or a specific minimal size, we can use type aliases defined in cstdint Overflow and Underflow \u00b6 The overflow (and underflow) is a common problem in most programming languages. The problem in C++ is that: overflows are not detected overflows can happen in many unexpected situations Dangerous situations \u00b6 In addition to the usual suspects like assigning a value to a variable of a smaller type, there are some less obvious situations that can cause overflows. Some examples: the result of an arithmetic operation is assigned to a variable of large enough type, but the overflow happens before the assignment itself: cpp short a = 32767; short b = 1; int c = a + b; // overflow happens beffore the assignment A solution to this problem is to use a numeric cast of the opperands (even one is enouhg): cpp short a = 32767; short b = 1; int c = static_cast<int>(a) + b; Detecting overflows \u00b6 There are some methods how to detect overflows automatically by suppliying arguments to the compiler. These are summarized here: MSVC : not implemented GCC : only detectes signed and floating point overflows, as the unsigned overflows are not considered as errors (the behaviour is defined in the standard). All undefined behaviour can be detected using the -fsanitize=undefined flag. Documentation Clang : Both signed and unsigned overflow can be detected. The undefined behaviour can be detected using the -fsanitize=undefined flag. Fo all integer overflows, the -fsanitize=integer flag can be used. Documentation The reasoning behind excluding the unsigned overflows from GCC are described here . It is also possible to do an ad-hoc overflow check in the code, the possible solutions are described in this SO question Characters \u00b6 Characters in C++ are represented by the char type, which is an integer type. This type can be signed or unsigned, and it is at least 8 bits long. Useful functions for working with characters are: std::isspace : checks if the character is a whitespace (space, tab, newline, etc.) std::toupper : converts the character to upper case Pointers \u00b6 cppreference Pointers to Functions \u00b6 Function pointers are declared as: <return_type> (*<pointer_name>)(<arg_1_type>, ..., <arg_n_type>) For example a function the_function returning bool and accepting int can be stored to pointer like this: bool (*ptr)(int) = &the_function The above example can be then simply called as bool b = ptr(2) Pointers to Member Objects \u00b6 Pointers to member objects has a cumbersome syntax declaration: <member type> <class type>::*<pointer name> = ... usage: <object name>.*<pointer name> = ... Example: class My_class{ public: int my_member; } int main{ // declaring the pointer int My_class::*ptr = &My_class::my_member; // creating the instance My_class inst; // using the pointer to a member object inst.*ptr = 2; } Pointers to Member Functions \u00b6 Pointers to member functions are even more scary in C++. We need to use the member object and the function adress and combine it in a obscure way: class My_class{ public: bool my_method(int par); } int main{ // creating the instance My_class inst; // assigning method address to a pointer bool (My_class::*ptr)(int) = &My_class::my_method; // using the pointer to a member function bool b = (inst.*ptr)(2) } The first unexpected change is the My_class before the name of the pointer. It's because unlike a pointer to function the_function which is of type (*)(int) , the pointer to my_method is of type (My_class::*)(int) The second difference is the call. We have t use the pointer to member binding operator .* to access the member of the specific instance inst . But this operator has a lower priority then the function call operator, so we must use the extra parantheses. References \u00b6 References serve as an alias to already existing objects. Standard ( Lvalue ) references works the same way as pointers, with two differences: they cannot be NULL they cannot be reassigned The second property is the most important, as the assignment is a common operation, which often happens under do hood. In conslusion, reference types cannot be used in most of the containers and objets that needs to be copied . Rvalue references \u00b6 Rvalue references are used to refer to temporary objects. They eneable to prevent copying local objets by extending lifetime of temporary objects. They are mostly used as function parameters: void f(int& x){ } f(3); // 3 needs to be copied to f, because it is a temporary variable // we can add the rvalue overload void f(int&& x){ } f(3) // rvalue overload called, no copy Forwarding references \u00b6 Forwarding references are references that preserves the value category (i.e. r/l-value reference, const ). They have two forms: function parameter forwarding references auto forwarding references Function parameter forwarding references \u00b6 In a function template, if we use the rvalue reference syntax for a function parameter of whose type is a function template parameter, the reference is actually a forwarding reference. Example: template<class T> void f(T&& arg) // parameter is T& or T&& depending on the supplied argument Important details: it works only for non const references the reference type has to be a function template argument, not a class template argument auto forwarding reference \u00b6 When we assign to `auto&&, it is a forwarding reference, not rvalue reference: auto&& a = f() // both type and value category depends on the return value of f() for(auto&& a: g(){ // same } Arrays \u00b6 cppreference There are two types of arrays: static , i.e., their size is known at compile type, and dynamic , the size of which is computed at runtime We can use the array name to access the first elemnt of the array as it is the pointer to that element. Static arrays \u00b6 Declaration: int a[nrows]; int a[nrwows][ncols]; // 2D int a[x_1]...[x_n]; // ND Initialization: int a[3] = {1, 2, 5} int b[3] = {} // zero initialization int c[3][2] = {{1,5}, {2,9}, {4,4}} // 2D int d[] = {1,5} // we can skip dimensions if their can be derived from data Note that the multi-dimensional syntax is just an abstraction for the programmers. The following code blocks are therefore equivalent: Matrix syntax const int rowns = 5; const int cols = 3; int matrix[rows][cols]; int main(){ for(int n = 0; n < rows; ++n){ for(int m = 0; m < cols; ++m){ Table[n][m] = (n + 1) * (m + 1); } } } Flat syntax const int rowns = 5; const int cols = 3; int matrix[rows * cols]; int main(){ for(int n = 0; n < rows; ++n){ for(int m = 0; m < cols; ++m){ Table[n * cols + m] = (n + 1) * (m + 1); } } } Using the matrix syntax adds the possibility to access the element of the array using multiple dimensions. But the underlying memory is the same. Dynamic arrays \u00b6 Declaration: int* a = new int[size] For multiple dimensions, this syntax does not scale, i.e, only one dimension can be dynamic: int(*a)[4] = new int[rows][4] // static column count int(*b)[cols] = new int[rows][cols] // does not compile unless cols is a constant! Array to pointer implicit conversion \u00b6 When we use the array name in an expression, it can be implicitly converted to a pointer to the first element of the array. This is true for both static and dynamic arrays. Example: int a[3] = {1, 2, 5} int* ptr = a; // ptr points to the first element of a This implicit conversion is called array-to-pointer decay . Mutli-dimensional dynamic arrays \u00b6 To simulate multi-dimensional dynamic arrays, we have two options: use the flat syntax, as demonstrated on static arrays use aray of pointers to arrays Method Pros Cons Flat Syntax Fast: single continuous allocations different access syntax than static 2D arrays Array of pointers Slow: one allocation per row, unrelated memory addresses between rows same access syntax as static 2D arrays Flat array \u00b6 int* a = new int[rows * cols] Then we can access the array as: a[x * cols + y] = 5 Array of pointers to array \u00b6 Declaration and Definition int** a= new int*[rows] for(int i = 0; i < rows; ++i){ a[i] = new int[cols] } Access is than like for static 2D array: a[x][y] = 5 . This works because the pointers can be also accessed using the array index operator ( [] ). In other words, it works \"by coincidence\", but we have not created a real 2D array. Auto dealocation of dynamic arrays \u00b6 We can replace the error-prone usage of new and delete by wraping the array into unique pointer: std:unique_ptr<int[]> a; a = std::make_unique<int[]>(size) References and Pointers to arrays \u00b6 cppreference The pointer to array is declared as <type> (*<pointer_name>)[<size>] : int a[5]; int (*ptr)[5] = &a; Analogously, the reference to array is declared as <type> (&<reference_name>)[<size>] : int a[5]; int (&ref)[5] = a; Function Type \u00b6 A function type consist from the function arguments and the return type. The function type is written as return_type(arg_1_type, ..., arg_n_type) . Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo), int(double, double)>) // TRUE Reference to Function and Pointer to Function Types \u00b6 cppreference A refrence to function has a type return_type(&)(arg_1_type, ..., arg_n_type) . Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo)&, int(&)(double, double)>); // TRUE A pointer to function has a type: return_type(*)(arg_1_type, ..., arg_n_type) Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo)*, int(*)(double, double)>); // TRUE Enumerations \u00b6 cppreference C++ supports simple enumerations, which are a set of named integer constants. The enumeration can be defined as: enum Color {red, green, blue}; // global scope enum class Color {red, green, blue}; // scoped, preferred There is no support for enum members like in Python, but we can use a wrapper class for that: class Color{ public: enum Value {red, green, blue}; Color(Value v): value(v){} // non-explicit constructor for easy initialization Value get_value() const {return value;} std::string to_string() const{ switch(value){ case Value::red: return \"red\"; case Value::green: return \"green\"; case Value::blue: return \"blue\"; } } private: Value value; } Color get_color(){ return Color::red; // this works due to the non-explicit constructor } int main(){ Color c = get_color(); std::cout << c.to_string() << std::endl; switch(c.get_value()){ case Color::red: std::cout << \"red\" << std::endl; case Color::green: std::cout << \"green\" << std::endl; case Color::blue: std::cout << \"blue\" << std::endl; } } For more complex code requireing automatic conversion to string and more, we can consider the magic_enum library . It supports the following features: enum to string conversion string to enum conversion enum iteration sequence of possible values Smart Pointers \u00b6 For managing resources in dynamic memory, smart pointers (sometimes called handles ) should be used. They manage the memory (alocation, dealocation) automatically, but their usage requires some practice. There are two types of smart pointers: std::unique_ptr for unique ownership std::shared_ptr for shared ownership Creation \u00b6 Usually, we create the pointer together with the target object in one call: std::make_unique<T>(<OBJECT PARAMS>) for unique pointer std::make_shared<T>(<OBJECT PARAMS>) for shared pointer These methods work well for objects, but cannot be used for arbitrary array initialization (only the empty/zero-initialized array can be created using these methods). For arbitrary array initialization, we need to use the smart pointer constructor: std::unique_ptr<int[]> ptr(new int[]{1, 2, 3}); Counter-intuitively, smart pointers created using the empty constructor of the respective pointer type does not default-construct the target object, but initialize the pointer to null instead: std::unique_ptr<My_class> ptr(std::null_ptr); // ptr is null std::unique_ptr<My_class> ptr(); // ptr is also null Shared Pointer \u00b6 Pointer to object with non-trivial ownership (owned by multiple objects). std::reference_wrapper \u00b6 cppreference Reference wrapper is a class template that can be used to store references in containers or aggregated objects. The disintinction from normal references is that the reference wrapper can be copied and assigned, so it does not prevent the copy/move operations on the object it belongs to. Otherwise, it behaves like a normal reference: it has to be assigned to a valid object and it cannot be null. Strings \u00b6 In C++, there are two types of strings: std::string is an owning class for a string. std::string_view is a non-owning class for a string. Also, there is a C-style string ( char* ), but it is not recommended to use it in modern C++. The difference between std::string and std::string_view is best explained by a table below: std::string std::string_view Owning Yes No Null-terminated Yes No Size Dynamic Static Lifetime Managed by the string Managed by the underlying char sequence Can be constexpr No Yes and the following code: std::string_view sv = \"hello\"; // sv is a view of the string literal \"hello\" std::string s = \"hello\"; // s stores a copy of the string literal \"hello\" String Literals \u00b6 cppreference The standard string literal is writen as \"literal\" . However, we need to escape some special characters in such literals, therefore, a raw string literal is sometimes more desirable: R\"(literal)\" . If our literal contains ( or ) , this is stil not enough, however, the delimiter can be extended to any string with a maximum length of 16 characters, for example: R\"lit(literal)lit\" . Raw string literals also useful for multi-line string literals . Formatting strings \u00b6 The usage of modern string formating is either std::format from the <format> header if the compiler supports C++20 string formatting ( compiler support ) or fmt::format from the fmt library if not. Either way, the usage is the same: format(<literal>, <arguments>) where the literal is a string literal with {} placeholders and the arguments are the values to be inserted into the placeholders. The placeholders can be filled width argument identification, if we want to use the same argument multiple times or change the order in the string while keep the order of arguments in the function call or format specification. These two parts are separated by : , both of them are optional. The most common format specifications are: data type: d for decimal integer f for floating point number s for string width and precision, in the format <width>.<precision> . Both values can be dynamic: std::format(\"{:{}.{}f}\", a, b, c) formats a float number a with width b and precision c . The formating reference can be found in the cppreference Spliting the string into tokens \u00b6 Unfortunately, the STL does not provide a simple way to split the string into tokens like Python's split method or PHP's explode function. It is not even planned for the future. If we want to split a string on a character or pattern, the easiest way is to use the split view from the ranges library, which has a std::ranges::subrange as its element type: // get a range of character subranges auto parts = std::ranges::views::split(str, '-'); // iterate over the parts for (auto part : parts) { std::cout << part << std::endl; // prints the part // convert part to string std::string s(part.begin(), part.end()); // convert part to string (C++23) std::string s(std::from_range, part); } The last string constructor is only available in C++23, and moreover, it requires the stl::from_range tag. The std::string_view is equiped with a range constructor which does not require the tag in C++23. However, it is explicit, so its usage is limited: std::string_view s(part) // invalid std::string_view s = std::string_view(part) // valid in C++23 Converting string to int \u00b6 There are simple functions for converting std::string to numbers, named std::stoi , std::stoul , etc. See cppreference for details. For C strings, the situation is more complicated. Substring \u00b6 A substring can be obtained using a member function substr : str.substr(str.size() - 1, 1)) // returns the last character as a string change the case \u00b6 Unfortunatelly, the STL has case changing functions only for characters, so we need to iterate over the string ourselfs. The boost has a solution, however: #include <boost/algorithm/string.hpp> auto upper = boost::to_upper(str); Alternatively, we can use the std::transform algorithm and the std::toupper or std::tolower functions: std::transform(str.begin(), str.end(), str.begin(), std::toupper); Building strings \u00b6 Unlike other languages, in C++, strings are mutable, so we can build them using the + operator without performance penalty. Alternatively, we can use the std::stringstream class. Testting for whitespace \u00b6 To test if a string contains only whitespace characters, we can use the std::all_of algorithm: std::all_of(str.begin(), str.end(), [](char c){return std::isspace(c);}) Date and time \u00b6 The date and time structure in C++ is std::tm . We can create it from the date and time string using std::get_time function: std::tm tm; std::istringstream ss(\"2011-Feb-18 23:12:34\"); ss >> std::get_time(&tm, \"%Y-%b-%d %H:%M:%S\"); Collections \u00b6 In C++, the collections are implemented as templates, so they can store any type. The most common collections are: std::array std::vector std::unordered_set std::unordered_map std::pair and std::tuple Currently, the collection semantic requirmenets are not imposed on the whole connection, bu on its member functions instead. Depending on the function used, there are different requirements for the stored types. This adds a lot of flexibility, as we can, for example, use move only types in collections when we refrain from using functions that require copying. On the other hand, it can make the debugging harder, as the compiler usually does not recognize the methods that caused the template to have stricter requirements but instead complains on the place where the template is instantiated. Sets \u00b6 Normal set collection for C++ is std::unordered_set . By default, the set uses a Hash , KeyEqual and Allocator template params provided by std functions. However, they need to exist, specifically: std::hash<Key> std::equal_to<Key> std::allocator<Key> So either those specializations needs to be provided by the snadard library (check cppreference), or you have to provide it. Providing custom hash function \u00b6 There are two options for providing custom hash function for a type T s: implementing an explicit specialization of the template function std::hash<T> providing the Hash template param when constructing the hash The first method is prefered if we want to provide a default hash function for some type for which there is no hash function specialization in the standard library. The second method is prefered only when we want some special hash function for a type T for which std::hash<T> is already defined. Implementing custom hash function \u00b6 First check whether the hash function is not provide by STL on cppreference . Then, many other hash specializations are implemented by boost, check the reference . If there is no implementation, we can implement the hash function as follows (example for set): template<> struct std::hash<std::unordered_set<const Request*>> { size_t operator()(const std::unordered_set<const Request*>& set) const { std::hash<const Request> hash_f; size_t sum{0}; for (const Request* r : set) { sum += hash_f(*r); } return sum; } }; Important implementation details: the function needs to be implemented inside std or annonymous namespace, not inside a custom namespace do not forget to add template<> above the function, this indicates that it is a template specialization. Maps \u00b6 The maps has similar requiremnts for keys as the requirements for set value types (see previous section). The hash map type is called std::unordered_map . Note that maps require the stored types to be complete . Geeting value by key \u00b6 To access the map element, the array operator ( [] ) can be used. Note however, that this operator does not check the existence of the key, even if we do not provide a value. Example: std::unordered_map<int,std::string> map; map[0] = \"hello\" map[0] = \"world\" // OK, tha value is overwritten a = map[1] // a == map[1] == \"\" unintuitively, the default value is inserted if the key does not exist Therefore, if we just read from the map, it is safer to use the at() member function. Inserting into map \u00b6 There are five options: map[key] = value; or map.insert({key, value}) map.emplace(key, value); map.try_emplace(key, value); map.insert_or_assign(key, value); The following table summarizes the differences: Method If key exists constructs in place returns value map[key] = value; overwrites no a reference to the value map.insert({key, value}); does not overwrite no a pair of iterator to value and bool set to true if insertion took place map.emplace(key, value); does not overwrite yes a pair of iterator to value and bool set to true if insertion took place map.try_emplace(key, value); does not overwrite yes same as emplace map.insert_or_assign(key, value); overwrites yes a pair of iterator to value and bool set to true if insertion took place There is only a small difference between emplace and try_emplace : the try_emplace does not create a new value if the key already exists, while the emplace can create a new value even if the key already exists (in which case, the value is then discarded). Tuples \u00b6 We have two standard class templates for tuples: std::pair for pairs std::tuple for tuples with unlimited size Although named differently, these class templates behaves mostly the same. Creating tuples \u00b6 There are two ways of creating a tuple: constructor ( auto p = std::pair(...) ) initializer ( auto p = {} ) Beware that by default , the deduced types are decayed, i.e., const and references are removed and the tuple stores value types . If you need to store the reference in a tuple, you have to specify the type: auto p = std::pair<int, constr std::string&>(...) Also, beware that the RVO does not apply for tuple members. This means that if we store values types in the tuple, the types are copied/moved, and in conclusion, they have to by copyable/movable! This is the reason why we frequently use smart pointers in tuples even though we would reurn directly by value if we returned a single value. Creating tuples with std::make_pair or std::make_tuple \u00b6 TLDR: from C++17, there is no reason to use make_pair / make_tuple . There are also factory methods make_pair / make_tuple . Before C++17, argument deduction did not work for constructors, so there is a dedicated method for creating tuples. However, now we can just call the constructor and the template arguments are deduced from the constructor arguments. Also, the make_pair / make_tuple functions can only produce tuples containing values, not references (even if we specify the reference type in the make_pair / make_tuple template argument, the returned tuple will be value-typed). Accessing tuple members \u00b6 The standard way to access the tuple/pair mamber is using the std::get function: auto tuple = std::tuple<int, std::string, float>(0, \"hello\", 1.5); auto hello = std::get<1>(tuple); Unpacking tuples into variables \u00b6 There are two scenarios of unpacking tuples into variables: unpacking into new variables : for that, we use structured binding . unpacking into existing variables : for that, we use std::tie function. Structured binding \u00b6 If we don't need the whole tuple objects, but only its members, we can use a structured binding . Example: std::pair<int, int> get_data(); void main(){ const auto& [x, y] = get_data(); } std::tie \u00b6 If we want to unpack the tuple into existing variables, we can use the std::tie function: std::pair<int, int> get_data(); void main(){ int x, y; std::tie(x, y) = get_data(); } Unpacking tuples to constructor params with std::make_from_tuple \u00b6 We cannot use structured binding to unpack tuple directly into function arguments. For normal functions, this is not a problem, as we can first use structured binding into local variables, and then we use those variables to call the function. However, it is a problem for parent/member initializer calls, as we cannot introduce any variables there. Luckily, there is a std::make_from_tuple template function prepared for this purpose. Example: std::tuple<int,float> get_data(){ ... } class Parent{ public: Parent(int a, float b){...} { class Child: public Parent{ public: Child(): Parent(std::make_from_tuple<Parent>(get_data())){} } std::optional \u00b6 cppreference std::optional<T> is a class template that can be used to store a value of type T or nothing. The advantage over other options like null pointers or is that the std::optional is a value type, so it can wrap stack objects as well. The type T must satisfy std::is_move_constructible_v<T> (must be either movable or copyable). The usage is easy as the class has a value constructor from T and a default constructor that creates an empty optional. Also, the type T is convertible to std::optional<T> , and std::nullopt is convertible to an empty optional. Finally, std::optional<T> is convertible to bool , so it can be used in if statements. A typical usage is: class My_class{ public: My_class(int a, int b); } std::optional<My_class> f(){ ... return My_class(a, b); // or return {a, b}; // or, in case of fail return std::nullopt; } std::optional<int> a = f(); if(a){ // a has a value } Unions and Variants \u00b6 The idea of a union is to store multiple types in the same memory location. Compared to the polymorphism, when we work with pointers and to templates, where the actual type is determined at compile time, the union actually has a shared memory for all the types. The union can be therefore used in cases where nor polymorphism neither templates are suitable. One example can be storing different unrelated types (e.g., std::string and int ) in a container. We cannot use templates as that require a single type. Nor we can use polymorphism, as the types are unrelated. The big disadvantage of unions is that they are not type safe. The compiler cannot check if the type we are accessing is the same as the type we stored. Therefore, we have to be very careful when using unions. Therefore, unless some special case, we should use std::variant instead of unions . std::variant \u00b6 The declaration of std::variant is similar to the declaration of std::tuple : std::variant<int, double> v; The std::variant can store any of the types specified in the template parameters. The only requirement is that the types are default constructible. Also, incompatible types cannot be stored in std::variant , as we cannot use them as template arguments. However, we can use pointers or references to incompatible types instead. The type of the stored value can be obtained using: std::holds_alternative method that returns a boolean value if the variant stores the type specified in the template parameter or std::variant::index method that returns the index of the stored value. this method can be used also in a switch statement as the index is integral The value can be accessed using: the std::get function, if we know the type stored in the variant or the std::get_if function if we are guesing the type. Both functions return a pointer to the stored value. Example: std::variant<int, double> v = 1; std::cout << v.index() << std::endl; // prints 0 std::cout << *std::get_if<int>(&v) << std::endl; // prints 1 A really usefull feature of std::variant is the std::visit method, which allows us to call a function on the stored value. The function is selected based on the type of the stored value. Example: std::variant<int, double> v = 1; std::visit([](auto&& arg) { std::cout << arg << std::endl; }, v); // prints 1 More on variants: cppreference cppstories Storing individual bits in a sequence \u00b6 Storing individual bits is a strategy for saving memory in high-performance applications, where each saved bit can have a dramatical impact on the performance. When working with small amounts of objects (less than millions), these strategies are not worth the effort, as we can store information in built-in types like int , char , or bool . There are multiple strategies for storing individual bits where each bit has its own meaning: using built-in arithmetic types and accessing individual bits using bit masks std::bitset using bitfields using std::vector<bool> The best strategy depends on the use case. The following table summarizes the pros and cons of the different strategies: Feature built-in arithmetic types std::bitset bitfields std::vector<bool> Maximum size per variable 64 bits unlimited 64 bits unlimited Dynamic size no no no yes Reading a subsequnece of bits \u00b6 For reading individual bits or the whole sequence of bits, each strategy has its own way of doing it. However, for reading a subsequence of bits, there is no machinery, so we have to resort to bit operations no matter of the strategy. For skipping first n bits, we use the right shift ( >> ) or the left shift operator, depending on endians. For skipping last n bits, we use a bitmask and the and ( & ) operator. Example: std::bitset<8> b = 0b10101010; std::cout << b.to_ulong() << std::endl; // prints 170 std::bitset \u00b6 cppreference std::bitset<N> is a class template that can store up to N bits. Reading \u00b6 To read a single bit , we can use: the test member function that returns a boolean value the operator[] operator that returns a boolean value To read more bits at once , we can use: the to_ulong member function that returns an unsigned long value the to_ullong member function that returns an unsigned long long value Both methods convert the whole bitset to an integer value of the corresponding type. If the bitset is larger than the integer type, an exception is thrown. There is no function for reading a specified sequence of bits . One way to overcome this is to read the whole bitset and apply a series of bit operations to get the desired bits (same as we would do with built-in arithmetic types). Another, slower option is to read the bits one by one using for loop. std::any : storing any type of value \u00b6 cppreference std::any is a class template that can store any type of value. It is a better alternative to traditional usage of void pointers. If the possible types are known at compile time, we should use std::variant instead. Value Categories \u00b6 [cppreferencepreerecege/value_category). In many contexts, the value category of an expression is important in deciding whether the code compiles or not, or which function or template overload is chosen. Therefore, it is usefull to be able to read value categories. expression value types: lvalue , meaning left-value. An expression typically on the left side of compound expression a statement, e.g. variable, member, or function name. Also, lvalues expressions are are: function ratoalls to fuctions returning lvalue assignments ++a , --a and similar pre operators *a indirection string literal cast prvalue , meaning pure rvalue. It is either a result of some operand ( + , / ) or a constructor/initializer result. The foloowing expressions are prvalues: literals with exception of string literals, e.g.: 4 , true , nullptr function or operator calls that return rvalue (non-reference) a++ , a-- and other post operators arithmetic and logical expressions &a address of expression this non-type template parameters, unless they are references lambda expressions requires expressions and concept spetializations xvalue , meaning expiring value. These valaues usually represent lvalues converted to rvalues. Xvalue expressions are: function call to functions returning rvalue reference (e.g., std::move ). member object expression ( a.m ) if a is an rvlaue and m is a non-reference type glvalue = lvalue || xvalue . rvalue = prvlaue || xvalue . Operators \u00b6 cppreferencen C++ supports almost all the standard operators known from other languages like Java, Python, or C#. Additionally, thsese operators can be overloaded. There are several categories of operators: arithmetic operators , including bitwise operators comparison operators logical operators assignment operators Note that the standard also supports alternative tokens for some operators (e.g., && -> and , || -> or , ! -> not ). However, these are not supported by all compilers. In MSVC, the /permissive- flag needs to be used to enable these tokens. User-defined Operators \u00b6 In C++ there are more operators than in other popular es like Python or Java. Additionally, these operators can be overloaded. See cppreferencen page for detailed description. Comparison Operators \u00b6 Default Comparison Operators \u00b6 cppreference . The != is usually not a problem, because it is implicitely generated as a negation of the == operator. However, the == is not generated by default, even for simple classes . To force the generation of a default member-wise comparison operator, we need to write: bool operator==(const My_class&) const = default; However, to do that, all members and base classes have to ae the operator == defined, otherwise the default operator will be implicitely deleted. The comparability can be checked with a std::equality_comparable<T> concept: staic_assert(std::equality_comparable<My_class>); Ternary Operator \u00b6 cppreference Ternary operator in C++ has the classical syntax of <condition> ? <true_expression> : <false_expression>; Note that both the true and false expressions must evaluate to the same type . Therefore, if we use polymorphism, we need to use manual type casting to the base type for at least one of the expressions (instead of relying on the implicit conversion when assigning to the result variable). typeid \u00b6 The typeid operator returns a std::type_info object that contains information about the type of the expression. Typically, we store the type_info in a std::type_index wrapper. Control Structures \u00b6 C++ supports the control structures known from other languages like Java, Python, or C#. Here, we focus on the specifics of C++. Switch Statement \u00b6 cppreference In C++, we can switch on integer types or enumeration types. Also, we can use classes that are implicitely convertible to integers or enums. Switch on string is not possible. The switch statement has the following syntax: switch(expression){ case value1: // code break; case value2: // code break; default: // code } However, it is usually a good idea to wrap each case in a block to create a separate scope for each case. Without it, the whole switch is a single block (contrary to if/else statements). The swich statements just jump to a case that matches the value, similarly to a goto statement. This can create problems, as for example variable initialization cannot be jumped over. The safe case statement looks like: switch(expression){ case value1:{ // code break; } case value2:{ // code break; } default:{ // code } } Classes and structs \u00b6 The only difference between a class and a struct is that in class, all members are private by default. Static Members \u00b6 cppreference Inside a class, the keyword static marks members that are not bound to any specific instance of the class, but to the class itself. They are declared as with the keyword static , but defined without it. To use a static member, we use the syntax <class_name>::<member_name> . Class Constants \u00b6 Class constants are static members that are defined as constants. They can be defined in two ways: static constexpr member variable if the constant type supports constexpr specifier, or static const member variable In the second case, we have to split the declaration and definition of the variable to avoid multiple definitions: // in the header file class My_class{ static const int a; } // in the cpp file const int My_class::a = 5; Friend declaration \u00b6 cppreference Sometimes, we need to provide an access to private members of a class to some other classes. In java, for example, we can put both classes to the same package and set the members as package private (no specifier). In C++, there is an even stronger concept of friend classes. We put a friend declaration to the body of a class whose private members should be accessible from some other class. The declaratiton can look as follows: Class To_be_accesssed { friend Has_access; } Now the Has_access class has access to the To_be_accesssed 's private members. Note that the friend relation is not transitive, nor symetric, and it is not inherited. Template friends \u00b6 If we want a template to be a friend, we can modify the code above: class To_be_accesssed { template<class T> friend class Has_access; } Now every Has_access<T> is a friend of To_be_accesssed . Note thet we need to use keyword class next to friend . We can also use only a template spetialization: class To_be_accesssed { friend class Has_access<int>; } or we can bound the allowed types of two templates togehter if both Has_access and of To_be_accesssed are templates: template<class T> class To_be_accesssed { friend class Has_access<T>; } Functions \u00b6 cppreference Function Declaration and Definition \u00b6 In C and C++, functions must have a: - declaration (signature) that specifies the function name, return type, and parameters - definition that specifies the function body The declaration has to be provided before the first use (call) of the function. The definition can be provided later. The declaration is typically provided in a header file, so that the function can be used outside the translation unit. The definition is typically provided in a source file. Merged Declaration and Definition \u00b6 If the function is not used outside the translation unit, the declaration and definition can be merged, i.e., the definition is itself a declaration. However, this is not recommended because after adding a corresponding declaration to one of the included headers (including libraries), the merged declaration/definition will become a definition of that function, which will be manifested as a linker error (multiple definitions of the function). Therefore, to control the visibility of the function, it is better to use other methods, provides in Section Visibility of Functions . Deciding between free function, member function and static member function \u00b6 Basically, you should decide as follows: Function needs access to instance -> member function Function should be called only by class members (i.e., member functions), so we want to limit its visibility, or we need to access static members of the class -> static member function Otherwise -> free function Argument-parameter Conversions \u00b6 Arg/param value reference rvalue value - - std::move reference implicit copy - copy constructor rvalue - not possible - Default Parameters \u00b6 Default function parameters in C++ works similarly to other languages: int add(int a, int b = 10); add(1, 2) // 3 add(1) // 11 However, the default parameters works only if we call the function by name. Therefore, we cannot use them in std::function and similar contexts. Example: std::function<int(int,int)> addf = add; std::function<int(int)> addf = add; // does not compile addf(1) // does not compile Also, the default parameters need to be values, not references or pointers. For references and pointers, we should use function overloading. Default Parameters and Inheritance \u00b6 TLDR: do not use default parameters in virtual functions. The default parameters are resolved at compile time. Therefore, the value does not depend on the actual type of the object, but on the declared type of the variable. This have following consequences: the default parameters are not inherited A* a = new B(); a->foo() will call B::foo() , with the default parameters of A::foo() To prevent confusion with inheritence we should use function overloading instead of default parameters in virtual functions (like in Java). Return values and NRVO \u00b6 For deciding the return value format, refer to the return value decision tree . Especially, note that NRVO is used in modern C++ and therefore, we can return all objects by value with no overhead most of the time. The NRVO works as follows: compiler tries to just tranfer the object to the parent stack frame (i. e. to the caller) without any move or copy if the above is not possible, the move constructor is called. if the above is not possible, the copy constructor is called. From C++17, the RVO is mandatory, therefore, it is unlikely that the compiler use a move/copy constructor. Consequently, most of the times, we can just return the local variable and let the rest to the compiler: unique_ptr<int> f(){ auto p = std::make_unique<int>(0); return p; // works, calls the move constructor automatically in the worst case (pre C++17 compiler) // return move( p ); // also works, but prevents NRVO } The NRVO is described also on cppreference together with initializer copy elision. Function Overlaoding \u00b6 Both normal and member funcions in C++ can be overloaded. The oveload mechanic, however, is quite complicated. There can be three results of overload resolution of some function call: no function fits -> error one function fits the best multiple functions fits the best -> error The whole algorithm of overload resolution can be found on cppreference . First, viable funcions are determined as functions with the same name and: with the same number of parameters with a greater number of parameters if the extra parameters has default arguments If there are no viable functions, the compilation fails. Otherwise, all viable functions are compared to get the best fit. The comparison has multiple levels. The basic principle is that if only one function fits the rules at certain level, it is chosen as a best fit. If there are multiple such functions, the compilation fails. Levels: Better conversion priority (most of the time, the best fit is found here, see conversion priority and ranking bellow) non-template constructor priority Conversion prioritiy and ranking \u00b6 cppreference When the conversion takes priority during the best viable function search, we say it is better . The (incomplete) algorithm of determining better conversion works as follows: standard conversion is better than user defined conversion user defined conversion is better then elipsis ( ... ) conversion comparing two standard conversions: if a conversion sequence S1 is a subsequence of conversion sequence S2, S1 is better then S2 lower rank priority rvalue over lvalue if both applicable ref over const ref if both applicable Conversion sequence ranks \u00b6 exact match promotion conversion : includes class to base conversion Constructor argument type resolution in list initialization \u00b6 When we use a list initailization and it results in a constructor call, it is not immediatelly clear which types will be used for arguments as the initialization list is not an expression. These types are, however, critical for the finding of best viable constructor. The following rules are used to determine the argument types (simplified): auto return type \u00b6 For functions that are defined inside declaration (template functions, lambdas), the return type can be automatically deduced if we use the auto keyword. The decision between value and reference return type is made according to the following rules: return type auto -> return by value return type auto& -> return by reference return type auto* -> return by pointer return type decltyype(auto) -> the return type is decltype(<RETURN EXPRESSION>) See more rules on cppreference Note that the auto return type is not allowed for functions defined outside the declaration (unless using the trailing return type). Function visibility \u00b6 The member function visibility is determined by the access specifier, in the same manner as the member variable visibility. For free functions , the visibility is determined by the linkage specifier . Without the specifier, the function is visible. To make it visible only in the current translation unit, we can use the static specifier. An equivalent way to make a function visible only in the current translation unit is to put it into an anonymous namespace : namespace { void f() {} } This way, the function is visible in the current translation unit, as the namespace is implicitly imported into it, but it is not visible in other translation units, because anonymous namespaces cannot be imported. One of the other approches frequently used in C++ is to put the function declaration into the source file so it cannot be included from the header. This solution is, however, flawed, unsafe, and therefore, not recommended . The problem is that this way, the function is still visible to the linker, and can be mistakenly used from another translation unit if somebody declare a function with the same name. Deleting functions \u00b6 cppreference We can delete functions using the delete keyword. This is mostly used for preventing the usage of copy/move constructors and assignment operators. However, it can be used for any function, as we illustrate in the following example: class My_class{ print_integer(int a){ std::cout << a << std::endl; } // we do not want to print doubles even they can be implicitly converted to int print_integer(double a) = delete; } Initialization and Assignment \u00b6 Loacal variables initialization/assignment \u00b6 Initialization happens in many contexts : in the declaration in new expression function parameter initialization return value initialization The syntax can be: (<expression list>) = expression list {<initializer list>} Finally, there are multiple initialization types, the resulting initialization type depends on both context and syntax: Value initialization: std::string s{}; Direct initialization: std::string s{\"value\"} Copy initialization: std::string s = \"value\" List initialization: std::string s{'v', 'a', 'l', 'u', 'e'} Aggregate initialization: char a[3] = {'a', 'b'} Reference initialization: char& c = a[0] Default initialization: std::string s List initialization \u00b6 List initialization initializes an object from a list. he list initialization has many forms, including: My_class c{arg_1, arg_2} My_class c = {arg_1, arg_2} my_func({arg_1, arg_2}) return {arg_1, arg_2} The list initialization of a type T can result in various initializations/constructios depending on many aspects. Here is the simplified algorithm: 1. T is aggregate -> aggregate initialization 2. The initializer list is empty and T has a default constructor -> value initialization 3. T has an constructor accepting std::initializer_list -> this constructor is called 4. other constructors of T are considered, excluding explicit constructors Value initialization \u00b6 cppreference This initializon is performed when we do not porvide any parameters for the initialization. Depending on the object, it results in either defualt or zero initialization. Aggregate initialization \u00b6 Aggregate initialization is an initialization for aggregate types. It is a form of list initialization. Example: My_class o1{arg_1, arg_2}; My_class o2 = {arg_1, arg_2}; // equivalent The list initialization of type T from an initializer list results in aggregate initialization if these conditions are fullfilled: the initializer list contains more then one element T is an aggregate type Nested initialization \u00b6 It is not possible to create nested initializatio statements like: class My_class{ int a, float b; public: My_class(ina a, float b): a(a), b(b) } std::tuple<int, My_class>{2, {3, 2.5}} // does not compile std::tuple<int, My_class>{2, My_class{3, 2.5}} // correnct version Member Initialization/Assignment \u00b6 There are two ways of member initialization: default member initialization initialization using member initializer list And then, there is an assignment option in constructor body . Reference: default member initialization constructor and initializer list One way or another, all members should be initialized at the constructor body at latest , even if we assign them again during all possible use cases. Reason: some types (numbers, enums, ...) can have arbitrary values when unassigned. This can lead to confusion when debugging the class, i.e., the member can appear as initialized even if it is not. easy support for overloading constructors, we can sometimes skip the call to the constructor with all arguments we can avoid default arguments in the constructor It is important to not use virtual functions in member initialization or constructor body , because the function table is not ready yet, so the calls are hard wired, and the results can be unpredictable, possibly compiler dependent. Default Member Initialization \u00b6 Either a brace initializer : My_class{ int member{1} } or an equals initializer : My_class{ int member = 1 } Member Initializer List \u00b6 Either using direct initialization (calling constructor of member ): My_class{ My_class(): member(1){ } or list initialization : My_class{ My_class(): member{1}{ } Constructor Body \u00b6 My_class{ My_class(){ member = 1 } } Comparison Table \u00b6 Ordered by priority, i.e., each method makes the methods bellow ignored/ovewritten if applied to the same member. Type In-place works for const members Constructor body no no Member initializer list yes yes Default member initializer yes, if we use direct initialization yes Constructors and Special Member Functions \u00b6 cppreference Special member functions are member functions that are someetimes defined implicitely by the compiler. The special member functions are: default (no parameter) constructor copy constructor copy assignment move constructor move assignment destructor These functions can be: defined implicitely by the compiler deleted implicitely by the compiler defaulted , i.e., defined by the compiler on our request cpp My_class() = default; Along with the comparison operators, these are the only functions that can be (see below). deleted , i.e., disabled by the compiler on our request cpp My_class(const My_class&) = delete; By default, all special member functions are defined implicitely if the members satisfy the requirements (see below). However, if we define any of the special member functions, the implicit definition is disabled. Therefore, typically, we define all special member functions or none of them. Constructor \u00b6 Defualt Variant \u00b6 The default constructor just create an empty object. The default constructor is not implicitly generated if: there is anothe constructor declared, including copy and move constructor there is some member that cannot be defaulty initialized Explicit constructor \u00b6 Sometimes, a normal constructor can lead to unexpected results, especially if it has only a single argument: class My_string { public: String(std::string string); // convert from std::string String(int length); // construct empty string with a preallocated size }; String s = 10; // surprise: empty string of size 10 istead of \"10\" To prevent these surprising conversion, we can mark the constructor explicit . The explicit keyword before the constructor name prevents the assigment using this constructor. The explicit constructor has to be explicitelly called. Call one constructor from another \u00b6 We can call one constructor from another using the delegating constructor . The syntax is: class My_class{ public: My_class(int a, int b): a(a), b(b){} My_class(int a): My_class(a, 0){} // delegating constructor } This way, we can call another constructor of the same class, or of the base class. Copy Constructor \u00b6 cppreference A copy constructor is called if an object is initialized from another object unless the move constructor is called as a better fit or the call is optimized out by copy elision . Some examples: initializing a new object from an existing object: My_class a; My_class b = a; // copy constructor called My_class c(a); // copy constructor called passing an object to a function by value: void f(My_class a){...} My_class a; f(a); // copy constructor called returning an object by value where the type is not movable and the compiler cannot optimize the call out. we call the copy constructor directly Implicit declaration and implicit deletion \u00b6 The copy constructor for type T is implicitely-declared if T has no declared user-defined copy constructors. If some there are some user-defined copy constructors, we can still force the implicit declaration of the copy constructor using the default keyword However, the implicit declaration does not mean that the copy constructor can be used! This is because the copy constructor can be implicitely defined as deleted . This happens if any of the following conditions is true: T has a non-static data member that cannot be copied. This can happen if any of the following is true: it has a deleted copy constructor, the copy constructor is inaccessible ( protected, private ) the copy constructor is ambiguous (e.g., multiple inheritance) T has a base class that cannot be copied, i.e., 1, 2, or 3 applies to at least one base class T has a non-static data member or base class with inaccessible destructor T has a rvlaue data member T has a user-defined move constructor or move assignment operator (this rule does not apply for defaulted copy constructor) The default implementationof copy constructor calls recursively the copy constructor of all base classes and on all members. For a pointer member, the copy object\u2019s member points to the same object as the original object\u2019s member Checking if a class is copy constructible \u00b6 We can check if a class is copy constructible using the std::is_copy_constructible type trait. Copy Assignment \u00b6 Copy Assignment is needed when we use the = operator with the existing class instances, e.g.: Class instanceA {}; Class instanceB; instanceB = instance A Move Constructor \u00b6 cppreference Move constructor semantic is that the new object takes the ownership of the resources of the old object. The state of the old object is unspecified, but it should not be used anymore. Move constructor is typically called when the object is initaialized from xvalue (but not prvalue!) of the same type. Examples: returning xvalue: Type f(){ Type t; return std::move(t); } passing argument as xvalue: f(Type t){ ... } Type t f(std::move(t)); initializing from xvalue: Type t; Type t2 = std::move(t); Note that for prvalues, the move call is eliminated by copy elision . Therefore, some calls that suggest move constructor call are actually optimized out: Type f(){ Type t; return t; // no move constructor call, copy elision } Type t = T(f()) // no move constructor call, copy elision Move constructor is needed: to cheaply move the object out from function if RVO is not possible to store the object in vector without copying it Note that a single class can have multiple move constructors, e.g.: both Type(Type&&) and Type(const Type&&) . Implicit declaration and implicit deletion \u00b6 The move constructor for type T is implicitely-declared if T has no declared copy constructors, copy assignment operators, move assignment operators, or destructors. If some of the above is declared, we can still force the implicit declaration of the move constructor using the default keyword However, that does not mean that the move constructor can be used! This is because the move constructor can be implicitely defined as deleted . This happens if any of the following conditions is true: T has a non-static data member that cannot be moved. A member cannot be moved if any of the following is true: it has a deleted, inaccessible (protected, private), or ambiguous move constructor, it is a reference, it is const -qualified T has a base class that cannot be moved, i.e., 1, 2, or 3 applies to at least one base class T has a non-static data member or base class with inaccessible destructor Checking if a class is move constructible \u00b6 We can check if a class is move constructible using the std::is_move_constructible type trait. However, the std::is_move_constructible does not check if the move constructor is accessible! Instead it checks if the call to the move constructor is valid (can success, compiles). The call can success if the move constructor is accessible, but it can also success if it is not accessible, but the class has a copy constructor, which is used instead. To check if the move constructor is accessible, we have to manually check the conditions, or disable the copy constructor. Move Assignment \u00b6 Trivial special member functions \u00b6 The special member functions are called trivial if they contain no operations other then copying/moving the members and base classes. For a special member function of type T to be trivial, all of the following conditions must be true: it is implicitly-declared or defaulted T has no virtual functions T has no virtual base classes the constructor for all direct base classes is trivial the constructor for all non-static data members is trivial Destructor \u00b6 We need destructor only if the object owns some resources that needs to be manually deallocated Typical usage \u00b6 It's mostly better to delete everything you don\u2019t need. Most likely, either - we need no custom constructors, or we need three (move and destructor), or we need all of them. Simple Temporary Object \u00b6 the object should live only in some local context we don\u2019t need anything Unique Object \u00b6 usually represents some real object usually, we need constructors for passing the ownership: move constructor move assignment Default Object \u00b6 copyable object We need copy constructor copy assignment move constructor move assignment Const vs non-const \u00b6 The const keyword can be used in two contexts: as a type qualifier, making the variable non-mutable ( cppreference ) as a member function qualifier, making the function work with const pointer to the instance ( cppreference ) A variable is non-mutable (const) if it is: const -qualified it is a member of a const object and it is not declared as mutable Non mutable variables: cannot be reassigned non-const member functions of them cannot be called A member function can be declared with const qualifier at the end of the declaration. A function declared with const: accesses its instance as const *<class_name> instead of <class_name>* by conclusion, see all members as const which means: non-const member functions of the object cannot be called non-const member variables of the object are accessed as const references For members, the const keyword can break the move operations on the object . For example we cannot move from a const std::unique_ptr<T> object. While this is also true for local variable, in members, it can lead to hard to find compilation errors, as a single const std::unique_ptr<T> member deep in the object hierarchy breaks the move semantic for the whole class and all subclasses. Avoiding duplication between const and non-const version of the same function \u00b6 To solve this problem without threatening the const-correctness, we need to implement the const version of a function and call it from the non-const one with double type cast: one that converts this to const, so we can call the const version of the function another one that removes const from the return value Example: const Content& get_content(unsigned index) const { Content content = ... // complicated code to get the right content return content; } Content& get_content(unsigned index){ return const_cast<Content&>(std::as_const(this*).get_content()); } Const/non const overloads and inheritance \u00b6 Normally, the compiler can safely choose the best match between const and non-const overloads. The problem can happen when each version is in a different place in the class hierarchy. Example: class Base { public: const int& get() const { return some; } protected: int some; }; class A : public virtual Base { public: int& get() { return some; } }; class B : public A {}; B test; test.get(); // ambiguous function error The problem is that the overload set is created for each class in the hierarchy separately. So if the overload was resolved prior the virtual function resolution, we would have only one version (non-const), which would be chosen, despite not being the best overload match in both overload sets. To prevent such unexpected result, some compilers (GCC) raise an ambiguous function error in such situations. To resolve that, we can merge the overload sets in class B : class B : public A { using Base:get; using A:get; }; IO and Filesystem \u00b6 The simple way to print to standard input is: std::cout << \"Hello world\" << std::endl; To return to the begining of the line and overwrite the previous output, we can use the '\\r' character: std::cout << \"Hello world\" << '\\r' << std::flush; File path manipulation \u00b6 Although we can use strings to work with file paths in C++, the standard format which is also easy to use is std::filesystem::path from the filesystem library . Basic operations: To create a path , we jusct call std::filesystem::path(<string path>) . We can easily join two paths by auto full_path = <path 1> / <path 2> ; To get the asolute path , we call std::filesystem::absolute(<path>) to get the path as CWD/<path> std::filesystem::canonical(<path>) to get the dots resolved. Note that this method throws exception if the path does not exists. The path to the current working directory can be obtained by calling std::filesystem::current_path() and set using std::filesystem::current_path(<path>) . To change the file extension (in the C++ representation, not in the filesystem), we can call the replace_extension method. Filesystem manipulation \u00b6 cppreference Copying \u00b6 To copy, we can use std::filesystem::copy(<source path>, <destination path>[, <options>]) function. The options parameter type is std::filesystem::copy_options . This enum is a bitmask type, therefore, multiple options can be combined using the | operator. Example: auto options = std::filesystem::copy_options::recursive | std::filesystem::copy_options::overwrite_existing; std::filesystem::copy(\"C:/temp/data\", \"c:/data/new\", options); Note that unlike the unix cp command, the copy function does not copy the directoy itself , even if the destination directory exists. Suppose we have two direcories: C:/temp/new C:/data/ And we want to copy the new folder, so that the result is: C:/data/new/ . In bash, this will be: cp -r C:/temp/new C:/data/ While in C++, we need to do: std::filesystem::copy(\"C:/temp/new\", \"C:/data/new\", std::filesystem::copy_options::recursive); Creating directories \u00b6 To create a directory, we can use std::filesystem::create_directory(<path>) function. This function fails if the parent directory does not exist. To create the parent directories as well, we can use std::filesystem::create_directories(<path>) function. Removing files and directories \u00b6 To remove a file or an empty directory, we can use std::filesystem::remove(<path>) function. To remove a content of a directory we can use std::filesystem::remove_all(<path>) function listed on the same page of cppreference. Other useful functions \u00b6 std::filesystem::exists(<path>) std::filesystem::is_directory(<path>) std::filesystem::is_regular_file(<path>) std::filesystem::is_empty(<path>) Getting a temporary folder \u00b6 To get a temporary folder, we can use the std::filesystem::temp_directory_path function. Manual text IO \u00b6 Input \u00b6 For input, we can use std::ifstream : std::ifstream file; file.open(<path>); ... file.close(); The important thing is that we need to check whether the open call was successful. The open function never throws an exception, even if the file does not exist , which is a common case. Instead, it only sets the failbit of the stream. Without some check, the failure is hidden as an ifstream in a fail state behaves as if it was empty. For reading line by line , we can use the std::getline function: std::string line; while (std::getline(file, line)) { // do something with the line } However, processing the line is currently not very convenient in C++ because functions from other languages like split are missing. For reading whitespace delimited tokens we can instead use the >> operator on the stream: // file content: \"01 Smith\" int id; std::string name; file >> id >> name; If we need to skip some tokens, its best to introduce a dummy string variable: // file content: \"01 2021-01-01 active Smith\" int id; std::string dummy; std::string name; file >> id >> dummy >> dummy >> name; Conveniently, the input streams have a bool operator that states whether the stream is in a state ready for reading. This way, we can easily stop the loop when the file is read, because the >> operator returns the stream itself: // read the whole file while (file >> id >> name) { ... } Output \u00b6 For line by line output, we use std::ofstream : std::ofstream file; file.open(<path>); batch_file << \"first line\" << std::endl; batch_file << \"second line\" << std::endl; ... batch_file.close(); Load whole file into string \u00b6 Again, we use the std::ifstream , but this time, we also use the std::istreambuf_iterator to read the whole file into a string: std::ifstream file(<path>); std::string content(std::istreambuf_iterator<char>{file}, {}); Here, the std::istreambuf_iterator<char> is created using initialization instead of the constructor so that the local variable is not confused with function declaration. The {} is used to create an empty string, which is the end of the range for the iterator. csv \u00b6 Input \u00b6 Output \u00b6 For csv output, we can usually use the general line-by-line approach. YAML \u00b6 For YAML, we can use the yaml-cpp library. We can load from file using YAML::LoadFile(<path>) from string using YAML::Load(<string>) for some reason, the YAML::Load function does not work with objects separated by indentation, so we need to use {} to separate the objects. To test whether a YAML::Node contains a certain key , we may use the [] operator, as it does not create a new node (unlike the stl containers): YAML::Node node; if (node[\"key\"]) { // do something } The iteration over the keys is done using YAML::const_iterator : for (YAML::const_iterator it = node.begin(); it != node.end(); ++it) { std::string key = it->first.as<std::string>(); YAML::Node value = it->second; } HDF \u00b6 To load data from HDF5 files, the HDF5 C++ API can be used. Typical usage: #include <H5Cpp.h> const H5::H5File file(\"file.h5\", H5F_ACC_RDONLY); const H5::DataSet dataset = file.openDataSet(\"dataset\"); const H5::DataSpace dataspace = dataset.getSpace(); hsize_t dims[2]; dataspace.getSimpleExtentDims(dims); H5::DataSpace memspace(2, dims); dataset.read( <pointer where to store the data>, H5::PredType::<data type>, memspace, dataspace ); If we do not know the dataset name, we can get the name by index: std::string dataset_name = file.getObjnameByIdx(0); // get the first dataset name To check if the dataset exists, we can use the exists method of the file: const H5::H5File file(\"file.h5\", H5F_ACC_RDONLY); if (file.exists(\"dataset\")) { ... } Inheritance \u00b6 Inheritance in C++ is similar to other languages, here are the important points: To enable overiding, a member function needs to be declared as virtual . Otherwise, it will be just hidden in a child with a function with the same name, and the override specifier cannot be used (see Shadowing). Multiple inheritance is possible. No interfaces. Instead, you can use abstract class with no data members. Virtual functions without implementation needs = 0 at the end of the declaration (e.g.: virtual void print() = 0; ) a type is polymorphic if it has at least one virtual function. I.e., the inheritance itself does not make the type polymorphic. Polymorphism \u00b6 Polymorphism is a concept for abstraction using which we can provide a single interface for multiple types that share the same parent. In C++, to use the polymorphism, we need to work with pointers or references . Imagine that we have these two class and a method that can process the base class: class Base { }; class Derived: public Base { }; void process_base(Base* base) { } Now we can use it lake this: Derived* derived = new Derived(); Base* base = derived; // we easilly can convert derived to base process_base(base); process_base(derived); // we can call the function that accepts a base pointer with a derived pointer We can do the same with smart pointers: void process_base_sh(std::shared_ptr<Base> base) { } std::shared_ptr<Derived> derived_sh = std::make_shared<Derived>(); std::shared_ptr<Base> base_sh = derived_sh; process_base_sh(base_sh); process_base_sh(derived_sh); Shadowing/Hiding: why is a function from parent not available in child? \u00b6 Members in child with a same name as another members in parent shadows those members (except the case when the parent member is virtual). When a member is shadowed/hiden, it is not available in the child class and it cannot be called using the child class instance. This can be counter-intuitive for functions as the shadowing considers only the name, not the signature . Example: class Base { public: void print() { printf(\"Base\\n\"); } }; class Child: public Base { public: void print(std::string type) { printf(\"Child \" + type + \"\\n\"); } }; int main() { Child child; child.print(); // does not compile, as the print() is hidden by print(std::string) return 0; } How to call a hidden function? \u00b6 There are two ways how to call a hideen function: we can use the using declaration in the child to introduce the hidden function: c++ class Child: public Base { public: using Base::print; // now the print() is available in Child void print(std::string type) { printf(\"Child \" + type + \"\\n\"); } }; Usiang a fully qualified name of the method: c++ int main() { Child child; child.Base::print(); return 0; } Constructors \u00b6 Parent constructor is always called from a child. By default, an empty constructor is called. Alternatively, we can call another constructor in the initializer. When we do not call the parent constructor in the child's initializer and the parent has no empty constructor, a compilation error is raised. Enablinging Parent Constructors in Child \u00b6 Implicitly, all methods from parent classes are visible in child, with exception of constructors. Constructors can be inherited manually with a using declaration, but only all at once. To enable only some constructors, we need to repeat them manually as child constructors and call parent construcors from them. Inheritance and Constructors/Destructors \u00b6 To prevent the future bugs with polymorphic destruction calls, it's a good habit to declare a public virtual destructor in each base class : class Base{ public: virtual ~Base() = default; } Otherwise, the following code will not call the child destructor: Child* child = new Child(); Base* base = (Base) child; delete base; But when defining destructor, constructor and move operations are not impliciotely generated. Moreover, the copy operations are generated enabling a polymorphic copy, which results in slicing. Therefore, the best approach for the base class is to: declare the virtual destrucor as default declare the default constructor . We need a default constructor, unless we use a diferent constructor and we want to disable the default one. declare the copy and move operations as protected . This way, the polymorpic copy is not possible, but proper copy/move operations are generated for every child class. Initializing base class members \u00b6 The base class members cannot be initialized in the child constructor initializer. Instead, we need to create a constructor in the base class and call it from the child constructor initializer. Slicing \u00b6 Polymorphism does not go well with value types. When a value type is copied, the only part that remains is the part writen in the code. That means that copying base_2 = base_1 result in a new Base object in base_2 , even if base_1 is an instance of child. Abstract classes therefore cannot be used as function value arguments at all . To pass a polymorphic type as a value to a library function, we need a copyable wrapper that forwards all calls to the undelying polymorphic type. Checking the Type \u00b6 There is no equivalent of Java's instanceof in C++. To check the type. it is possible to use dynamic cast: Child& child = dynamic_cast<Child&>(parent) In case of failure, std::bad_cast is thrown. To prevent exceptions (i.e., we need the type check for branching), we can use pointers: Child* child = dynamic_cast<Child*>(&parent) In this case, if the cast fails, then child == nullptr . Note that to use the dynamic_cast on a type, the type, the type needs to have at least one virtual method . However, this should not be an issue as the type should have at least a virtual destructor. Covariant Return Type \u00b6 Covariant return type is a concept of returning a narower type id derived class than the return type specified in base. Example: class Base { public: virtual Base& get() = 0; }; class Derived: public Base{ public: Derived& get() override { return *this; } }; It works with template classes too: template<class T> class Derived_template: public Base { public: Derived_template<T>& get() override { return &this; } }; Use Method from Parent to Override a Method from Other Parent \u00b6 Unlike in java, a parent method cannot be used to implement an interface of a child . Example: class Interface { public: virtual void print() = 0; }; class Base { public: virtual void print() { printf(\"Base\\n\"); } }; class Child: public Base, public Interface { public: }; int main() { Child child; // does not compile, as Child is an abstract class child.print(); return 0; } The above code does not compile as in C++, the parent print() method is not used as an impementation of print() from the interface (like it works e.g. in Java). There simplest solution to this problem is to override the method in Child and call the parent method staticaly: class Child: public Base, public Interface { public: void print() override { Base::print(); } }; Multiple inheritance and virtual base classes \u00b6 wiki cppreference Multiple inheritance is possible in C++. However, it can lead to some problems. Consider the following example: class A { public: int a; }; class B: public A {}; class C: public A {}; class D: public B, public C {}; It may not be obvious, but the class D has two instances of A in it. This is because the B and C both have their own instance of A . This is certainly not what we want as this way, we have two copies of A::a in D , which are only accessible using qualified names ( D::B::a and D::C::a ) and which can have different values. Virtual Inheritance \u00b6 To mitigate this problem, we can use the virtual inheritance . The virtual inheritance is used when we want to have only one instance of a base class in a child class, even if the base class is inherited multiple times. To use the virtual inheritance, we need to declare the base class as virtual in all child classes: class A { public: int a; }; class B: public virtual A {}; class C: public virtual A {}; class D: public B, public C {}; Multiple copy/move calls with virtual inheritance \u00b6 However, this solves only the problem of having multiple instances of the same base class. But there are also problems with the copy and move operations. In the above example, if the class D is copied or moved, it calls the copy/move operations of B and C , which in turn call the copy/move operations of A . This means that the A is copied/moved twice , which is not what we want. To solve this we need to manually define the copy/move operations of classes in the hierarchy so that the copy/move operations of the base class are called only once. However this can be a complex task. Also, it can backfire later when we extend the hierarchy. Other sources \u00b6 SO answer SO answer 2 Templates \u00b6 The templates are a powerful tool for: generic programming, zero-overhead interfaces, and metaprogramming. Although they have similar syntax as generics in Java, they are principialy different both in the way they are implemented and in the way they are used. There are two types of templates: function templates class templates Syntax \u00b6 Template Declaration \u00b6 Both for classes and functions, the template declaration has the following form: template<<template parameters>> Template definition \u00b6 The definition of template functions or functions fo the template class requires the template declaration to be present. The definition has the following form: template<<template parameters>> <standard function definition> Here, the template parameters are the function template parameters if we define a template function, or the class template parameters if we define a function of a template class. If the template function is a member of a template class, we have to specify both the template parameters of the function and the template parameters of the class: template<<class template parameters>> template<<function template parameters>> <standard class function definition> Note that the template definition has to be in the header file , either directly or included from another header file. This includes the member function definitions of a template class, even if they are not templated themselves and does not use the template parameters of the class. Template Parameters \u00b6 cppreference The template parameters can be: type parameters: class T value parameters: int T template parameters: template<class T> class TT Template parameters can be restricted by in several ways: typename T : any complete type class T : a class type <Concept> T : a type constrained by a concept by a requires expression, e.g.: template<typename T> requires std::is_integral_v<T> Organization rules \u00b6 *.h : declarations *.tpp template definitions *.cpp non-template definitions. For simplicity, we include the tpp files at the end of corresponding header files. If we need to speed up the compilation, we can include the tpp files only in the source files that needs the implementations , as described on SE To speed up the build it is also desireble to move any non-template code to source files , even through inheritance, if needed. Templates and Namespaces \u00b6 If the templated code resides in a namespace, it can be tempting to save few lines of code by sorrounding both .h and .tpp files using one namespace expression: // structs.h hile namespace my_namespace { // declarations... #include 'structs.tpp' } // structs.tpp // definitions However, this can confuse some IDEs (e.g., false positive errors in IntelliSense), so it is better to introduce the namespace in both files: // structs.h hile namespace my_namespace { // declarations... } #include 'structs.tpp' // structs.tpp namespace my_namespace { // definitions } Don't forget to close the file and reopen it after the change to clear the errors. Providing Template Arguments \u00b6 A template can be instantiated only if all the template arguments are provided. The templete arguments need to be complete types . Arguments can be provided explicitly: std::vector<int> v; or sum<int>(1,2) , deduced from the initialization (classes): std::vector v = {1,2,3}; from the context (functions): sum(1,2); , or defaulted ```cpp template class A {}; template int sum (T a, T b = 0) { return a + b; } auto s = sum(1, 2); A a(); ``` If we want the template arguments to be deduced or defaulted, we usually use the <> : template<class T = int> class A {}; A<> a(); // default argument is used std::vector<A<>> v; // default argument is used In some cases, the <> can be ommited, e.g., when declaring a variable: A a; // default argument is used // but std::vector<A> v; // error, the A is considered a template here, not the instantiation The rules for omitting the <> are quite complex. Therefore, it is better to always use the <> when we want to use the default arguments. Rules for omitting the <> \u00b6 We can ommite the <> in the following cases: when declaring a variable: A a; when using the type in a function call: f(A()); when instantiating a template class: class B: public A {}; We cannot ommite the <> in the following cases: When we use the template as a nested type: std::vector<A<>> v; , not std::vector<A> v; in the return type of a function: A<> f() , not A f() When declaring an alias: using B = A<> not using B = A for template template parameters. Default Template Arguments \u00b6 Default template arguments can be used to provide a default value for any template parameter except parameter packs. For template classes, there is a restriction that after a default argument is used, all the following parameters must have a default argument as well, except the last one wchich can be parameter pack. Template Argument Deduction \u00b6 Details on cppreference . Template argument deduction should work for: constructors function and operator calls storing the function pointer Class Template Argument Deduction (CTAD) \u00b6 Details on cppreference . The main difference from the function templete argument deduction is that in CTAD, all the template arguments needs to be specified, or all must not be specified and must be deducible. Apart from that, there are more subtle differences arising of a complex procedure that is behind CTAD. We explain CTAD principle using a new concept (not a C++ concept :) ) called deduction guides . Deduction Guides \u00b6 The CTAD use so called deductione guides to deduce the template parameters. Deduction guides can be either implicit or explicit. To demonstrate the principle, let's first start with user-defined deduction guides. User defined deduction guides \u00b6 Let's have an iterator wrapper class below: template<class E, Iterator<E> I> class Iter_wrapper{ public: explicit Iter_wrapper(I iterator){ ... } ... }; Here, the argument E cannot be deduced from argument I , despite the use of the Iterator concept may indicate otherwise. We can still enable the deduction by adding the following deduction guide: template<class I> Iter_wrapper(I iterator) -> Iter_wrapper<decltype(*iterator),I>; Here, the part left from -> represents the constructor call that should be guided, and the part right from -> defines the argument types we want to deduce. Some more details about user defined deduction guides are also on the Microsoft Blog . Implicit deduction guides \u00b6 The vast majority of deduction guidedes used in CTAD are implicit. The most important implicit deduction guides are: constructor deduction guides copy deduction guides The copy deduction guide has the following form: template<<class template parameters>> <class>(<class><class template parameters> obj) -> <class><class template parameters>; For a simple wrapper class: template<class C> class Wrapper{ ... } template<class C> Wrapper(Wrapper<C> obj) -> Wrapper<C>; // implicitelly defined copy deduction guide The constructor deduction guides has the following form: template<<class template parameters>> <class>(<constructor arguments>) -> <class><class template parameters>; For a simple wrapper class: template<class C> class Wrapper{ Wrapper(T&& ref); } template<class C> Wrapper(C&&) -> Wrapper<C>; // implicitelly defined constructor deduction guide Deduction guides resolution \u00b6 Note that CTAD is a process independent of the constructor overload! . First an appropriate deduction guide is used to deduce the class template argumnets, this process can fail if there is no guide. Only then, the overload resolution begins. Most of the time, it is not so important and we can just look at the constructor that is chosen by the constructor overload resolution process and see the used deduction guids and consequently, the resulting template arguments. Sometimes, however, this simplified understanding can lead to confusing results: template<class C> class Wrapper{ Wrapper(T&& ref); Wrapper(double&& ref); // special overload for double } auto w1 = Wrapper(1.5) // the double overload is called In the above example, it may be surprising that the second constructor can be called, as it does not have the class argument present, so the implicit deduction guide cannot work: template<class C> Wrapper(double&&) -> Wrapper<C>; // C unknown! However, it compiles and works, because the deduction guide from the first constructor is used for CTAD, and then, the second constructor is chosen by the constructor overload. Template Specialization \u00b6 Template specialization is a way to provide a different implementation of a template for a specific type. For example, we can provide a different implementation of a template for a std::string type. Imagine following class: // declaration template<class T> class Object{ public: void print(T value) }; // definition template<class T> void Object<T>::print(T value){ std::cout << value << std::endl; } Now, we can provide a different implementation for std::string : // declaration template<> class Object{ public: void print(std::string value) }; template<> void Object<std::string>::print(std::string value){ std::cout << value << std::endl; } There are two types of template specialization: full specialization : exact specification for all template arguments partial specialization : exact specification for a subset of template arguments and/or non-type template arguments To demonstrate the difference, let's have a look at the following example: // declaration template<class T, class C> class Object{}; // primary template // full specialization template<> class Object<int, std::string>{}; // full specialization // partial specializations template<class C> class Object<int, C>{}; // not a full specialization, as C is not specified template<std::integral T, My_concept C> class Object<T, C>{}; // not a full specialization, types are not exactly specified While behaving similarly, there are some important differences between the full and partial specialization : Declaration and definition: Full specialization is a new type. Therefore, it must be declared in the header and defined in the source file ( .cpp ). additionaly, if the full specialization is a member of a class, the declaration must be outside the class: ```cpp template<> class Object{ public: // member function template template void print(T value){ ... // template definition } // template full specialization declaration - wrong. This will not compile in GCC template<> void print(std::string value){ ... // template definition } }; // template full specialization declaration - correct template<> void Object::print(std::string value); ``` Partial specialization is still just a template, so it must be defined in the header file ( .h or .tpp ). For functions, we cannot provide a partial specialization . For member functions we can solve this by specializing the whole class. For free functions, we have to use other techniques like compile-time branching . Type Erasure \u00b6 cppreference When working with templates, one soon realizes that they are contiguous. Once we use a template with a parameter T , we have to use T in the calling code if we do not know the exact type, and that is also true for the calling code of the calling code, etc, all the way to the point where we know the exact type. Often times, this is principially unavoidable, because we need to use the template parameter to keep the contract on the template parameter type. However, there are situation where this is absolutely useless. Imagine this scenario: template<class T> class Vector_statistic{ void print_vector_size(const std::vector<T>& vector){ return vector.size(); } }; Here, there is no type contract on the template parameter T needed. Yet, we have to implement Vector_statistic as a template class, otherwise it won't compile. To avoid this, we can use the technic called type erasure . The principle of type erasure is to create a parent class without the template parameter T that contain the necessary methods that does not depend on the type T . Then, we use this polymorphic interface to avoid the need for the template parameter T . For the above example, we also need a wrapper class as we cannot change the std::vector class template to inherit from our parent class. class Vector_statistic_wrapper{ public: virtual void print_vector_size() = 0; }; template<class T> class Vector_statistic_wrapper_impl: public Vector_statistic_wrapper{ public: Vector_statistic_wrapper_impl(const std::vector<T>& vector): vector(vector){} void print_vector_size() override{ return vector.size(); } private: std::vector<T>& vector; }; class Vector_statistic{ public: Vector_statistic(){} void print_vector_size(const Vector_statistic_wrapper& wrapper){ return wrapper.print_vector_size(); } }; Note that for this example, it is probably simpler to use the class template even though we do not need the template parameter T . However, if the call chain independent of the type T is complex, the type erasure solution can become much simpler and more readable. Also note that the type erasure solution can never be more efficient. We can save a little bit on the compile time and code size, but the runtime performance will be strictly lower (due to the virtual function calls) Sources - https://davekilian.com/cpp-type-erasure.html Template parameter packs \u00b6 cppreference on all parameter packs, but mostly template parameter packs From many programming languages, ve know the concept of function parameter packs (Java), or variable length argument lists (Python). These are available in C++ as well. However, in C++, we also have a much more complex and powerful concept of template parameter packs. The differnece from function parameter packs is that template parameter packs are resolved at compile time, efectively creating function or class types with a variable number of arguments. A practical example can be a template function that measures the time of the execution of a function: template<typename F, typename... A> auto measure_time(F func, A... args){ auto start = std::chrono::high_resolution_clock::now(); func(args...); auto end = std::chrono::high_resolution_clock::now(); return std::chrono::duration_cast<std::chrono::milliseconds>(end - start).count(); } Syntax \u00b6 Be aware that the dots ( ... ) placement is different for: declaration of the template parameter pack: dots after the parameter restriction use of the template parameter pack as a type specifier: dots after the type name using the parameter pack in an expression: dots after the parameter pack name Example: template<typename... T> // dots after the parameter restriction void func(T... args) { // dots after the parameter type auto result = other_func(args...); // dots after the parameter pack name } Parameter Pack Expansion \u00b6 When using the dots ( ... ), the parameter pack is expanded. There are many allowed contexts for the expansion, with different rules for the expansion. When having a parameter pack A... , available as A... args , it can be used: in a function call: func(args...) expands to func(arg1, arg2, ..., argN) class initialization: My_class(args...) expands to My_class(arg1, arg2, ..., argN) brace initialization: {args...} expands to {arg1, arg2, ..., argN} template argument list: std::tuple<args...> expands to std::tuple<arg1, arg2, ..., argN> function parameter list: template<typename... A> void func(A... args) expands to void func(A1 arg1, A2 arg2, ..., AN argN) class template parameter list: template<typename... A> class My_class<A...> expands to My_class<A1, A2, ..., AN> base class specifier: class My_class: public Args... expands to class My_class: public A1, public A2, ..., public AN lambda capture: [args...] expands to [arg1, arg2, ..., argN] Fold Expressions \u00b6 cppreference Fold expressions is a powerful feature that allows to cleanly implement operations on the whole parameter pack at once, which would otherwise be very difficult to implement. There are four possible patterns for fold expressions: (<pack> <operator> ...) (... <operator> <pack>) (<pack> <operator> ... <operator> <initial value>) (<initial value> <operator> ... <operator> <pack>) Here, the <operator> is any binary operator (arithmetic, logical, bitwise, assignment, , ,...). the <initial value> is an expression without any operator with higher precedence than the <operator> . The fold expression is evaluated as follows: (T... <operator> ...) evaluates to (<T1> <operator> (<T2> <operator> ... <operator> <TN>)) (... <operator> T...) evaluates to ((<T1> <operator> <T2>) <operator> ... <operator> <TN>) (T... <operator> ... <operator> <initial value>) evaluates to (<T1> <operator> (<T2> <operator> ... <operator> (<TN> <operator> <initial value>))) (<initial value> <operator> ... <operator> <T...>) evaluates to ((((<initial value> <operator> <T1>) <operator> <T2>) <operator> ... <operator> <TN>) Note that we can use fold expressions anywhere where we can use parameter pack expansion. Therefore, we can use them even in e.g. requires expression. Examples \u00b6 An example of a fold expression used for a sum function template : template<typename... Args> auto sum(Args... args){ return (... + args); } Fold expression in the requires expression : template<typename... NO> template<template <typename, class...> class S> requires(DARP_benchmark_solver_constructor_interface<S,NO> && ...) The above requires expression will be expanded to: (DARP_benchmark_solver_constructor_interface<S,NO1> && DARP_benchmark_solver_constructor_interface<S,NO2> && ... && DARP_benchmark_solver_constructor_interface<S,NON>) Using Complicated Types as Template Arguments \u00b6 Sometimes, it can be very tricky to determine the template argument we need in order to use the template. The correct argument can be for example a return value of some function, templete function, or even member function of a template instanciation which has other templates as argument... To make it easier, we can, istead of suplying the correct arguments, evaluate an expression that returns the correct type and then use the decltype specifier. For more info, see the Determining Type from Expressions section. Type Traits \u00b6 The purpose of type traits is to create predicates involving teplate parameters. Using type traits, we can ask questios about template parameters. With the answer to these questions, we can even implement conditional compilation, i.e., select a correct template based on parameter type. Most of the STL type traits are defined in header type_traits . A type trate is a template with a constant that holds the result of the predicate, i.e., the answer to the question. More about type traits Usefull Type Traits \u00b6 std::is_same std::is_base_of std::is_convertible std::conditional : enables if-else type selection Replacement for old type traits \u00b6 Some of the old type traits are no longer needed as they can be replaced by new language features, which are more readable and less error prone. Some examples: std::enable_if can be replaced by concepts: // old: enable_if template<class T> void f(T x, typename std::enable_if_t<std::is_integral_v<T>, void> = 0) { std::cout << x << '\\n'; } // new: concepts template<std::integral T> void f(T x) { std::cout << x << '\\n'; } Concepts \u00b6 cppreference Concepts are named sets of requiremnets. They can be used instead of class / typename keywords to restrict the template types. The syntax is: template<class T, ....> concept concept-name = constraint-expression The concept can have multiple template parameters. The first one in the declaration stands for the concept itself, so it can be refered in the constraint expression. More template parameters can be optionally added and their purpose is to make the concept generic. Constraints \u00b6 Constraints can be composed using && and || operatos. For atomic constaints declaration, we can use: Type traits: template<class T> concept Integral = std::is_integral<T>::value; Concepts: template<class T> concept UnsignedIntegral = Integral<T> && !SignedIntegral<T>; Requires expression: template<typename T> concept Addable = requires (T x) { x + x; }; Either form we chose, the atomic constraint have to always evaluate to bool. Requires Expression \u00b6 Requires expressions ar ethe most powerfull conctraints. The syntax is: requires(<parameter list>) { <requirements> } Here, the <parameter list> is a list of objects (instances of types) that we need to use in the <requirements> . There are four types of <requirements> that can appear in the requires expression: simple requiremnet : a requirement that can contain any expression. Evaluates to true if the expression is valid. cpp requires (T x) { x + x; }; type requirement : a requiremnt checking the validity of a type: cpp requires { typename T::inner; // required nested member name typename S<T>; // required class template specialization typename Ref<T>; // required alias template substitution }; compound requirement : Checks the arguments and the return type of some call. It has the form: {expression} -> return-type-requirement; cpp requires(T x) { {*x} -> std::convertible_to<typename T::inner>; } other useful type traits can be used instead of std::convertible_to . Nested requirement : a require expression inside another requires expression: cpp requires(T a, size_t n) { requires Same<T*, decltype(&a)>; // nested } Auto filling the first template argument \u00b6 Concepts have a special feature that their first argument can be autoffiled from outer context. Consequentlly, you then fill only the remaining arguments. Examples: //When using the concept template<class T, class U> concept Derived = std::is_base_of<U, T>::value; template<Derived<Base> T> void f(T); // T is constrained by Derived<T, Base> // When defining the concept template<typename S> concept Stock = requires(S stock) { // return value is constrained by std::same_as<decltype(stock), double> {stock.get_value()} -> std::same_as<double>; } STL Concepts \u00b6 iterator concepts Usefull Patterns \u00b6 Constrain a Template Argument \u00b6 Imagine that you have a template function load and an abstract class Loadable_interface that works as an interface: class Loadable_interface{ virtual void load() = 0; }; template<class T> void load(T to_load){ ... to load.load() ... }; Typically you want to constraint the template argument T to the Loadable_interface type, so that other developer clearly see the interface requirement, and receives a clear error message if the requirement is not met. In Java, we have an extend keyword for this purpose that can constraint the template argument. In C++, this can be solved with concepts. First we have to define a concept that requires the interface: template<typename L> concept Loadable = std::is_base_of_v<Loadable_interface, L>; Than we can use the concept like this: template<Loadable T> void load(T to_load){ ... to load.load() ... }; Constraint a Concept Argument \u00b6 Imagine that you have a concept Loadable that requires a method load to return a type T restricted by a concept Loadable_type . One would expect to write the loadable concept like this: template<typename L, Loadable_type LT> concept Loadable = requires(L loadable) { {loadable.load()} -> LT; }; However, this is not possible, as there is a rule that concept cannot not have associated constraints . The solution is to use an unrestricted template argument and constrain it inside the concept definition: template<typename L, typename LT> concept Loadable = Loadable_type<LT> && requires(L loadable) { {loadable.load()} -> LT; }; Sources \u00b6 https://en.cppreference.com/w/cpp/language/constraints Requires expression explained Interfaces \u00b6 In programming, an interface is usualy a set of requirements that restricts the function or template parameters, so that all types fulfiling the requiremnet can be used as arguments. Therte are two ways how to create an interface in C++: using the polymorphism using templates argument restriction While the polymorphism is easier to implement, the templating is more powerful and it has zero overhead. The most important thing is probably that despite these concepts can be used together in one application, not all \"combinations\" are allowed especialy when using tamplates and polymorphism in the same type. Note that in C++, polymorphism option work only for function argument restriction, but we cannot directly use it to constrain template arguments (unlike in Java). To demonstrate all possible options, imagine an interface that constraints a type that it must have the following two functions: int get_value(); void set_value(int date); The following sections we will demonstrate how to achieve this using multiple techniques. Interface using polymorfism \u00b6 Unlike in java, there are no interface types in C++. However, we can implement polymorfic interface using abstract class. The following class can be used as an interface: class Value_interface{ virtual int get_value() = 0; virtual void set_value(int date) = 0; } To use this interface as a fuction argument or return value, follow this example: std::unique_ptr<Value_interface> increment(std::unique_ptr<Value_interface> orig_value){ return orig_value->set_value(orig_value->get_value() + 1); } This system works in C++ because it supports multiple inheritance. Do not forget to use the virtual keyword, otherwise, the method cannot be overriden. Note that unlike in other languages, in C++, the polymorphism cannot be directly use as a template (generic) interface. Therefore, we cannot use the polymorfism alone to restrict a type. Using template argument restriction as an interface \u00b6 To use template argument restriction as an interface, we can use concepts. The following concept impose the same requirements as the interface from the polymorphism section: To use template argument restriction as an interface, we can use concepts. The following concept impose the same requirements as the interface from the polymorphism section: template<class V> concept Value_interface = requires(V value_interface){{value_interface.get_value()} -> std::same_as<int>; } && requires(V value_interface, int value){{value_interface.set_value(value)} -> std::same_as<void>; } Remember that the return type of the function has to defined by a concept , the type cannot be used directly. Therefore, the following require statement is invalid: requires{(V value_interface){value_interface.get_value()} -> int; } To use this interface as an template argument in class use: template<Value_interface V> class ... And in function arguments and return types: template<Value_interface V> V increment(V orig_value){ return orig_value.set_value(orig_value.get_value() + 1); Restricting the member function to be const \u00b6 To restrict the member function to be const, we neet to make the type value const in the requires expression: template<class V> concept Value_interface = requires{(const V value_interface) {valvalue_interfaceue.get_value() -> std::same_as<int>;}; }; Using concepts and polymorphism together to restrict template parameters with abstract class \u00b6 We cannot restrict template parameters by polymorphic interface directly, however, we can combine it with concept. The folowing concept can be used together with the interface from the polymorphic interface section: template<class V> concept Value_interface_concept = requires std::is_base_of<Value_interface,V> Neverthless, as much as this combination can seem to be clear and elegent, it brings some problems. . We can use concepts to imposed many interfaces on a single type, but with this solution, it can lead to a polymorphic hell. While there is no problem with two concepts that directly requires the same method to be present with abstract classes, this can be problematic. Moreover, we will lose the zero overhead advantage of the concepts, as the polymorphism will be used to implement the interface. The Conflict Between Templates and Polymorphism \u00b6 As described above, messing with polymorphism and templates together can be tricky. Some examples: No Virtual Member Function with Template Parameters \u00b6 An example: a virtual (abstract) function cannot be a template function ( member template function cannot be virtual), so it cannot use template parameters outside of those defined by the class template. Polymorphism cannot be used inside template params \u00b6 If the functin accepts MyContainer<Animal> we cannot call it with MyContainer<Cat> , even if Cat is an instance of Animal. Possible solutions for conflicts \u00b6 do not use templates -> more complicated polymorphism ( type erasure for members/containers) do not use polymorphism -> use templates for interfaces an adapter can be used Polymorphic members and containers \u00b6 When we need to store various object in the same member or container, we can use both templates and polymorphism. However, both techniques has its limits, summarized in the table below: | | Polymorphism | Templates | | -- | -- | -- | | The concrete type has to be known at compile time | No | Yes | For multiple member initializations, the member can contain any element. | No , the elements have to share base class. | Yes | | For a single initialization, the containar can contain multiple types of objects | Yes , if they have the same base class | No | We can work with value members | No | Yes | When using the interface, we need to use downcasting and upcasting | Yes | No Deciding between template and polymorphism \u00b6 Frequently, we need some entity(class, function) to accept multiple objects through some interface. We have to decide, whether we use templates, or polymorphism for that interface. Some decision points: We need to return the same type we enter to the class/function -> use templates We have to access the interface (from outside) without knowing the exact type -> use polymorphism We need to restrict the member/parametr type in the child -> use templates for the template parameter if you need to fix the relation between method parameters/members or template arguments of thouse, you need to use templates If there are space considerations, be aware that every parent class adds an 8 byte pointer to the atribute table In general, the polymorphic interface have the following adventages: easy to implement easy to undestand similar to what people know from other languages On the other hand, the interface using concepts has the following adventages: no need for type cast all types check on compile time -> no runtime errors zero overhead no object slicing -> you don't have to use pointers when working with this kind of interface we can save memory because we don't need the vtable pointers Iterators, STL algorithms, and ranges \u00b6 If we want to iterate over elements in some programming language, we need to fulfill some interface. In Java, this interface is called Iterable . Also, there is usually some interface that formalize the underlying work, in Java, for example, it is called Iterator . In C++, however, the interface for iteration is not handled by polymorphism. Instead, it is handled using type traits and concepts. On top of that, there are multiple interfaces for iteration: legacy iteration, e.g., for (auto it = v.begin(); it != v.end(); ++it) STL algorithms, e.g., std::find(v.begin(), v.end(), 42) STL range algorithms, e.g., std::ranges::find(v, 42) STL range views, e.g., std::ranges::views::filter(v, [](int x){return x > 0;}) The following table summarizes the differences between the interfaces: |---| Plain iteration | STL algorithms | STL range algorithms | STL range views | |---|---|---|---|---| | Interface | type traits | type traits | concepts | concepts | | Iteration | eager | eager | eager | lazy | | Modify the underlying range | no | yes | yes | no | | Can work on temporaries * | yes | yes | yes | no | *If the operation modifies the data, i.e., sorting, shuffling, transforming, etc. The examples below demonstrate the differences between the interfaces on the following task: create a vector of 10 elements with values 0,1,2,...,9, i.e., the same as Python range(10) . // plain iteration std::vector<int> vec(10); int i = 0; for (auto it = vec.begin(); it != vec.end(); ++it) { *it = i; ++i; } // legacy algorithm std::vector<int> vec(10); std::iota(vec.begin(), vec.end(), 0); // C++11 way, legacy interface using type traits // range algorithm std::vector<int> vec(10); std::ranges::iota(vec.begin(), vec.end(), 0); // basically the same, but the constructor arguments are constrained with concepts // same using adaptor auto range = std::views::iota(0, 10); std::vector vec{range.begin(), range.end()}; // in-place vector construction Terminology \u00b6 range : the object we iterate over (Iterable in Java) iterator : the object which does the real work (Iterator in Java) Usually, a range is composed of two iterators: begin : points to the beginning of the range, returned by <range_object>.begin() end : points to the end of the object, returned by <range_object>.end() Each iterator implements the dereference ( * ) operator that acces the element of the range the iterator is pointing to. Depending on the iterator type, the iterator also supports other operations: ++ , -- to iterate along the range, array index operator ( [] ) for random access, etc. Most of the STL collections (vector, set,...) are also ranges. How to choose the correct interface? \u00b6 when deciding which interface to use, we can use the following rules: If the number of tasks and the complexity of the tasks is high, use the legacy iteration . It is hard to write a 20 line for loop with various function calls as algorithm or adaptor and the result would be hard to read. Otherwise, if you need to preserve the original range as it is or you need to compose multiple operations, use the STL range adaptors . Otherwise, use the STL range algorithms . Note that the in this guide, we do not consider the legacy STL algorithms. With the availability of the STL range algorithms, there is no reason to use the legacy algorithms, except for the backward compatibility or for the algorithms that are not yet implemented in the STL. Also note that some STL algorithms are principially non-modifying, e.g., std::ranges::find or std::ranges::count . These algorithms logically do not have the adaptor equivalent. STL ranges and views \u00b6 https://en.cppreference.com/w/cpp/ranges In C++ 20 there is a new range library that provides functional operations for iterators. It is similar to functional addon in Java 8. As explained in the beginning of this chapter, there are two ways how to use the STL ranges: using the range algorithms ( ranges::<alg name> ) that are invoked eagerly. using the range views ( ranges::views::<view name> ) that are invoked lazily. Note that the range algorithms and adaptors cannot produce result without an input, i.e., we always need a range or collection on which we want to apply our algorithm/view. STL range views \u00b6 The difference of range view to range algorithms is that the views are lazy, i.e., they do not produce any result until they are iterated over. This is similar to the Python generators. The advantage is that we can chain multiple views together and the result is computed only when we iterate over the final view. Note that due to the lazy nature of the views, the underlying range has to be alive during the whole iteration . Therefore, we cannot use the views on temporaries, e.g., we cannot useviews directly in the constructor of a vector, or we cannot use the views on a temporary range returned by a function. A custom view can be created so that it can be chained with STL views. However, it has to satisfy the view concept , and more importantly, it should satisfy the view semantic, i.e., it should be cheap to copy and move (without copying the underlying data). Usefull views \u00b6 std::views::iota : generates a sequence of numbers std::views::filter : filters the elements of the range Projections \u00b6 Unlike in Java, we cannot refer to member functions when lambda functions are required. However, we can use these member functions when the algorithm or adaptor has a projection parameter. Example: struct Data { int a; std::string b; ... }; std::vector<Data> data = get_data(); std::sort(data, {}, &Data::b); The trick here is that we can only provide the member that we want to use for sorting, but the sorting logic ( first < second ...) is handeled by a standard comparator (the second argument initialized with {} ). We can have even more complicated projections that are not just references to member functions: std::vector<My_class> objects = get_objects(); std::vector<unsigned> indexes = get_indexes(objects); auto proj = [&objects](std::size_t i) -> const std::string& { return objects[i].get_name(); }; std::ranges::sort(indexes, {}, proj) // sort indexes using the property of objects Useful range algorithms \u00b6 Note that the most frequently used algorithms have a separate section in the Iterators chapter. std::shuffle : shuffles the elements in the range (formerly std::random_shuffle ). std::adjacent_find : finds the first two adjacent elements that are equal. Can be used to find duplicates if the range is sorted. std::ranges::unique : moves the duplicates to the end of the range and returns the iterator to the first duplicate. Only consecutive duplicates are found. std::ranges::min : finds the smallest element in the range. We can use either natural sorting, or a comparator, or a projection. If the range is empty, the behavior is undefined. std::ranges::min_element : finds the smallest element in the range. Unlike std::ranges::min , this function returns an iterator to the smallest element. std::ranges::empty : checks whether the range is empty. Other Resources \u00b6 https://www.modernescpp.com/index.php/c-20-the-ranges-library Boost ranges \u00b6 In addition to the STL range algorithms and adaptors, boost has it's own range library with other more complex algorithms and adaptors. Boost range requirements \u00b6 Sometimes, it is hard to say why a type does not satisfy some of the requirements for boos ranges. Fortunatelly, the boost provides concepts for checking whether a type satisfy each specific range model. Example: BOOST_CONCEPT_ASSERT(( boost::SinglePassRangeConcept<std::vector<int>> )); // true Also, it is necessary to check whether the value of the iterator can be accessed: BOOST_CONCEPT_ASSERT(( boost_concepts::ReadableIteratorConcept< typename boost::range_iterator<std::vector<int>>::type > )); // true Most likely, the compiler will complain that boost::range_iterator<R>::type does not exist for your range R . The boost range library generate this type by a macro from the R::iterator type. Therefore, make sure that your range has an iterator type defined, either as: a type alias to an existing iterator an iterator nested class Note that <RANGE CLASS>::iterator and <RANGE CLASS>::const_iterator has to be accessible (public). Sequences \u00b6 The iota algortihm/adapter is used to create a sequence: auto range = std::views::iota(0, 10); auto vec = std::vector(range.begin(), range.end()); Note that we cannot pass the view directly to the vector, as the vector does not have a range constructor. Zip \u00b6 The classical Python like zip iteration is available using the zip adapator , which is not yet supported in MSVC. However, boost provides a similar functionality boost::combine . boost::combine \u00b6 boost::combine example: std::vector<int> va{1, 2, 3}; std::vectro<float> vb{0.5, 1, 1.5}; for(const auto& [a, b]: boost::combine(va, vb)){ ... } Each argument of combine must satisfy boost::SinglePassRange Enumerating \u00b6 There is no function in standard library equivalent to the python enumerate. We can use a similar boost solution: #include <boost/range/adaptor/indexed.hpp> for(auto const& el: <range> | boost::adaptors::indexed(0)){ std::cout << el.index() << \": \" << el.value() << std::endl; } However, inside the loop, we have to call the index and value functions, so it is probably easier to stick to the good old extra variable: size_t i = 0; for(auto const& el: <range>) { std::cout << i << \": \" << el << std::endl; ++i; } Sorting \u00b6 There is no sorted view or something simmiler, so in order to sort a range, we need to: really sort the object in the range create an adaptor/view from the range, and then sort the view There are two functions for sorting in the STL algorithm library: std::sort : old supports parallelization directly by the policy param std::ranges::sort : new supports comparison using projections There are three types of sorting: natural sorting using the < operator of T : std::sort(<RANGE<T>>) sorting using a comparator: std::sort(<RANGE>, <COMPARATOR>) , where comparator is a fuction with parameters and return value analogous to the natural sorting operator. sorting using projection (only availeble in std::ranges::sort ): std::ranges::sort(<RANGE>, <STANDARD GENERIC COMPARATOR>, <PROJECTION> Sorting using projection \u00b6 When we want to sort the objects by a single property different then natural sorting, the easiest way is to use projection. Example: struct Data { int a; std::string b; ... }; std::vector<Data> data = get_data(); std::sort(data, {}, &Data::b); The trick here is that we can only provide the member that we want to use for sorting, but the sorting logic ( first < second ...) is handeled by a standard comparator (the second argument initialized with {} ). We can have even more complicated projections: std::vector<My_class> objects = get_objects(); std::vector<unsigned> indexes = get_indexes(objects); auto proj = [&objects](std::size_t i) -> const std::string& { return objects[i].get_name(); }; std::ranges::sort(indexes, {}, proj) // sort indexes using the property of objects Transformation \u00b6 Transformation alg/views transforms an input range according to a callable. As with other operation, there are thre options: classical algorithm: std::transform with a direct paralellization using the policy parameter range algorithm: std::ranges::transform with a support for projections range view: std::ranges::views::transform - a lazy variant The algorithms (but not the view) also supports binary transformations , i.e., create an output range using two input ranges. Transform view example: std::vector<int> in(3, 0); // [0, 0, 0] auto ad = std::ranges::transform_view(in, [](const auto in){return in + 1;}); std::vector<int> out(ad.begin(), ad.end()); The transform view can be only constructed from an object satisfying ranges::input_range . If we want to use a general range (e.g., vector), we need to call the addapter, which has a same signature like the view constructor itself. The important thing here is that the adapter return type is not a std::ranges::views::transform<<RANGE>> but std::ranges::views::transform<std::ranges::ref_view<RANGE>>> ( std::ranges::ref_view ). Supporting various collections is therefore possible only with teplates, but not with inheritance. Note that unlike in Java, it is not possible to use a member reference as a transformation function (e.g.: &MyClass::to_sting() ). We have to always use lambda functions, std::bind or similar to create the callable. Aggregating (sum, product, etc.) \u00b6 These operations can be done using the std::accumulate algorithm. This algorithm is about to be replaced by the std::ranges::fold algorithm, but it is not yet implemented in Clang. Examples: // default accumulation -> sum std::vector<int> vec{1, 2, 3, 4, 5}; int sum = std::accumulate(vec.begin(), vec.end(), 0); // product int product = std::accumulate(vec.begin(), vec.end(), 1, std::multiplies<int>()); Implementing a custom range \u00b6 There are different requirements for different types of ranges. Moreover, there are different requirements for the range-based for loop (for each) , or the legacy STL algorithms. Here we focus on requirements for ranges. Not however, that the range requirements are more strict than the requirements for the range-based for loop or the legacy STL algorithms. Therefore, the described approach should work for all three cases. Usually, we proceed as follows: Choose the right range (Iterable) concept for your range from the STL range concepts . The most common is the std::ranges::input_range concept. Implement the range concept for the range. Either, we can do it by using the interface of the undelying range we usein our class (i.e, we just forward the calls to the methods of std::vector or std::unordered_map ) or implement the interface from scratch. For that, we also need to implement the iterator class that fulfills the corresponding iterator concept (e.g., std::input_iterator for the std::ranges::input_range ). Implementing an input range \u00b6 The input range is the most common range type. The only requirement for the input range is that it has to have the begin and end methods that return the input iterator. Example: class My_range { private: std::vector<int> data; public: My_range(std::vector<int> data): data(data) {} auto begin() {return data.begin();} auto end() {return data.end();} // usually, we also want a const version of the range auto begin() const {return data.begin();} auto end() const {return data.end();} }; Boost Iterator Templates \u00b6 The boost.iterator library provides some templates to implement iteratores easily, typically using some existing iterators and modifying just a small part of it: for pointer to type (dereference) iterator, you can use boost indirect iterator zip iterator for Python like iteration over multiple collections [transform iteratorather useful iterators are also included in the boost.iterator library for using another iterator and just modify the access ( * ) opindex.html). including: zip iterator. counting_iterator to create number sequence like Python range gentransform iterator There are also two general (most powerfull) classes: iterator adapter iterator facade Resources \u00b6 How to write a legacy iterator iter_value_t Lambda Functions \u00b6 In c++ lambda functions are defined as: [<capture>](<params>) -> <return_type> { <code> } The rerurn type is optional, but sometimes required (see below). Since C++23, the parantheses are optional if there are no functon parameters. Captures \u00b6 Anything that we want to use from outside has to appear in capture. To prevent copying, we should capture by reference, using & before the name of the variable. [&var_1] // capture by reference [var_1] // capture by value [&] // default capture by reference For the detailed explanation of the captures, see cppreference . Return type \u00b6 The return type of lambda functions can be set only using the trailing return type syntax ( -> <RETURN TYPE> after the function params). The return type can be omited. Note however, that the default return type is auto , so in case we want to return by reference, we need to add at least -> auto , or even a more specific return type. Specifiers \u00b6 Lambda functions can have special specifiers: mutable : lambda can modify function parameters capture by copy Exceptions \u00b6 In C++, exceptions works simillarly as in other languages. Standard runtime error can be thrown using the std::runtime_error class: throw std::runtime_error(\"message\"); Always catch exception by reference! Note that unlike in Java or Python, there is no default exception handler in C++. Therefore, if an exception is not caught and, in conclusion, the program is terminated, there is no useful information about the exception in the standard output. Instead, we only receive the exit code. For this reason, it is a good practice to catch all exceptions in the main function and print the error message. Example: int main() { try { <the code of the whole program here> } catch(...) { const std::exception_ptr& eptr = std::current_exception(); if (!eptr) { throw std::bad_exception(); } /*char* message;*/ std::string message; try { std::rethrow_exception(eptr); } catch (const std::exception& e) { message = e.what(); } catch (const std::string& e) { message = e; } catch (const char* e) { message = e; } catch(const GRBException& ex) { message = fmt::format(\"{}: {}\", ex.getErrorCode(), ex.getMessage()); } catch (...) { message = \"Unknown error\"; } spdlog::error(message); return message; } } Rethrowing Exceptions \u00b6 We can rethrow an exception like this: catch(const std::exception& ex){ // do ssomething ... throw; } Note that in parallel regions, the exception have to be caught before the end of the parallel region , otherwise the thread is killed. How to Catch Any Exception \u00b6 In C++, we can catch any exception with: catch (...) { } However, this way, we cannot access the exception object. As there is no base class for exceptions in C++, there is no way to catch all kind of exception objects in C++. noexcept specification \u00b6 A lot of templates in C++ requires functions to be noexcept which is usually checked by a type trait std::is_nothrow_invocable . We can easily modify our function to satisfy this by adding a noexcept to the function declariaton. There are no requirements for a noexcept function. It can call functions without noexcept or even throw exceptions itself. The only difference it that uncought exceptions from a noexcept function are not passed to the caller. Instead the program is terminated by calling std::terminate , which otherwise happens only if the main function throws. By default, only constructors, destructors, and copy/move operations are noexcept. Stack traces \u00b6 Unlike most other languages, C++ does not print stack trace on program termination. The only way to get a stack trace for all exceptions is to set up a custom terminate handler an inside it, print the stack trace. However, as of 2023, all the stack trace printing/generating libraries requires platform dependent configuration and fails to work in some platforms or configurations. Example: void terminate_handler_with_stacktrace() { try { <stack trace generation here>; } catch (...) {} std::abort(); } std::set_terminate(&terminate_handler_with_stacktrace); To create the stacktrace, we can use one of the stacktrace libraries: stacktrace header from the standard library if the compiler supports it (C++ 23) as of 2024-04, only MSVC supports this functionality cpptrace boost stacktrace Logging \u00b6 There is no build in logging in C++. However, there are some libraries that can be used for logging. In this section we will present logging using the spdlog library. We can log using the spdlog::<LEVEL> functions: spdlog::info(\"Hello, {}!\", \"World\"); By default, the log is written to console. In order to write also to a file, we need to create loggers manually and set the list of sinks as a default logger: const auto console_sink = std::make_shared<spdlog::sinks::stdout_sink_st>(); console_sink->set_level(spdlog::level::info); // log level for console sink auto file_sink = std::make_shared<spdlog::sinks::basic_file_sink_st>(<log filepath>, true); std::initializer_list<spdlog::sink_ptr> sink_list{console_sink, file_sink}; const auto logger = std::make_shared<spdlog::logger>(<LOGGER NAME>, sink_list); logger->set_level(spdlog::level::debug); //log level for the whole logger spdlog::set_default_logger(logger); To save performance in case of an intensive logging, we can set an extended flushing period: spdlog::flush_every(std::chrono::seconds(5)); Levels \u00b6 The log levels are defined in the spdlog::level::level_enum . The levels are: trace debug info warn error critical Colors \u00b6 By default, the logger uses colors for different log levels. However, this capability is lost when: using custom sinks or using custom formatters To keep the colors, we need to a) use the color sink and b) explicitly set the usage of the color in the formatter: auto console_sink = std::make_shared<spdlog::sinks::stdout_color_sink_mt>(); auto logger = std::make_shared<spdlog::logger>(\"console\", console_sink); logger->set_pattern(\"[%^%l%$] %v\"); Here %^ and %$ are the color start and end markers. Type Aliases \u00b6 Type aliases are short names bound to some other types. We can introduce it either with typedef or with using keyword. Examples (equvalent): typedef int number; using number = int; typedef void func(int,int); using func = void(int, int) The using new syntax is more readable, as the alias is at the begining of the expression. But why to use type aliases? Two strong motivations can be: iImprove the readebility : When we work with a type with a very long declaration, it is wise to use an alias. We can partialy solve this issue by using auto, but that is not a complete solution Make the refactoring easier : When w work with aliases, it is easy to change the type we work with, just by redefining the alias. Note that type aliases cannot have the same name as variables in the same scope . So it is usually safer to name type aliases with this in mind, i.e., using id_type = .. insted of using id = .. Template Aliasis \u00b6 We can also create template aliases as follows: template<class A, typename B> class some_template{ ... }; template<class T> using my_template_alias = some_template<T, int>; Aliases inside classes \u00b6 The type alias can also be placed inside a class. From outside the class, it can be accessed as <CLASS NAME>::<ALIAS NAME> : class My_class{ public: using number = unsigned long long number n = 0; } My_class::number number = 5; Constant Expressions \u00b6 cppreference . A constant expression is an expression that can be evaluated at compile time. The result of constant expression can be used in static context, i.e., it can be: assigned to a constexpr variable, tested for true using static_assert Unfortunatelly, there is no universal way how to determine if an expression is a constant expression . Compile Time Branching \u00b6 For compile time branching, we can use the if constexpr : template<class T, class C> class Object{ public: bool process(T value, C config){ if constexpr (std::is_same_v<T, std::string>){ return process_string(value, config); } else { return process_value(value, config); } } }; Note that here, the if constexpr requires the corresponding else branch. Otherwise, the code cannot be discarded during the compilation. Example: template<class T, class C> class Object{ public: bool process(T value, C config){ if constexpr (std::is_same_v<T, std::string>){ return process_string(value, config); } return process_value(value, config); // this compiles even if T is std::string } }; Regular expressions \u00b6 The regex patern is stored in a std::regex object: const std::regex regex{R\"regex(Plan (\\d+))regex\"}; Note that we use the raw string so we do not have to escape the pattern. Also, note that std::regex cannot be constexpr Matching the result \u00b6 We use the std::regex_search to search for the occurence of the pattern in a string. The result is stored in a std::smatch object which contains the whole match on the 0th index and then the macthed groups on subsequent indices. A typical operation: std::smatch matches; const auto found = std::regex_search(string, matches, regex); if(found){ auto plan_id = matches[1].str(); // finds the first group } Note that matches[0] is not the first matched group, but the whole match. Namespaces \u00b6 cppreference Namespace provides duplicit-name protection, it is a similar concept to Java packages. Contrary to java packages and modules, the C++ namespaces are unrelated to the directory structure. namespace my_namespace { ... } The namespaces are used in both declaration and definition (both in header and source files). The inner namespace has access to outer namespaces. For using some namespace inside our namespace without full qualification, we can write: using namespace <NAMESPACE NAME> Anonymous namespaces \u00b6 Anonymous namespaces are declared as: namespace { ... } Each anonnymous namespaces has a different and unknown ID. Therefore, the content of the annonymous namespace cannot be accessed from outside the namespace, with exception of the file where the namespace is declared which has an implicit access to it. Namespace aliases \u00b6 We can create a namespace alias using the namespace keyword to short the nested namespace names. Typicall example: namespace fs = std::filesystem; decltype : Determining Type from Expressions \u00b6 Sometimes, it is usefull to declare a type from expression, instead of do it manualy. Using decltype specifier, we can get the resulting type of an expression as if it was evaluated. Examples: struct A { double x; }; const A* a; decltype(a->x) // evaluates to double decltype(std::accumulate(a, [](double sum, double val){return sum + val;})) // evalutes to double We can use the decltype in any context where type is required. Examples: int i = 1 decltype(i) j = 3 std::vector<decltype(j)> numbers; The Value Category of decltype \u00b6 The value category of decltype is resolved depending on the value category of an expression inside it: deltype(<XVALUE>) -> T&& deltype(<LVALUE>) -> T& deltype(<RVALUE>) -> T The rvalue conversion can lead to unexpected results, in context, where the value type matters: static_assert(std::is_same_v<decltype(0), decltype(std::identity()(0))>); // error The above expressions fails because: decltype(0) , 0 is an rvalue -> the decltype result is int decltype(std::identity()(0)) result of std::identity() is an xvalue -> the decltype result is int&& . Determining Type from Expressions Sometimes, it is usefull to declare a type from expression, instead of do it manualy. Using decltype specifier, we can get the resulting type of an expression as if it was evaluated. Examples: struct A { double x; }; const A* a; decltype(a->x) // evaluates to double decltype(std::accumulate(a, [](double sum, double val){return sum + val;})) // evalutes to double We can use the decltype in any context where type is required. Examples: int i = 1 decltype(i) j = 3 std::vector<decltype(j)> numbers; Determining the Return Value Type of a Function \u00b6 As we can see above, we can use decltype to determine the return value type. But also, there is a type trait for that: std::invoke_result_t (formerly std::result_of ). The std::invoke_result_t should vbe equal to decltype when aplied to return type, with the following limitations: - we cannot use abstract classes as arguments of std::invoke_result_t , while we can use them inside decltype (using std::declval , see below). \u00b6 Construct object inside decltype with std::declval \u00b6 std::declval is a usefull function designed to be used only in static contexts, inside decltype . It enables using member functions inside decltype without using constructors. Without std::declval , some type expressions are hard or even impossible to costruct. Example: class Complex_class{ Complex_class(int a, bool b, ...) ... int compute() } // without declval decltype<Complex_class(1, false, ...).compute()> // using declval decltype(std::declval<Complex_class>().compute()) decltype and Overloading \u00b6 in static context, there is no overloading, the vtable is not available. Therefore, we have to hint the compiler which specific overloaded function we want to evaluate. This also applies to const vs non const overloading. The following example shows how to get the const iterator type of a vector: std::vector<anything> vec // non const iter decltype(vec.begin()) // const iter decltype<std::declval<const decltype(vec)>().begin()> Another example shows how to use the const overload inside std::bind : decltype(std::bind(static_cast<const ActionData<N>&(std::vector<ActionData<N>>::*)(size_t) const>(&std::vector<ActionData<N>>::operator[]), action_data)), Above, we used static cast for choosing the const version of the vector array operator. Instead, we can use explicit template argument for std::bind : decltype(std::bind<const ActionData<N>& (std::vector<ActionData<N>>::*)(size_t) const>(&std::vector<ActionData<N>>::operator[], action_data)), Parallelization \u00b6 While there wa no support of parallelization i earlier versions of C++ , now there are many tools. Standard Threads \u00b6 For-each with Parallel Execution Policy \u00b6 The function std::for_each can be run with a parallel execution policy to process the loop in parallel. Async tasks \u00b6 Tasks for asznchronous execution, like file downloads, db queries, etc. The main function is std::async . Open-MP \u00b6 In MSVC, the Open MP library is automatically included and linked. In GCC, we need to find the libs in CmakeLists.txt : find_package(OpenMP REQUIRED) Standard Templates for Callables \u00b6 Using std::invoke to call the member function \u00b6 using std::invoke , the cal syntax bool b = (inst.*ptr)() can be replaced with longer but more straighforward call: bool b = std::invoke(ptr, inst, 2) Using std::mem_fn to Store a Pointer to Member Function in a Callable \u00b6 With std::mem_fn , we can store the pointer to a member function in a callable object. Later, we can call the object without the pointer to the member function. Example: auto mem_ptr = std::mem_fn(&My_class::my_method) bool b = mem_ptr(inst, 2) Using a Pointer to Member Function as a Functor \u00b6 A normal function can be usually send instead of functor, as it can be invoked in the same way. However, in case of member function, we usually need to somehow bind the function pointer to the instance. We can use the std::bind function exactly for that: auto functor = std::bind(&My_class::my_method, inst); bool b = functor(2) Advanteges: we do not need an access to instance in the context from which we call the member function we do not have to remember the complex syntax of a pointer to a member function declaration we receive a callable object, which usage is even simpler than using std::invoke Note that in case we want to bind only some parameters, we need to supply placeholders for the remaining parameters ( std::placeholders ). Using Lambdas Instead of std::bind \u00b6 For more readable code and better compile error messages, it is usefull to replace std::bind callls with labda functions. The above example can be rewritten as: auto functor = [inst](int num){return inst.my_method(num);); bool b = functor(2) Store the Result of std::bind \u00b6 Sometimes, we need to know the return type of the std::bind . In many context, we need to provide the type instead of using auto . But luckily, there is a type exactly for that: std::function . Example: std::function<bool(int)> functor = std::bind(&My_class::my_method, inst); bool b = functor(2) A lambda can also be stored to std::function . But be carefull to add an explicit return type to it, if it returns by a reference. Example: My_class{ public: int my_member } My_class inst; std::function f = [inst](){return &inst.my_member; } // wrong, reference to a temporary due to return type deduction std::function f = [inst]() -> const int& {return &inst.my_member; } // correct More detailed information about pointers to member functions std::mem_fn and Data Members \u00b6 Data member pointers can be aslo stored as std::mem_fn . A call to this object with an instance as the only argument then return the data member value. The plain syntax is <type> <class name>.*<pointer name> = <class name>.<member name> , and the pointer is then accessed as <instance>.*<pointer name> . Example: int Car::*pSpeed = &Car::speed; c1.*pSpeed = 2; Usefull STL functions std::for_each : iterates over iterable objects and call a callable for each iteration std::bind : Binds a function call to a variable that can be called some parameters of the function can be fixed in the variable, while others can be provided for each call each reference parameter has to be wrapped as a reference_wrapper std:mem_fn : Creates a variable that represents a callable that calls member function std::function \u00b6 The std::function template can hold any callable. It can be initialized from: function pointer/reference, member function pointer/reference, lambda function functor It can be easily passed to functions, used as template parameter, etc. The template parameters for std::function has the form of std::function<<RETURN TYPE>(<ARGUMENTS>)> . Example: auto lambda = [](std::size_t i) -> My_class { return My_class(i); }; std::function<My_class(std::size_t)> f{lambda} std::function and overloading \u00b6 one of the traps when using std::function is the ambiguity when using an overloade function: int add(int, int); double add(double, double); std::function<int(int, int)> func = add; // fails due to ambiguity. The solution is to cant the function to its type first and then assign it to the template: std::function<int(int, int)> func = static_cast<int(*)(int, int)>add; Preprocessor Directives \u00b6 The C language has a preprocessor that uses a specific syntax to modify the code before the compilation. This preprocessor is also used in C++. The most used tasks are: including files ( #include ): equivalent to Java or Python import statement conditional compilation based on OS, compiler, or other conditions Also, preprocessor had some other purposes, now replaced by other tools: defining constants ( #define ): replaced by const and constexpr A simple constant can be defined as: #define PI 3.14159 . The variable can be used in the code as PI . metaprogramming: replaced by templates Include \u00b6 cppreference There are two types of include directives. For both types, the behavior is implementation dependent. However, the most common behavior is: #include <file> : the file is searched in the system directories #include \"file\" : the file is searched relative to the current file Conditional include \u00b6 Sometimes, we need a conditional include based on what is available in the system. We can use two mechanisms: __has_include(<file>) : Basically, we test the header availability during compilation: cpp #if __has_include(<format>) #include <format> using namespace format = std::format; #else #include <fmt/format.h> using namespace format = fmt; #endif only available since C++17 predefined compiler variable: We define some variable during project configuration and then use it in the preprocessor control structure: cmake set(USE_FMT ON) cpp #if USE_FMT #include <fmt/format.h> using namespace format = fmt; #else #include <format> using namespace format = std::format; #endif never use this for public headers , it is not portable as each client project now has to set the variable in the cmake configuration Control structures \u00b6 #ifdef <MACRO> ... #elif <MACRO> ... #else ... #endif Instead of #ifdef <MACRO> , we can use #if defined(<MACRO>) . In this case, we can use multiple conditions in one #if directive: #if defined(MACRO_1) && defined(MACRO_2) ... #endif Predefined macros for detecting compiler, OS, etc. \u00b6 To detect the Operating system , use: #ifdef _WIN32 for Windows #ifdef __linux__ for Linux #ifdef unix for Unix-like systems (but not MacOS) #ifdef __APPLE__ for MacOS Debugging preprocessor directives \u00b6 Sometimes, it may be usefull to print the value of a macro, or show which branch of the #if directive was taken. This can be done using the #pragma message directive: #ifdef MACRO #pragma message(\"MACRO is defined\") #else #pragma message(\"MACRO is not defined\") #endif Attributes \u00b6 cppreference Atrributes mechanism provides compilers with a standard way to extend the language with new features (as opose to preprocessor macros). Additionally, some standard attributes are listed in the C++ standard. The syntax is: [[ <attribute> ]] where <attribute> is the attribute name (potentially prefixed with the namespace of the attribute) optionally followed by arguments. Attributes may appear almost anywhere in the code, however, the usage of each attribute is typically restricted to a specific context. Standard attributes are: [[nodiscard]] : marks a function that the return value should not be ignored. If the return value is ignored, a warning is issued. [[maybe_unused]] : marks a variable that may be unused. This suppresses the unused variable warning in modern compilers. Comments \u00b6 The C++ standard does not specify the comment syntax. However, the most common comments are: // single-line comment /* multi-line comment */ Also, there are special comment blocks for documentation systems like Doxygen. Those, like in Java, are typically enclosed in /** and */ blocks. /** * This is a multi-line comment. * It can contain multiple lines. */ Function Comment Blocks \u00b6 For function comment blocks, there is a special syntax in Doxygen: /** * @brief This is a function comment block. * It can contain multiple lines. * @param param1 This is a parameter. * @param param2 This is a parameter. * @return This is a return value. */ Additionaly, we can refer to the parameters in the text using @p <param name> . Resources \u00b6 In C++, there is no facility for resource management like in Java or Python. Instead, resources have to be loaded like standard files. Moreover, there is no built-in way how to determine the localtion of the running executable so that we can load the resources from the same directory. Typically, this has to be implemented for each platform separately: #include <iostream> #include <filesystem> #ifdef _WIN32 #include <windows.h> #else #include <unistd.h> #endif std::string get_executable_path() { char buffer[1024]; #ifdef _WIN32 GetModuleFileNameA(NULL, buffer, sizeof(buffer)); #else ssize_t count = readlink(\"/proc/self/exe\", buffer, sizeof(buffer)); if (count == -1) throw std::runtime_error(\"Failed to get executable path\"); buffer[count] = '\\0'; #endif return std::filesystem::path(buffer).parent_path().string(); } Testing with Google Test \u00b6 Private method testing \u00b6 The testing of private method is not easy with Google Test, but that is common also for other tets frameworks or even computer languages (see the common manual). Some solutions are described in this SO question . Usually, the easiest solution is to aplly some naming/namespace convention and make the function accessible. For free functions: namespace internal { void private_function(){ ... } } For member functions: class MyClass{ public: void _private_function(); Memory Alignment \u00b6 Allignment FAQ Wikipedia SO question In most cases, a sane programmer does not have to worry about memory optimization beyond choosing the right data types. However, in high-performance applications, where millions of objects are processed, even some details regarding the memory layout can have a significant impact on the performance. Therefore, we introduce some terminology and tools for memory optimization. To make the memory access faster, the compiler aligns the data in the memory so that the memory can be read by the chunks natural for the architecture which are typically multiple of bytes. This affects both how the data structure members are stored ( padding ) and how the data structure itself is stored ( alignment ). First, let's look at how much a data structure takes in memory. The data structure memory usage consists of: the size of its members, including base classes the size of the padding between the members if the strucrure use virtual methods, the size of the vtable Apart of the above, a structure itself may take even more memory due to the alignment . Finally, there are strategies how to optimize the structure size called packing . See the packing section on the Eric S. Raymond's website for more details. Padding \u00b6 The padding is added by the compiler so that the members are aligned. Typically, the members are aligned to their natural size. This means that a boolean variable, which is 1 byte, is aligned to 1 byte, while a 4-byte integer is aligned to 4 bytes. Note that the compiler is not allowed to change the order of the members, and therefore, the order of the members affects the size of the structure in memory! Example: struct A{ // 4 bytes bool b; // 1 byte bool b2; // 1 byte short a; // 2 bytes }; struct B{ // 4 bytes short a; // 2 bytes bool b; // 1 byte bool b2; // 1 byte }; struct C{ // 5 bytes bool b; // 1 byte + 1 byte padding so that the short is aligned short a; // 2 bytes bool b2; // 1 byte The natural alignment apply to the basic types. The composed types (e.g., structs, classes, unions) are typically aligned to the largest member. Structure Alignment (or final padding) \u00b6 Appart from padding, each data structure is alligned to its largest member. If the data structure size (including padding) is not a multiple of its alignment, it effectively takes more memory so that it is aligned. specific tasks \u00b6 Conditional Function Execution \u00b6 W know it from other languages: if the function can be run in two (or more) modes, there is a function parameter that controls the execution. Usually, most of the function is the same (otherwise, we eould create multiple fuctions), and the switch controls just a small part. Unlike in other langueges. C++ has not one, but three options how to implement this. They are described below in atable together with theai properties. function parameter template parameter compiler directive good readability yes no no compiler optimization no yes yes conditional code compilation no no yes Function Parameter \u00b6 void(bool switch = true){ if(switch){ ... } else{ ... } } Template Parameter \u00b6 template<bool S = true> void(){ if(S){ ... } else{ ... } } Compiler Directive \u00b6 void(){ #ifdef SWITCH ... #else ... #endif } Ignoring warnings for specific line of code \u00b6 Sometimes, we want to suppress some warnings, mostly in libraries we are including. The syntax is, unfortunatelly, different for each compiler. Example: #if defined(_MSC_VER) #pragma warning(push) #pragma warning(disable: <WARNING CODE>) #elif defined(__GNUC__) #pragma GCC diagnostic push #pragma GCC diagnostic ignored \"<WARNING TYPE GCC>\" #elif defined(__clang__) #pragma clang diagnostic push #pragma clang diagnostic ignored \"<WARNING TYPE CLANG>\" #endif .. affected code... #if defined(_MSC_VER) #pragma warning(pop) #elif defined(__GNUC__) #pragma GCC diagnostic pop #elif defined(__clang__) #pragma clang diagnostic pop #endif Here, the <WARNING CODE> is the code of the warning to be suppressed without the C prefix. Note that warnings related to the preprocessor macros cannot be suppressed this way in GCC due to a bug (fixed in GCC 13). The same is true for conditions: #if 0 #pragma sdhdhs // unknown pragma raises warning, despite unreachcable #endif Measuring used resource \u00b6 Memory \u00b6 MSVC \u00b6 In MSVC, we can measure the peak used memory using the following code: #include <psapi.h> PROCESS_MEMORY_COUNTERS pmc; K32GetProcessMemoryInfo(GetCurrentProcess(), &pmc, sizeof(pmc)); auto max_mem = pmc.PeakWorkingSetSize Working with tabular data \u00b6 Potential libs similar to Python Pandas: Arrow Dataframe Executing external commands \u00b6 The support for executing external commands in C++ is unsatisfactory. The most common solution is to use the system function. However, the system calls are not portable, e.g., the quotes around the command are not supported in Windows Another option is to use the Boost Process library. Command Line Interface \u00b6 For CLI, please follow the CLI manual . Here we focus on setting up the TCLAP library. TCLAP use Jinja-like Templating \u00b6 For working with Jinja-like templates, we can use the Inja template engine. Exceptions \u00b6 There are the following exceptions types: ParserError thrown on parse_template method RenderError thrown on write method Render Errors \u00b6 empty expression : this signalize that some expression is empty. Unfortunatelly, the line number is incorrect (it is always 1). Look for empty conditions, loops, etc. (e.g., {% if %} , {% for %} , {% else if %} ). Design Patterns \u00b6 This section describe how to implement various design patterns in C++. Visitor Pattern \u00b6 C++ has a much more clear and efficient way how to implement the visitor pattern than other languages like Java. Instead of classical polymorphism, we use: std::variant to list all possible types std::visit to implement a visitor The advantage is that the visitor use a procedural way to choose the appropriate function for the given type, which is much more efficient than the runtime dispatching of the classical visitor pattern. Also there is no circular dependency between the visitor and the visited classes. Compile-time Plugins using Static Registration \u00b6 Sometimes, we want to enable extension of the functionality of some executable at compile time. The problem is: how we can call the extended code from main function, if we do not know it? The answer is to use the static registration pattern. The principle is simple: in the executable, we define a registry of available plugins in the plugin, we call some registration function to register the plugin in the registry. As this registration cannot be called from the main function, we use the static variable initialization to register the plugin. The static registration in the plugin looks like this: #include <plugin_registry.h> struct Plugin{ // constructor Plugin(){ PLugin_registry::register_plugin(<parameters>); } }; Plugin plugin; // here, the constructor is called as the variable has static storage duration. This technique has some important pitfall. Because the main executable does not reference the plugin, the compiler may discard the whole plugin object and not link it to the executable. To prevent this, several techniques can be used: - link the plugin with /WHOLEARCHIVE flag. \u00b6","title":"C++ Manual"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#source-and-header-files","text":"In C++, there are: header files, that contain declarations of functions, classes, variables, etc. and, for templated code, the code itself. they have the .h or .hpp extension source files, that contain the definitions of the functions, classes, variables, etc. they have the .cpp extension Each source file added to the target (see CMake Manual for adding source files to the target in CMake projects) is compiled into an object file - a translation unit . The header files are not compiled, but insted, they serve as a promise that there will be some translation unit that will contain the code that will be linked to the final executable. The above mechanism provides a flexible and practical interface, but a special care must be taken to avoid mistakes, that typically result in a liker error. Most important here is the One Definition Rule (ODR) : each entity (e.g., class, function, variable, etc.) must be defined exactly once in the whole program. If something is defined in multiple translation units, it will result in a multiple definition error. If we forgot to define some entity, or we do not add the source file with the definition to the target, it will result in an undefined reference error. Typically, each header has a corresponding source file, that contains the definition of all entities declared in the header. However, there is no requirement to have a single source file for each header, we can separate the code into multiple source files. Note that all entities in the translation units added to the target are compiled, even if they are not used . This is in contrast with the statements inside functions, which are not compiled if they are not used.","title":"Source and Header Files"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#storage-duration","text":"cppreference The storage duration of a variable is the period for which the storage is guaranteed to be available. There are the following storage durations : static storage duration : these variables are initialized at the start of the program before the main function is called. They are destroyed at the end of the program, after the main function returns. thread storage duration : these variables are initialized at the start of the thread before the thread function is called. They are destroyed at the end of the thread, after the thread function returns. automatic storage duration : these variables are initialized at the point of their declaration and destroyed at the end of the block they are declared in. dynamic storage duration : these variables are initialized by some memory management mechanism, and they are destroyed by an analogous mechanism. To determine the scope of a variable, we can use the following rules : Variables within the namespace scope (including the undeclared global namespace) have static storage duration unless they are declared with the thread_local specifier, in which case they have thread storage duration. block scope variables have automatic storage duration unless they are declared with the static specifier, in which case they have static storage duration. block static variables, unlike namespace static variables, are not initialized at the start of the program, but at the first time the line with the declaration is executed. parameters have automatic storage duration. objects created with mechanics like new / delete , malloc / free , new[] / delete[] , or std::make_unique , std::make_shared , etc. have dynamic storage duration. An important lesson from these rules is that the global variables have static storage duration even without the static specifier. The reason for using the static specifier for globals is to control the linkage of the variable.","title":"Storage Duration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#type-system","text":"cppreference Type is a property of each: object reference function expression","title":"Type System"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#complete-and-incomplete-types","text":"In many context, we have to supply a type with a requirement of being a complete type. So what types are incomplete? The void type is always incomplete Any structure without definition (e.g. using struct structure *ps; , without defining structure .) An array without dimensions is an incomplete type: int a[]; is incomplete, while int a[5]; is complete. An array of incomplete elements is incomplete. A type trait that can be used to determine whether a type is complete is described here .","title":"Complete and Incomplete Types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#aggregate-types","text":"Aggregate types are: array types class types that fullfill the following conditions no private or protected members no constructores declared (including inherited constructors) no private or protected base classes no virtual member functions The elements of the aggregate types can and are ment to be constructed using the aggregate initialization (see the local variable initialization section).","title":"Aggregate types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#type-conversion","text":"cppreference: implicit conversion In some context, an implicit type conversion is aplied. This happens if we use a value of one type in a context that expects a different type. The conversion is applied automatically by the compiler, but it can be also applied explicitly using the static_cast operator. In some cases where the conversion is potentially dangerous, the static_cast is the only way to prevent compiler warnings.","title":"Type Conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#numeric-conversion","text":"There are two basic types of numeric conversion: standard implicit conversion that can be of many types: this conversion is applied if we use an expression of type T in a context that expects a type U . Example: ```cpp void print_int(int a){ std::cout << a << std::endl; } int main(){ short a = 5; print_int(a); // a is implicitly converted to int } ``` usual arithmetic conversion which is applied when we use two different types in an arithmetic binary operation. Example: cpp int main(){ short a = 5; int b = 2; int c = a + b; // a is converted to int }","title":"Numeric Conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implicit-numeric-conversion","text":"","title":"Implicit Numeric Conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#integral-promotion","text":"Integral promotion is a coversion of an integer type to a larger integer type. The promotion should be safe in a sense that it never changes the value. Important promotions are: bool is promoted to int : false -> 0 , true -> 1","title":"Integral Promotion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#integral-conversion","text":"Unlike integral promotion, integral conversion coverts to a smaller type, so the value can be changed. The conversion is safe only if the value is in the range of the target type. Important conversions are:","title":"Integral Conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#usual-arithmetic-conversion","text":"cppreference This conversion is applied when we use two different types in an arithmetic binary operation. The purpose of this conversion is convert both operands to the same type before the operation is applied. The result of the conversion is then the type of the operands. The conversion has the following steps steps: lvalue to rvalue conversion of both operands special step for enum types special step for floating point types conversion of both operands to the common type The last step: the conversion of both operands to the common type is performed using the following rules: If both operands have the same type, no conversion is performed. If both operands have signed integer types or both have unsigned integer types, the operand with the type of lesser integer conversion rank (size) is converted to the type of the operand with greater rank. otherwise, we have a mix of signed and unsigned types. The following rules are applied: If the unsigned type has conversion rank greater or equal to the rank of the signed type, then the unsigned type is used. Otherwise, if the signed type can represent all values of the unsigned type, then the signed type is used. Otherwise, both operands are converted to the unsigned type corresponding to the signed type (same rank). Here especially the rule 3.1 leads to many unexpected results and hard to find bugs. Example: int main(){ unsigned int a = 10; int b = -1; auto c = b - a; // c is unsigned and the value is 4294967285 } To avoid this problem, always use the static_cast operator if dealing with mixed signed/unsigned types .","title":"Usual Arithmetic Conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#show-the-type-at-runtime","text":"It may be useful to show the type of a variable at runtime: for debugging purposes for logging to compare the types of two variables Note however, that in C++, there is no reflection support. Therefore, we cannot retrieve the name of the type at runtime in a reliable way . Instead, the name retrieved by the methods described below can depend on the compiler and the compiler settings.","title":"Show the Type at Runtime"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#resolved-complicated-types","text":"Sometimes, it is useful to print the type, so that we can see the real type of some complicated template code. For that, the following template can be used: #include <string_view> template <typename T> constexpr auto type_name() { std::string_view name, prefix, suffix; #ifdef __clang__ name = __PRETTY_FUNCTION__; prefix = \"auto type_name() [T = \"; suffix = \"]\"; #elif defined(__GNUC__) name = __PRETTY_FUNCTION__; prefix = \"constexpr auto type_name() [with T = \"; suffix = \"]\"; #elif defined(_MSC_VER) name = __FUNCSIG__; prefix = \"auto __cdecl type_name<\"; suffix = \">(void)\"; #endif name.remove_prefix(prefix.size()); name.remove_suffix(suffix.size()); return name; } Usage: std::cout << type_name<std::remove_pointer_t<typename std::vector<std::string>::iterator::value_type>>() << std::endl; // Prints: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > Source on SO","title":"Resolved complicated types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#show-the-user-provided-types-stdtype_info","text":"If we want to show the type of a variable provided by the user (e.g., by a function accepting std::any ), we can use the typeid operator which returns a std::type_info object.","title":"Show the user-provided types (std::type_info)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#built-in-and-stl-types","text":"","title":"Built-in and STL Types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#arithmetic-types","text":"cppreference","title":"Arithmetic Types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#integers","text":"Integer types varies in the sign and size. Unfortunatelly, the minimum sizes guaranteed by the standard are not usable, because the real size is different and it differs even between platforms . Especially the long type. To use an integer with a specific size, or a specific minimal size, we can use type aliases defined in cstdint","title":"Integers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#overflow-and-underflow","text":"The overflow (and underflow) is a common problem in most programming languages. The problem in C++ is that: overflows are not detected overflows can happen in many unexpected situations","title":"Overflow and Underflow"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#dangerous-situations","text":"In addition to the usual suspects like assigning a value to a variable of a smaller type, there are some less obvious situations that can cause overflows. Some examples: the result of an arithmetic operation is assigned to a variable of large enough type, but the overflow happens before the assignment itself: cpp short a = 32767; short b = 1; int c = a + b; // overflow happens beffore the assignment A solution to this problem is to use a numeric cast of the opperands (even one is enouhg): cpp short a = 32767; short b = 1; int c = static_cast<int>(a) + b;","title":"Dangerous situations"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#detecting-overflows","text":"There are some methods how to detect overflows automatically by suppliying arguments to the compiler. These are summarized here: MSVC : not implemented GCC : only detectes signed and floating point overflows, as the unsigned overflows are not considered as errors (the behaviour is defined in the standard). All undefined behaviour can be detected using the -fsanitize=undefined flag. Documentation Clang : Both signed and unsigned overflow can be detected. The undefined behaviour can be detected using the -fsanitize=undefined flag. Fo all integer overflows, the -fsanitize=integer flag can be used. Documentation The reasoning behind excluding the unsigned overflows from GCC are described here . It is also possible to do an ad-hoc overflow check in the code, the possible solutions are described in this SO question","title":"Detecting overflows"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#characters","text":"Characters in C++ are represented by the char type, which is an integer type. This type can be signed or unsigned, and it is at least 8 bits long. Useful functions for working with characters are: std::isspace : checks if the character is a whitespace (space, tab, newline, etc.) std::toupper : converts the character to upper case","title":"Characters"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#pointers","text":"cppreference","title":"Pointers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#pointers-to-functions","text":"Function pointers are declared as: <return_type> (*<pointer_name>)(<arg_1_type>, ..., <arg_n_type>) For example a function the_function returning bool and accepting int can be stored to pointer like this: bool (*ptr)(int) = &the_function The above example can be then simply called as bool b = ptr(2)","title":"Pointers to Functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#pointers-to-member-objects","text":"Pointers to member objects has a cumbersome syntax declaration: <member type> <class type>::*<pointer name> = ... usage: <object name>.*<pointer name> = ... Example: class My_class{ public: int my_member; } int main{ // declaring the pointer int My_class::*ptr = &My_class::my_member; // creating the instance My_class inst; // using the pointer to a member object inst.*ptr = 2; }","title":"Pointers to Member Objects"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#pointers-to-member-functions","text":"Pointers to member functions are even more scary in C++. We need to use the member object and the function adress and combine it in a obscure way: class My_class{ public: bool my_method(int par); } int main{ // creating the instance My_class inst; // assigning method address to a pointer bool (My_class::*ptr)(int) = &My_class::my_method; // using the pointer to a member function bool b = (inst.*ptr)(2) } The first unexpected change is the My_class before the name of the pointer. It's because unlike a pointer to function the_function which is of type (*)(int) , the pointer to my_method is of type (My_class::*)(int) The second difference is the call. We have t use the pointer to member binding operator .* to access the member of the specific instance inst . But this operator has a lower priority then the function call operator, so we must use the extra parantheses.","title":"Pointers to Member Functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#references","text":"References serve as an alias to already existing objects. Standard ( Lvalue ) references works the same way as pointers, with two differences: they cannot be NULL they cannot be reassigned The second property is the most important, as the assignment is a common operation, which often happens under do hood. In conslusion, reference types cannot be used in most of the containers and objets that needs to be copied .","title":"References"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#rvalue-references","text":"Rvalue references are used to refer to temporary objects. They eneable to prevent copying local objets by extending lifetime of temporary objects. They are mostly used as function parameters: void f(int& x){ } f(3); // 3 needs to be copied to f, because it is a temporary variable // we can add the rvalue overload void f(int&& x){ } f(3) // rvalue overload called, no copy","title":"Rvalue references"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#forwarding-references","text":"Forwarding references are references that preserves the value category (i.e. r/l-value reference, const ). They have two forms: function parameter forwarding references auto forwarding references","title":"Forwarding references"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-parameter-forwarding-references","text":"In a function template, if we use the rvalue reference syntax for a function parameter of whose type is a function template parameter, the reference is actually a forwarding reference. Example: template<class T> void f(T&& arg) // parameter is T& or T&& depending on the supplied argument Important details: it works only for non const references the reference type has to be a function template argument, not a class template argument","title":"Function parameter forwarding references"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#auto-forwarding-reference","text":"When we assign to `auto&&, it is a forwarding reference, not rvalue reference: auto&& a = f() // both type and value category depends on the return value of f() for(auto&& a: g(){ // same }","title":"auto forwarding reference"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#arrays","text":"cppreference There are two types of arrays: static , i.e., their size is known at compile type, and dynamic , the size of which is computed at runtime We can use the array name to access the first elemnt of the array as it is the pointer to that element.","title":"Arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#static-arrays","text":"Declaration: int a[nrows]; int a[nrwows][ncols]; // 2D int a[x_1]...[x_n]; // ND Initialization: int a[3] = {1, 2, 5} int b[3] = {} // zero initialization int c[3][2] = {{1,5}, {2,9}, {4,4}} // 2D int d[] = {1,5} // we can skip dimensions if their can be derived from data Note that the multi-dimensional syntax is just an abstraction for the programmers. The following code blocks are therefore equivalent: Matrix syntax const int rowns = 5; const int cols = 3; int matrix[rows][cols]; int main(){ for(int n = 0; n < rows; ++n){ for(int m = 0; m < cols; ++m){ Table[n][m] = (n + 1) * (m + 1); } } } Flat syntax const int rowns = 5; const int cols = 3; int matrix[rows * cols]; int main(){ for(int n = 0; n < rows; ++n){ for(int m = 0; m < cols; ++m){ Table[n * cols + m] = (n + 1) * (m + 1); } } } Using the matrix syntax adds the possibility to access the element of the array using multiple dimensions. But the underlying memory is the same.","title":"Static arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#dynamic-arrays","text":"Declaration: int* a = new int[size] For multiple dimensions, this syntax does not scale, i.e, only one dimension can be dynamic: int(*a)[4] = new int[rows][4] // static column count int(*b)[cols] = new int[rows][cols] // does not compile unless cols is a constant!","title":"Dynamic arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#array-to-pointer-implicit-conversion","text":"When we use the array name in an expression, it can be implicitly converted to a pointer to the first element of the array. This is true for both static and dynamic arrays. Example: int a[3] = {1, 2, 5} int* ptr = a; // ptr points to the first element of a This implicit conversion is called array-to-pointer decay .","title":"Array to pointer implicit conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#mutli-dimensional-dynamic-arrays","text":"To simulate multi-dimensional dynamic arrays, we have two options: use the flat syntax, as demonstrated on static arrays use aray of pointers to arrays Method Pros Cons Flat Syntax Fast: single continuous allocations different access syntax than static 2D arrays Array of pointers Slow: one allocation per row, unrelated memory addresses between rows same access syntax as static 2D arrays","title":"Mutli-dimensional dynamic arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#flat-array","text":"int* a = new int[rows * cols] Then we can access the array as: a[x * cols + y] = 5","title":"Flat array"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#array-of-pointers-to-array","text":"Declaration and Definition int** a= new int*[rows] for(int i = 0; i < rows; ++i){ a[i] = new int[cols] } Access is than like for static 2D array: a[x][y] = 5 . This works because the pointers can be also accessed using the array index operator ( [] ). In other words, it works \"by coincidence\", but we have not created a real 2D array.","title":"Array of pointers to array"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#auto-dealocation-of-dynamic-arrays","text":"We can replace the error-prone usage of new and delete by wraping the array into unique pointer: std:unique_ptr<int[]> a; a = std::make_unique<int[]>(size)","title":"Auto dealocation of dynamic arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#references-and-pointers-to-arrays","text":"cppreference The pointer to array is declared as <type> (*<pointer_name>)[<size>] : int a[5]; int (*ptr)[5] = &a; Analogously, the reference to array is declared as <type> (&<reference_name>)[<size>] : int a[5]; int (&ref)[5] = a;","title":"References and Pointers to arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-type","text":"A function type consist from the function arguments and the return type. The function type is written as return_type(arg_1_type, ..., arg_n_type) . Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo), int(double, double)>) // TRUE","title":"Function Type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#reference-to-function-and-pointer-to-function-types","text":"cppreference A refrence to function has a type return_type(&)(arg_1_type, ..., arg_n_type) . Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo)&, int(&)(double, double)>); // TRUE A pointer to function has a type: return_type(*)(arg_1_type, ..., arg_n_type) Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo)*, int(*)(double, double)>); // TRUE","title":"Reference to Function and Pointer to Function Types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#enumerations","text":"cppreference C++ supports simple enumerations, which are a set of named integer constants. The enumeration can be defined as: enum Color {red, green, blue}; // global scope enum class Color {red, green, blue}; // scoped, preferred There is no support for enum members like in Python, but we can use a wrapper class for that: class Color{ public: enum Value {red, green, blue}; Color(Value v): value(v){} // non-explicit constructor for easy initialization Value get_value() const {return value;} std::string to_string() const{ switch(value){ case Value::red: return \"red\"; case Value::green: return \"green\"; case Value::blue: return \"blue\"; } } private: Value value; } Color get_color(){ return Color::red; // this works due to the non-explicit constructor } int main(){ Color c = get_color(); std::cout << c.to_string() << std::endl; switch(c.get_value()){ case Color::red: std::cout << \"red\" << std::endl; case Color::green: std::cout << \"green\" << std::endl; case Color::blue: std::cout << \"blue\" << std::endl; } } For more complex code requireing automatic conversion to string and more, we can consider the magic_enum library . It supports the following features: enum to string conversion string to enum conversion enum iteration sequence of possible values","title":"Enumerations"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#smart-pointers","text":"For managing resources in dynamic memory, smart pointers (sometimes called handles ) should be used. They manage the memory (alocation, dealocation) automatically, but their usage requires some practice. There are two types of smart pointers: std::unique_ptr for unique ownership std::shared_ptr for shared ownership","title":"Smart Pointers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#creation","text":"Usually, we create the pointer together with the target object in one call: std::make_unique<T>(<OBJECT PARAMS>) for unique pointer std::make_shared<T>(<OBJECT PARAMS>) for shared pointer These methods work well for objects, but cannot be used for arbitrary array initialization (only the empty/zero-initialized array can be created using these methods). For arbitrary array initialization, we need to use the smart pointer constructor: std::unique_ptr<int[]> ptr(new int[]{1, 2, 3}); Counter-intuitively, smart pointers created using the empty constructor of the respective pointer type does not default-construct the target object, but initialize the pointer to null instead: std::unique_ptr<My_class> ptr(std::null_ptr); // ptr is null std::unique_ptr<My_class> ptr(); // ptr is also null","title":"Creation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#shared-pointer","text":"Pointer to object with non-trivial ownership (owned by multiple objects).","title":"Shared Pointer"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdreference_wrapper","text":"cppreference Reference wrapper is a class template that can be used to store references in containers or aggregated objects. The disintinction from normal references is that the reference wrapper can be copied and assigned, so it does not prevent the copy/move operations on the object it belongs to. Otherwise, it behaves like a normal reference: it has to be assigned to a valid object and it cannot be null.","title":"std::reference_wrapper"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#strings","text":"In C++, there are two types of strings: std::string is an owning class for a string. std::string_view is a non-owning class for a string. Also, there is a C-style string ( char* ), but it is not recommended to use it in modern C++. The difference between std::string and std::string_view is best explained by a table below: std::string std::string_view Owning Yes No Null-terminated Yes No Size Dynamic Static Lifetime Managed by the string Managed by the underlying char sequence Can be constexpr No Yes and the following code: std::string_view sv = \"hello\"; // sv is a view of the string literal \"hello\" std::string s = \"hello\"; // s stores a copy of the string literal \"hello\"","title":"Strings"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#string-literals","text":"cppreference The standard string literal is writen as \"literal\" . However, we need to escape some special characters in such literals, therefore, a raw string literal is sometimes more desirable: R\"(literal)\" . If our literal contains ( or ) , this is stil not enough, however, the delimiter can be extended to any string with a maximum length of 16 characters, for example: R\"lit(literal)lit\" . Raw string literals also useful for multi-line string literals .","title":"String Literals"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#formatting-strings","text":"The usage of modern string formating is either std::format from the <format> header if the compiler supports C++20 string formatting ( compiler support ) or fmt::format from the fmt library if not. Either way, the usage is the same: format(<literal>, <arguments>) where the literal is a string literal with {} placeholders and the arguments are the values to be inserted into the placeholders. The placeholders can be filled width argument identification, if we want to use the same argument multiple times or change the order in the string while keep the order of arguments in the function call or format specification. These two parts are separated by : , both of them are optional. The most common format specifications are: data type: d for decimal integer f for floating point number s for string width and precision, in the format <width>.<precision> . Both values can be dynamic: std::format(\"{:{}.{}f}\", a, b, c) formats a float number a with width b and precision c . The formating reference can be found in the cppreference","title":"Formatting strings"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#spliting-the-string-into-tokens","text":"Unfortunately, the STL does not provide a simple way to split the string into tokens like Python's split method or PHP's explode function. It is not even planned for the future. If we want to split a string on a character or pattern, the easiest way is to use the split view from the ranges library, which has a std::ranges::subrange as its element type: // get a range of character subranges auto parts = std::ranges::views::split(str, '-'); // iterate over the parts for (auto part : parts) { std::cout << part << std::endl; // prints the part // convert part to string std::string s(part.begin(), part.end()); // convert part to string (C++23) std::string s(std::from_range, part); } The last string constructor is only available in C++23, and moreover, it requires the stl::from_range tag. The std::string_view is equiped with a range constructor which does not require the tag in C++23. However, it is explicit, so its usage is limited: std::string_view s(part) // invalid std::string_view s = std::string_view(part) // valid in C++23","title":"Spliting the string into tokens"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#converting-string-to-int","text":"There are simple functions for converting std::string to numbers, named std::stoi , std::stoul , etc. See cppreference for details. For C strings, the situation is more complicated.","title":"Converting string to int"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#substring","text":"A substring can be obtained using a member function substr : str.substr(str.size() - 1, 1)) // returns the last character as a string","title":"Substring"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#change-the-case","text":"Unfortunatelly, the STL has case changing functions only for characters, so we need to iterate over the string ourselfs. The boost has a solution, however: #include <boost/algorithm/string.hpp> auto upper = boost::to_upper(str); Alternatively, we can use the std::transform algorithm and the std::toupper or std::tolower functions: std::transform(str.begin(), str.end(), str.begin(), std::toupper);","title":"change the case"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#building-strings","text":"Unlike other languages, in C++, strings are mutable, so we can build them using the + operator without performance penalty. Alternatively, we can use the std::stringstream class.","title":"Building strings"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#testting-for-whitespace","text":"To test if a string contains only whitespace characters, we can use the std::all_of algorithm: std::all_of(str.begin(), str.end(), [](char c){return std::isspace(c);})","title":"Testting for whitespace"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#date-and-time","text":"The date and time structure in C++ is std::tm . We can create it from the date and time string using std::get_time function: std::tm tm; std::istringstream ss(\"2011-Feb-18 23:12:34\"); ss >> std::get_time(&tm, \"%Y-%b-%d %H:%M:%S\");","title":"Date and time"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#collections","text":"In C++, the collections are implemented as templates, so they can store any type. The most common collections are: std::array std::vector std::unordered_set std::unordered_map std::pair and std::tuple Currently, the collection semantic requirmenets are not imposed on the whole connection, bu on its member functions instead. Depending on the function used, there are different requirements for the stored types. This adds a lot of flexibility, as we can, for example, use move only types in collections when we refrain from using functions that require copying. On the other hand, it can make the debugging harder, as the compiler usually does not recognize the methods that caused the template to have stricter requirements but instead complains on the place where the template is instantiated.","title":"Collections"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#sets","text":"Normal set collection for C++ is std::unordered_set . By default, the set uses a Hash , KeyEqual and Allocator template params provided by std functions. However, they need to exist, specifically: std::hash<Key> std::equal_to<Key> std::allocator<Key> So either those specializations needs to be provided by the snadard library (check cppreference), or you have to provide it.","title":"Sets"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#providing-custom-hash-function","text":"There are two options for providing custom hash function for a type T s: implementing an explicit specialization of the template function std::hash<T> providing the Hash template param when constructing the hash The first method is prefered if we want to provide a default hash function for some type for which there is no hash function specialization in the standard library. The second method is prefered only when we want some special hash function for a type T for which std::hash<T> is already defined.","title":"Providing custom hash function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implementing-custom-hash-function","text":"First check whether the hash function is not provide by STL on cppreference . Then, many other hash specializations are implemented by boost, check the reference . If there is no implementation, we can implement the hash function as follows (example for set): template<> struct std::hash<std::unordered_set<const Request*>> { size_t operator()(const std::unordered_set<const Request*>& set) const { std::hash<const Request> hash_f; size_t sum{0}; for (const Request* r : set) { sum += hash_f(*r); } return sum; } }; Important implementation details: the function needs to be implemented inside std or annonymous namespace, not inside a custom namespace do not forget to add template<> above the function, this indicates that it is a template specialization.","title":"Implementing custom hash function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#maps","text":"The maps has similar requiremnts for keys as the requirements for set value types (see previous section). The hash map type is called std::unordered_map . Note that maps require the stored types to be complete .","title":"Maps"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#geeting-value-by-key","text":"To access the map element, the array operator ( [] ) can be used. Note however, that this operator does not check the existence of the key, even if we do not provide a value. Example: std::unordered_map<int,std::string> map; map[0] = \"hello\" map[0] = \"world\" // OK, tha value is overwritten a = map[1] // a == map[1] == \"\" unintuitively, the default value is inserted if the key does not exist Therefore, if we just read from the map, it is safer to use the at() member function.","title":"Geeting value by key"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#inserting-into-map","text":"There are five options: map[key] = value; or map.insert({key, value}) map.emplace(key, value); map.try_emplace(key, value); map.insert_or_assign(key, value); The following table summarizes the differences: Method If key exists constructs in place returns value map[key] = value; overwrites no a reference to the value map.insert({key, value}); does not overwrite no a pair of iterator to value and bool set to true if insertion took place map.emplace(key, value); does not overwrite yes a pair of iterator to value and bool set to true if insertion took place map.try_emplace(key, value); does not overwrite yes same as emplace map.insert_or_assign(key, value); overwrites yes a pair of iterator to value and bool set to true if insertion took place There is only a small difference between emplace and try_emplace : the try_emplace does not create a new value if the key already exists, while the emplace can create a new value even if the key already exists (in which case, the value is then discarded).","title":"Inserting into map"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#tuples","text":"We have two standard class templates for tuples: std::pair for pairs std::tuple for tuples with unlimited size Although named differently, these class templates behaves mostly the same.","title":"Tuples"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#creating-tuples","text":"There are two ways of creating a tuple: constructor ( auto p = std::pair(...) ) initializer ( auto p = {} ) Beware that by default , the deduced types are decayed, i.e., const and references are removed and the tuple stores value types . If you need to store the reference in a tuple, you have to specify the type: auto p = std::pair<int, constr std::string&>(...) Also, beware that the RVO does not apply for tuple members. This means that if we store values types in the tuple, the types are copied/moved, and in conclusion, they have to by copyable/movable! This is the reason why we frequently use smart pointers in tuples even though we would reurn directly by value if we returned a single value.","title":"Creating tuples"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#creating-tuples-with-stdmake_pair-or-stdmake_tuple","text":"TLDR: from C++17, there is no reason to use make_pair / make_tuple . There are also factory methods make_pair / make_tuple . Before C++17, argument deduction did not work for constructors, so there is a dedicated method for creating tuples. However, now we can just call the constructor and the template arguments are deduced from the constructor arguments. Also, the make_pair / make_tuple functions can only produce tuples containing values, not references (even if we specify the reference type in the make_pair / make_tuple template argument, the returned tuple will be value-typed).","title":"Creating tuples with std::make_pair or std::make_tuple"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#accessing-tuple-members","text":"The standard way to access the tuple/pair mamber is using the std::get function: auto tuple = std::tuple<int, std::string, float>(0, \"hello\", 1.5); auto hello = std::get<1>(tuple);","title":"Accessing tuple members"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#unpacking-tuples-into-variables","text":"There are two scenarios of unpacking tuples into variables: unpacking into new variables : for that, we use structured binding . unpacking into existing variables : for that, we use std::tie function.","title":"Unpacking tuples into variables"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#structured-binding","text":"If we don't need the whole tuple objects, but only its members, we can use a structured binding . Example: std::pair<int, int> get_data(); void main(){ const auto& [x, y] = get_data(); }","title":"Structured binding"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdtie","text":"If we want to unpack the tuple into existing variables, we can use the std::tie function: std::pair<int, int> get_data(); void main(){ int x, y; std::tie(x, y) = get_data(); }","title":"std::tie"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#unpacking-tuples-to-constructor-params-with-stdmake_from_tuple","text":"We cannot use structured binding to unpack tuple directly into function arguments. For normal functions, this is not a problem, as we can first use structured binding into local variables, and then we use those variables to call the function. However, it is a problem for parent/member initializer calls, as we cannot introduce any variables there. Luckily, there is a std::make_from_tuple template function prepared for this purpose. Example: std::tuple<int,float> get_data(){ ... } class Parent{ public: Parent(int a, float b){...} { class Child: public Parent{ public: Child(): Parent(std::make_from_tuple<Parent>(get_data())){} }","title":"Unpacking tuples to constructor params with std::make_from_tuple"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdoptional","text":"cppreference std::optional<T> is a class template that can be used to store a value of type T or nothing. The advantage over other options like null pointers or is that the std::optional is a value type, so it can wrap stack objects as well. The type T must satisfy std::is_move_constructible_v<T> (must be either movable or copyable). The usage is easy as the class has a value constructor from T and a default constructor that creates an empty optional. Also, the type T is convertible to std::optional<T> , and std::nullopt is convertible to an empty optional. Finally, std::optional<T> is convertible to bool , so it can be used in if statements. A typical usage is: class My_class{ public: My_class(int a, int b); } std::optional<My_class> f(){ ... return My_class(a, b); // or return {a, b}; // or, in case of fail return std::nullopt; } std::optional<int> a = f(); if(a){ // a has a value }","title":"std::optional"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#unions-and-variants","text":"The idea of a union is to store multiple types in the same memory location. Compared to the polymorphism, when we work with pointers and to templates, where the actual type is determined at compile time, the union actually has a shared memory for all the types. The union can be therefore used in cases where nor polymorphism neither templates are suitable. One example can be storing different unrelated types (e.g., std::string and int ) in a container. We cannot use templates as that require a single type. Nor we can use polymorphism, as the types are unrelated. The big disadvantage of unions is that they are not type safe. The compiler cannot check if the type we are accessing is the same as the type we stored. Therefore, we have to be very careful when using unions. Therefore, unless some special case, we should use std::variant instead of unions .","title":"Unions and Variants"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdvariant","text":"The declaration of std::variant is similar to the declaration of std::tuple : std::variant<int, double> v; The std::variant can store any of the types specified in the template parameters. The only requirement is that the types are default constructible. Also, incompatible types cannot be stored in std::variant , as we cannot use them as template arguments. However, we can use pointers or references to incompatible types instead. The type of the stored value can be obtained using: std::holds_alternative method that returns a boolean value if the variant stores the type specified in the template parameter or std::variant::index method that returns the index of the stored value. this method can be used also in a switch statement as the index is integral The value can be accessed using: the std::get function, if we know the type stored in the variant or the std::get_if function if we are guesing the type. Both functions return a pointer to the stored value. Example: std::variant<int, double> v = 1; std::cout << v.index() << std::endl; // prints 0 std::cout << *std::get_if<int>(&v) << std::endl; // prints 1 A really usefull feature of std::variant is the std::visit method, which allows us to call a function on the stored value. The function is selected based on the type of the stored value. Example: std::variant<int, double> v = 1; std::visit([](auto&& arg) { std::cout << arg << std::endl; }, v); // prints 1 More on variants: cppreference cppstories","title":"std::variant"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#storing-individual-bits-in-a-sequence","text":"Storing individual bits is a strategy for saving memory in high-performance applications, where each saved bit can have a dramatical impact on the performance. When working with small amounts of objects (less than millions), these strategies are not worth the effort, as we can store information in built-in types like int , char , or bool . There are multiple strategies for storing individual bits where each bit has its own meaning: using built-in arithmetic types and accessing individual bits using bit masks std::bitset using bitfields using std::vector<bool> The best strategy depends on the use case. The following table summarizes the pros and cons of the different strategies: Feature built-in arithmetic types std::bitset bitfields std::vector<bool> Maximum size per variable 64 bits unlimited 64 bits unlimited Dynamic size no no no yes","title":"Storing individual bits in a sequence"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#reading-a-subsequnece-of-bits","text":"For reading individual bits or the whole sequence of bits, each strategy has its own way of doing it. However, for reading a subsequence of bits, there is no machinery, so we have to resort to bit operations no matter of the strategy. For skipping first n bits, we use the right shift ( >> ) or the left shift operator, depending on endians. For skipping last n bits, we use a bitmask and the and ( & ) operator. Example: std::bitset<8> b = 0b10101010; std::cout << b.to_ulong() << std::endl; // prints 170","title":"Reading a subsequnece of bits"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdbitset","text":"cppreference std::bitset<N> is a class template that can store up to N bits.","title":"std::bitset"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#reading","text":"To read a single bit , we can use: the test member function that returns a boolean value the operator[] operator that returns a boolean value To read more bits at once , we can use: the to_ulong member function that returns an unsigned long value the to_ullong member function that returns an unsigned long long value Both methods convert the whole bitset to an integer value of the corresponding type. If the bitset is larger than the integer type, an exception is thrown. There is no function for reading a specified sequence of bits . One way to overcome this is to read the whole bitset and apply a series of bit operations to get the desired bits (same as we would do with built-in arithmetic types). Another, slower option is to read the bits one by one using for loop.","title":"Reading"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdany-storing-any-type-of-value","text":"cppreference std::any is a class template that can store any type of value. It is a better alternative to traditional usage of void pointers. If the possible types are known at compile time, we should use std::variant instead.","title":"std::any: storing any type of value"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#value-categories","text":"[cppreferencepreerecege/value_category). In many contexts, the value category of an expression is important in deciding whether the code compiles or not, or which function or template overload is chosen. Therefore, it is usefull to be able to read value categories. expression value types: lvalue , meaning left-value. An expression typically on the left side of compound expression a statement, e.g. variable, member, or function name. Also, lvalues expressions are are: function ratoalls to fuctions returning lvalue assignments ++a , --a and similar pre operators *a indirection string literal cast prvalue , meaning pure rvalue. It is either a result of some operand ( + , / ) or a constructor/initializer result. The foloowing expressions are prvalues: literals with exception of string literals, e.g.: 4 , true , nullptr function or operator calls that return rvalue (non-reference) a++ , a-- and other post operators arithmetic and logical expressions &a address of expression this non-type template parameters, unless they are references lambda expressions requires expressions and concept spetializations xvalue , meaning expiring value. These valaues usually represent lvalues converted to rvalues. Xvalue expressions are: function call to functions returning rvalue reference (e.g., std::move ). member object expression ( a.m ) if a is an rvlaue and m is a non-reference type glvalue = lvalue || xvalue . rvalue = prvlaue || xvalue .","title":"Value Categories"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#operators","text":"cppreferencen C++ supports almost all the standard operators known from other languages like Java, Python, or C#. Additionally, thsese operators can be overloaded. There are several categories of operators: arithmetic operators , including bitwise operators comparison operators logical operators assignment operators Note that the standard also supports alternative tokens for some operators (e.g., && -> and , || -> or , ! -> not ). However, these are not supported by all compilers. In MSVC, the /permissive- flag needs to be used to enable these tokens.","title":"Operators"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#user-defined-operators","text":"In C++ there are more operators than in other popular es like Python or Java. Additionally, these operators can be overloaded. See cppreferencen page for detailed description.","title":"User-defined Operators"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#comparison-operators","text":"","title":"Comparison Operators"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-comparison-operators","text":"cppreference . The != is usually not a problem, because it is implicitely generated as a negation of the == operator. However, the == is not generated by default, even for simple classes . To force the generation of a default member-wise comparison operator, we need to write: bool operator==(const My_class&) const = default; However, to do that, all members and base classes have to ae the operator == defined, otherwise the default operator will be implicitely deleted. The comparability can be checked with a std::equality_comparable<T> concept: staic_assert(std::equality_comparable<My_class>);","title":"Default Comparison Operators"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#ternary-operator","text":"cppreference Ternary operator in C++ has the classical syntax of <condition> ? <true_expression> : <false_expression>; Note that both the true and false expressions must evaluate to the same type . Therefore, if we use polymorphism, we need to use manual type casting to the base type for at least one of the expressions (instead of relying on the implicit conversion when assigning to the result variable).","title":"Ternary Operator"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#typeid","text":"The typeid operator returns a std::type_info object that contains information about the type of the expression. Typically, we store the type_info in a std::type_index wrapper.","title":"typeid"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#control-structures","text":"C++ supports the control structures known from other languages like Java, Python, or C#. Here, we focus on the specifics of C++.","title":"Control Structures"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#switch-statement","text":"cppreference In C++, we can switch on integer types or enumeration types. Also, we can use classes that are implicitely convertible to integers or enums. Switch on string is not possible. The switch statement has the following syntax: switch(expression){ case value1: // code break; case value2: // code break; default: // code } However, it is usually a good idea to wrap each case in a block to create a separate scope for each case. Without it, the whole switch is a single block (contrary to if/else statements). The swich statements just jump to a case that matches the value, similarly to a goto statement. This can create problems, as for example variable initialization cannot be jumped over. The safe case statement looks like: switch(expression){ case value1:{ // code break; } case value2:{ // code break; } default:{ // code } }","title":"Switch Statement"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#classes-and-structs","text":"The only difference between a class and a struct is that in class, all members are private by default.","title":"Classes and structs"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#static-members","text":"cppreference Inside a class, the keyword static marks members that are not bound to any specific instance of the class, but to the class itself. They are declared as with the keyword static , but defined without it. To use a static member, we use the syntax <class_name>::<member_name> .","title":"Static Members"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#class-constants","text":"Class constants are static members that are defined as constants. They can be defined in two ways: static constexpr member variable if the constant type supports constexpr specifier, or static const member variable In the second case, we have to split the declaration and definition of the variable to avoid multiple definitions: // in the header file class My_class{ static const int a; } // in the cpp file const int My_class::a = 5;","title":"Class Constants"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#friend-declaration","text":"cppreference Sometimes, we need to provide an access to private members of a class to some other classes. In java, for example, we can put both classes to the same package and set the members as package private (no specifier). In C++, there is an even stronger concept of friend classes. We put a friend declaration to the body of a class whose private members should be accessible from some other class. The declaratiton can look as follows: Class To_be_accesssed { friend Has_access; } Now the Has_access class has access to the To_be_accesssed 's private members. Note that the friend relation is not transitive, nor symetric, and it is not inherited.","title":"Friend declaration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-friends","text":"If we want a template to be a friend, we can modify the code above: class To_be_accesssed { template<class T> friend class Has_access; } Now every Has_access<T> is a friend of To_be_accesssed . Note thet we need to use keyword class next to friend . We can also use only a template spetialization: class To_be_accesssed { friend class Has_access<int>; } or we can bound the allowed types of two templates togehter if both Has_access and of To_be_accesssed are templates: template<class T> class To_be_accesssed { friend class Has_access<T>; }","title":"Template friends"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#functions","text":"cppreference","title":"Functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-declaration-and-definition","text":"In C and C++, functions must have a: - declaration (signature) that specifies the function name, return type, and parameters - definition that specifies the function body The declaration has to be provided before the first use (call) of the function. The definition can be provided later. The declaration is typically provided in a header file, so that the function can be used outside the translation unit. The definition is typically provided in a source file.","title":"Function Declaration and Definition"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#merged-declaration-and-definition","text":"If the function is not used outside the translation unit, the declaration and definition can be merged, i.e., the definition is itself a declaration. However, this is not recommended because after adding a corresponding declaration to one of the included headers (including libraries), the merged declaration/definition will become a definition of that function, which will be manifested as a linker error (multiple definitions of the function). Therefore, to control the visibility of the function, it is better to use other methods, provides in Section Visibility of Functions .","title":"Merged Declaration and Definition"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deciding-between-free-function-member-function-and-static-member-function","text":"Basically, you should decide as follows: Function needs access to instance -> member function Function should be called only by class members (i.e., member functions), so we want to limit its visibility, or we need to access static members of the class -> static member function Otherwise -> free function","title":"Deciding between free function, member function and static member function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#argument-parameter-conversions","text":"Arg/param value reference rvalue value - - std::move reference implicit copy - copy constructor rvalue - not possible -","title":"Argument-parameter Conversions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-parameters","text":"Default function parameters in C++ works similarly to other languages: int add(int a, int b = 10); add(1, 2) // 3 add(1) // 11 However, the default parameters works only if we call the function by name. Therefore, we cannot use them in std::function and similar contexts. Example: std::function<int(int,int)> addf = add; std::function<int(int)> addf = add; // does not compile addf(1) // does not compile Also, the default parameters need to be values, not references or pointers. For references and pointers, we should use function overloading.","title":"Default Parameters"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-parameters-and-inheritance","text":"TLDR: do not use default parameters in virtual functions. The default parameters are resolved at compile time. Therefore, the value does not depend on the actual type of the object, but on the declared type of the variable. This have following consequences: the default parameters are not inherited A* a = new B(); a->foo() will call B::foo() , with the default parameters of A::foo() To prevent confusion with inheritence we should use function overloading instead of default parameters in virtual functions (like in Java).","title":"Default Parameters and Inheritance"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#return-values-and-nrvo","text":"For deciding the return value format, refer to the return value decision tree . Especially, note that NRVO is used in modern C++ and therefore, we can return all objects by value with no overhead most of the time. The NRVO works as follows: compiler tries to just tranfer the object to the parent stack frame (i. e. to the caller) without any move or copy if the above is not possible, the move constructor is called. if the above is not possible, the copy constructor is called. From C++17, the RVO is mandatory, therefore, it is unlikely that the compiler use a move/copy constructor. Consequently, most of the times, we can just return the local variable and let the rest to the compiler: unique_ptr<int> f(){ auto p = std::make_unique<int>(0); return p; // works, calls the move constructor automatically in the worst case (pre C++17 compiler) // return move( p ); // also works, but prevents NRVO } The NRVO is described also on cppreference together with initializer copy elision.","title":"Return values and NRVO"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-overlaoding","text":"Both normal and member funcions in C++ can be overloaded. The oveload mechanic, however, is quite complicated. There can be three results of overload resolution of some function call: no function fits -> error one function fits the best multiple functions fits the best -> error The whole algorithm of overload resolution can be found on cppreference . First, viable funcions are determined as functions with the same name and: with the same number of parameters with a greater number of parameters if the extra parameters has default arguments If there are no viable functions, the compilation fails. Otherwise, all viable functions are compared to get the best fit. The comparison has multiple levels. The basic principle is that if only one function fits the rules at certain level, it is chosen as a best fit. If there are multiple such functions, the compilation fails. Levels: Better conversion priority (most of the time, the best fit is found here, see conversion priority and ranking bellow) non-template constructor priority","title":"Function Overlaoding"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#conversion-prioritiy-and-ranking","text":"cppreference When the conversion takes priority during the best viable function search, we say it is better . The (incomplete) algorithm of determining better conversion works as follows: standard conversion is better than user defined conversion user defined conversion is better then elipsis ( ... ) conversion comparing two standard conversions: if a conversion sequence S1 is a subsequence of conversion sequence S2, S1 is better then S2 lower rank priority rvalue over lvalue if both applicable ref over const ref if both applicable","title":"Conversion prioritiy and ranking"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#conversion-sequence-ranks","text":"exact match promotion conversion : includes class to base conversion","title":"Conversion sequence ranks"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constructor-argument-type-resolution-in-list-initialization","text":"When we use a list initailization and it results in a constructor call, it is not immediatelly clear which types will be used for arguments as the initialization list is not an expression. These types are, however, critical for the finding of best viable constructor. The following rules are used to determine the argument types (simplified):","title":"Constructor argument type resolution in list initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#auto-return-type","text":"For functions that are defined inside declaration (template functions, lambdas), the return type can be automatically deduced if we use the auto keyword. The decision between value and reference return type is made according to the following rules: return type auto -> return by value return type auto& -> return by reference return type auto* -> return by pointer return type decltyype(auto) -> the return type is decltype(<RETURN EXPRESSION>) See more rules on cppreference Note that the auto return type is not allowed for functions defined outside the declaration (unless using the trailing return type).","title":"auto return type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-visibility","text":"The member function visibility is determined by the access specifier, in the same manner as the member variable visibility. For free functions , the visibility is determined by the linkage specifier . Without the specifier, the function is visible. To make it visible only in the current translation unit, we can use the static specifier. An equivalent way to make a function visible only in the current translation unit is to put it into an anonymous namespace : namespace { void f() {} } This way, the function is visible in the current translation unit, as the namespace is implicitly imported into it, but it is not visible in other translation units, because anonymous namespaces cannot be imported. One of the other approches frequently used in C++ is to put the function declaration into the source file so it cannot be included from the header. This solution is, however, flawed, unsafe, and therefore, not recommended . The problem is that this way, the function is still visible to the linker, and can be mistakenly used from another translation unit if somebody declare a function with the same name.","title":"Function visibility"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deleting-functions","text":"cppreference We can delete functions using the delete keyword. This is mostly used for preventing the usage of copy/move constructors and assignment operators. However, it can be used for any function, as we illustrate in the following example: class My_class{ print_integer(int a){ std::cout << a << std::endl; } // we do not want to print doubles even they can be implicitly converted to int print_integer(double a) = delete; }","title":"Deleting functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#initialization-and-assignment","text":"","title":"Initialization and Assignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#loacal-variables-initializationassignment","text":"Initialization happens in many contexts : in the declaration in new expression function parameter initialization return value initialization The syntax can be: (<expression list>) = expression list {<initializer list>} Finally, there are multiple initialization types, the resulting initialization type depends on both context and syntax: Value initialization: std::string s{}; Direct initialization: std::string s{\"value\"} Copy initialization: std::string s = \"value\" List initialization: std::string s{'v', 'a', 'l', 'u', 'e'} Aggregate initialization: char a[3] = {'a', 'b'} Reference initialization: char& c = a[0] Default initialization: std::string s","title":"Loacal variables initialization/assignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#list-initialization","text":"List initialization initializes an object from a list. he list initialization has many forms, including: My_class c{arg_1, arg_2} My_class c = {arg_1, arg_2} my_func({arg_1, arg_2}) return {arg_1, arg_2} The list initialization of a type T can result in various initializations/constructios depending on many aspects. Here is the simplified algorithm: 1. T is aggregate -> aggregate initialization 2. The initializer list is empty and T has a default constructor -> value initialization 3. T has an constructor accepting std::initializer_list -> this constructor is called 4. other constructors of T are considered, excluding explicit constructors","title":"List initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#value-initialization","text":"cppreference This initializon is performed when we do not porvide any parameters for the initialization. Depending on the object, it results in either defualt or zero initialization.","title":"Value initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#aggregate-initialization","text":"Aggregate initialization is an initialization for aggregate types. It is a form of list initialization. Example: My_class o1{arg_1, arg_2}; My_class o2 = {arg_1, arg_2}; // equivalent The list initialization of type T from an initializer list results in aggregate initialization if these conditions are fullfilled: the initializer list contains more then one element T is an aggregate type","title":"Aggregate initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#nested-initialization","text":"It is not possible to create nested initializatio statements like: class My_class{ int a, float b; public: My_class(ina a, float b): a(a), b(b) } std::tuple<int, My_class>{2, {3, 2.5}} // does not compile std::tuple<int, My_class>{2, My_class{3, 2.5}} // correnct version","title":"Nested initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#member-initializationassignment","text":"There are two ways of member initialization: default member initialization initialization using member initializer list And then, there is an assignment option in constructor body . Reference: default member initialization constructor and initializer list One way or another, all members should be initialized at the constructor body at latest , even if we assign them again during all possible use cases. Reason: some types (numbers, enums, ...) can have arbitrary values when unassigned. This can lead to confusion when debugging the class, i.e., the member can appear as initialized even if it is not. easy support for overloading constructors, we can sometimes skip the call to the constructor with all arguments we can avoid default arguments in the constructor It is important to not use virtual functions in member initialization or constructor body , because the function table is not ready yet, so the calls are hard wired, and the results can be unpredictable, possibly compiler dependent.","title":"Member Initialization/Assignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-member-initialization","text":"Either a brace initializer : My_class{ int member{1} } or an equals initializer : My_class{ int member = 1 }","title":"Default Member Initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#member-initializer-list","text":"Either using direct initialization (calling constructor of member ): My_class{ My_class(): member(1){ } or list initialization : My_class{ My_class(): member{1}{ }","title":"Member Initializer List"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constructor-body","text":"My_class{ My_class(){ member = 1 } }","title":"Constructor Body"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#comparison-table","text":"Ordered by priority, i.e., each method makes the methods bellow ignored/ovewritten if applied to the same member. Type In-place works for const members Constructor body no no Member initializer list yes yes Default member initializer yes, if we use direct initialization yes","title":"Comparison Table"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constructors-and-special-member-functions","text":"cppreference Special member functions are member functions that are someetimes defined implicitely by the compiler. The special member functions are: default (no parameter) constructor copy constructor copy assignment move constructor move assignment destructor These functions can be: defined implicitely by the compiler deleted implicitely by the compiler defaulted , i.e., defined by the compiler on our request cpp My_class() = default; Along with the comparison operators, these are the only functions that can be (see below). deleted , i.e., disabled by the compiler on our request cpp My_class(const My_class&) = delete; By default, all special member functions are defined implicitely if the members satisfy the requirements (see below). However, if we define any of the special member functions, the implicit definition is disabled. Therefore, typically, we define all special member functions or none of them.","title":"Constructors and Special Member Functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constructor","text":"","title":"Constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#defualt-variant","text":"The default constructor just create an empty object. The default constructor is not implicitly generated if: there is anothe constructor declared, including copy and move constructor there is some member that cannot be defaulty initialized","title":"Defualt Variant"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#explicit-constructor","text":"Sometimes, a normal constructor can lead to unexpected results, especially if it has only a single argument: class My_string { public: String(std::string string); // convert from std::string String(int length); // construct empty string with a preallocated size }; String s = 10; // surprise: empty string of size 10 istead of \"10\" To prevent these surprising conversion, we can mark the constructor explicit . The explicit keyword before the constructor name prevents the assigment using this constructor. The explicit constructor has to be explicitelly called.","title":"Explicit constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#call-one-constructor-from-another","text":"We can call one constructor from another using the delegating constructor . The syntax is: class My_class{ public: My_class(int a, int b): a(a), b(b){} My_class(int a): My_class(a, 0){} // delegating constructor } This way, we can call another constructor of the same class, or of the base class.","title":"Call one constructor from another"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#copy-constructor","text":"cppreference A copy constructor is called if an object is initialized from another object unless the move constructor is called as a better fit or the call is optimized out by copy elision . Some examples: initializing a new object from an existing object: My_class a; My_class b = a; // copy constructor called My_class c(a); // copy constructor called passing an object to a function by value: void f(My_class a){...} My_class a; f(a); // copy constructor called returning an object by value where the type is not movable and the compiler cannot optimize the call out. we call the copy constructor directly","title":"Copy Constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implicit-declaration-and-implicit-deletion","text":"The copy constructor for type T is implicitely-declared if T has no declared user-defined copy constructors. If some there are some user-defined copy constructors, we can still force the implicit declaration of the copy constructor using the default keyword However, the implicit declaration does not mean that the copy constructor can be used! This is because the copy constructor can be implicitely defined as deleted . This happens if any of the following conditions is true: T has a non-static data member that cannot be copied. This can happen if any of the following is true: it has a deleted copy constructor, the copy constructor is inaccessible ( protected, private ) the copy constructor is ambiguous (e.g., multiple inheritance) T has a base class that cannot be copied, i.e., 1, 2, or 3 applies to at least one base class T has a non-static data member or base class with inaccessible destructor T has a rvlaue data member T has a user-defined move constructor or move assignment operator (this rule does not apply for defaulted copy constructor) The default implementationof copy constructor calls recursively the copy constructor of all base classes and on all members. For a pointer member, the copy object\u2019s member points to the same object as the original object\u2019s member","title":"Implicit declaration and implicit deletion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#checking-if-a-class-is-copy-constructible","text":"We can check if a class is copy constructible using the std::is_copy_constructible type trait.","title":"Checking if a class is copy constructible"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#copy-assignment","text":"Copy Assignment is needed when we use the = operator with the existing class instances, e.g.: Class instanceA {}; Class instanceB; instanceB = instance A","title":"Copy Assignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#move-constructor","text":"cppreference Move constructor semantic is that the new object takes the ownership of the resources of the old object. The state of the old object is unspecified, but it should not be used anymore. Move constructor is typically called when the object is initaialized from xvalue (but not prvalue!) of the same type. Examples: returning xvalue: Type f(){ Type t; return std::move(t); } passing argument as xvalue: f(Type t){ ... } Type t f(std::move(t)); initializing from xvalue: Type t; Type t2 = std::move(t); Note that for prvalues, the move call is eliminated by copy elision . Therefore, some calls that suggest move constructor call are actually optimized out: Type f(){ Type t; return t; // no move constructor call, copy elision } Type t = T(f()) // no move constructor call, copy elision Move constructor is needed: to cheaply move the object out from function if RVO is not possible to store the object in vector without copying it Note that a single class can have multiple move constructors, e.g.: both Type(Type&&) and Type(const Type&&) .","title":"Move Constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implicit-declaration-and-implicit-deletion_1","text":"The move constructor for type T is implicitely-declared if T has no declared copy constructors, copy assignment operators, move assignment operators, or destructors. If some of the above is declared, we can still force the implicit declaration of the move constructor using the default keyword However, that does not mean that the move constructor can be used! This is because the move constructor can be implicitely defined as deleted . This happens if any of the following conditions is true: T has a non-static data member that cannot be moved. A member cannot be moved if any of the following is true: it has a deleted, inaccessible (protected, private), or ambiguous move constructor, it is a reference, it is const -qualified T has a base class that cannot be moved, i.e., 1, 2, or 3 applies to at least one base class T has a non-static data member or base class with inaccessible destructor","title":"Implicit declaration and implicit deletion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#checking-if-a-class-is-move-constructible","text":"We can check if a class is move constructible using the std::is_move_constructible type trait. However, the std::is_move_constructible does not check if the move constructor is accessible! Instead it checks if the call to the move constructor is valid (can success, compiles). The call can success if the move constructor is accessible, but it can also success if it is not accessible, but the class has a copy constructor, which is used instead. To check if the move constructor is accessible, we have to manually check the conditions, or disable the copy constructor.","title":"Checking if a class is move constructible"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#move-assignment","text":"","title":"Move Assignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#trivial-special-member-functions","text":"The special member functions are called trivial if they contain no operations other then copying/moving the members and base classes. For a special member function of type T to be trivial, all of the following conditions must be true: it is implicitly-declared or defaulted T has no virtual functions T has no virtual base classes the constructor for all direct base classes is trivial the constructor for all non-static data members is trivial","title":"Trivial special member functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#destructor","text":"We need destructor only if the object owns some resources that needs to be manually deallocated","title":"Destructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#typical-usage","text":"It's mostly better to delete everything you don\u2019t need. Most likely, either - we need no custom constructors, or we need three (move and destructor), or we need all of them.","title":"Typical usage"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#simple-temporary-object","text":"the object should live only in some local context we don\u2019t need anything","title":"Simple Temporary Object"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#unique-object","text":"usually represents some real object usually, we need constructors for passing the ownership: move constructor move assignment","title":"Unique Object"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-object","text":"copyable object We need copy constructor copy assignment move constructor move assignment","title":"Default Object"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#const-vs-non-const","text":"The const keyword can be used in two contexts: as a type qualifier, making the variable non-mutable ( cppreference ) as a member function qualifier, making the function work with const pointer to the instance ( cppreference ) A variable is non-mutable (const) if it is: const -qualified it is a member of a const object and it is not declared as mutable Non mutable variables: cannot be reassigned non-const member functions of them cannot be called A member function can be declared with const qualifier at the end of the declaration. A function declared with const: accesses its instance as const *<class_name> instead of <class_name>* by conclusion, see all members as const which means: non-const member functions of the object cannot be called non-const member variables of the object are accessed as const references For members, the const keyword can break the move operations on the object . For example we cannot move from a const std::unique_ptr<T> object. While this is also true for local variable, in members, it can lead to hard to find compilation errors, as a single const std::unique_ptr<T> member deep in the object hierarchy breaks the move semantic for the whole class and all subclasses.","title":"Const vs non-const"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#avoiding-duplication-between-const-and-non-const-version-of-the-same-function","text":"To solve this problem without threatening the const-correctness, we need to implement the const version of a function and call it from the non-const one with double type cast: one that converts this to const, so we can call the const version of the function another one that removes const from the return value Example: const Content& get_content(unsigned index) const { Content content = ... // complicated code to get the right content return content; } Content& get_content(unsigned index){ return const_cast<Content&>(std::as_const(this*).get_content()); }","title":"Avoiding duplication between const and non-const version of the same function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constnon-const-overloads-and-inheritance","text":"Normally, the compiler can safely choose the best match between const and non-const overloads. The problem can happen when each version is in a different place in the class hierarchy. Example: class Base { public: const int& get() const { return some; } protected: int some; }; class A : public virtual Base { public: int& get() { return some; } }; class B : public A {}; B test; test.get(); // ambiguous function error The problem is that the overload set is created for each class in the hierarchy separately. So if the overload was resolved prior the virtual function resolution, we would have only one version (non-const), which would be chosen, despite not being the best overload match in both overload sets. To prevent such unexpected result, some compilers (GCC) raise an ambiguous function error in such situations. To resolve that, we can merge the overload sets in class B : class B : public A { using Base:get; using A:get; };","title":"Const/non const overloads and inheritance"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#io-and-filesystem","text":"The simple way to print to standard input is: std::cout << \"Hello world\" << std::endl; To return to the begining of the line and overwrite the previous output, we can use the '\\r' character: std::cout << \"Hello world\" << '\\r' << std::flush;","title":"IO and Filesystem"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#file-path-manipulation","text":"Although we can use strings to work with file paths in C++, the standard format which is also easy to use is std::filesystem::path from the filesystem library . Basic operations: To create a path , we jusct call std::filesystem::path(<string path>) . We can easily join two paths by auto full_path = <path 1> / <path 2> ; To get the asolute path , we call std::filesystem::absolute(<path>) to get the path as CWD/<path> std::filesystem::canonical(<path>) to get the dots resolved. Note that this method throws exception if the path does not exists. The path to the current working directory can be obtained by calling std::filesystem::current_path() and set using std::filesystem::current_path(<path>) . To change the file extension (in the C++ representation, not in the filesystem), we can call the replace_extension method.","title":"File path manipulation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#filesystem-manipulation","text":"cppreference","title":"Filesystem manipulation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#copying","text":"To copy, we can use std::filesystem::copy(<source path>, <destination path>[, <options>]) function. The options parameter type is std::filesystem::copy_options . This enum is a bitmask type, therefore, multiple options can be combined using the | operator. Example: auto options = std::filesystem::copy_options::recursive | std::filesystem::copy_options::overwrite_existing; std::filesystem::copy(\"C:/temp/data\", \"c:/data/new\", options); Note that unlike the unix cp command, the copy function does not copy the directoy itself , even if the destination directory exists. Suppose we have two direcories: C:/temp/new C:/data/ And we want to copy the new folder, so that the result is: C:/data/new/ . In bash, this will be: cp -r C:/temp/new C:/data/ While in C++, we need to do: std::filesystem::copy(\"C:/temp/new\", \"C:/data/new\", std::filesystem::copy_options::recursive);","title":"Copying"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#creating-directories","text":"To create a directory, we can use std::filesystem::create_directory(<path>) function. This function fails if the parent directory does not exist. To create the parent directories as well, we can use std::filesystem::create_directories(<path>) function.","title":"Creating directories"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#removing-files-and-directories","text":"To remove a file or an empty directory, we can use std::filesystem::remove(<path>) function. To remove a content of a directory we can use std::filesystem::remove_all(<path>) function listed on the same page of cppreference.","title":"Removing files and directories"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#other-useful-functions","text":"std::filesystem::exists(<path>) std::filesystem::is_directory(<path>) std::filesystem::is_regular_file(<path>) std::filesystem::is_empty(<path>)","title":"Other useful functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#getting-a-temporary-folder","text":"To get a temporary folder, we can use the std::filesystem::temp_directory_path function.","title":"Getting a temporary folder"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#manual-text-io","text":"","title":"Manual text IO"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#input","text":"For input, we can use std::ifstream : std::ifstream file; file.open(<path>); ... file.close(); The important thing is that we need to check whether the open call was successful. The open function never throws an exception, even if the file does not exist , which is a common case. Instead, it only sets the failbit of the stream. Without some check, the failure is hidden as an ifstream in a fail state behaves as if it was empty. For reading line by line , we can use the std::getline function: std::string line; while (std::getline(file, line)) { // do something with the line } However, processing the line is currently not very convenient in C++ because functions from other languages like split are missing. For reading whitespace delimited tokens we can instead use the >> operator on the stream: // file content: \"01 Smith\" int id; std::string name; file >> id >> name; If we need to skip some tokens, its best to introduce a dummy string variable: // file content: \"01 2021-01-01 active Smith\" int id; std::string dummy; std::string name; file >> id >> dummy >> dummy >> name; Conveniently, the input streams have a bool operator that states whether the stream is in a state ready for reading. This way, we can easily stop the loop when the file is read, because the >> operator returns the stream itself: // read the whole file while (file >> id >> name) { ... }","title":"Input"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#output","text":"For line by line output, we use std::ofstream : std::ofstream file; file.open(<path>); batch_file << \"first line\" << std::endl; batch_file << \"second line\" << std::endl; ... batch_file.close();","title":"Output"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#load-whole-file-into-string","text":"Again, we use the std::ifstream , but this time, we also use the std::istreambuf_iterator to read the whole file into a string: std::ifstream file(<path>); std::string content(std::istreambuf_iterator<char>{file}, {}); Here, the std::istreambuf_iterator<char> is created using initialization instead of the constructor so that the local variable is not confused with function declaration. The {} is used to create an empty string, which is the end of the range for the iterator.","title":"Load whole file into string"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#csv","text":"","title":"csv"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#input_1","text":"","title":"Input"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#output_1","text":"For csv output, we can usually use the general line-by-line approach.","title":"Output"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#yaml","text":"For YAML, we can use the yaml-cpp library. We can load from file using YAML::LoadFile(<path>) from string using YAML::Load(<string>) for some reason, the YAML::Load function does not work with objects separated by indentation, so we need to use {} to separate the objects. To test whether a YAML::Node contains a certain key , we may use the [] operator, as it does not create a new node (unlike the stl containers): YAML::Node node; if (node[\"key\"]) { // do something } The iteration over the keys is done using YAML::const_iterator : for (YAML::const_iterator it = node.begin(); it != node.end(); ++it) { std::string key = it->first.as<std::string>(); YAML::Node value = it->second; }","title":"YAML"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#hdf","text":"To load data from HDF5 files, the HDF5 C++ API can be used. Typical usage: #include <H5Cpp.h> const H5::H5File file(\"file.h5\", H5F_ACC_RDONLY); const H5::DataSet dataset = file.openDataSet(\"dataset\"); const H5::DataSpace dataspace = dataset.getSpace(); hsize_t dims[2]; dataspace.getSimpleExtentDims(dims); H5::DataSpace memspace(2, dims); dataset.read( <pointer where to store the data>, H5::PredType::<data type>, memspace, dataspace ); If we do not know the dataset name, we can get the name by index: std::string dataset_name = file.getObjnameByIdx(0); // get the first dataset name To check if the dataset exists, we can use the exists method of the file: const H5::H5File file(\"file.h5\", H5F_ACC_RDONLY); if (file.exists(\"dataset\")) { ... }","title":"HDF"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#inheritance","text":"Inheritance in C++ is similar to other languages, here are the important points: To enable overiding, a member function needs to be declared as virtual . Otherwise, it will be just hidden in a child with a function with the same name, and the override specifier cannot be used (see Shadowing). Multiple inheritance is possible. No interfaces. Instead, you can use abstract class with no data members. Virtual functions without implementation needs = 0 at the end of the declaration (e.g.: virtual void print() = 0; ) a type is polymorphic if it has at least one virtual function. I.e., the inheritance itself does not make the type polymorphic.","title":"Inheritance"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#polymorphism","text":"Polymorphism is a concept for abstraction using which we can provide a single interface for multiple types that share the same parent. In C++, to use the polymorphism, we need to work with pointers or references . Imagine that we have these two class and a method that can process the base class: class Base { }; class Derived: public Base { }; void process_base(Base* base) { } Now we can use it lake this: Derived* derived = new Derived(); Base* base = derived; // we easilly can convert derived to base process_base(base); process_base(derived); // we can call the function that accepts a base pointer with a derived pointer We can do the same with smart pointers: void process_base_sh(std::shared_ptr<Base> base) { } std::shared_ptr<Derived> derived_sh = std::make_shared<Derived>(); std::shared_ptr<Base> base_sh = derived_sh; process_base_sh(base_sh); process_base_sh(derived_sh);","title":"Polymorphism"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#shadowinghiding-why-is-a-function-from-parent-not-available-in-child","text":"Members in child with a same name as another members in parent shadows those members (except the case when the parent member is virtual). When a member is shadowed/hiden, it is not available in the child class and it cannot be called using the child class instance. This can be counter-intuitive for functions as the shadowing considers only the name, not the signature . Example: class Base { public: void print() { printf(\"Base\\n\"); } }; class Child: public Base { public: void print(std::string type) { printf(\"Child \" + type + \"\\n\"); } }; int main() { Child child; child.print(); // does not compile, as the print() is hidden by print(std::string) return 0; }","title":"Shadowing/Hiding: why is a function from parent not available in child?"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#how-to-call-a-hidden-function","text":"There are two ways how to call a hideen function: we can use the using declaration in the child to introduce the hidden function: c++ class Child: public Base { public: using Base::print; // now the print() is available in Child void print(std::string type) { printf(\"Child \" + type + \"\\n\"); } }; Usiang a fully qualified name of the method: c++ int main() { Child child; child.Base::print(); return 0; }","title":"How to call a hidden function?"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constructors","text":"Parent constructor is always called from a child. By default, an empty constructor is called. Alternatively, we can call another constructor in the initializer. When we do not call the parent constructor in the child's initializer and the parent has no empty constructor, a compilation error is raised.","title":"Constructors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#enablinging-parent-constructors-in-child","text":"Implicitly, all methods from parent classes are visible in child, with exception of constructors. Constructors can be inherited manually with a using declaration, but only all at once. To enable only some constructors, we need to repeat them manually as child constructors and call parent construcors from them.","title":"Enablinging Parent Constructors in Child"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#inheritance-and-constructorsdestructors","text":"To prevent the future bugs with polymorphic destruction calls, it's a good habit to declare a public virtual destructor in each base class : class Base{ public: virtual ~Base() = default; } Otherwise, the following code will not call the child destructor: Child* child = new Child(); Base* base = (Base) child; delete base; But when defining destructor, constructor and move operations are not impliciotely generated. Moreover, the copy operations are generated enabling a polymorphic copy, which results in slicing. Therefore, the best approach for the base class is to: declare the virtual destrucor as default declare the default constructor . We need a default constructor, unless we use a diferent constructor and we want to disable the default one. declare the copy and move operations as protected . This way, the polymorpic copy is not possible, but proper copy/move operations are generated for every child class.","title":"Inheritance and Constructors/Destructors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#initializing-base-class-members","text":"The base class members cannot be initialized in the child constructor initializer. Instead, we need to create a constructor in the base class and call it from the child constructor initializer.","title":"Initializing base class members"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#slicing","text":"Polymorphism does not go well with value types. When a value type is copied, the only part that remains is the part writen in the code. That means that copying base_2 = base_1 result in a new Base object in base_2 , even if base_1 is an instance of child. Abstract classes therefore cannot be used as function value arguments at all . To pass a polymorphic type as a value to a library function, we need a copyable wrapper that forwards all calls to the undelying polymorphic type.","title":"Slicing"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#checking-the-type","text":"There is no equivalent of Java's instanceof in C++. To check the type. it is possible to use dynamic cast: Child& child = dynamic_cast<Child&>(parent) In case of failure, std::bad_cast is thrown. To prevent exceptions (i.e., we need the type check for branching), we can use pointers: Child* child = dynamic_cast<Child*>(&parent) In this case, if the cast fails, then child == nullptr . Note that to use the dynamic_cast on a type, the type, the type needs to have at least one virtual method . However, this should not be an issue as the type should have at least a virtual destructor.","title":"Checking the Type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#covariant-return-type","text":"Covariant return type is a concept of returning a narower type id derived class than the return type specified in base. Example: class Base { public: virtual Base& get() = 0; }; class Derived: public Base{ public: Derived& get() override { return *this; } }; It works with template classes too: template<class T> class Derived_template: public Base { public: Derived_template<T>& get() override { return &this; } };","title":"Covariant Return Type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#use-method-from-parent-to-override-a-method-from-other-parent","text":"Unlike in java, a parent method cannot be used to implement an interface of a child . Example: class Interface { public: virtual void print() = 0; }; class Base { public: virtual void print() { printf(\"Base\\n\"); } }; class Child: public Base, public Interface { public: }; int main() { Child child; // does not compile, as Child is an abstract class child.print(); return 0; } The above code does not compile as in C++, the parent print() method is not used as an impementation of print() from the interface (like it works e.g. in Java). There simplest solution to this problem is to override the method in Child and call the parent method staticaly: class Child: public Base, public Interface { public: void print() override { Base::print(); } };","title":"Use Method from Parent to Override a Method from Other Parent"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#multiple-inheritance-and-virtual-base-classes","text":"wiki cppreference Multiple inheritance is possible in C++. However, it can lead to some problems. Consider the following example: class A { public: int a; }; class B: public A {}; class C: public A {}; class D: public B, public C {}; It may not be obvious, but the class D has two instances of A in it. This is because the B and C both have their own instance of A . This is certainly not what we want as this way, we have two copies of A::a in D , which are only accessible using qualified names ( D::B::a and D::C::a ) and which can have different values.","title":"Multiple inheritance and virtual base classes"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#virtual-inheritance","text":"To mitigate this problem, we can use the virtual inheritance . The virtual inheritance is used when we want to have only one instance of a base class in a child class, even if the base class is inherited multiple times. To use the virtual inheritance, we need to declare the base class as virtual in all child classes: class A { public: int a; }; class B: public virtual A {}; class C: public virtual A {}; class D: public B, public C {};","title":"Virtual Inheritance"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#multiple-copymove-calls-with-virtual-inheritance","text":"However, this solves only the problem of having multiple instances of the same base class. But there are also problems with the copy and move operations. In the above example, if the class D is copied or moved, it calls the copy/move operations of B and C , which in turn call the copy/move operations of A . This means that the A is copied/moved twice , which is not what we want. To solve this we need to manually define the copy/move operations of classes in the hierarchy so that the copy/move operations of the base class are called only once. However this can be a complex task. Also, it can backfire later when we extend the hierarchy.","title":"Multiple copy/move calls with virtual inheritance"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#other-sources","text":"SO answer SO answer 2","title":"Other sources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#templates","text":"The templates are a powerful tool for: generic programming, zero-overhead interfaces, and metaprogramming. Although they have similar syntax as generics in Java, they are principialy different both in the way they are implemented and in the way they are used. There are two types of templates: function templates class templates","title":"Templates"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#syntax","text":"","title":"Syntax"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-declaration","text":"Both for classes and functions, the template declaration has the following form: template<<template parameters>>","title":"Template Declaration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-definition","text":"The definition of template functions or functions fo the template class requires the template declaration to be present. The definition has the following form: template<<template parameters>> <standard function definition> Here, the template parameters are the function template parameters if we define a template function, or the class template parameters if we define a function of a template class. If the template function is a member of a template class, we have to specify both the template parameters of the function and the template parameters of the class: template<<class template parameters>> template<<function template parameters>> <standard class function definition> Note that the template definition has to be in the header file , either directly or included from another header file. This includes the member function definitions of a template class, even if they are not templated themselves and does not use the template parameters of the class.","title":"Template definition"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-parameters","text":"cppreference The template parameters can be: type parameters: class T value parameters: int T template parameters: template<class T> class TT Template parameters can be restricted by in several ways: typename T : any complete type class T : a class type <Concept> T : a type constrained by a concept by a requires expression, e.g.: template<typename T> requires std::is_integral_v<T>","title":"Template Parameters"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#organization-rules","text":"*.h : declarations *.tpp template definitions *.cpp non-template definitions. For simplicity, we include the tpp files at the end of corresponding header files. If we need to speed up the compilation, we can include the tpp files only in the source files that needs the implementations , as described on SE To speed up the build it is also desireble to move any non-template code to source files , even through inheritance, if needed.","title":"Organization rules"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#templates-and-namespaces","text":"If the templated code resides in a namespace, it can be tempting to save few lines of code by sorrounding both .h and .tpp files using one namespace expression: // structs.h hile namespace my_namespace { // declarations... #include 'structs.tpp' } // structs.tpp // definitions However, this can confuse some IDEs (e.g., false positive errors in IntelliSense), so it is better to introduce the namespace in both files: // structs.h hile namespace my_namespace { // declarations... } #include 'structs.tpp' // structs.tpp namespace my_namespace { // definitions } Don't forget to close the file and reopen it after the change to clear the errors.","title":"Templates and Namespaces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#providing-template-arguments","text":"A template can be instantiated only if all the template arguments are provided. The templete arguments need to be complete types . Arguments can be provided explicitly: std::vector<int> v; or sum<int>(1,2) , deduced from the initialization (classes): std::vector v = {1,2,3}; from the context (functions): sum(1,2); , or defaulted ```cpp template class A {}; template int sum (T a, T b = 0) { return a + b; } auto s = sum(1, 2); A a(); ``` If we want the template arguments to be deduced or defaulted, we usually use the <> : template<class T = int> class A {}; A<> a(); // default argument is used std::vector<A<>> v; // default argument is used In some cases, the <> can be ommited, e.g., when declaring a variable: A a; // default argument is used // but std::vector<A> v; // error, the A is considered a template here, not the instantiation The rules for omitting the <> are quite complex. Therefore, it is better to always use the <> when we want to use the default arguments.","title":"Providing Template Arguments"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#rules-for-omitting-the","text":"We can ommite the <> in the following cases: when declaring a variable: A a; when using the type in a function call: f(A()); when instantiating a template class: class B: public A {}; We cannot ommite the <> in the following cases: When we use the template as a nested type: std::vector<A<>> v; , not std::vector<A> v; in the return type of a function: A<> f() , not A f() When declaring an alias: using B = A<> not using B = A for template template parameters.","title":"Rules for omitting the &lt;&gt;"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-template-arguments","text":"Default template arguments can be used to provide a default value for any template parameter except parameter packs. For template classes, there is a restriction that after a default argument is used, all the following parameters must have a default argument as well, except the last one wchich can be parameter pack.","title":"Default Template Arguments"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-argument-deduction","text":"Details on cppreference . Template argument deduction should work for: constructors function and operator calls storing the function pointer","title":"Template Argument Deduction"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#class-template-argument-deduction-ctad","text":"Details on cppreference . The main difference from the function templete argument deduction is that in CTAD, all the template arguments needs to be specified, or all must not be specified and must be deducible. Apart from that, there are more subtle differences arising of a complex procedure that is behind CTAD. We explain CTAD principle using a new concept (not a C++ concept :) ) called deduction guides .","title":"Class Template Argument Deduction (CTAD)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deduction-guides","text":"The CTAD use so called deductione guides to deduce the template parameters. Deduction guides can be either implicit or explicit. To demonstrate the principle, let's first start with user-defined deduction guides.","title":"Deduction Guides"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#user-defined-deduction-guides","text":"Let's have an iterator wrapper class below: template<class E, Iterator<E> I> class Iter_wrapper{ public: explicit Iter_wrapper(I iterator){ ... } ... }; Here, the argument E cannot be deduced from argument I , despite the use of the Iterator concept may indicate otherwise. We can still enable the deduction by adding the following deduction guide: template<class I> Iter_wrapper(I iterator) -> Iter_wrapper<decltype(*iterator),I>; Here, the part left from -> represents the constructor call that should be guided, and the part right from -> defines the argument types we want to deduce. Some more details about user defined deduction guides are also on the Microsoft Blog .","title":"User defined deduction guides"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implicit-deduction-guides","text":"The vast majority of deduction guidedes used in CTAD are implicit. The most important implicit deduction guides are: constructor deduction guides copy deduction guides The copy deduction guide has the following form: template<<class template parameters>> <class>(<class><class template parameters> obj) -> <class><class template parameters>; For a simple wrapper class: template<class C> class Wrapper{ ... } template<class C> Wrapper(Wrapper<C> obj) -> Wrapper<C>; // implicitelly defined copy deduction guide The constructor deduction guides has the following form: template<<class template parameters>> <class>(<constructor arguments>) -> <class><class template parameters>; For a simple wrapper class: template<class C> class Wrapper{ Wrapper(T&& ref); } template<class C> Wrapper(C&&) -> Wrapper<C>; // implicitelly defined constructor deduction guide","title":"Implicit deduction guides"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deduction-guides-resolution","text":"Note that CTAD is a process independent of the constructor overload! . First an appropriate deduction guide is used to deduce the class template argumnets, this process can fail if there is no guide. Only then, the overload resolution begins. Most of the time, it is not so important and we can just look at the constructor that is chosen by the constructor overload resolution process and see the used deduction guids and consequently, the resulting template arguments. Sometimes, however, this simplified understanding can lead to confusing results: template<class C> class Wrapper{ Wrapper(T&& ref); Wrapper(double&& ref); // special overload for double } auto w1 = Wrapper(1.5) // the double overload is called In the above example, it may be surprising that the second constructor can be called, as it does not have the class argument present, so the implicit deduction guide cannot work: template<class C> Wrapper(double&&) -> Wrapper<C>; // C unknown! However, it compiles and works, because the deduction guide from the first constructor is used for CTAD, and then, the second constructor is chosen by the constructor overload.","title":"Deduction guides resolution"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-specialization","text":"Template specialization is a way to provide a different implementation of a template for a specific type. For example, we can provide a different implementation of a template for a std::string type. Imagine following class: // declaration template<class T> class Object{ public: void print(T value) }; // definition template<class T> void Object<T>::print(T value){ std::cout << value << std::endl; } Now, we can provide a different implementation for std::string : // declaration template<> class Object{ public: void print(std::string value) }; template<> void Object<std::string>::print(std::string value){ std::cout << value << std::endl; } There are two types of template specialization: full specialization : exact specification for all template arguments partial specialization : exact specification for a subset of template arguments and/or non-type template arguments To demonstrate the difference, let's have a look at the following example: // declaration template<class T, class C> class Object{}; // primary template // full specialization template<> class Object<int, std::string>{}; // full specialization // partial specializations template<class C> class Object<int, C>{}; // not a full specialization, as C is not specified template<std::integral T, My_concept C> class Object<T, C>{}; // not a full specialization, types are not exactly specified While behaving similarly, there are some important differences between the full and partial specialization : Declaration and definition: Full specialization is a new type. Therefore, it must be declared in the header and defined in the source file ( .cpp ). additionaly, if the full specialization is a member of a class, the declaration must be outside the class: ```cpp template<> class Object{ public: // member function template template void print(T value){ ... // template definition } // template full specialization declaration - wrong. This will not compile in GCC template<> void print(std::string value){ ... // template definition } }; // template full specialization declaration - correct template<> void Object::print(std::string value); ``` Partial specialization is still just a template, so it must be defined in the header file ( .h or .tpp ). For functions, we cannot provide a partial specialization . For member functions we can solve this by specializing the whole class. For free functions, we have to use other techniques like compile-time branching .","title":"Template Specialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#type-erasure","text":"cppreference When working with templates, one soon realizes that they are contiguous. Once we use a template with a parameter T , we have to use T in the calling code if we do not know the exact type, and that is also true for the calling code of the calling code, etc, all the way to the point where we know the exact type. Often times, this is principially unavoidable, because we need to use the template parameter to keep the contract on the template parameter type. However, there are situation where this is absolutely useless. Imagine this scenario: template<class T> class Vector_statistic{ void print_vector_size(const std::vector<T>& vector){ return vector.size(); } }; Here, there is no type contract on the template parameter T needed. Yet, we have to implement Vector_statistic as a template class, otherwise it won't compile. To avoid this, we can use the technic called type erasure . The principle of type erasure is to create a parent class without the template parameter T that contain the necessary methods that does not depend on the type T . Then, we use this polymorphic interface to avoid the need for the template parameter T . For the above example, we also need a wrapper class as we cannot change the std::vector class template to inherit from our parent class. class Vector_statistic_wrapper{ public: virtual void print_vector_size() = 0; }; template<class T> class Vector_statistic_wrapper_impl: public Vector_statistic_wrapper{ public: Vector_statistic_wrapper_impl(const std::vector<T>& vector): vector(vector){} void print_vector_size() override{ return vector.size(); } private: std::vector<T>& vector; }; class Vector_statistic{ public: Vector_statistic(){} void print_vector_size(const Vector_statistic_wrapper& wrapper){ return wrapper.print_vector_size(); } }; Note that for this example, it is probably simpler to use the class template even though we do not need the template parameter T . However, if the call chain independent of the type T is complex, the type erasure solution can become much simpler and more readable. Also note that the type erasure solution can never be more efficient. We can save a little bit on the compile time and code size, but the runtime performance will be strictly lower (due to the virtual function calls) Sources - https://davekilian.com/cpp-type-erasure.html","title":"Type Erasure"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-parameter-packs","text":"cppreference on all parameter packs, but mostly template parameter packs From many programming languages, ve know the concept of function parameter packs (Java), or variable length argument lists (Python). These are available in C++ as well. However, in C++, we also have a much more complex and powerful concept of template parameter packs. The differnece from function parameter packs is that template parameter packs are resolved at compile time, efectively creating function or class types with a variable number of arguments. A practical example can be a template function that measures the time of the execution of a function: template<typename F, typename... A> auto measure_time(F func, A... args){ auto start = std::chrono::high_resolution_clock::now(); func(args...); auto end = std::chrono::high_resolution_clock::now(); return std::chrono::duration_cast<std::chrono::milliseconds>(end - start).count(); }","title":"Template parameter packs"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#syntax_1","text":"Be aware that the dots ( ... ) placement is different for: declaration of the template parameter pack: dots after the parameter restriction use of the template parameter pack as a type specifier: dots after the type name using the parameter pack in an expression: dots after the parameter pack name Example: template<typename... T> // dots after the parameter restriction void func(T... args) { // dots after the parameter type auto result = other_func(args...); // dots after the parameter pack name }","title":"Syntax"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#parameter-pack-expansion","text":"When using the dots ( ... ), the parameter pack is expanded. There are many allowed contexts for the expansion, with different rules for the expansion. When having a parameter pack A... , available as A... args , it can be used: in a function call: func(args...) expands to func(arg1, arg2, ..., argN) class initialization: My_class(args...) expands to My_class(arg1, arg2, ..., argN) brace initialization: {args...} expands to {arg1, arg2, ..., argN} template argument list: std::tuple<args...> expands to std::tuple<arg1, arg2, ..., argN> function parameter list: template<typename... A> void func(A... args) expands to void func(A1 arg1, A2 arg2, ..., AN argN) class template parameter list: template<typename... A> class My_class<A...> expands to My_class<A1, A2, ..., AN> base class specifier: class My_class: public Args... expands to class My_class: public A1, public A2, ..., public AN lambda capture: [args...] expands to [arg1, arg2, ..., argN]","title":"Parameter Pack Expansion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#fold-expressions","text":"cppreference Fold expressions is a powerful feature that allows to cleanly implement operations on the whole parameter pack at once, which would otherwise be very difficult to implement. There are four possible patterns for fold expressions: (<pack> <operator> ...) (... <operator> <pack>) (<pack> <operator> ... <operator> <initial value>) (<initial value> <operator> ... <operator> <pack>) Here, the <operator> is any binary operator (arithmetic, logical, bitwise, assignment, , ,...). the <initial value> is an expression without any operator with higher precedence than the <operator> . The fold expression is evaluated as follows: (T... <operator> ...) evaluates to (<T1> <operator> (<T2> <operator> ... <operator> <TN>)) (... <operator> T...) evaluates to ((<T1> <operator> <T2>) <operator> ... <operator> <TN>) (T... <operator> ... <operator> <initial value>) evaluates to (<T1> <operator> (<T2> <operator> ... <operator> (<TN> <operator> <initial value>))) (<initial value> <operator> ... <operator> <T...>) evaluates to ((((<initial value> <operator> <T1>) <operator> <T2>) <operator> ... <operator> <TN>) Note that we can use fold expressions anywhere where we can use parameter pack expansion. Therefore, we can use them even in e.g. requires expression.","title":"Fold Expressions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#examples","text":"An example of a fold expression used for a sum function template : template<typename... Args> auto sum(Args... args){ return (... + args); } Fold expression in the requires expression : template<typename... NO> template<template <typename, class...> class S> requires(DARP_benchmark_solver_constructor_interface<S,NO> && ...) The above requires expression will be expanded to: (DARP_benchmark_solver_constructor_interface<S,NO1> && DARP_benchmark_solver_constructor_interface<S,NO2> && ... && DARP_benchmark_solver_constructor_interface<S,NON>)","title":"Examples"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-complicated-types-as-template-arguments","text":"Sometimes, it can be very tricky to determine the template argument we need in order to use the template. The correct argument can be for example a return value of some function, templete function, or even member function of a template instanciation which has other templates as argument... To make it easier, we can, istead of suplying the correct arguments, evaluate an expression that returns the correct type and then use the decltype specifier. For more info, see the Determining Type from Expressions section.","title":"Using Complicated Types as Template Arguments"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#type-traits","text":"The purpose of type traits is to create predicates involving teplate parameters. Using type traits, we can ask questios about template parameters. With the answer to these questions, we can even implement conditional compilation, i.e., select a correct template based on parameter type. Most of the STL type traits are defined in header type_traits . A type trate is a template with a constant that holds the result of the predicate, i.e., the answer to the question. More about type traits","title":"Type Traits"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#usefull-type-traits","text":"std::is_same std::is_base_of std::is_convertible std::conditional : enables if-else type selection","title":"Usefull Type Traits"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#replacement-for-old-type-traits","text":"Some of the old type traits are no longer needed as they can be replaced by new language features, which are more readable and less error prone. Some examples: std::enable_if can be replaced by concepts: // old: enable_if template<class T> void f(T x, typename std::enable_if_t<std::is_integral_v<T>, void> = 0) { std::cout << x << '\\n'; } // new: concepts template<std::integral T> void f(T x) { std::cout << x << '\\n'; }","title":"Replacement for old type traits"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#concepts","text":"cppreference Concepts are named sets of requiremnets. They can be used instead of class / typename keywords to restrict the template types. The syntax is: template<class T, ....> concept concept-name = constraint-expression The concept can have multiple template parameters. The first one in the declaration stands for the concept itself, so it can be refered in the constraint expression. More template parameters can be optionally added and their purpose is to make the concept generic.","title":"Concepts"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constraints","text":"Constraints can be composed using && and || operatos. For atomic constaints declaration, we can use: Type traits: template<class T> concept Integral = std::is_integral<T>::value; Concepts: template<class T> concept UnsignedIntegral = Integral<T> && !SignedIntegral<T>; Requires expression: template<typename T> concept Addable = requires (T x) { x + x; }; Either form we chose, the atomic constraint have to always evaluate to bool.","title":"Constraints"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#requires-expression","text":"Requires expressions ar ethe most powerfull conctraints. The syntax is: requires(<parameter list>) { <requirements> } Here, the <parameter list> is a list of objects (instances of types) that we need to use in the <requirements> . There are four types of <requirements> that can appear in the requires expression: simple requiremnet : a requirement that can contain any expression. Evaluates to true if the expression is valid. cpp requires (T x) { x + x; }; type requirement : a requiremnt checking the validity of a type: cpp requires { typename T::inner; // required nested member name typename S<T>; // required class template specialization typename Ref<T>; // required alias template substitution }; compound requirement : Checks the arguments and the return type of some call. It has the form: {expression} -> return-type-requirement; cpp requires(T x) { {*x} -> std::convertible_to<typename T::inner>; } other useful type traits can be used instead of std::convertible_to . Nested requirement : a require expression inside another requires expression: cpp requires(T a, size_t n) { requires Same<T*, decltype(&a)>; // nested }","title":"Requires Expression"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#auto-filling-the-first-template-argument","text":"Concepts have a special feature that their first argument can be autoffiled from outer context. Consequentlly, you then fill only the remaining arguments. Examples: //When using the concept template<class T, class U> concept Derived = std::is_base_of<U, T>::value; template<Derived<Base> T> void f(T); // T is constrained by Derived<T, Base> // When defining the concept template<typename S> concept Stock = requires(S stock) { // return value is constrained by std::same_as<decltype(stock), double> {stock.get_value()} -> std::same_as<double>; }","title":"Auto filling the first template argument"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stl-concepts","text":"iterator concepts","title":"STL Concepts"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#usefull-patterns","text":"","title":"Usefull Patterns"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constrain-a-template-argument","text":"Imagine that you have a template function load and an abstract class Loadable_interface that works as an interface: class Loadable_interface{ virtual void load() = 0; }; template<class T> void load(T to_load){ ... to load.load() ... }; Typically you want to constraint the template argument T to the Loadable_interface type, so that other developer clearly see the interface requirement, and receives a clear error message if the requirement is not met. In Java, we have an extend keyword for this purpose that can constraint the template argument. In C++, this can be solved with concepts. First we have to define a concept that requires the interface: template<typename L> concept Loadable = std::is_base_of_v<Loadable_interface, L>; Than we can use the concept like this: template<Loadable T> void load(T to_load){ ... to load.load() ... };","title":"Constrain a Template Argument"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constraint-a-concept-argument","text":"Imagine that you have a concept Loadable that requires a method load to return a type T restricted by a concept Loadable_type . One would expect to write the loadable concept like this: template<typename L, Loadable_type LT> concept Loadable = requires(L loadable) { {loadable.load()} -> LT; }; However, this is not possible, as there is a rule that concept cannot not have associated constraints . The solution is to use an unrestricted template argument and constrain it inside the concept definition: template<typename L, typename LT> concept Loadable = Loadable_type<LT> && requires(L loadable) { {loadable.load()} -> LT; };","title":"Constraint a Concept Argument"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#sources","text":"https://en.cppreference.com/w/cpp/language/constraints Requires expression explained","title":"Sources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#interfaces","text":"In programming, an interface is usualy a set of requirements that restricts the function or template parameters, so that all types fulfiling the requiremnet can be used as arguments. Therte are two ways how to create an interface in C++: using the polymorphism using templates argument restriction While the polymorphism is easier to implement, the templating is more powerful and it has zero overhead. The most important thing is probably that despite these concepts can be used together in one application, not all \"combinations\" are allowed especialy when using tamplates and polymorphism in the same type. Note that in C++, polymorphism option work only for function argument restriction, but we cannot directly use it to constrain template arguments (unlike in Java). To demonstrate all possible options, imagine an interface that constraints a type that it must have the following two functions: int get_value(); void set_value(int date); The following sections we will demonstrate how to achieve this using multiple techniques.","title":"Interfaces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#interface-using-polymorfism","text":"Unlike in java, there are no interface types in C++. However, we can implement polymorfic interface using abstract class. The following class can be used as an interface: class Value_interface{ virtual int get_value() = 0; virtual void set_value(int date) = 0; } To use this interface as a fuction argument or return value, follow this example: std::unique_ptr<Value_interface> increment(std::unique_ptr<Value_interface> orig_value){ return orig_value->set_value(orig_value->get_value() + 1); } This system works in C++ because it supports multiple inheritance. Do not forget to use the virtual keyword, otherwise, the method cannot be overriden. Note that unlike in other languages, in C++, the polymorphism cannot be directly use as a template (generic) interface. Therefore, we cannot use the polymorfism alone to restrict a type.","title":"Interface using polymorfism"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-template-argument-restriction-as-an-interface","text":"To use template argument restriction as an interface, we can use concepts. The following concept impose the same requirements as the interface from the polymorphism section: To use template argument restriction as an interface, we can use concepts. The following concept impose the same requirements as the interface from the polymorphism section: template<class V> concept Value_interface = requires(V value_interface){{value_interface.get_value()} -> std::same_as<int>; } && requires(V value_interface, int value){{value_interface.set_value(value)} -> std::same_as<void>; } Remember that the return type of the function has to defined by a concept , the type cannot be used directly. Therefore, the following require statement is invalid: requires{(V value_interface){value_interface.get_value()} -> int; } To use this interface as an template argument in class use: template<Value_interface V> class ... And in function arguments and return types: template<Value_interface V> V increment(V orig_value){ return orig_value.set_value(orig_value.get_value() + 1);","title":"Using template argument restriction as an interface"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#restricting-the-member-function-to-be-const","text":"To restrict the member function to be const, we neet to make the type value const in the requires expression: template<class V> concept Value_interface = requires{(const V value_interface) {valvalue_interfaceue.get_value() -> std::same_as<int>;}; };","title":"Restricting the member function to be const"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-concepts-and-polymorphism-together-to-restrict-template-parameters-with-abstract-class","text":"We cannot restrict template parameters by polymorphic interface directly, however, we can combine it with concept. The folowing concept can be used together with the interface from the polymorphic interface section: template<class V> concept Value_interface_concept = requires std::is_base_of<Value_interface,V> Neverthless, as much as this combination can seem to be clear and elegent, it brings some problems. . We can use concepts to imposed many interfaces on a single type, but with this solution, it can lead to a polymorphic hell. While there is no problem with two concepts that directly requires the same method to be present with abstract classes, this can be problematic. Moreover, we will lose the zero overhead advantage of the concepts, as the polymorphism will be used to implement the interface.","title":"Using concepts and polymorphism together to restrict template parameters with abstract class"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#the-conflict-between-templates-and-polymorphism","text":"As described above, messing with polymorphism and templates together can be tricky. Some examples:","title":"The Conflict Between Templates and Polymorphism"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#no-virtual-member-function-with-template-parameters","text":"An example: a virtual (abstract) function cannot be a template function ( member template function cannot be virtual), so it cannot use template parameters outside of those defined by the class template.","title":"No Virtual Member Function with Template Parameters"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#polymorphism-cannot-be-used-inside-template-params","text":"If the functin accepts MyContainer<Animal> we cannot call it with MyContainer<Cat> , even if Cat is an instance of Animal.","title":"Polymorphism cannot be used inside template params"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#possible-solutions-for-conflicts","text":"do not use templates -> more complicated polymorphism ( type erasure for members/containers) do not use polymorphism -> use templates for interfaces an adapter can be used","title":"Possible solutions for conflicts"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#polymorphic-members-and-containers","text":"When we need to store various object in the same member or container, we can use both templates and polymorphism. However, both techniques has its limits, summarized in the table below: | | Polymorphism | Templates | | -- | -- | -- | | The concrete type has to be known at compile time | No | Yes | For multiple member initializations, the member can contain any element. | No , the elements have to share base class. | Yes | | For a single initialization, the containar can contain multiple types of objects | Yes , if they have the same base class | No | We can work with value members | No | Yes | When using the interface, we need to use downcasting and upcasting | Yes | No","title":"Polymorphic members and containers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deciding-between-template-and-polymorphism","text":"Frequently, we need some entity(class, function) to accept multiple objects through some interface. We have to decide, whether we use templates, or polymorphism for that interface. Some decision points: We need to return the same type we enter to the class/function -> use templates We have to access the interface (from outside) without knowing the exact type -> use polymorphism We need to restrict the member/parametr type in the child -> use templates for the template parameter if you need to fix the relation between method parameters/members or template arguments of thouse, you need to use templates If there are space considerations, be aware that every parent class adds an 8 byte pointer to the atribute table In general, the polymorphic interface have the following adventages: easy to implement easy to undestand similar to what people know from other languages On the other hand, the interface using concepts has the following adventages: no need for type cast all types check on compile time -> no runtime errors zero overhead no object slicing -> you don't have to use pointers when working with this kind of interface we can save memory because we don't need the vtable pointers","title":"Deciding between template and polymorphism"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#iterators-stl-algorithms-and-ranges","text":"If we want to iterate over elements in some programming language, we need to fulfill some interface. In Java, this interface is called Iterable . Also, there is usually some interface that formalize the underlying work, in Java, for example, it is called Iterator . In C++, however, the interface for iteration is not handled by polymorphism. Instead, it is handled using type traits and concepts. On top of that, there are multiple interfaces for iteration: legacy iteration, e.g., for (auto it = v.begin(); it != v.end(); ++it) STL algorithms, e.g., std::find(v.begin(), v.end(), 42) STL range algorithms, e.g., std::ranges::find(v, 42) STL range views, e.g., std::ranges::views::filter(v, [](int x){return x > 0;}) The following table summarizes the differences between the interfaces: |---| Plain iteration | STL algorithms | STL range algorithms | STL range views | |---|---|---|---|---| | Interface | type traits | type traits | concepts | concepts | | Iteration | eager | eager | eager | lazy | | Modify the underlying range | no | yes | yes | no | | Can work on temporaries * | yes | yes | yes | no | *If the operation modifies the data, i.e., sorting, shuffling, transforming, etc. The examples below demonstrate the differences between the interfaces on the following task: create a vector of 10 elements with values 0,1,2,...,9, i.e., the same as Python range(10) . // plain iteration std::vector<int> vec(10); int i = 0; for (auto it = vec.begin(); it != vec.end(); ++it) { *it = i; ++i; } // legacy algorithm std::vector<int> vec(10); std::iota(vec.begin(), vec.end(), 0); // C++11 way, legacy interface using type traits // range algorithm std::vector<int> vec(10); std::ranges::iota(vec.begin(), vec.end(), 0); // basically the same, but the constructor arguments are constrained with concepts // same using adaptor auto range = std::views::iota(0, 10); std::vector vec{range.begin(), range.end()}; // in-place vector construction","title":"Iterators, STL algorithms, and ranges"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#terminology","text":"range : the object we iterate over (Iterable in Java) iterator : the object which does the real work (Iterator in Java) Usually, a range is composed of two iterators: begin : points to the beginning of the range, returned by <range_object>.begin() end : points to the end of the object, returned by <range_object>.end() Each iterator implements the dereference ( * ) operator that acces the element of the range the iterator is pointing to. Depending on the iterator type, the iterator also supports other operations: ++ , -- to iterate along the range, array index operator ( [] ) for random access, etc. Most of the STL collections (vector, set,...) are also ranges.","title":"Terminology"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#how-to-choose-the-correct-interface","text":"when deciding which interface to use, we can use the following rules: If the number of tasks and the complexity of the tasks is high, use the legacy iteration . It is hard to write a 20 line for loop with various function calls as algorithm or adaptor and the result would be hard to read. Otherwise, if you need to preserve the original range as it is or you need to compose multiple operations, use the STL range adaptors . Otherwise, use the STL range algorithms . Note that the in this guide, we do not consider the legacy STL algorithms. With the availability of the STL range algorithms, there is no reason to use the legacy algorithms, except for the backward compatibility or for the algorithms that are not yet implemented in the STL. Also note that some STL algorithms are principially non-modifying, e.g., std::ranges::find or std::ranges::count . These algorithms logically do not have the adaptor equivalent.","title":"How to choose the correct interface?"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stl-ranges-and-views","text":"https://en.cppreference.com/w/cpp/ranges In C++ 20 there is a new range library that provides functional operations for iterators. It is similar to functional addon in Java 8. As explained in the beginning of this chapter, there are two ways how to use the STL ranges: using the range algorithms ( ranges::<alg name> ) that are invoked eagerly. using the range views ( ranges::views::<view name> ) that are invoked lazily. Note that the range algorithms and adaptors cannot produce result without an input, i.e., we always need a range or collection on which we want to apply our algorithm/view.","title":"STL ranges and views"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stl-range-views","text":"The difference of range view to range algorithms is that the views are lazy, i.e., they do not produce any result until they are iterated over. This is similar to the Python generators. The advantage is that we can chain multiple views together and the result is computed only when we iterate over the final view. Note that due to the lazy nature of the views, the underlying range has to be alive during the whole iteration . Therefore, we cannot use the views on temporaries, e.g., we cannot useviews directly in the constructor of a vector, or we cannot use the views on a temporary range returned by a function. A custom view can be created so that it can be chained with STL views. However, it has to satisfy the view concept , and more importantly, it should satisfy the view semantic, i.e., it should be cheap to copy and move (without copying the underlying data).","title":"STL range views"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#usefull-views","text":"std::views::iota : generates a sequence of numbers std::views::filter : filters the elements of the range","title":"Usefull views"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#projections","text":"Unlike in Java, we cannot refer to member functions when lambda functions are required. However, we can use these member functions when the algorithm or adaptor has a projection parameter. Example: struct Data { int a; std::string b; ... }; std::vector<Data> data = get_data(); std::sort(data, {}, &Data::b); The trick here is that we can only provide the member that we want to use for sorting, but the sorting logic ( first < second ...) is handeled by a standard comparator (the second argument initialized with {} ). We can have even more complicated projections that are not just references to member functions: std::vector<My_class> objects = get_objects(); std::vector<unsigned> indexes = get_indexes(objects); auto proj = [&objects](std::size_t i) -> const std::string& { return objects[i].get_name(); }; std::ranges::sort(indexes, {}, proj) // sort indexes using the property of objects","title":"Projections"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#useful-range-algorithms","text":"Note that the most frequently used algorithms have a separate section in the Iterators chapter. std::shuffle : shuffles the elements in the range (formerly std::random_shuffle ). std::adjacent_find : finds the first two adjacent elements that are equal. Can be used to find duplicates if the range is sorted. std::ranges::unique : moves the duplicates to the end of the range and returns the iterator to the first duplicate. Only consecutive duplicates are found. std::ranges::min : finds the smallest element in the range. We can use either natural sorting, or a comparator, or a projection. If the range is empty, the behavior is undefined. std::ranges::min_element : finds the smallest element in the range. Unlike std::ranges::min , this function returns an iterator to the smallest element. std::ranges::empty : checks whether the range is empty.","title":"Useful range algorithms"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#other-resources","text":"https://www.modernescpp.com/index.php/c-20-the-ranges-library","title":"Other Resources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#boost-ranges","text":"In addition to the STL range algorithms and adaptors, boost has it's own range library with other more complex algorithms and adaptors.","title":"Boost ranges"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#boost-range-requirements","text":"Sometimes, it is hard to say why a type does not satisfy some of the requirements for boos ranges. Fortunatelly, the boost provides concepts for checking whether a type satisfy each specific range model. Example: BOOST_CONCEPT_ASSERT(( boost::SinglePassRangeConcept<std::vector<int>> )); // true Also, it is necessary to check whether the value of the iterator can be accessed: BOOST_CONCEPT_ASSERT(( boost_concepts::ReadableIteratorConcept< typename boost::range_iterator<std::vector<int>>::type > )); // true Most likely, the compiler will complain that boost::range_iterator<R>::type does not exist for your range R . The boost range library generate this type by a macro from the R::iterator type. Therefore, make sure that your range has an iterator type defined, either as: a type alias to an existing iterator an iterator nested class Note that <RANGE CLASS>::iterator and <RANGE CLASS>::const_iterator has to be accessible (public).","title":"Boost range requirements"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#sequences","text":"The iota algortihm/adapter is used to create a sequence: auto range = std::views::iota(0, 10); auto vec = std::vector(range.begin(), range.end()); Note that we cannot pass the view directly to the vector, as the vector does not have a range constructor.","title":"Sequences"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#zip","text":"The classical Python like zip iteration is available using the zip adapator , which is not yet supported in MSVC. However, boost provides a similar functionality boost::combine .","title":"Zip"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#boostcombine","text":"boost::combine example: std::vector<int> va{1, 2, 3}; std::vectro<float> vb{0.5, 1, 1.5}; for(const auto& [a, b]: boost::combine(va, vb)){ ... } Each argument of combine must satisfy boost::SinglePassRange","title":"boost::combine"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#enumerating","text":"There is no function in standard library equivalent to the python enumerate. We can use a similar boost solution: #include <boost/range/adaptor/indexed.hpp> for(auto const& el: <range> | boost::adaptors::indexed(0)){ std::cout << el.index() << \": \" << el.value() << std::endl; } However, inside the loop, we have to call the index and value functions, so it is probably easier to stick to the good old extra variable: size_t i = 0; for(auto const& el: <range>) { std::cout << i << \": \" << el << std::endl; ++i; }","title":"Enumerating"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#sorting","text":"There is no sorted view or something simmiler, so in order to sort a range, we need to: really sort the object in the range create an adaptor/view from the range, and then sort the view There are two functions for sorting in the STL algorithm library: std::sort : old supports parallelization directly by the policy param std::ranges::sort : new supports comparison using projections There are three types of sorting: natural sorting using the < operator of T : std::sort(<RANGE<T>>) sorting using a comparator: std::sort(<RANGE>, <COMPARATOR>) , where comparator is a fuction with parameters and return value analogous to the natural sorting operator. sorting using projection (only availeble in std::ranges::sort ): std::ranges::sort(<RANGE>, <STANDARD GENERIC COMPARATOR>, <PROJECTION>","title":"Sorting"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#sorting-using-projection","text":"When we want to sort the objects by a single property different then natural sorting, the easiest way is to use projection. Example: struct Data { int a; std::string b; ... }; std::vector<Data> data = get_data(); std::sort(data, {}, &Data::b); The trick here is that we can only provide the member that we want to use for sorting, but the sorting logic ( first < second ...) is handeled by a standard comparator (the second argument initialized with {} ). We can have even more complicated projections: std::vector<My_class> objects = get_objects(); std::vector<unsigned> indexes = get_indexes(objects); auto proj = [&objects](std::size_t i) -> const std::string& { return objects[i].get_name(); }; std::ranges::sort(indexes, {}, proj) // sort indexes using the property of objects","title":"Sorting using projection"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#transformation","text":"Transformation alg/views transforms an input range according to a callable. As with other operation, there are thre options: classical algorithm: std::transform with a direct paralellization using the policy parameter range algorithm: std::ranges::transform with a support for projections range view: std::ranges::views::transform - a lazy variant The algorithms (but not the view) also supports binary transformations , i.e., create an output range using two input ranges. Transform view example: std::vector<int> in(3, 0); // [0, 0, 0] auto ad = std::ranges::transform_view(in, [](const auto in){return in + 1;}); std::vector<int> out(ad.begin(), ad.end()); The transform view can be only constructed from an object satisfying ranges::input_range . If we want to use a general range (e.g., vector), we need to call the addapter, which has a same signature like the view constructor itself. The important thing here is that the adapter return type is not a std::ranges::views::transform<<RANGE>> but std::ranges::views::transform<std::ranges::ref_view<RANGE>>> ( std::ranges::ref_view ). Supporting various collections is therefore possible only with teplates, but not with inheritance. Note that unlike in Java, it is not possible to use a member reference as a transformation function (e.g.: &MyClass::to_sting() ). We have to always use lambda functions, std::bind or similar to create the callable.","title":"Transformation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#aggregating-sum-product-etc","text":"These operations can be done using the std::accumulate algorithm. This algorithm is about to be replaced by the std::ranges::fold algorithm, but it is not yet implemented in Clang. Examples: // default accumulation -> sum std::vector<int> vec{1, 2, 3, 4, 5}; int sum = std::accumulate(vec.begin(), vec.end(), 0); // product int product = std::accumulate(vec.begin(), vec.end(), 1, std::multiplies<int>());","title":"Aggregating (sum, product, etc.)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implementing-a-custom-range","text":"There are different requirements for different types of ranges. Moreover, there are different requirements for the range-based for loop (for each) , or the legacy STL algorithms. Here we focus on requirements for ranges. Not however, that the range requirements are more strict than the requirements for the range-based for loop or the legacy STL algorithms. Therefore, the described approach should work for all three cases. Usually, we proceed as follows: Choose the right range (Iterable) concept for your range from the STL range concepts . The most common is the std::ranges::input_range concept. Implement the range concept for the range. Either, we can do it by using the interface of the undelying range we usein our class (i.e, we just forward the calls to the methods of std::vector or std::unordered_map ) or implement the interface from scratch. For that, we also need to implement the iterator class that fulfills the corresponding iterator concept (e.g., std::input_iterator for the std::ranges::input_range ).","title":"Implementing a custom range"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implementing-an-input-range","text":"The input range is the most common range type. The only requirement for the input range is that it has to have the begin and end methods that return the input iterator. Example: class My_range { private: std::vector<int> data; public: My_range(std::vector<int> data): data(data) {} auto begin() {return data.begin();} auto end() {return data.end();} // usually, we also want a const version of the range auto begin() const {return data.begin();} auto end() const {return data.end();} };","title":"Implementing an input range"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#boost-iterator-templates","text":"The boost.iterator library provides some templates to implement iteratores easily, typically using some existing iterators and modifying just a small part of it: for pointer to type (dereference) iterator, you can use boost indirect iterator zip iterator for Python like iteration over multiple collections [transform iteratorather useful iterators are also included in the boost.iterator library for using another iterator and just modify the access ( * ) opindex.html). including: zip iterator. counting_iterator to create number sequence like Python range gentransform iterator There are also two general (most powerfull) classes: iterator adapter iterator facade","title":"Boost Iterator Templates"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#resources","text":"How to write a legacy iterator iter_value_t","title":"Resources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#lambda-functions","text":"In c++ lambda functions are defined as: [<capture>](<params>) -> <return_type> { <code> } The rerurn type is optional, but sometimes required (see below). Since C++23, the parantheses are optional if there are no functon parameters.","title":"Lambda Functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#captures","text":"Anything that we want to use from outside has to appear in capture. To prevent copying, we should capture by reference, using & before the name of the variable. [&var_1] // capture by reference [var_1] // capture by value [&] // default capture by reference For the detailed explanation of the captures, see cppreference .","title":"Captures"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#return-type","text":"The return type of lambda functions can be set only using the trailing return type syntax ( -> <RETURN TYPE> after the function params). The return type can be omited. Note however, that the default return type is auto , so in case we want to return by reference, we need to add at least -> auto , or even a more specific return type.","title":"Return type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#specifiers","text":"Lambda functions can have special specifiers: mutable : lambda can modify function parameters capture by copy","title":"Specifiers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#exceptions","text":"In C++, exceptions works simillarly as in other languages. Standard runtime error can be thrown using the std::runtime_error class: throw std::runtime_error(\"message\"); Always catch exception by reference! Note that unlike in Java or Python, there is no default exception handler in C++. Therefore, if an exception is not caught and, in conclusion, the program is terminated, there is no useful information about the exception in the standard output. Instead, we only receive the exit code. For this reason, it is a good practice to catch all exceptions in the main function and print the error message. Example: int main() { try { <the code of the whole program here> } catch(...) { const std::exception_ptr& eptr = std::current_exception(); if (!eptr) { throw std::bad_exception(); } /*char* message;*/ std::string message; try { std::rethrow_exception(eptr); } catch (const std::exception& e) { message = e.what(); } catch (const std::string& e) { message = e; } catch (const char* e) { message = e; } catch(const GRBException& ex) { message = fmt::format(\"{}: {}\", ex.getErrorCode(), ex.getMessage()); } catch (...) { message = \"Unknown error\"; } spdlog::error(message); return message; } }","title":"Exceptions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#rethrowing-exceptions","text":"We can rethrow an exception like this: catch(const std::exception& ex){ // do ssomething ... throw; } Note that in parallel regions, the exception have to be caught before the end of the parallel region , otherwise the thread is killed.","title":"Rethrowing Exceptions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#how-to-catch-any-exception","text":"In C++, we can catch any exception with: catch (...) { } However, this way, we cannot access the exception object. As there is no base class for exceptions in C++, there is no way to catch all kind of exception objects in C++.","title":"How to Catch Any Exception"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#noexcept-specification","text":"A lot of templates in C++ requires functions to be noexcept which is usually checked by a type trait std::is_nothrow_invocable . We can easily modify our function to satisfy this by adding a noexcept to the function declariaton. There are no requirements for a noexcept function. It can call functions without noexcept or even throw exceptions itself. The only difference it that uncought exceptions from a noexcept function are not passed to the caller. Instead the program is terminated by calling std::terminate , which otherwise happens only if the main function throws. By default, only constructors, destructors, and copy/move operations are noexcept.","title":"noexcept specification"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stack-traces","text":"Unlike most other languages, C++ does not print stack trace on program termination. The only way to get a stack trace for all exceptions is to set up a custom terminate handler an inside it, print the stack trace. However, as of 2023, all the stack trace printing/generating libraries requires platform dependent configuration and fails to work in some platforms or configurations. Example: void terminate_handler_with_stacktrace() { try { <stack trace generation here>; } catch (...) {} std::abort(); } std::set_terminate(&terminate_handler_with_stacktrace); To create the stacktrace, we can use one of the stacktrace libraries: stacktrace header from the standard library if the compiler supports it (C++ 23) as of 2024-04, only MSVC supports this functionality cpptrace boost stacktrace","title":"Stack traces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#logging","text":"There is no build in logging in C++. However, there are some libraries that can be used for logging. In this section we will present logging using the spdlog library. We can log using the spdlog::<LEVEL> functions: spdlog::info(\"Hello, {}!\", \"World\"); By default, the log is written to console. In order to write also to a file, we need to create loggers manually and set the list of sinks as a default logger: const auto console_sink = std::make_shared<spdlog::sinks::stdout_sink_st>(); console_sink->set_level(spdlog::level::info); // log level for console sink auto file_sink = std::make_shared<spdlog::sinks::basic_file_sink_st>(<log filepath>, true); std::initializer_list<spdlog::sink_ptr> sink_list{console_sink, file_sink}; const auto logger = std::make_shared<spdlog::logger>(<LOGGER NAME>, sink_list); logger->set_level(spdlog::level::debug); //log level for the whole logger spdlog::set_default_logger(logger); To save performance in case of an intensive logging, we can set an extended flushing period: spdlog::flush_every(std::chrono::seconds(5));","title":"Logging"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#levels","text":"The log levels are defined in the spdlog::level::level_enum . The levels are: trace debug info warn error critical","title":"Levels"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#colors","text":"By default, the logger uses colors for different log levels. However, this capability is lost when: using custom sinks or using custom formatters To keep the colors, we need to a) use the color sink and b) explicitly set the usage of the color in the formatter: auto console_sink = std::make_shared<spdlog::sinks::stdout_color_sink_mt>(); auto logger = std::make_shared<spdlog::logger>(\"console\", console_sink); logger->set_pattern(\"[%^%l%$] %v\"); Here %^ and %$ are the color start and end markers.","title":"Colors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#type-aliases","text":"Type aliases are short names bound to some other types. We can introduce it either with typedef or with using keyword. Examples (equvalent): typedef int number; using number = int; typedef void func(int,int); using func = void(int, int) The using new syntax is more readable, as the alias is at the begining of the expression. But why to use type aliases? Two strong motivations can be: iImprove the readebility : When we work with a type with a very long declaration, it is wise to use an alias. We can partialy solve this issue by using auto, but that is not a complete solution Make the refactoring easier : When w work with aliases, it is easy to change the type we work with, just by redefining the alias. Note that type aliases cannot have the same name as variables in the same scope . So it is usually safer to name type aliases with this in mind, i.e., using id_type = .. insted of using id = ..","title":"Type Aliases"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-aliasis","text":"We can also create template aliases as follows: template<class A, typename B> class some_template{ ... }; template<class T> using my_template_alias = some_template<T, int>;","title":"Template Aliasis"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#aliases-inside-classes","text":"The type alias can also be placed inside a class. From outside the class, it can be accessed as <CLASS NAME>::<ALIAS NAME> : class My_class{ public: using number = unsigned long long number n = 0; } My_class::number number = 5;","title":"Aliases inside classes"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constant-expressions","text":"cppreference . A constant expression is an expression that can be evaluated at compile time. The result of constant expression can be used in static context, i.e., it can be: assigned to a constexpr variable, tested for true using static_assert Unfortunatelly, there is no universal way how to determine if an expression is a constant expression .","title":"Constant Expressions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#compile-time-branching","text":"For compile time branching, we can use the if constexpr : template<class T, class C> class Object{ public: bool process(T value, C config){ if constexpr (std::is_same_v<T, std::string>){ return process_string(value, config); } else { return process_value(value, config); } } }; Note that here, the if constexpr requires the corresponding else branch. Otherwise, the code cannot be discarded during the compilation. Example: template<class T, class C> class Object{ public: bool process(T value, C config){ if constexpr (std::is_same_v<T, std::string>){ return process_string(value, config); } return process_value(value, config); // this compiles even if T is std::string } };","title":"Compile Time Branching"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#regular-expressions","text":"The regex patern is stored in a std::regex object: const std::regex regex{R\"regex(Plan (\\d+))regex\"}; Note that we use the raw string so we do not have to escape the pattern. Also, note that std::regex cannot be constexpr","title":"Regular expressions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#matching-the-result","text":"We use the std::regex_search to search for the occurence of the pattern in a string. The result is stored in a std::smatch object which contains the whole match on the 0th index and then the macthed groups on subsequent indices. A typical operation: std::smatch matches; const auto found = std::regex_search(string, matches, regex); if(found){ auto plan_id = matches[1].str(); // finds the first group } Note that matches[0] is not the first matched group, but the whole match.","title":"Matching the result"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#namespaces","text":"cppreference Namespace provides duplicit-name protection, it is a similar concept to Java packages. Contrary to java packages and modules, the C++ namespaces are unrelated to the directory structure. namespace my_namespace { ... } The namespaces are used in both declaration and definition (both in header and source files). The inner namespace has access to outer namespaces. For using some namespace inside our namespace without full qualification, we can write: using namespace <NAMESPACE NAME>","title":"Namespaces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#anonymous-namespaces","text":"Anonymous namespaces are declared as: namespace { ... } Each anonnymous namespaces has a different and unknown ID. Therefore, the content of the annonymous namespace cannot be accessed from outside the namespace, with exception of the file where the namespace is declared which has an implicit access to it.","title":"Anonymous namespaces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#namespace-aliases","text":"We can create a namespace alias using the namespace keyword to short the nested namespace names. Typicall example: namespace fs = std::filesystem;","title":"Namespace aliases"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#decltype-determining-type-from-expressions","text":"Sometimes, it is usefull to declare a type from expression, instead of do it manualy. Using decltype specifier, we can get the resulting type of an expression as if it was evaluated. Examples: struct A { double x; }; const A* a; decltype(a->x) // evaluates to double decltype(std::accumulate(a, [](double sum, double val){return sum + val;})) // evalutes to double We can use the decltype in any context where type is required. Examples: int i = 1 decltype(i) j = 3 std::vector<decltype(j)> numbers;","title":"decltype: Determining Type from Expressions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#the-value-category-of-decltype","text":"The value category of decltype is resolved depending on the value category of an expression inside it: deltype(<XVALUE>) -> T&& deltype(<LVALUE>) -> T& deltype(<RVALUE>) -> T The rvalue conversion can lead to unexpected results, in context, where the value type matters: static_assert(std::is_same_v<decltype(0), decltype(std::identity()(0))>); // error The above expressions fails because: decltype(0) , 0 is an rvalue -> the decltype result is int decltype(std::identity()(0)) result of std::identity() is an xvalue -> the decltype result is int&& . Determining Type from Expressions Sometimes, it is usefull to declare a type from expression, instead of do it manualy. Using decltype specifier, we can get the resulting type of an expression as if it was evaluated. Examples: struct A { double x; }; const A* a; decltype(a->x) // evaluates to double decltype(std::accumulate(a, [](double sum, double val){return sum + val;})) // evalutes to double We can use the decltype in any context where type is required. Examples: int i = 1 decltype(i) j = 3 std::vector<decltype(j)> numbers;","title":"The Value Category of decltype"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#determining-the-return-value-type-of-a-function","text":"As we can see above, we can use decltype to determine the return value type. But also, there is a type trait for that: std::invoke_result_t (formerly std::result_of ). The std::invoke_result_t should vbe equal to decltype when aplied to return type, with the following limitations:","title":"Determining the Return Value Type of a Function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#-we-cannot-use-abstract-classes-as-arguments-of-stdinvoke_result_t-while-we-can-use-them-inside-decltype-using-stddeclval-see-below","text":"","title":"- we cannot use abstract classes as arguments of std::invoke_result_t, while we can use them inside decltype (using std::declval, see below)."},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#construct-object-inside-decltype-with-stddeclval","text":"std::declval is a usefull function designed to be used only in static contexts, inside decltype . It enables using member functions inside decltype without using constructors. Without std::declval , some type expressions are hard or even impossible to costruct. Example: class Complex_class{ Complex_class(int a, bool b, ...) ... int compute() } // without declval decltype<Complex_class(1, false, ...).compute()> // using declval decltype(std::declval<Complex_class>().compute())","title":"Construct object inside decltype with std::declval"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#decltype-and-overloading","text":"in static context, there is no overloading, the vtable is not available. Therefore, we have to hint the compiler which specific overloaded function we want to evaluate. This also applies to const vs non const overloading. The following example shows how to get the const iterator type of a vector: std::vector<anything> vec // non const iter decltype(vec.begin()) // const iter decltype<std::declval<const decltype(vec)>().begin()> Another example shows how to use the const overload inside std::bind : decltype(std::bind(static_cast<const ActionData<N>&(std::vector<ActionData<N>>::*)(size_t) const>(&std::vector<ActionData<N>>::operator[]), action_data)), Above, we used static cast for choosing the const version of the vector array operator. Instead, we can use explicit template argument for std::bind : decltype(std::bind<const ActionData<N>& (std::vector<ActionData<N>>::*)(size_t) const>(&std::vector<ActionData<N>>::operator[], action_data)),","title":"decltype and Overloading"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#parallelization","text":"While there wa no support of parallelization i earlier versions of C++ , now there are many tools.","title":"Parallelization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#standard-threads","text":"","title":"Standard Threads"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#for-each-with-parallel-execution-policy","text":"The function std::for_each can be run with a parallel execution policy to process the loop in parallel.","title":"For-each with Parallel Execution Policy"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#async-tasks","text":"Tasks for asznchronous execution, like file downloads, db queries, etc. The main function is std::async .","title":"Async tasks"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#open-mp","text":"In MSVC, the Open MP library is automatically included and linked. In GCC, we need to find the libs in CmakeLists.txt : find_package(OpenMP REQUIRED)","title":"Open-MP"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#standard-templates-for-callables","text":"","title":"Standard Templates for Callables"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-stdinvoke-to-call-the-member-function","text":"using std::invoke , the cal syntax bool b = (inst.*ptr)() can be replaced with longer but more straighforward call: bool b = std::invoke(ptr, inst, 2)","title":"Using std::invoke to call the member function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-stdmem_fn-to-store-a-pointer-to-member-function-in-a-callable","text":"With std::mem_fn , we can store the pointer to a member function in a callable object. Later, we can call the object without the pointer to the member function. Example: auto mem_ptr = std::mem_fn(&My_class::my_method) bool b = mem_ptr(inst, 2)","title":"Using std::mem_fn to Store a Pointer to Member Function in a Callable"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-a-pointer-to-member-function-as-a-functor","text":"A normal function can be usually send instead of functor, as it can be invoked in the same way. However, in case of member function, we usually need to somehow bind the function pointer to the instance. We can use the std::bind function exactly for that: auto functor = std::bind(&My_class::my_method, inst); bool b = functor(2) Advanteges: we do not need an access to instance in the context from which we call the member function we do not have to remember the complex syntax of a pointer to a member function declaration we receive a callable object, which usage is even simpler than using std::invoke Note that in case we want to bind only some parameters, we need to supply placeholders for the remaining parameters ( std::placeholders ).","title":"Using a Pointer to Member Function as a Functor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-lambdas-instead-of-stdbind","text":"For more readable code and better compile error messages, it is usefull to replace std::bind callls with labda functions. The above example can be rewritten as: auto functor = [inst](int num){return inst.my_method(num);); bool b = functor(2)","title":"Using Lambdas Instead of std::bind"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#store-the-result-of-stdbind","text":"Sometimes, we need to know the return type of the std::bind . In many context, we need to provide the type instead of using auto . But luckily, there is a type exactly for that: std::function . Example: std::function<bool(int)> functor = std::bind(&My_class::my_method, inst); bool b = functor(2) A lambda can also be stored to std::function . But be carefull to add an explicit return type to it, if it returns by a reference. Example: My_class{ public: int my_member } My_class inst; std::function f = [inst](){return &inst.my_member; } // wrong, reference to a temporary due to return type deduction std::function f = [inst]() -> const int& {return &inst.my_member; } // correct More detailed information about pointers to member functions","title":"Store the Result of std::bind"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdmem_fn-and-data-members","text":"Data member pointers can be aslo stored as std::mem_fn . A call to this object with an instance as the only argument then return the data member value. The plain syntax is <type> <class name>.*<pointer name> = <class name>.<member name> , and the pointer is then accessed as <instance>.*<pointer name> . Example: int Car::*pSpeed = &Car::speed; c1.*pSpeed = 2; Usefull STL functions std::for_each : iterates over iterable objects and call a callable for each iteration std::bind : Binds a function call to a variable that can be called some parameters of the function can be fixed in the variable, while others can be provided for each call each reference parameter has to be wrapped as a reference_wrapper std:mem_fn : Creates a variable that represents a callable that calls member function","title":"std::mem_fn and Data Members"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdfunction","text":"The std::function template can hold any callable. It can be initialized from: function pointer/reference, member function pointer/reference, lambda function functor It can be easily passed to functions, used as template parameter, etc. The template parameters for std::function has the form of std::function<<RETURN TYPE>(<ARGUMENTS>)> . Example: auto lambda = [](std::size_t i) -> My_class { return My_class(i); }; std::function<My_class(std::size_t)> f{lambda}","title":"std::function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdfunction-and-overloading","text":"one of the traps when using std::function is the ambiguity when using an overloade function: int add(int, int); double add(double, double); std::function<int(int, int)> func = add; // fails due to ambiguity. The solution is to cant the function to its type first and then assign it to the template: std::function<int(int, int)> func = static_cast<int(*)(int, int)>add;","title":"std::function and overloading"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#preprocessor-directives","text":"The C language has a preprocessor that uses a specific syntax to modify the code before the compilation. This preprocessor is also used in C++. The most used tasks are: including files ( #include ): equivalent to Java or Python import statement conditional compilation based on OS, compiler, or other conditions Also, preprocessor had some other purposes, now replaced by other tools: defining constants ( #define ): replaced by const and constexpr A simple constant can be defined as: #define PI 3.14159 . The variable can be used in the code as PI . metaprogramming: replaced by templates","title":"Preprocessor Directives"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#include","text":"cppreference There are two types of include directives. For both types, the behavior is implementation dependent. However, the most common behavior is: #include <file> : the file is searched in the system directories #include \"file\" : the file is searched relative to the current file","title":"Include"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#conditional-include","text":"Sometimes, we need a conditional include based on what is available in the system. We can use two mechanisms: __has_include(<file>) : Basically, we test the header availability during compilation: cpp #if __has_include(<format>) #include <format> using namespace format = std::format; #else #include <fmt/format.h> using namespace format = fmt; #endif only available since C++17 predefined compiler variable: We define some variable during project configuration and then use it in the preprocessor control structure: cmake set(USE_FMT ON) cpp #if USE_FMT #include <fmt/format.h> using namespace format = fmt; #else #include <format> using namespace format = std::format; #endif never use this for public headers , it is not portable as each client project now has to set the variable in the cmake configuration","title":"Conditional include"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#control-structures_1","text":"#ifdef <MACRO> ... #elif <MACRO> ... #else ... #endif Instead of #ifdef <MACRO> , we can use #if defined(<MACRO>) . In this case, we can use multiple conditions in one #if directive: #if defined(MACRO_1) && defined(MACRO_2) ... #endif","title":"Control structures"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#predefined-macros-for-detecting-compiler-os-etc","text":"To detect the Operating system , use: #ifdef _WIN32 for Windows #ifdef __linux__ for Linux #ifdef unix for Unix-like systems (but not MacOS) #ifdef __APPLE__ for MacOS","title":"Predefined macros for detecting compiler, OS, etc."},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#debugging-preprocessor-directives","text":"Sometimes, it may be usefull to print the value of a macro, or show which branch of the #if directive was taken. This can be done using the #pragma message directive: #ifdef MACRO #pragma message(\"MACRO is defined\") #else #pragma message(\"MACRO is not defined\") #endif","title":"Debugging preprocessor directives"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#attributes","text":"cppreference Atrributes mechanism provides compilers with a standard way to extend the language with new features (as opose to preprocessor macros). Additionally, some standard attributes are listed in the C++ standard. The syntax is: [[ <attribute> ]] where <attribute> is the attribute name (potentially prefixed with the namespace of the attribute) optionally followed by arguments. Attributes may appear almost anywhere in the code, however, the usage of each attribute is typically restricted to a specific context. Standard attributes are: [[nodiscard]] : marks a function that the return value should not be ignored. If the return value is ignored, a warning is issued. [[maybe_unused]] : marks a variable that may be unused. This suppresses the unused variable warning in modern compilers.","title":"Attributes"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#comments","text":"The C++ standard does not specify the comment syntax. However, the most common comments are: // single-line comment /* multi-line comment */ Also, there are special comment blocks for documentation systems like Doxygen. Those, like in Java, are typically enclosed in /** and */ blocks. /** * This is a multi-line comment. * It can contain multiple lines. */","title":"Comments"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-comment-blocks","text":"For function comment blocks, there is a special syntax in Doxygen: /** * @brief This is a function comment block. * It can contain multiple lines. * @param param1 This is a parameter. * @param param2 This is a parameter. * @return This is a return value. */ Additionaly, we can refer to the parameters in the text using @p <param name> .","title":"Function Comment Blocks"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#resources_1","text":"In C++, there is no facility for resource management like in Java or Python. Instead, resources have to be loaded like standard files. Moreover, there is no built-in way how to determine the localtion of the running executable so that we can load the resources from the same directory. Typically, this has to be implemented for each platform separately: #include <iostream> #include <filesystem> #ifdef _WIN32 #include <windows.h> #else #include <unistd.h> #endif std::string get_executable_path() { char buffer[1024]; #ifdef _WIN32 GetModuleFileNameA(NULL, buffer, sizeof(buffer)); #else ssize_t count = readlink(\"/proc/self/exe\", buffer, sizeof(buffer)); if (count == -1) throw std::runtime_error(\"Failed to get executable path\"); buffer[count] = '\\0'; #endif return std::filesystem::path(buffer).parent_path().string(); }","title":"Resources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#testing-with-google-test","text":"","title":"Testing with Google Test"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#private-method-testing","text":"The testing of private method is not easy with Google Test, but that is common also for other tets frameworks or even computer languages (see the common manual). Some solutions are described in this SO question . Usually, the easiest solution is to aplly some naming/namespace convention and make the function accessible. For free functions: namespace internal { void private_function(){ ... } } For member functions: class MyClass{ public: void _private_function();","title":"Private method testing"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#memory-alignment","text":"Allignment FAQ Wikipedia SO question In most cases, a sane programmer does not have to worry about memory optimization beyond choosing the right data types. However, in high-performance applications, where millions of objects are processed, even some details regarding the memory layout can have a significant impact on the performance. Therefore, we introduce some terminology and tools for memory optimization. To make the memory access faster, the compiler aligns the data in the memory so that the memory can be read by the chunks natural for the architecture which are typically multiple of bytes. This affects both how the data structure members are stored ( padding ) and how the data structure itself is stored ( alignment ). First, let's look at how much a data structure takes in memory. The data structure memory usage consists of: the size of its members, including base classes the size of the padding between the members if the strucrure use virtual methods, the size of the vtable Apart of the above, a structure itself may take even more memory due to the alignment . Finally, there are strategies how to optimize the structure size called packing . See the packing section on the Eric S. Raymond's website for more details.","title":"Memory Alignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#padding","text":"The padding is added by the compiler so that the members are aligned. Typically, the members are aligned to their natural size. This means that a boolean variable, which is 1 byte, is aligned to 1 byte, while a 4-byte integer is aligned to 4 bytes. Note that the compiler is not allowed to change the order of the members, and therefore, the order of the members affects the size of the structure in memory! Example: struct A{ // 4 bytes bool b; // 1 byte bool b2; // 1 byte short a; // 2 bytes }; struct B{ // 4 bytes short a; // 2 bytes bool b; // 1 byte bool b2; // 1 byte }; struct C{ // 5 bytes bool b; // 1 byte + 1 byte padding so that the short is aligned short a; // 2 bytes bool b2; // 1 byte The natural alignment apply to the basic types. The composed types (e.g., structs, classes, unions) are typically aligned to the largest member.","title":"Padding"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#structure-alignment-or-final-padding","text":"Appart from padding, each data structure is alligned to its largest member. If the data structure size (including padding) is not a multiple of its alignment, it effectively takes more memory so that it is aligned.","title":"Structure Alignment (or final padding)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#specific-tasks","text":"","title":"specific tasks"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#conditional-function-execution","text":"W know it from other languages: if the function can be run in two (or more) modes, there is a function parameter that controls the execution. Usually, most of the function is the same (otherwise, we eould create multiple fuctions), and the switch controls just a small part. Unlike in other langueges. C++ has not one, but three options how to implement this. They are described below in atable together with theai properties. function parameter template parameter compiler directive good readability yes no no compiler optimization no yes yes conditional code compilation no no yes","title":"Conditional Function Execution"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-parameter","text":"void(bool switch = true){ if(switch){ ... } else{ ... } }","title":"Function Parameter"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-parameter","text":"template<bool S = true> void(){ if(S){ ... } else{ ... } }","title":"Template Parameter"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#compiler-directive","text":"void(){ #ifdef SWITCH ... #else ... #endif }","title":"Compiler Directive"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#ignoring-warnings-for-specific-line-of-code","text":"Sometimes, we want to suppress some warnings, mostly in libraries we are including. The syntax is, unfortunatelly, different for each compiler. Example: #if defined(_MSC_VER) #pragma warning(push) #pragma warning(disable: <WARNING CODE>) #elif defined(__GNUC__) #pragma GCC diagnostic push #pragma GCC diagnostic ignored \"<WARNING TYPE GCC>\" #elif defined(__clang__) #pragma clang diagnostic push #pragma clang diagnostic ignored \"<WARNING TYPE CLANG>\" #endif .. affected code... #if defined(_MSC_VER) #pragma warning(pop) #elif defined(__GNUC__) #pragma GCC diagnostic pop #elif defined(__clang__) #pragma clang diagnostic pop #endif Here, the <WARNING CODE> is the code of the warning to be suppressed without the C prefix. Note that warnings related to the preprocessor macros cannot be suppressed this way in GCC due to a bug (fixed in GCC 13). The same is true for conditions: #if 0 #pragma sdhdhs // unknown pragma raises warning, despite unreachcable #endif","title":"Ignoring warnings for specific line of code"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#measuring-used-resource","text":"","title":"Measuring used resource"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#memory","text":"","title":"Memory"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#msvc","text":"In MSVC, we can measure the peak used memory using the following code: #include <psapi.h> PROCESS_MEMORY_COUNTERS pmc; K32GetProcessMemoryInfo(GetCurrentProcess(), &pmc, sizeof(pmc)); auto max_mem = pmc.PeakWorkingSetSize","title":"MSVC"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#working-with-tabular-data","text":"Potential libs similar to Python Pandas: Arrow Dataframe","title":"Working with tabular data"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#executing-external-commands","text":"The support for executing external commands in C++ is unsatisfactory. The most common solution is to use the system function. However, the system calls are not portable, e.g., the quotes around the command are not supported in Windows Another option is to use the Boost Process library.","title":"Executing external commands"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#command-line-interface","text":"For CLI, please follow the CLI manual . Here we focus on setting up the TCLAP library. TCLAP use","title":"Command Line Interface"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#jinja-like-templating","text":"For working with Jinja-like templates, we can use the Inja template engine.","title":"Jinja-like Templating"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#exceptions_1","text":"There are the following exceptions types: ParserError thrown on parse_template method RenderError thrown on write method","title":"Exceptions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#render-errors","text":"empty expression : this signalize that some expression is empty. Unfortunatelly, the line number is incorrect (it is always 1). Look for empty conditions, loops, etc. (e.g., {% if %} , {% for %} , {% else if %} ).","title":"Render Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#design-patterns","text":"This section describe how to implement various design patterns in C++.","title":"Design Patterns"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#visitor-pattern","text":"C++ has a much more clear and efficient way how to implement the visitor pattern than other languages like Java. Instead of classical polymorphism, we use: std::variant to list all possible types std::visit to implement a visitor The advantage is that the visitor use a procedural way to choose the appropriate function for the given type, which is much more efficient than the runtime dispatching of the classical visitor pattern. Also there is no circular dependency between the visitor and the visited classes.","title":"Visitor Pattern"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#compile-time-plugins-using-static-registration","text":"Sometimes, we want to enable extension of the functionality of some executable at compile time. The problem is: how we can call the extended code from main function, if we do not know it? The answer is to use the static registration pattern. The principle is simple: in the executable, we define a registry of available plugins in the plugin, we call some registration function to register the plugin in the registry. As this registration cannot be called from the main function, we use the static variable initialization to register the plugin. The static registration in the plugin looks like this: #include <plugin_registry.h> struct Plugin{ // constructor Plugin(){ PLugin_registry::register_plugin(<parameters>); } }; Plugin plugin; // here, the constructor is called as the variable has static storage duration. This technique has some important pitfall. Because the main executable does not reference the plugin, the compiler may discard the whole plugin object and not link it to the executable. To prevent this, several techniques can be used:","title":"Compile-time Plugins using Static Registration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#-link-the-plugin-with-wholearchive-flag","text":"","title":"- link the plugin with /WHOLEARCHIVE flag."},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/","text":"Introduction \u00b6 In C++, the workflow and especially the build pipeline is much more complicated than in other languages. Therefore, we start with brief overview of the C++ build pipelines. The following scheme (see the source for links) shows the possible build pipelines for C++, starting from Integrated developemnt tools (IDE) and ending with the linker. This guide presents mostly the following workflows: Clion or Visual Studio IDE CMake any sequencing tool (these are discribed only briefly as they are configured automaticvally by CMake) MSVC and GCC compiler toolchains Appart from the build pipeline, we also cover the dependency management. For this, we focus on the dependency manager vcpkg. Compiler Toolchains \u00b6 There are various toolchains available on Windows and Linux, but we limit this guide for only some of them, specifically those which are frequently updated and works great with Clion. MSYS2 (Windows) \u00b6 download follow the installation guide on the homepage install MinGW64 using: pacman -S mingw-w64-x86_64-gcc MSVC (Windows) \u00b6 Visual Studio Comunity Edition Setting up the compiler environment \u00b6 Most of the time, the compiler environment is set up automatically. However, beware that failing to setup the MSVC compiler environment can lead to hard to debug errors or silent selection of another compiler! . MSVC compiler is automatically set up: by Visual Studio and CLion IDEs by developer command prompt and developer PowerShell anytime when MSBuild is used as build sequencing tool On the other hand, a typical example of a situation where the compiler environment is not set up is when using a non-developer shell (e.g., PowerShell) and using a non-default build sequencing tool (e.g., Ninja). To test if the compiler environment is set up correctly , we can use call the cl command. If the compiler environment is not set up correctly, the command will not be found. To set up the compiler environment manually, you can use the following commands: $vswhere = \"${env:ProgramFiles(x86)}\\Microsoft Visual Studio\\Installer\\vswhere.exe\" $vs = & $vswhere -latest -products * -requires Microsoft.VisualStudio.Component.VC.Tools.x86.x64 -property installationPath Import-Module \"$vs\\Common7\\Tools\\Microsoft.VisualStudio.DevShell.dll\" Enter-VsDevShell -VsInstallPath $vs -DevCmdArguments '-arch=x64 -host_arch=x64' Common Compiler Flags \u00b6 /nologo : do not print the copyright banner and information messages /EH : exception handeling flags GCC (Linux/WSL) \u00b6 Installation \u00b6 If on Windows, Install the WSL first (WSL 2) check if GCC is installed by typing gcc --version If GCC is not installed: sudo apt-get update sudo apt-get upgrade sudo apt-get install gcc-<version> g++ \u00b6 g++ is the main executable for the GCC compiler. It is botha compiler and a wrapper for the linker. Typical usage is: g++ -o <output file> <cpp files and library files> : compilation and linking g++ -c <cpp files> : compilation only g++ -o <output file> <object files and library files> : linking only Other frequently used flags are: -g : include debug information -l<library> : link the library named lib<library> . Unlike direct library specification, here the linkers searches for the library in standard directories. Build Sequencing Tools \u00b6 Make \u00b6 Most of the time, make scripts should not be written manually, but generated by build script generators like CMake. Therefore, here, we describe the structure of make scripts as they are generated by CMake. The structure is: Makefile : the main file that contains the rules for building the project CMakeFiles : a directory containing the files referenced in the Makefile <target name>.dir directory for each target link.txt : the file containing the linking command MSBuild \u00b6 The MSBuild is a XML-based build sequencing tool. The scripts are typically generated by CMake or directly by Visual Studio. The main file for each target is the <target name>.vcxproj file. All properties are stored in the root element <Project> . The structure inside the project element is as follows (only the most important elements are listed): <ItemDefinitionGroup Condition=\"<configuration>\"> : contains the properties for the given configuration. <ItemGroup><multiple <ClCompile> elements><ItemGroup> : contains the source files that are compiled <ItemGroup><multiple <ProjectReference> elements><ItemGroup> : contains references to other targets (dependencies). Theses targets are built before the current target. The propertis for each configuration are structured as follows: <ClCompile> : contains the properties for the compilation of the source files (e.g., language standard, runtime library, etc.) <ResourceCompile> : <PreprocessorDefinitions> : the preprocessor definitions used for the target. These are introduced either by the target itself or by the dependencies. <Link> : contains the properties for the linking of the target (e.g., the libraries to link, the output file, etc.) Cmake \u00b6 Windows: Install CMake from https://cmake.org/download/ if your CMake is too old (e.g. error: \u201cCMake 3.15 or higher is required\u201d), update CMake (same as new install) Linux: If cmake is installed already, uninstall it! Do not use the cmake from linux repositories!! Download CMake sh installer from https://cmake.org/download/ install: sudo chmod +x <INSTALLER> sudo <INSTALLER> sudo rm <INSTALLER> add cmake executable to path Other details about CMake can be found in the CMake Manual . vcpkg \u00b6 For detailed information about vcpkg, see the vcpkg Manual . IDE \u00b6 Clion \u00b6 Configuration \u00b6 Set default layout \u00b6 Window -> Layouts -> Save changes in current layout Set up the new Nova engine \u00b6 The new Nova engine is a new Clion's engine that is faster and have more features. It is the engine that is used in Visual Studio Resharper C++ plugin. To enable it, go to settings -> Advanced Settings and under Clion check the Use the ReSharper C++ language engine (Clion Nova) . Apart from the new features, the Nova engine can prevent some false errors that are caused by the clangd engine used by the old Clion engine. Set up new surround with template \u00b6 In Clion, there are two types of surround with templates: surrond with and surround with live template . The first type use simple predefined templates and cannot be modified. However, the second type can be modified and new templates can be added. Toolchain configuration \u00b6 Go to settings -> Build, Execution, Deployment -> toolchain , add new toolchain and set: Name to whatever you want The environment should point to your toolchain: MSVC: C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community MSYS: C:\\MSYS2 WSL: From the drop-down list, choose the environment you configured for using with CLion in the previous steps Credentials (WSL) click to the setting button next to the credentials and fill host: localhost port: 2222 user and password according to your WSL system credentials Architecture (non WSL): amd64 CMake: C:\\Program Files\\CMake\\bin\\cmake.exe , for WSL, leave it as it is other fields should be filled automatically Multiple WSL toolchains \u00b6 When using multiple WSL toolchains, we need to manually set the compilers. To do so, fill also the C Compiler and C++ Compiler fields in the toolchain settings with the path to the compiler executable. Project configuration \u00b6 Most project settings resides (hereinafter Project settings ) in settings -> Build, Execution, Deployment -> CMake . For each build configuration, add a new template and set: Name to whatever you want Build type to debug To Cmake options , add: path to vcpkg toolchain file: Linux: -DCMAKE_TOOLCHAIN_FILE=/opt/vcpkg/scripts/buildsystems/vcpkg.cmake Windows: -DCMAKE_TOOLCHAIN_FILE=C:/vcpkg/scripts/buildsystems/vcpkg.cmake Set the correct vcpkg triplet MSVC: -DVCPKG_TARGET_TRIPLET=x64-windows MinGW: -DVCPKG_TARGET_TRIPLET=x64-MinGW Linux: -DVCPKG_TARGET_TRIPLET=x64-linux WSL extra configuration \u00b6 The CLion does not see the WSL's environment variables (as of 2023-03, see here ). To fix it, go to Project settings and set add the necessary environment variables to Environment field. Configuring only some CMake profiles \u00b6 When we click on the CMake reconiguration button, all profiles are reconfigured. Unfortunately, there is no way how to configure only some profiles. To work around this, we can deactivate the profiles we do not want to configure. To do so: go to settings -> Build, Execution, Deployment -> CMake select the profile you want to deactivate uncheck the Enable profile checkbox located at the top of the profile settings Troubleshooting \u00b6 Editor reports errors despite the code compiles in all compilers \u00b6 This can be caused by the clangd engine used by the old Clion engine. To fix it, enable the new Nova engine. Editor actions are not available, Ctrl + click does nothing \u00b6 check whether the \"Go to Reference\" action is triggered. If there is a tooltip claiming no references, go to next step. Otherwise, the problem is in the keymap check the right side of the status bar (bottom right corner of the editor). There should be an indicator in the format of C++ | <target name> | <debugging configuration> . If there is another indicator, CLion does not know the context of the project. Probably, the project configuration needs to be fixed Visual Studio \u00b6 Installation \u00b6 Install Visual Studio Open/Create a CMake project Install ReSharper C++ Setting Synchronization \u00b6 Sign-in in Visual Studio using a Mictosoft account. A lot of settings should be synchronized automatically . Apply the layout: Window -> Apply Window Layout -> <Layout Name> Sync ReSharper settings: you can share the file: %APPDATA%\\JetBrains\\Shared\\vAny\\ ( ~\\AppData\\Roaming\\JetBrains\\Shared\\vAny\\ ). This does not work good though as the files are changed on both sides constantly. unfortunately, as of 01/2023, there is no good way how to share resharper settings Install roamed plugins Basic Configuration \u00b6 Add 120 char guideline install the extension add the guideline in command window: Edit.AddGuideline 120 if there is an error extension ... did not load properly , you need to install the developer analytic tools package to the Visual Studio: Visual Studio Installer -> modify Go to the Individual Components tab search for the extension and select it proceed with the Visual Studio Modification If you need to use the system CMake, configure it now (described below) If you use *.tpp file, configure a support for them (described below). installation Enable template implementation files ( .*tpp ) syntax highlighting: \u00b6 Go to Tools -> Options -> Text Editor -> File Extension select Microsoft Visual C++ write tpp to the field and click add (reopen the file to see changes) To Change the Build Verbosity \u00b6 Go to Tools -> Options -> Projects and Solutions -> Build and Run Change the value of the MSBuild project build output verbosity. Project Setting \u00b6 Configure Visual Studio to use system CMake: \u00b6 Go to Project -> CMake Settings it should open the CMakeSettings.json file Scroll to the bottom and click on show advanced settings Set the CMake executable to point to the cmake.exe file of your system CMake Build Setting and Enviromental Variables \u00b6 The build configuration is in the file CMakePresets.json , located in the root of the project. The file can be also opened by right clicking on CMakeLists.txt ad selecting Edit CMake presets . Set the CMake Toolchain File \u00b6 To set the vcpkg toolchain file add the following value to the base configuration cacheVariables dictionary: \"CMAKE_TOOLCHAIN_FILE\": { \"value\": \"C:/vcpkg/scripts/buildsystems/vcpkg.cmake\", \"type\": \"FILEPATH\" } Set the Compiler \u00b6 The MSVC toolchain has two compiler executables, default one, and clang. The default compiler configuration looks like this: \"cacheVariables\": { ... \"CMAKE_C_COMPILER\": \"cl.exe\", \"CMAKE_CXX_COMPILER\": \"cl.exe\" ... }, To change the compiler to clang, replace cl.exe by clang-cl.exe in both rows. Old Method Using CMakeSettings.json \u00b6 We can open the build setting by right click on CMakeList.txt -> Cmake Settings To configure configure vcpkg toolchain file: Under General , fill to the Cmake toolchain file the following: C:/vcpkg/scripts/buildsystems/vcpkg.cmake To configure the enviromental variable, edit the CmakeSettings.json file directly. The global variables can be set in the environments array, the per configuration ones in <config object>/environments ( exmaple ). Launch Setting \u00b6 The launch settings determins the launch configuration, most importantly, the run arguments. To modify the run arguments: 1. open the launch.vs.json file: - use the context menu: - Right-click on CMakeLists.txt -> Add Debug Configuration - select default - or open the file directly, it is stored in <PROJECT DIR>/.vs/ 2. in launch.vs.json configure: - type : default for MSVC or cppgdb for WSL - projectTarget : the name of the target (executable) - name : the display name in Visual Studio - args : json array with arguments as strings - arguments with spaces have to be quoted with escaped quotes 3. Select the launch configuration in the drop-down menu next to the play button If the configuration is not visible in the drop-down menu, double-check the launch.vs.json file. The file is not validated, so it is easy to make a typo. If there is any problem, insted of an error, the launch configuration is not available. The following problems are common: syntax error in the json (should be marked by red squiggly line) typo in the target name Other launch.vs.json options \u00b6 cwd : the working directory Microsoft reference for launch.vs.json WSL Configuration \u00b6 For using GCC 10: go to CmakeSettings.json -> CMake variables and cache select show advanced variables checkbox set CMAKE_CXX_COMPILER variable to /usr/bin/g++-10 Other Configuration \u00b6 show white spaces: Edit -> Advanced -> View White Space . configure indentation: described here Determine Visual Studio version \u00b6 At total, there are 5 different versionigs related to Visual Studio . The version which the compiler support table refers to is the version of the compiler ( cl.exe ). we can find it be examining the compiler executable stored in C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.34.31933\\bin\\Hostx64\\x64 . Problems & solutions \u00b6 Cannot regenerate Cmake cache \u00b6 go to ./vs and look for file named CmakeWorkspaceSettings . It most likelz contains a line with disable = true . Just delete the file, or the specific line. Visual Studio Code \u00b6 In VS Code, the C++ support is provided by multiple extensions. CMake support extension \u00b6 The CMake support extension is provided by the CMake Tools extension. To set the vcpkg toolchain file, it is best to edit the extension settings, so that the toolchain file is set for all projects: In the extension settings, go to CMake: Configure Args Add argument --toolchain Add argument with the path to the vcpkg toolchain file To run the configuration or other CMake tasks, open the newly installed CMake view. By default, the CMake extension uses CMake presets to configure the project. To use CMake withou presets, go to the extension settings and set CMake: Use CMake Presets to never . Cursor \u00b6 For Cursor, the situation is almost the same as for VS Code. Important differences: The main, but important difference is that Cursor does not support the Visual Studio Debugger (cppvsdbg) [source] , [source 2] . Therefore, Cursor is hardly usable as the all-in-one IDE for C++, it has to be used in combination with another IDE. Cursor does not have a built-in IntelliSense. Instead, it relies on clangd to provide the functionality. Cursor does not have the Microsoft C++ Extension enabled. Instead, it has another C++ extension from Anysphere. This is the only C++ extension that needs to be installed manually, as it installs automatically: the CMake Tools extension the clangd extension the CodeLLDB extension Call the MSVC compiler from Cursor \u00b6 To partially overcome this limitation, we can at least run the program without debugger as a task: \"tasks\": [ { \"label\": \"Run DC 3-minute instance\", \"type\": \"process\", \"command\": \"${command:cmake.launchTargetFilename}\", \"options\": { \"cwd\": \"${command:cmake.launchTargetDirectory}\" }, \"args\": [ \"--instance\", \"C:/Google Drive AIC/My Drive/AIC Experiment Data/DARP/Instances/DC/instances/start_18-00/duration_01_min/max_delay_03_min\" ], \"problemMatcher\": [] } ] Here the executable is defined by: ${command:cmake.launchTargetFilename} : the executable ${command:cmake.launchTargetDirectory} : the path to the build bin directory Both these variables are defined by the CMake Tools extension. Troubleshooting \u00b6 The Ctrl + click on a function call does not open the definition \u00b6 Unlike in VS Code, Cursor does not have a built-in IntelliSense. Instead, it relies on clangd to provide the functionality. To conclude what is wrong, first try to reset the clangd server: View -> Command Palette type clangd: Restart language server Typical directory structure \u00b6 The typical directory structure for a C++ project is as follows: data : the directory containing the data files, including test data and default configuration files include : the directory containing public header files this directory is for the headers that are meant to be included by other projects. Typically, only library projects have this directory the projects own headers should be in the include/<project name> directory to mimic the installation directory structure install : the directory containing the installation scripts. Only needed for library projects or projects that are meant to be distributed port : the directory containing the vcpkg port files. Only needed for libraries that are meant to be distributed via vcpkg src : the directory containing the source files and private headers test : the directory containing the test files <various build dirs> : the directories containing the build files. The root directory typically contains the following files: .gitignore : the git ignore file CMakeLists.txt : the main cmake file CTestConfig.cmake : the CTest configuration file Dependencies \u00b6 Unlike in Python and Java, that have pip and maven, C++ does not have a standard package manager. Instead, there are plenty of ways how to install the dependencies and CMake then tries to find them. In the following table, we list the most common ways how to install the dependencies together with theri pros and cons. Method Platform-independent Automatic install of the dependency tree Can install build configuration tools Support easily determined CMake --install yes no yes no vcpkg yes yes yes yes CMake FetchContent yes no no no CMake ExternalProject yes Vcpkg Libraries \u00b6 type vcpkg list , if the library you need is not listed, continue to the next steps type vcpkg search <library simple name> and inspect the result to determine the exact name of the package you need if the library is not listed, check the presence in vcpkg repo if the library is in repo, but search does not find it, update vcpkg type vcpkg install <exact name> to install the package at the end of the installation log, there will be a cmake command needed to integrate the library, put it to the appropriate place to your CMakeList.txt file To display the cmake commands for the installed libraries, just run vcpkg install <exact name> again. Boost \u00b6 With boost, we should install only the necessary components. Then to include boost, we need: find_package(Boost REQUIRED) with all compiled components listed target_include_directories(<YOUR TARGET NAME> PUBLIC ${Boost_INCLUDE_DIRS}) Sometimes, it may be usefull to find out which boost components require linking. The list in the boost documentation, both for Unix and Windows . JNI \u00b6 for JNI, a JAVA_HOME system property needs to be set to the absolute path to the JDK, e.g., C:\\Program Files\\Java\\jdk-15.0.1 Gurobi \u00b6 If you don\u2019t have Gurobi installed, do it now, and check that the installation is working Windows: just install as usual Linux: download the archive to /opt sudo tar xvfz <gurobi archive> add the the file that introduce environment variables needed for gurobi to etc/profile.d Linux only: it is necessary to build the C++ library for your version of the compiler. Steps: 1. cd <GUROBI DIR>/linux64/src/build/ 2. make 3. mv libgurobi_c++.a ../../lib/libgurobi_c++_<some id for you, like version>.a 4. cd ../../lib/ 5. ln -sf ./libgurobi_c++<some id for you, like version>.a libgurobi_c++.a Follow this guide , specifically: 1. put the attached our custom FindGUROBI script to: - Windows: C:\\Program Files\\CMake\\share\\cmake-<your cmake version>\\Modules/ - Linux: /opt/<CMAKNAME>/share/cmake-<VERSION>/Modules 2. to your CMakeLists.txt , add: - find_package(GUROBI REQUIRED) - target_include_directories(<your executable> PRIVATE ${GUROBI_INCLUDE_DIRS}) - target_link_libraries(<your executable> PRIVATE ${GUROBI_LIBRARY}) - target_link_libraries(<your executable> PRIVATE optimized ${GUROBI_CXX_LIBRARY} debug ${GUROBI_CXX_DEBUG_LIBRARY}) 3. try to load the cmake projects (i.e., generate the build scripts using cmake). 4. if the C++ library is not found ( Gurobi c++ library not found ), check whether the correct C++ library is in the gurobi home, the file <library name>.lib has to be in the lib directory of the gurobi installation. If the file is not there, it is possible that your gurobi version is too old Update Gurobi \u00b6 Updating is done by installing the new version and generating and using new licence key . after update, you need to delete your build dir in order to prevent using of cached path to old Gurobi install Also, you need to update the library name on line 10 of the FindGUROBI.cmake script. Other Libraries Not Available in vcpkg \u00b6 Test Library linking/inclusion \u00b6 For testing purposes, we can follow this simple pattern: build the library include the library: target_include_directories(<target name> PUBLIC <path to include dir>) , where include dir is the directory with the main header file of the library. if the library is not the header only library, we need to: 3.1 link the library: target_link_libraries(<target name> PUBLIC <path to lib file>) , where path to lib file is the path to the dynamic library file used for linking ( .so on Linux, .lib on Windows). 3.2. add the dynamic library to some path visible for the executable - here the library file is .so on Linux and .dll on Windows - there are plenty options for the visible path, the most common being the system PATH variable, or the directory with the executable. Dependencies with WSL and CLion \u00b6 In WSL, when combined with CLion, some find scripts does not work, because they depend on system variables, that are not correctly passed from CLIon SSH connection to CMake. Therefore, it is necessary to add hints with absolute path to these scripts. Some of them can be downloaded here . Package that require these hints: JNI Gurobi Refactoring \u00b6 The refactoring of C++ code is a complex process, so the number of supported refactoring operations is limited. In Visual Studio, the only supported refactoring operation is renaming. In IntelliJ tools (CLion, ReSharper C++), there are more tools available, but still, the refactoring is not as powerful nor reliable as in Java or Python. Other alternative is to implement the refactoring manually, with a help of some compiler tools like clang Refactoring Engine ( example project ). Changing Method Signature \u00b6 As of 2023-10, there is no reliable way how to change the method signature in C++. The most efficient tool is the method signature refactorin in either CLion or ReSharper C++. However, it does not work in all cases, so it is necessary to check and fix the code manually. Standard Library \u00b6 The strucure and organization of the standard library is different on Windows and Linux. On Windows (specificly when using MSVC) the library is called MSVC C Runtime (CRT). It is divided between: VCRuntime<version>.dll : contains the parts that interacts with the OS new versions are released with new versions of Visual Studio ucrtbase.dll : contains other parts of the CRT non-versioned, the same for all versions of Visual Studio many other libraries On Linux , the library is divided between C and C++ standard libraries: C++ Standard Library: contains the standard C++ features described in the C++ standard. GCC: GNU C++ Library ( libstdc++ ) the backward compatibility is maintained by the exported symbols marked as GLIBCXX_<version> . The max GLIBCXX version per GCC version is listed in the ABI page LLVM: libc++ C Standard Library ( glibc ) : contains the standard C features described in the C standard. To get the version, run ldd --version Linking to the Standard Library \u00b6 On both platforms: the standard library is linked automatically the standard library is linked dynamically by default However, Windows and Linux have different attitudes towards static linking of the standard library: Windows : static linking is pretty common and easy to setup: pros: easy way to distribute the application to older systems, possibly decades old cons: even begginers have to check that the executable and all its dependencies link to the same (static vs dynamic) version of the CRT Linux : static linking is not common and not recommended: pros: the mismatch of the standard library linking between the executable and the dependencies is not a problem cons: the executable is typically not portable to other systems, unless the same version of the standard library is installed ( glibc is usually stable, but libstdc++ and libc++ are not) Setting up the correct CRT with MSVC \u00b6 official documentation It is critical to set the runtime library for the executable we are building so that it matches the runtime library used by the dependencies. If the mismatch occurs, it is manifested depending on the situation: when we link to a dynamic CRT, but dependencies link to a static CRT, the linker will throw an error, typically for each such dependency plaintext LNK2038 mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in .. when we link to a staticCRT, but dependencies link to a dynamic CRT, the linker proceeds without errors, but the program crashes at runtime. Typically, some allocation/deallocation error occurs. The CRT is set by compiler flags (see the table below). These flags are typically passed to the compiler by the build system based on the build configuration files (e.g., MSBuild files). They affect all parts of the CRT If we generate thes files by the IDE (Visual Studio), we have to set the CRT in the IDE. If they are generated by CMake , there are three possible situations: we use the dynamic CRT (default): nothing has to be done the build is handled by vcpkg (libraries installed with vcpgk install ): the runtime library is set by the VCPKG_CRT_LINKAGE variable in the triplet file. Nothing has to be done. we use the static CRT: we have to set the CMAKE_MSVC_RUNTIME_LIBRARY variable in the CMakeLists.txt file. if we use vcpgk libraries, we should set it based on the triplet used: cmake if (VCPKG_TARGET_TRIPLET MATCHES \"-static\") set(CMAKE_MSVC_RUNTIME_LIBRARY \"MultiThreaded$<$<CONFIG:Debug>:Debug>\") endif() To determine if the type of the runtime library used by the target , we can explore the build script for the target, For MSBuild, the runtime library is set by the RuntimeLibrary property. To determine the runtime library used by a library , we can use the dumpbin tool. This tool is part of the Visual Studio installation and can be run from the Developer Command Prompt or Developer PowerShell. The following command will display the runtime library used by the library: dumpbin /directives <path to the library> The following compiler flags are available: Option Copiler flag MSBuild name Description Linged library Multi-threaded Dynamic /MD MultiThreadedDLL The default option. The application uses the dynamic version of the runtime library. msvcrt.lib Multi-threaded Dynamic Debug /MDd MultiThreadedDebugDLL The debug version of the dynamic runtime library. msvcrtd.lib Multi-threaded Static /MT MultiThreaded The application uses the static version of the runtime library. libcmt.lib Multi-threaded Static Debug /MTd MultiThreadedDebug The debug version of the static runtime library. libcmtd.lib DLL /LD The application is compiled as a DLL. - DLL Debug /LDd The debug version of the DLL. - By default, the /MD / /MDd flags are used depending on the build type. Resouces \u00b6 MSVC CRT on Wikipedia CRT reorganization blogpost from 2015 GitHub repo with the relesed CRT sources, useful information, and links Exporting symbols for shared libraries \u00b6 When creating a shared library, we have to specify which symbols are exported. These are the only symbols that can be directly used from the client code. This is done using special keywords. Because the keywords are different for different compilers, usually, some macros are used instead. Typically, these macros: use the correct keyword for the compiler support disabling the keyword for building static libraries or executables The macros are typically defined in a dedicated header file called export header . This file is then included in every header file that defines a symbol that should be exported. For the whole export machinery to work, we need to: create the export header file and include it in every header file that defines an exported symbol mark the symbols that should be exported with the export macro use CMake to supply the correct compiler flags used in the export header Creating the Export Header \u00b6 We can generate the export header file using the GenerateExportHeader module. We can get it by the following code: add_library(<target name> SHARED <files>) generate_export_header(<target name>) This will generate the export header file in the build directory. However, the export file is different for different compilers. Therefore, it is best to copy the file for each compiler and then merge the macros to create a universal export header file. Alternatively, we can use some proven export header file. Finally, we can store the export header file in the source directory and include it in every header file that defines an exported symbol. Marking the Symbols \u00b6 Usually, we mark the following symbols for export: classes: class <export macro> MyClass{...} functions: <export macro> <return type> my_function(...) Other symbols does not have to be exported as they are automatically exported by the compiler: enums and enum classes constants templates Additionaly, only the symbols needed by external code should be exported. The exeption is when the the interface use templates. In this case, all symbols used by the template (but not the template itself) should be exported. The <export macro> from the GenerateExportHeader is named <target name>_EXPORT , we can check the exact name in the export header file. CMake Configuration \u00b6 Now the shared library build should work correctly. However, the static library build will be full of warnings, because the export macro is not intended for static libraries. The same is true for executables. Therefore, we need to set a special property for each target that uses the export header and is not a shared library: target_compile_definitions(<static/executable target name> PUBLIC <static condition macro >) The static condition macro from the GenerateExportHeader is named <target name>_STATIC_DEFINE . If not clear, we can find the exact macros' names in the export header file. Resouces \u00b6 tutorial on decovar Compilation for a specific CPU \u00b6 MSVC \u00b6 MSVC cannot compile for a specific CPU or CPU series. It can, however, use new instructions sets more efficiently if it compiles the code without the support for CPUs thad does not support these instruction sets. The command for the compiler is: `/arch: (see MSVC documentation for details). ## GCC In GCC, the march option enables compilation for a specific hardware. ml) option enables compilation for a specific hardware. pects that you use vcpkg in a per-project configuration. To make it work, add: -DCMAKE_TOOLCHAIN_FILE=<vcpkg location>/scripts/buildsystems/vcpkg.cmake - To change build options ( option in CMakeLists.txt ), run cmake with -D <option name>=<option value> <build dir> . Example: cmake -D BUILD_TESTING=OFF . Building \u00b6 For building, use: cmake --build <build dir> where build dir is the directory containing the build scripts ( CmakeFiles folder). To list the build options: cmake -L Specify the build type (Debug, Release) \u00b6 To build in release mode, or any other build mode except for the default, we need to specify the parameters for CMake. Unfortunately, these parameters depends on the build system: Single-configuration systems (Unix, MinGW) Multi-configuration systems (Visual Studio) Single-configuration systems \u00b6 Single configuration systems have the build type hardcoded in the build scripts. Therefore, we need to specify the build type for CMake when we generate the build scripts: cmake ../ -DCMAKE_BUILD_TYPE=Release By default, the build type is Release . Multi-configuration systems \u00b6 In multi-configuration systems, the -DCMAKE_BUILD_TYPE parameter is ignored, because the build configuration is supposed to be determined when building the code (i.e., same build scripts for debug and for release). Therefore, we omit it, and instead specify the --config parameter when building the code: cmake --build . --config Release Specify the target \u00b6 We can use the --target parameter for that: cmake --build . --target <TARGET NAME> Clean the source files \u00b6 Run: cmake --build . --target clean Handling Case Insensitivity \u00b6 Windows builds are, in line with the OS, case insensitive. Moreover, the Visual Studio does some magic with names internally, so the build is case insensitive even on VS WSL builds. The case insensitivity can bring inconsistencies that later breake Unix builds. Therefore, it is desirable to have the build case sensitive even on Windows. Fortunatelly, we can toggle the case sensitivity at the OS level using this PowerShell command: Get-ChildItem <PROJECT ROOT PATH> -Recurse -Directory | ForEach-Object { fsutil.exe file setCaseSensitiveInfo $_.FullName enable } Note that this can break the git commits, so it is necessary to also configure git in your case-sensitive repo: git config core.ignorecase false","title":"C++ Workflow"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#introduction","text":"In C++, the workflow and especially the build pipeline is much more complicated than in other languages. Therefore, we start with brief overview of the C++ build pipelines. The following scheme (see the source for links) shows the possible build pipelines for C++, starting from Integrated developemnt tools (IDE) and ending with the linker. This guide presents mostly the following workflows: Clion or Visual Studio IDE CMake any sequencing tool (these are discribed only briefly as they are configured automaticvally by CMake) MSVC and GCC compiler toolchains Appart from the build pipeline, we also cover the dependency management. For this, we focus on the dependency manager vcpkg.","title":"Introduction"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#compiler-toolchains","text":"There are various toolchains available on Windows and Linux, but we limit this guide for only some of them, specifically those which are frequently updated and works great with Clion.","title":"Compiler Toolchains"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#msys2-windows","text":"download follow the installation guide on the homepage install MinGW64 using: pacman -S mingw-w64-x86_64-gcc","title":"MSYS2 (Windows)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#msvc-windows","text":"Visual Studio Comunity Edition","title":"MSVC (Windows)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#setting-up-the-compiler-environment","text":"Most of the time, the compiler environment is set up automatically. However, beware that failing to setup the MSVC compiler environment can lead to hard to debug errors or silent selection of another compiler! . MSVC compiler is automatically set up: by Visual Studio and CLion IDEs by developer command prompt and developer PowerShell anytime when MSBuild is used as build sequencing tool On the other hand, a typical example of a situation where the compiler environment is not set up is when using a non-developer shell (e.g., PowerShell) and using a non-default build sequencing tool (e.g., Ninja). To test if the compiler environment is set up correctly , we can use call the cl command. If the compiler environment is not set up correctly, the command will not be found. To set up the compiler environment manually, you can use the following commands: $vswhere = \"${env:ProgramFiles(x86)}\\Microsoft Visual Studio\\Installer\\vswhere.exe\" $vs = & $vswhere -latest -products * -requires Microsoft.VisualStudio.Component.VC.Tools.x86.x64 -property installationPath Import-Module \"$vs\\Common7\\Tools\\Microsoft.VisualStudio.DevShell.dll\" Enter-VsDevShell -VsInstallPath $vs -DevCmdArguments '-arch=x64 -host_arch=x64'","title":"Setting up the compiler environment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#common-compiler-flags","text":"/nologo : do not print the copyright banner and information messages /EH : exception handeling flags","title":"Common Compiler Flags"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#gcc-linuxwsl","text":"","title":"GCC (Linux/WSL)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#installation","text":"If on Windows, Install the WSL first (WSL 2) check if GCC is installed by typing gcc --version If GCC is not installed: sudo apt-get update sudo apt-get upgrade sudo apt-get install gcc-<version>","title":"Installation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#g","text":"g++ is the main executable for the GCC compiler. It is botha compiler and a wrapper for the linker. Typical usage is: g++ -o <output file> <cpp files and library files> : compilation and linking g++ -c <cpp files> : compilation only g++ -o <output file> <object files and library files> : linking only Other frequently used flags are: -g : include debug information -l<library> : link the library named lib<library> . Unlike direct library specification, here the linkers searches for the library in standard directories.","title":"g++"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#build-sequencing-tools","text":"","title":"Build Sequencing Tools"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#make","text":"Most of the time, make scripts should not be written manually, but generated by build script generators like CMake. Therefore, here, we describe the structure of make scripts as they are generated by CMake. The structure is: Makefile : the main file that contains the rules for building the project CMakeFiles : a directory containing the files referenced in the Makefile <target name>.dir directory for each target link.txt : the file containing the linking command","title":"Make"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#msbuild","text":"The MSBuild is a XML-based build sequencing tool. The scripts are typically generated by CMake or directly by Visual Studio. The main file for each target is the <target name>.vcxproj file. All properties are stored in the root element <Project> . The structure inside the project element is as follows (only the most important elements are listed): <ItemDefinitionGroup Condition=\"<configuration>\"> : contains the properties for the given configuration. <ItemGroup><multiple <ClCompile> elements><ItemGroup> : contains the source files that are compiled <ItemGroup><multiple <ProjectReference> elements><ItemGroup> : contains references to other targets (dependencies). Theses targets are built before the current target. The propertis for each configuration are structured as follows: <ClCompile> : contains the properties for the compilation of the source files (e.g., language standard, runtime library, etc.) <ResourceCompile> : <PreprocessorDefinitions> : the preprocessor definitions used for the target. These are introduced either by the target itself or by the dependencies. <Link> : contains the properties for the linking of the target (e.g., the libraries to link, the output file, etc.)","title":"MSBuild"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#cmake","text":"Windows: Install CMake from https://cmake.org/download/ if your CMake is too old (e.g. error: \u201cCMake 3.15 or higher is required\u201d), update CMake (same as new install) Linux: If cmake is installed already, uninstall it! Do not use the cmake from linux repositories!! Download CMake sh installer from https://cmake.org/download/ install: sudo chmod +x <INSTALLER> sudo <INSTALLER> sudo rm <INSTALLER> add cmake executable to path Other details about CMake can be found in the CMake Manual .","title":"Cmake"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#vcpkg","text":"For detailed information about vcpkg, see the vcpkg Manual .","title":"vcpkg"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#ide","text":"","title":"IDE"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#clion","text":"","title":"Clion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#configuration","text":"","title":"Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#set-default-layout","text":"Window -> Layouts -> Save changes in current layout","title":"Set default layout"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#set-up-the-new-nova-engine","text":"The new Nova engine is a new Clion's engine that is faster and have more features. It is the engine that is used in Visual Studio Resharper C++ plugin. To enable it, go to settings -> Advanced Settings and under Clion check the Use the ReSharper C++ language engine (Clion Nova) . Apart from the new features, the Nova engine can prevent some false errors that are caused by the clangd engine used by the old Clion engine.","title":"Set up the new Nova engine"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#set-up-new-surround-with-template","text":"In Clion, there are two types of surround with templates: surrond with and surround with live template . The first type use simple predefined templates and cannot be modified. However, the second type can be modified and new templates can be added.","title":"Set up new surround with template"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#toolchain-configuration","text":"Go to settings -> Build, Execution, Deployment -> toolchain , add new toolchain and set: Name to whatever you want The environment should point to your toolchain: MSVC: C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community MSYS: C:\\MSYS2 WSL: From the drop-down list, choose the environment you configured for using with CLion in the previous steps Credentials (WSL) click to the setting button next to the credentials and fill host: localhost port: 2222 user and password according to your WSL system credentials Architecture (non WSL): amd64 CMake: C:\\Program Files\\CMake\\bin\\cmake.exe , for WSL, leave it as it is other fields should be filled automatically","title":"Toolchain configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#multiple-wsl-toolchains","text":"When using multiple WSL toolchains, we need to manually set the compilers. To do so, fill also the C Compiler and C++ Compiler fields in the toolchain settings with the path to the compiler executable.","title":"Multiple WSL toolchains"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#project-configuration","text":"Most project settings resides (hereinafter Project settings ) in settings -> Build, Execution, Deployment -> CMake . For each build configuration, add a new template and set: Name to whatever you want Build type to debug To Cmake options , add: path to vcpkg toolchain file: Linux: -DCMAKE_TOOLCHAIN_FILE=/opt/vcpkg/scripts/buildsystems/vcpkg.cmake Windows: -DCMAKE_TOOLCHAIN_FILE=C:/vcpkg/scripts/buildsystems/vcpkg.cmake Set the correct vcpkg triplet MSVC: -DVCPKG_TARGET_TRIPLET=x64-windows MinGW: -DVCPKG_TARGET_TRIPLET=x64-MinGW Linux: -DVCPKG_TARGET_TRIPLET=x64-linux","title":"Project configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#wsl-extra-configuration","text":"The CLion does not see the WSL's environment variables (as of 2023-03, see here ). To fix it, go to Project settings and set add the necessary environment variables to Environment field.","title":"WSL extra configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#configuring-only-some-cmake-profiles","text":"When we click on the CMake reconiguration button, all profiles are reconfigured. Unfortunately, there is no way how to configure only some profiles. To work around this, we can deactivate the profiles we do not want to configure. To do so: go to settings -> Build, Execution, Deployment -> CMake select the profile you want to deactivate uncheck the Enable profile checkbox located at the top of the profile settings","title":"Configuring only some CMake profiles"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#editor-reports-errors-despite-the-code-compiles-in-all-compilers","text":"This can be caused by the clangd engine used by the old Clion engine. To fix it, enable the new Nova engine.","title":"Editor reports errors despite the code compiles in all compilers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#editor-actions-are-not-available-ctrl-click-does-nothing","text":"check whether the \"Go to Reference\" action is triggered. If there is a tooltip claiming no references, go to next step. Otherwise, the problem is in the keymap check the right side of the status bar (bottom right corner of the editor). There should be an indicator in the format of C++ | <target name> | <debugging configuration> . If there is another indicator, CLion does not know the context of the project. Probably, the project configuration needs to be fixed","title":"Editor actions are not available, Ctrl + click does nothing"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#visual-studio","text":"","title":"Visual Studio"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#installation_1","text":"Install Visual Studio Open/Create a CMake project Install ReSharper C++","title":"Installation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#setting-synchronization","text":"Sign-in in Visual Studio using a Mictosoft account. A lot of settings should be synchronized automatically . Apply the layout: Window -> Apply Window Layout -> <Layout Name> Sync ReSharper settings: you can share the file: %APPDATA%\\JetBrains\\Shared\\vAny\\ ( ~\\AppData\\Roaming\\JetBrains\\Shared\\vAny\\ ). This does not work good though as the files are changed on both sides constantly. unfortunately, as of 01/2023, there is no good way how to share resharper settings Install roamed plugins","title":"Setting Synchronization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#basic-configuration","text":"Add 120 char guideline install the extension add the guideline in command window: Edit.AddGuideline 120 if there is an error extension ... did not load properly , you need to install the developer analytic tools package to the Visual Studio: Visual Studio Installer -> modify Go to the Individual Components tab search for the extension and select it proceed with the Visual Studio Modification If you need to use the system CMake, configure it now (described below) If you use *.tpp file, configure a support for them (described below). installation","title":"Basic Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#enable-template-implementation-files-tpp-syntax-highlighting","text":"Go to Tools -> Options -> Text Editor -> File Extension select Microsoft Visual C++ write tpp to the field and click add (reopen the file to see changes)","title":"Enable template implementation files (.*tpp) syntax highlighting:"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#to-change-the-build-verbosity","text":"Go to Tools -> Options -> Projects and Solutions -> Build and Run Change the value of the MSBuild project build output verbosity.","title":"To Change the Build Verbosity"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#project-setting","text":"","title":"Project Setting"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#configure-visual-studio-to-use-system-cmake","text":"Go to Project -> CMake Settings it should open the CMakeSettings.json file Scroll to the bottom and click on show advanced settings Set the CMake executable to point to the cmake.exe file of your system CMake","title":"Configure Visual Studio to use system CMake:"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#build-setting-and-enviromental-variables","text":"The build configuration is in the file CMakePresets.json , located in the root of the project. The file can be also opened by right clicking on CMakeLists.txt ad selecting Edit CMake presets .","title":"Build Setting and Enviromental Variables"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#set-the-cmake-toolchain-file","text":"To set the vcpkg toolchain file add the following value to the base configuration cacheVariables dictionary: \"CMAKE_TOOLCHAIN_FILE\": { \"value\": \"C:/vcpkg/scripts/buildsystems/vcpkg.cmake\", \"type\": \"FILEPATH\" }","title":"Set the CMake Toolchain File"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#set-the-compiler","text":"The MSVC toolchain has two compiler executables, default one, and clang. The default compiler configuration looks like this: \"cacheVariables\": { ... \"CMAKE_C_COMPILER\": \"cl.exe\", \"CMAKE_CXX_COMPILER\": \"cl.exe\" ... }, To change the compiler to clang, replace cl.exe by clang-cl.exe in both rows.","title":"Set the Compiler"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#old-method-using-cmakesettingsjson","text":"We can open the build setting by right click on CMakeList.txt -> Cmake Settings To configure configure vcpkg toolchain file: Under General , fill to the Cmake toolchain file the following: C:/vcpkg/scripts/buildsystems/vcpkg.cmake To configure the enviromental variable, edit the CmakeSettings.json file directly. The global variables can be set in the environments array, the per configuration ones in <config object>/environments ( exmaple ).","title":"Old Method Using CMakeSettings.json"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#launch-setting","text":"The launch settings determins the launch configuration, most importantly, the run arguments. To modify the run arguments: 1. open the launch.vs.json file: - use the context menu: - Right-click on CMakeLists.txt -> Add Debug Configuration - select default - or open the file directly, it is stored in <PROJECT DIR>/.vs/ 2. in launch.vs.json configure: - type : default for MSVC or cppgdb for WSL - projectTarget : the name of the target (executable) - name : the display name in Visual Studio - args : json array with arguments as strings - arguments with spaces have to be quoted with escaped quotes 3. Select the launch configuration in the drop-down menu next to the play button If the configuration is not visible in the drop-down menu, double-check the launch.vs.json file. The file is not validated, so it is easy to make a typo. If there is any problem, insted of an error, the launch configuration is not available. The following problems are common: syntax error in the json (should be marked by red squiggly line) typo in the target name","title":"Launch Setting"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#other-launchvsjson-options","text":"cwd : the working directory Microsoft reference for launch.vs.json","title":"Other launch.vs.json options"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#wsl-configuration","text":"For using GCC 10: go to CmakeSettings.json -> CMake variables and cache select show advanced variables checkbox set CMAKE_CXX_COMPILER variable to /usr/bin/g++-10","title":"WSL Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#other-configuration","text":"show white spaces: Edit -> Advanced -> View White Space . configure indentation: described here","title":"Other Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#determine-visual-studio-version","text":"At total, there are 5 different versionigs related to Visual Studio . The version which the compiler support table refers to is the version of the compiler ( cl.exe ). we can find it be examining the compiler executable stored in C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.34.31933\\bin\\Hostx64\\x64 .","title":"Determine Visual Studio version"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#problems-solutions","text":"","title":"Problems &amp; solutions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#cannot-regenerate-cmake-cache","text":"go to ./vs and look for file named CmakeWorkspaceSettings . It most likelz contains a line with disable = true . Just delete the file, or the specific line.","title":"Cannot regenerate Cmake cache"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#visual-studio-code","text":"In VS Code, the C++ support is provided by multiple extensions.","title":"Visual Studio Code"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#cmake-support-extension","text":"The CMake support extension is provided by the CMake Tools extension. To set the vcpkg toolchain file, it is best to edit the extension settings, so that the toolchain file is set for all projects: In the extension settings, go to CMake: Configure Args Add argument --toolchain Add argument with the path to the vcpkg toolchain file To run the configuration or other CMake tasks, open the newly installed CMake view. By default, the CMake extension uses CMake presets to configure the project. To use CMake withou presets, go to the extension settings and set CMake: Use CMake Presets to never .","title":"CMake support extension"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#cursor","text":"For Cursor, the situation is almost the same as for VS Code. Important differences: The main, but important difference is that Cursor does not support the Visual Studio Debugger (cppvsdbg) [source] , [source 2] . Therefore, Cursor is hardly usable as the all-in-one IDE for C++, it has to be used in combination with another IDE. Cursor does not have a built-in IntelliSense. Instead, it relies on clangd to provide the functionality. Cursor does not have the Microsoft C++ Extension enabled. Instead, it has another C++ extension from Anysphere. This is the only C++ extension that needs to be installed manually, as it installs automatically: the CMake Tools extension the clangd extension the CodeLLDB extension","title":"Cursor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#call-the-msvc-compiler-from-cursor","text":"To partially overcome this limitation, we can at least run the program without debugger as a task: \"tasks\": [ { \"label\": \"Run DC 3-minute instance\", \"type\": \"process\", \"command\": \"${command:cmake.launchTargetFilename}\", \"options\": { \"cwd\": \"${command:cmake.launchTargetDirectory}\" }, \"args\": [ \"--instance\", \"C:/Google Drive AIC/My Drive/AIC Experiment Data/DARP/Instances/DC/instances/start_18-00/duration_01_min/max_delay_03_min\" ], \"problemMatcher\": [] } ] Here the executable is defined by: ${command:cmake.launchTargetFilename} : the executable ${command:cmake.launchTargetDirectory} : the path to the build bin directory Both these variables are defined by the CMake Tools extension.","title":"Call the MSVC compiler from Cursor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#troubleshooting_1","text":"","title":"Troubleshooting"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#the-ctrl-click-on-a-function-call-does-not-open-the-definition","text":"Unlike in VS Code, Cursor does not have a built-in IntelliSense. Instead, it relies on clangd to provide the functionality. To conclude what is wrong, first try to reset the clangd server: View -> Command Palette type clangd: Restart language server","title":"The Ctrl + click on a function call does not open the definition"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#typical-directory-structure","text":"The typical directory structure for a C++ project is as follows: data : the directory containing the data files, including test data and default configuration files include : the directory containing public header files this directory is for the headers that are meant to be included by other projects. Typically, only library projects have this directory the projects own headers should be in the include/<project name> directory to mimic the installation directory structure install : the directory containing the installation scripts. Only needed for library projects or projects that are meant to be distributed port : the directory containing the vcpkg port files. Only needed for libraries that are meant to be distributed via vcpkg src : the directory containing the source files and private headers test : the directory containing the test files <various build dirs> : the directories containing the build files. The root directory typically contains the following files: .gitignore : the git ignore file CMakeLists.txt : the main cmake file CTestConfig.cmake : the CTest configuration file","title":"Typical directory structure"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#dependencies","text":"Unlike in Python and Java, that have pip and maven, C++ does not have a standard package manager. Instead, there are plenty of ways how to install the dependencies and CMake then tries to find them. In the following table, we list the most common ways how to install the dependencies together with theri pros and cons. Method Platform-independent Automatic install of the dependency tree Can install build configuration tools Support easily determined CMake --install yes no yes no vcpkg yes yes yes yes CMake FetchContent yes no no no CMake ExternalProject yes","title":"Dependencies"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#vcpkg-libraries","text":"type vcpkg list , if the library you need is not listed, continue to the next steps type vcpkg search <library simple name> and inspect the result to determine the exact name of the package you need if the library is not listed, check the presence in vcpkg repo if the library is in repo, but search does not find it, update vcpkg type vcpkg install <exact name> to install the package at the end of the installation log, there will be a cmake command needed to integrate the library, put it to the appropriate place to your CMakeList.txt file To display the cmake commands for the installed libraries, just run vcpkg install <exact name> again.","title":"Vcpkg Libraries"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#boost","text":"With boost, we should install only the necessary components. Then to include boost, we need: find_package(Boost REQUIRED) with all compiled components listed target_include_directories(<YOUR TARGET NAME> PUBLIC ${Boost_INCLUDE_DIRS}) Sometimes, it may be usefull to find out which boost components require linking. The list in the boost documentation, both for Unix and Windows .","title":"Boost"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#jni","text":"for JNI, a JAVA_HOME system property needs to be set to the absolute path to the JDK, e.g., C:\\Program Files\\Java\\jdk-15.0.1","title":"JNI"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#gurobi","text":"If you don\u2019t have Gurobi installed, do it now, and check that the installation is working Windows: just install as usual Linux: download the archive to /opt sudo tar xvfz <gurobi archive> add the the file that introduce environment variables needed for gurobi to etc/profile.d Linux only: it is necessary to build the C++ library for your version of the compiler. Steps: 1. cd <GUROBI DIR>/linux64/src/build/ 2. make 3. mv libgurobi_c++.a ../../lib/libgurobi_c++_<some id for you, like version>.a 4. cd ../../lib/ 5. ln -sf ./libgurobi_c++<some id for you, like version>.a libgurobi_c++.a Follow this guide , specifically: 1. put the attached our custom FindGUROBI script to: - Windows: C:\\Program Files\\CMake\\share\\cmake-<your cmake version>\\Modules/ - Linux: /opt/<CMAKNAME>/share/cmake-<VERSION>/Modules 2. to your CMakeLists.txt , add: - find_package(GUROBI REQUIRED) - target_include_directories(<your executable> PRIVATE ${GUROBI_INCLUDE_DIRS}) - target_link_libraries(<your executable> PRIVATE ${GUROBI_LIBRARY}) - target_link_libraries(<your executable> PRIVATE optimized ${GUROBI_CXX_LIBRARY} debug ${GUROBI_CXX_DEBUG_LIBRARY}) 3. try to load the cmake projects (i.e., generate the build scripts using cmake). 4. if the C++ library is not found ( Gurobi c++ library not found ), check whether the correct C++ library is in the gurobi home, the file <library name>.lib has to be in the lib directory of the gurobi installation. If the file is not there, it is possible that your gurobi version is too old","title":"Gurobi"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#update-gurobi","text":"Updating is done by installing the new version and generating and using new licence key . after update, you need to delete your build dir in order to prevent using of cached path to old Gurobi install Also, you need to update the library name on line 10 of the FindGUROBI.cmake script.","title":"Update Gurobi"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#other-libraries-not-available-in-vcpkg","text":"","title":"Other Libraries Not Available in vcpkg"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#test-library-linkinginclusion","text":"For testing purposes, we can follow this simple pattern: build the library include the library: target_include_directories(<target name> PUBLIC <path to include dir>) , where include dir is the directory with the main header file of the library. if the library is not the header only library, we need to: 3.1 link the library: target_link_libraries(<target name> PUBLIC <path to lib file>) , where path to lib file is the path to the dynamic library file used for linking ( .so on Linux, .lib on Windows). 3.2. add the dynamic library to some path visible for the executable - here the library file is .so on Linux and .dll on Windows - there are plenty options for the visible path, the most common being the system PATH variable, or the directory with the executable.","title":"Test Library linking/inclusion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#dependencies-with-wsl-and-clion","text":"In WSL, when combined with CLion, some find scripts does not work, because they depend on system variables, that are not correctly passed from CLIon SSH connection to CMake. Therefore, it is necessary to add hints with absolute path to these scripts. Some of them can be downloaded here . Package that require these hints: JNI Gurobi","title":"Dependencies with WSL and CLion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#refactoring","text":"The refactoring of C++ code is a complex process, so the number of supported refactoring operations is limited. In Visual Studio, the only supported refactoring operation is renaming. In IntelliJ tools (CLion, ReSharper C++), there are more tools available, but still, the refactoring is not as powerful nor reliable as in Java or Python. Other alternative is to implement the refactoring manually, with a help of some compiler tools like clang Refactoring Engine ( example project ).","title":"Refactoring"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#changing-method-signature","text":"As of 2023-10, there is no reliable way how to change the method signature in C++. The most efficient tool is the method signature refactorin in either CLion or ReSharper C++. However, it does not work in all cases, so it is necessary to check and fix the code manually.","title":"Changing Method Signature"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#standard-library","text":"The strucure and organization of the standard library is different on Windows and Linux. On Windows (specificly when using MSVC) the library is called MSVC C Runtime (CRT). It is divided between: VCRuntime<version>.dll : contains the parts that interacts with the OS new versions are released with new versions of Visual Studio ucrtbase.dll : contains other parts of the CRT non-versioned, the same for all versions of Visual Studio many other libraries On Linux , the library is divided between C and C++ standard libraries: C++ Standard Library: contains the standard C++ features described in the C++ standard. GCC: GNU C++ Library ( libstdc++ ) the backward compatibility is maintained by the exported symbols marked as GLIBCXX_<version> . The max GLIBCXX version per GCC version is listed in the ABI page LLVM: libc++ C Standard Library ( glibc ) : contains the standard C features described in the C standard. To get the version, run ldd --version","title":"Standard Library"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#linking-to-the-standard-library","text":"On both platforms: the standard library is linked automatically the standard library is linked dynamically by default However, Windows and Linux have different attitudes towards static linking of the standard library: Windows : static linking is pretty common and easy to setup: pros: easy way to distribute the application to older systems, possibly decades old cons: even begginers have to check that the executable and all its dependencies link to the same (static vs dynamic) version of the CRT Linux : static linking is not common and not recommended: pros: the mismatch of the standard library linking between the executable and the dependencies is not a problem cons: the executable is typically not portable to other systems, unless the same version of the standard library is installed ( glibc is usually stable, but libstdc++ and libc++ are not)","title":"Linking to the Standard Library"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#setting-up-the-correct-crt-with-msvc","text":"official documentation It is critical to set the runtime library for the executable we are building so that it matches the runtime library used by the dependencies. If the mismatch occurs, it is manifested depending on the situation: when we link to a dynamic CRT, but dependencies link to a static CRT, the linker will throw an error, typically for each such dependency plaintext LNK2038 mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in .. when we link to a staticCRT, but dependencies link to a dynamic CRT, the linker proceeds without errors, but the program crashes at runtime. Typically, some allocation/deallocation error occurs. The CRT is set by compiler flags (see the table below). These flags are typically passed to the compiler by the build system based on the build configuration files (e.g., MSBuild files). They affect all parts of the CRT If we generate thes files by the IDE (Visual Studio), we have to set the CRT in the IDE. If they are generated by CMake , there are three possible situations: we use the dynamic CRT (default): nothing has to be done the build is handled by vcpkg (libraries installed with vcpgk install ): the runtime library is set by the VCPKG_CRT_LINKAGE variable in the triplet file. Nothing has to be done. we use the static CRT: we have to set the CMAKE_MSVC_RUNTIME_LIBRARY variable in the CMakeLists.txt file. if we use vcpgk libraries, we should set it based on the triplet used: cmake if (VCPKG_TARGET_TRIPLET MATCHES \"-static\") set(CMAKE_MSVC_RUNTIME_LIBRARY \"MultiThreaded$<$<CONFIG:Debug>:Debug>\") endif() To determine if the type of the runtime library used by the target , we can explore the build script for the target, For MSBuild, the runtime library is set by the RuntimeLibrary property. To determine the runtime library used by a library , we can use the dumpbin tool. This tool is part of the Visual Studio installation and can be run from the Developer Command Prompt or Developer PowerShell. The following command will display the runtime library used by the library: dumpbin /directives <path to the library> The following compiler flags are available: Option Copiler flag MSBuild name Description Linged library Multi-threaded Dynamic /MD MultiThreadedDLL The default option. The application uses the dynamic version of the runtime library. msvcrt.lib Multi-threaded Dynamic Debug /MDd MultiThreadedDebugDLL The debug version of the dynamic runtime library. msvcrtd.lib Multi-threaded Static /MT MultiThreaded The application uses the static version of the runtime library. libcmt.lib Multi-threaded Static Debug /MTd MultiThreadedDebug The debug version of the static runtime library. libcmtd.lib DLL /LD The application is compiled as a DLL. - DLL Debug /LDd The debug version of the DLL. - By default, the /MD / /MDd flags are used depending on the build type.","title":"Setting up the correct CRT with MSVC"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#resouces","text":"MSVC CRT on Wikipedia CRT reorganization blogpost from 2015 GitHub repo with the relesed CRT sources, useful information, and links","title":"Resouces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#exporting-symbols-for-shared-libraries","text":"When creating a shared library, we have to specify which symbols are exported. These are the only symbols that can be directly used from the client code. This is done using special keywords. Because the keywords are different for different compilers, usually, some macros are used instead. Typically, these macros: use the correct keyword for the compiler support disabling the keyword for building static libraries or executables The macros are typically defined in a dedicated header file called export header . This file is then included in every header file that defines a symbol that should be exported. For the whole export machinery to work, we need to: create the export header file and include it in every header file that defines an exported symbol mark the symbols that should be exported with the export macro use CMake to supply the correct compiler flags used in the export header","title":"Exporting symbols for shared libraries"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#creating-the-export-header","text":"We can generate the export header file using the GenerateExportHeader module. We can get it by the following code: add_library(<target name> SHARED <files>) generate_export_header(<target name>) This will generate the export header file in the build directory. However, the export file is different for different compilers. Therefore, it is best to copy the file for each compiler and then merge the macros to create a universal export header file. Alternatively, we can use some proven export header file. Finally, we can store the export header file in the source directory and include it in every header file that defines an exported symbol.","title":"Creating the Export Header"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#marking-the-symbols","text":"Usually, we mark the following symbols for export: classes: class <export macro> MyClass{...} functions: <export macro> <return type> my_function(...) Other symbols does not have to be exported as they are automatically exported by the compiler: enums and enum classes constants templates Additionaly, only the symbols needed by external code should be exported. The exeption is when the the interface use templates. In this case, all symbols used by the template (but not the template itself) should be exported. The <export macro> from the GenerateExportHeader is named <target name>_EXPORT , we can check the exact name in the export header file.","title":"Marking the Symbols"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#cmake-configuration","text":"Now the shared library build should work correctly. However, the static library build will be full of warnings, because the export macro is not intended for static libraries. The same is true for executables. Therefore, we need to set a special property for each target that uses the export header and is not a shared library: target_compile_definitions(<static/executable target name> PUBLIC <static condition macro >) The static condition macro from the GenerateExportHeader is named <target name>_STATIC_DEFINE . If not clear, we can find the exact macros' names in the export header file.","title":"CMake Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#resouces_1","text":"tutorial on decovar","title":"Resouces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#compilation-for-a-specific-cpu","text":"","title":"Compilation for a specific CPU"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#msvc","text":"MSVC cannot compile for a specific CPU or CPU series. It can, however, use new instructions sets more efficiently if it compiles the code without the support for CPUs thad does not support these instruction sets. The command for the compiler is: `/arch: (see MSVC documentation for details). ## GCC In GCC, the march option enables compilation for a specific hardware. ml) option enables compilation for a specific hardware. pects that you use vcpkg in a per-project configuration. To make it work, add: -DCMAKE_TOOLCHAIN_FILE=<vcpkg location>/scripts/buildsystems/vcpkg.cmake - To change build options ( option in CMakeLists.txt ), run cmake with -D <option name>=<option value> <build dir> . Example: cmake -D BUILD_TESTING=OFF .","title":"MSVC"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#building","text":"For building, use: cmake --build <build dir> where build dir is the directory containing the build scripts ( CmakeFiles folder). To list the build options: cmake -L","title":"Building"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#specify-the-build-type-debug-release","text":"To build in release mode, or any other build mode except for the default, we need to specify the parameters for CMake. Unfortunately, these parameters depends on the build system: Single-configuration systems (Unix, MinGW) Multi-configuration systems (Visual Studio)","title":"Specify the build type (Debug, Release)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#single-configuration-systems","text":"Single configuration systems have the build type hardcoded in the build scripts. Therefore, we need to specify the build type for CMake when we generate the build scripts: cmake ../ -DCMAKE_BUILD_TYPE=Release By default, the build type is Release .","title":"Single-configuration systems"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#multi-configuration-systems","text":"In multi-configuration systems, the -DCMAKE_BUILD_TYPE parameter is ignored, because the build configuration is supposed to be determined when building the code (i.e., same build scripts for debug and for release). Therefore, we omit it, and instead specify the --config parameter when building the code: cmake --build . --config Release","title":"Multi-configuration systems"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#specify-the-target","text":"We can use the --target parameter for that: cmake --build . --target <TARGET NAME>","title":"Specify the target"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#clean-the-source-files","text":"Run: cmake --build . --target clean","title":"Clean the source files"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#handling-case-insensitivity","text":"Windows builds are, in line with the OS, case insensitive. Moreover, the Visual Studio does some magic with names internally, so the build is case insensitive even on VS WSL builds. The case insensitivity can bring inconsistencies that later breake Unix builds. Therefore, it is desirable to have the build case sensitive even on Windows. Fortunatelly, we can toggle the case sensitivity at the OS level using this PowerShell command: Get-ChildItem <PROJECT ROOT PATH> -Recurse -Directory | ForEach-Object { fsutil.exe file setCaseSensitiveInfo $_.FullName enable } Note that this can break the git commits, so it is necessary to also configure git in your case-sensitive repo: git config core.ignorecase false","title":"Handling Case Insensitivity"},{"location":"Programming/C%2B%2B/CMake%20Manual/","text":"Introduction \u00b6 CMake is a cross-platform build system generator that generates build scripts for various build systems (e.g., make, ninja, Visual Studio, Xcode) a name of the language used to write the configuration files for the build system generator and other related scripts. CMake can run in two modes: project mode : is the standard mode. This mode is used when the project is configured using the cmake command. Therefore, the CMakeLists.txt file is executed in this mode. script mode : is used when the cmake command is run with the -P option. In this mode, the CMakeLists.txt file is not executed, but the script specified by the -P option is executed. Main resources: CMake documentation CMake tutorial Generators \u00b6 CMake not only supports multiple platforms but also multiple build systems on each platform. These build systems are called generators . To get a list of available generators, run: cmake -E capabilities The cmake command \u00b6 documentation Configuration: Generating Build scripts \u00b6 General syntax is: cmake <source dir> Here <source dir> is the CMakeLists.txt directory. The build scripts are build in current directory. We can set any cache variable using the -D argument. Example: cmake <source dir> -D <variable name>=<variable value> # or equivalently cmake <source dir> -D<variable name>=<variable value> Toolchain file \u00b6 To work with package managers, a link to toolchain file has to be provided as an argument. For vcpkg , the argument is as follows: # new version cmake <dir> --toolchain <vcpkg location>/scripts/buildsystems/vcpkg.cmake # old version cmake <dir> -DCMAKE_TOOLCHAIN_FILE=<vcpkg location>/scripts/buildsystems/vcpkg.cmake Note that the toolchain is only loaded at the beginnning of the generation process. Once you forgot it, you need to delete the build scripts diectory content to make this argument work for subsequent cmake commands. Other Usefull arguments \u00b6 -B <build dir> to specify the build directory. By default, the build directory is the current directory. -LH to see cmake nonadvanced variables together with the description. -LHA to see also the advanced variables. Note that this prints only cached variables, to print all variables, we have to edit the CmakeLists.txt. --graphviz=<file name> to generate a graphviz file of the dependencies. Note that only visible CMake dependencies are included Legacy arguments \u00b6 -H : to specify the source directory (where the CMakeLists.txt file is located). Now it is specified as the positional argument or using -S . Building \u00b6 For building, use: cmake --build <build dir> where build dir is the directory containing the build scripts ( CmakeFiles folder). To list the build options: cmake -L Specify the target \u00b6 By default, all targets are built. We can use the --target to specify a single target: cmake --build . --target <TARGET NAME> There is also a special target all that builds all targets, which is equivalent to not specifying the --target argument. Specify the build type (Debug, Release) \u00b6 In CMake, we use a specific build type string instead of compiler and linker flags: Debug - Debug build Release - Release build RelWithDebInfo - Release build with debug information MinSizeRel - Release build with minimal size Unfortunately, the way how the build type should be specified depends on the build system: Single-configuration systems (GCC, Clang, MinGW) Multi-configuration systems (MSVC) Single-configuration systems \u00b6 Single configuration systems have the compiler flags hardcoded in the build scripts. Therefore, we need to specify the build type for CMake when we generate the build scripts: cmake ../ -DCMAKE_BUILD_TYPE=Release By default, the build type is an empty string . This means that no extra flags are added to the compiler and linker so the compiler and linker run with their default settings. Interesting info can be found in this SO question . Multi-configuration systems \u00b6 In multi-configuration systems, the -DCMAKE_BUILD_TYPE parameter is ignored, because the build configuration is supposed to be determined when building the code (i.e., same build scripts for debug and for release). Therefore, we omit it, and instead specify the --config parameter when building the code: cmake --build . --config Release Rebuild or Clean the build binaries \u00b6 To rebuild, we use the --clean-first argument: cmake --build . --clean-first If we want just to clean the build binaries without building, we use the clean target: cmake --build . --target clean Install \u00b6 To be able to install the project, it needs to be configured to do so. For this, check the installation configuration . To install the project, run: cmake --install <build dir> Note that the project needs to be built first. If it is not, we can build and install in one step using the --build with the --target install argument: cmake --build <build dir> --target install Usually, we want to use a different directory when testing the installation. To do that, we need to configure the project with the CMAKE_INSTALL_PREFIX variable. Example: cmake -DCMAKE_INSTALL_PREFIX=<test install dir> <source dir> A note for Windows installations : The default CMAKE_INSTALL_PREFIX is C:/Program Files (x86)/<project name> , even if the project is a 64-bit project. To override this, configure the project with the following argument: cmake <other arguments> -A x64 CMake command-line tools \u00b6 documentation Apart from standard commands listed in previous sections, CMake provides several command-line tools that are not directly related to the build process. These tools wrap the system commands so that we are able to use them in a cross-platform way. To run these tools, execute: cmake -E <tool name> <arguments> The most useful tools are: copy - copy files and directories capabilities - print the properties of the system related to the build process Copy tool \u00b6 The copy tool has two signatures: copy <source> <destination> copy -t <destination> <source> ( only available in CMake 3.26 and later ) Here, <source> can be a directory, a file, or a list of files. The <destination> can be a directory or a file. Execute a CMake script \u00b6 To execute a CMake script, use the cmake -P <script> command. Syntax \u00b6 Variables \u00b6 In order to use a variable in the CMakeLists.txt file, we have to use the ${} syntax: message(STATUS \"dir=${dir}\") In conditions, we can use the variable using its name: if(DEFINED <name>) ... Variables are set using the set command: set(<variable name> <variable value>) Variable types \u00b6 CMake has two types of variables: string variables : the most common type of variable. A string variable is created if the <variable value> is a single word or a quoted string. Example: cmake set(dir \"C:/Program Files\") list variables : a list variable is created if the <variable value> is a list of words. Example: cmake set(dirs \"C:/Program Files\" \"C:/Program Files (x86)\") Enviromental variables \u00b6 We can use environmental variables using the ENV variable: if(DEFINED ENV{<name>}) ... Be aware that in string, we use only one pair of curly braces (see variable references manual ): message(STATUS \"dir=$ENV{dir}\") Built-in variables \u00b6 documentation There are some variable generated by default by CMake. Howver, these differ between the script and config modes. Variables available in both modes \u00b6 CMAKE_SOURCE_DIR : the directory where the top level CMakeLists.txt file is located. This variable is rarely the right one, because our project may be used as a subproject in another project. in the script mode, this variable is set to the directory where the script is located. CMAKE_CURRENT_SOURCE_DIR : the directory where the currently processed CMakeLists.txt file is located. in the script mode, this variable is set to the directory where the script is located. CMAKE_BINARY_DIR : the top level build directory. Most of the time, it is appropriate to use CMAKE_CURRENT_BINARY_DIR instead of this variable. in the script mode, this variable is set to the directory where the script is located. CMAKE_CURRENT_BINARY_DIR : the directory where the build scripts for the current target are located and where the build process is executed. For some generators, this is also the directory where the binaries are stored. in the script mode, this variable is set to the directory where the script is located. Config mode variables \u00b6 CMAKE_CURRENT_LIST_DIR : the directory where the currently processed CMakeLists.txt file is located. PROJECT_SOURCE_DIR : the nearest directory up in the directory tree where the CMakeLists.txt with the project command is located. CMAKE_<LANG>_COMPILER_ID : the compiler ID for the language <LANG> . CMAKE_<LANG>_COMPILER_VERSION : the compiler version for the language <LANG> . CMAKE_PROJECT_NAME : the name of the top level project. Most of the time, it is appropriate to use PROJECT_NAME instead of this variable. PROJECT_NAME : the name of the project. MSVC : set to TRUE if the compiler is MSVC. Script mode variables \u00b6 CMAKE_CURRENT_LIST_DIR : the directory where the currently processed script is located Unix installation directories \u00b6 There are also variables for installation directories typical for Unix systems. Touse them, we have to include the GNUInstallDirs module . The variables have two formats: CMAKE_INSTALL_<dir> : the directory relative to the installation prefix. These variables have to be used in the install command and other commands that use the installation prefix. CMAKE_INSTALL_FULL_<dir> : the full path to the directory. Notable variables are: BINDIR : the directory for executables ( bin ) LIBDIR : the directory for libraries ( lib ) INCLUDEDIR : the directory for headers ( include ) List variables \u00b6 List variables are defined similarly to scalar variables using the set command: set(<name> <value 1> <value 2> ...) Then, we can use the list variable in: commands that accept lists (e.g., add_executable , add_library , target_link_libraries ) for loops Print all variables \u00b6 To print all variables, the following function can be used: function(dump_cmake_variables) if (ARGV0) message(STATUS \"Printing variables matching '${ARGV0}'\") else() message(STATUS \"Printing all variables\") endif() get_cmake_property(_variableNames VARIABLES) list (SORT _variableNames) foreach (_variableName ${_variableNames}) if (ARGV0) unset(MATCHED) string(REGEX MATCH ${ARGV0} MATCHED ${_variableName}) if (NOT MATCHED) continue() endif() endif() message(STATUS \"${_variableName}=${${_variableName}}\") endforeach() message(STATUS \"Printing variables - END\") endfunction() To print all variables related to HDF5 lib, call dump_cmake_variables(HDF) after the find_package call. The option command \u00b6 The option command is used to define a boolean variable that can: be set by the user using the -D argument when running the cmake command, have a default value, have a description that will be printed when the variable is set, and set a cache variable (see CMake cache ). The syntax is: option(<option name> <option description> <default value>) The behavior of the option command is as follows: If variable is already set (either a cache variable or a normal variable), the option command is ignored. Otherwise, a cache variable is created if we are in the project mode, and a normal variable is created if we are in the script mode. Control structures \u00b6 if \u00b6 The if command has the following syntax: if(<condition>) ... elseif(<condition>) ... else() ... endif() The condition can be: a variable an expression Each expression can use some of the supported operators: logical operators: AND , OR , NOT comparison operators: EQUAL , LESS , GREATER , LESS_EQUAL , GREATER_EQUAL , STREQUAL , STRLESS , STRGREATER , STRLESS_EQUAL , STRGREATER_EQUAL file operators: EXISTS , IS_DIRECTORY , IS_REGULAR_FILE , IS_SYMLINK , IS_ABSOLUTE , IS_RELATIVE , IS_NEWER_THAN , IS_OLDER_THAN string operators: MATCHES , LESS , GREATER , LESS_EQUAL , GREATER_EQUAL , STREQUAL , STRLESS , STRGREATER , STRLESS_EQUAL , STRGREATER_EQUAL The MATCHES operator requires a regex, not a simplified filesystem filter pattern. See the regex documentation for more. version operators: VERSION_EQUAL , VERSION_LESS , VERSION_GREATER , VERSION_LESS_EQUAL , VERSION_GREATER_EQUAL these are ment to be used with version string variables created by the find_package command: cmake find_package(<package name> CONFIG REQUIRED) if(<package name>_VERSION VERSION_LESS <version>) ... endif() and more... For the full list of operators, see the if command documentation . foreach \u00b6 The foreach command has the following syntax: foreach(<variable name> <items>) ... endforeach() Generator expressions \u00b6 Manual Generator expressions are a very useful tool to control the build process based on the build type, compiler type, or similar properties. CMake use them to generate mutliple build scripts from a single CMakeLists.txt file. The syntax for a basic condition expression is: \"$<$<condition>:<this will be printed if condition is satisfied>>\" Unlike, variables, the generator expressions are evaluated during the build process, not during the configuration process . Therefore, they cannot be dumped during the configuration process, and they cannot be used in the if command. However, they can be still used in variables and commands, if the evaluation is not needed during the configuration process. Notable variable expressions: $<TARGET_FILE_DIR:<target name>> - the directory where the target will be built $<TARGET_RUNTIME_DLLS:<target name>> - the list of runtime dependencies of the target note that this property is only available (and necessary) for MSBuild generator and it is only available in the POST_BUILD phase. Evaluating generator expressions during configuration \u00b6 In case we need to see the evaluated generator expressions during cmake configuration, we can try to cheat using the following command: file(GENERATE OUTPUT <filename> CONTENT <string-with-generator-expression>) This way, we receive the evaluated value of the generator expression in the file for one of the build configurations. File operations \u00b6 To perform file operations, use the file command. The most useful subcommands are: MAKE_DIRECTORY <directory> - create a directory RENAME <from> <to> - renames/moves a file or directory. The <from> must exist, and the parent directory of <to> must exist. REMOVE <file> - remove a file REMOVE_RECURSE <directory> - remove a directory and all its content Path operations \u00b6 Path operations are performed using the cmake_path command. The sytax for this command varies based on the subcommand. Path decomposition \u00b6 The syntax for the path decomposition is: cmake_path(GET <path> <path part> <output variable>) Here, <path part> can be: PARENT_PATH - the parent directory of the path List operations \u00b6 For list operations, use the list command. The most useful subcommands are: APPEND - append an element to the list: list(APPEND <list name> <elements>) JOIN - join the list elements into a string: list(JOIN <list name> <separator> <output variable>) Functions \u00b6 Functions are defined using the function command. The syntax is: function(<function name> <argument 1> <argument 2> ...) ... endfunction() This way, we have a function with simple positional arguments. These arguments can be used in the function body as variables: function(print_arguments arg1 arg2) message(STATUS \"arg1=${arg1}\") message(STATUS \"arg2=${arg2}\") endfunction() To call the function, use the following syntax: print_arguments(\"value 1\" \"value 2\") More resources: https://hsf-training.github.io/hsf-training-cmake-webpage/11-functions/index.html Named arguments \u00b6 We can notice that a typical cmake function has named arguments, e.g., add_custom_command(TARGET <target name> POST_BUILD COMMAND <command>) . To achieve this, we can use the cmake_parse_arguments command. The syntax is: function(<function name>) cmake_parse_arguments( PARSE_ARGV <positional args count> <variable prefix> <options> <one_value_keywords> <multi_value_keywords> ) Here: <positional args count> is the number of positional arguments that are skipped by the cmake_parse_arguments command, <variable prefix> is the prefix for variables created from named arguments (the name will be <variable prefix>_<variable_name> ), and <options> , <one_value_keywords> , and <multi_value_keywords> are the lists of named arguments of each type. the list have to be specified as a string divided by a semicolon. The <options> are the arguments that can be either present or not, the <one_value_keywords> are the arguments that have a single value, and the <multi_value_keywords> are the arguments that have multiple values. Default values for arguments \u00b6 There is no specific syntax for default values for arguments. We can achieve this, for example, by using the if command: if(NOT DEFINED <variable>) set(<variable> <default value>) endif() Return values \u00b6 There is a return statement in CMake, but in general, the value is returned by setting a variable with a parent scope: function(return_value) set(<return val name> <value> PARENT_SCOPE) endfunction() If we want to determine the return variable name by the caller, we have to pass the variable name as an argument: function(return_value return_var_name) set(${return_var_name} <value> PARENT_SCOPE) endfunction() Useful functions \u00b6 message \u00b6 The message command is used to print messages during the configuration process. The syntax is: message(<mode> <message>) The <mode> can be: STATUS - the message is printed as a status message WARNING - the message is printed as a warning AUTHOR_WARNING - the message is printed as an author warning SEND_ERROR - the message is printed as an error and the configuration process is stopped FATAL_ERROR - the message is printed as a fatal error and the configuration process is stopped DEPRECATION - the message is printed as a deprecation warning execute_process \u00b6 The execute_process command is used to execute an external command. The syntax is: execute_process(COMMAND <command> [OPTIONS]) Important options: RESULT_VARIABLE <variable name> : the variable that will store the result (exit code) of the command. find_program \u00b6 The find_program command is used to find an executable in the system. It's adventages over providing our own path/logic is: it is cross-platform - it automatically searches for the executable with the correct extension for the current platform it can be configured to raise an error if the executable is not found it can automatically search for the executable in the system paths The syntax is: find_program(<variable name> <executable name> [OTHER_ARGUMENTS]) The full path to the executable is stored in the <variable name> variable. The OTHER_ARGUMENTS are: REQUIRED : if the executable is not found, the configuration process is stopped HINTS <path> , PATHS <path> : the path where the executable should be searched. HINTS and PATHS are equivalent exept the priority: HINTS are searched before the standard paths, PATHS are searched at the end with the lowest priority. CMakeLists.txt \u00b6 The CMakeLists.txt file is the main configuration file for any CMake project. This file is executed during the configuration step (when the cmake command is run without arguments specifying another step). It contains commands written in the CMake language that are used to configure the build process. The typical structure of the CMakeLists.txt file is as follows: Top section contains project wide setting like name, minimum cmake version, and the language specification. Targets sections containing: the target definition together with sources used target includes target linking Typical Top section content \u00b6 The typical content of the top section is: minimum cmake version: cmake_minimum_required(VERSION <version>) project name and version: project(<name> [VERSION <version>]) language specification: enable_language(<language>) cmake variables setup, e.g.: set(CMAKE_CXX_STANDARD <version>) compile options: add_compile_options(<option 1> <option 2> ...) cmake module inclusion: include(<module name>) Language standards \u00b6 The language standard is set using the set command together with the CMAKE_<LANG>_STANDARD variable. Example: set(CMAKE_CXX_STANDARD 17) This way, the standard is set for all targets and the compiler should be configured for that standard. However, if the compiler does not support the standard, the build script generation continues and the failure will appear later during the compilation. To avoid that, we can make the standard a requirement using the set command together with the CMAKE_<LANG>_STANDARD_REQUIRED variable. Example: set(CMAKE_CXX_STANDARD_REQUIRED ON) However, the required standard is not always correctly supported by the compiler (e.g., GCC up to version 13 does not support C++20). Therefore, we need to specify the minimum version for these compilers: if(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 13.0.0) message(FATAL_ERROR \"GCC version must be at least 13.0.0!\") endif() Set the runtime library type for MSVC \u00b6 In MSVC, it is crucial that both the target and all its dependencies are compiled with the same (standard) runtime library type. To set the library type for the target in CMake, we use the CMKAE_MSVC_RUNTIME_LIBRARY variable. The possible values are: MultiThreadedDLL - the dynamic library (default in Release mode) MultiThreadedDebugDLL - the dynamic library with debug information (default in Debug mode) MultiThreaded - the static library MultiThreadedDebug - the static library with debug information By default, the dynamic library is used. To set the static library, use: set(CMAKE_MSVC_RUNTIME_LIBRARY \"MultiThreaded$<$<CONFIG:Debug>:Debug>\") This way, the static library with debug information is used in the Debug mode, and the static library is used in the Release mode. Note that the CMAKE_MSVC_RUNTIME_LIBRARY variable was introduced in CMake 3.15. Therefore, you have to set cmake_minimum_required(VERSION 3.15) in the CMakeLists.txt file, or set the CMP0091 policy to NEW using the cmake_policy command. Compile options \u00b6 Most of the compile options are now sets automatically based on the declarations in the CMakeLists.txt file. However, some notable exceptions exists. To set such options, we have to use the add_compile_options command: add_compile_options(<option 1> <option 2> ...) MSVC \u00b6 /permissive- to enable the strictest mode of the compiler GCC \u00b6 -pedantic-errors to report all cases where non-standard GCC extension is used and treat them as errors Linker Options \u00b6 Linker options can be set with add_link_options command. Example: add_link_options(\"/STACK: 10000000\") Dependency management \u00b6 There are many ways how to manage dependencies in CMake, for complete overview, see the documentation . Although it is possible to hard-code the paths for includes and linking, it is usually better to initialize the paths automatically using a rich set of commands cmake offers. It has the following advatages: Hardcoding the paths is error-prone, while cmake commands usually deliver correct paths It boost the productivity as we do not have to investigate where each library is installed The resulting CMakeLists.txt file is more portable And most importantly, potential errors concerning missing libraries are reported prior to the compilation/linking . Most of the libraries have CMake support, so their CMake variables can be initialized simply by: calling the find_package command described below, or, if it is a simple header-only library using the find_path command For packages without the CMake support, we have to use lower-level cmake commands like find_path or find_libraries . For convinience, we can put these command to our own Find<name> script that can be used by multiple project or even shared. How to recognize a CMake package? \u00b6 CMake packages have either: their own cmake config (cmake-aware libs usually installed through the package manager like vcpkg ) or they have a Find<package name> script created by someone else that heuristically search for the packege (The default location for these scripts is CMake/share/cmake-<version>/Modules ). Standard way: find_package \u00b6 The find_package command is the primary command for dependencies. It tries to find the correct variables for a library. The command sets: the <PackageName>_FOUND variable to TRUE or 1 if the package is found include paths linking paths platform/toolchain specific enviromental variables There are two modes of operation for the command module mode , which uses the Find<library name> cmake scripts, typically provided not by the library developers, but somebody else who wants the libraries to be accessible by CMake. All modules provided by CMake itself are listed in the documentation . config mode which uses CMake scripts with name <PackageName>Config.cmake or <lowercasePackageName>-config.cmake provided by the developers of the package. They are typically distributed with the source code and downloaded by the package manager. To find out which of the operation modes is used, the find_package command uses the following logic: if the MODULE parameter is used, the module mode is used if the CONFIG or No_MODULE parameter is used, the config mode is used if some parameters from the full (advanced) signature of the find_package command are used (e.g.: NAMES ), the config mode is used otherwise, by default, the module mode is used with the fallback to the config mode if the module is not found Config mode \u00b6 Config packages are CMake modules that were created as cmake projects by their developers. They are therefore naturally integrated into Cmake. The configuration files are executed as follows: Package version file: <package name>-config-version.cmake or <package name>ConfigVersion.cmake . This file handles the version compatibility, i.e., it ensures that the installed version of the package is compatible with the version requested in the find_package command. Package configuration file: <package name>-config.cmake or <package name>Config.cmake . The process of searching for these files is very complex. For the full description, see the documentation . The most important steps are: Search the directory specified by the CMAKE_FIND_PACKAGE_REDIRECT_DIR variable. Typically, this variable is set to <build dir>/CMakeFiles/pkgRedirects . Search specified subdirectories of a <prefix path> . Multiple <prefix path> variables are considered in the following order: Package-specific prefix paths, in the following order: <PackageName>_ROOT CMake variable <PACKAGENAME>_ROOT} CMake variable <PackageName>_ROOT environment variable <PACKAGENAME>_ROOT} environment variable Prefix paths specified in CMake cache variables, in the following order: CMAKE_PREFIX_PATH CMake variable CMAKE_FRAMEWORK_PATH CMake variable CMAKE_APPBUNDLE_PATH CMake variable Prefix paths specified in CMake environment variables, in the following order: <PackageName>_DIR CMake variable CMAKE_PREFIX_PATH environment variable CMAKE_FRAMEWORK_PATH environment variable CMAKE_APPBUNDLE_PATH environment variable And much more... Module mode \u00b6 Module packages are packages that are not cmake projects themselves, but are hooked into cmake using custom find module scrips. These scripts are automatically executed by find_package . The find module script is named Find<package name>.cmake . The find_package command searches for these scripts in: the CMAKE_MODULE_PATH directories, and then int the CMake installation, e.g.: CMake/share/cmake-3.22/Modules/Find<package name>.cmake . Searching for include directories with find_path \u00b6 The find_path command is intended to find the path (e.g., an include directory). It is either used for non-CMake packages or for header-only libraries. A simple syntax is: find_path( <var name> NAMES <file names> ) Here: <var name> is the name of the resulting variable <file names> are all possible file names split by space. At least one of the files needs to be present in a path for it to be considered to be the found path. By default, only some default paths are considered. To consider other paths, we have some options: PATHS <paths> , where <paths> are full candidate paths split by space PATH_SUFFIXES <relative paths> , where <relative paths> are relative paths that will be appended to all paths considered. Important parameters: REQUIRED : if the path is not found, the configuration process is stopped. Similar to the REQUIRED parameter of the find_package command. Versions \u00b6 We may specify the version of package by using the VERSION parameter. The syntax is: VERSION <minimum version> VERSION <minimum version>...<maximum version> The version here is the cmake version used in the cmake version file (i.e., <package name>-config-version.cmake or <package name>ConfigVersion.cmake ) of the package. Typically, this is the project version of the package, specified in the project command. Low level command: find_library \u00b6 The find_library command is used to populate a variable with a result of a specific file search optimized for libraries. The search algorithm works as follows: ? Search package paths order: <CurrentPackage>_ROOT , ENV{<CurrentPackage>_ROOT} , <ParentPackage>_ROOT , ENV{<ParentPackage>_ROOT} this only happens if the find_library command is called from within a find_<module> or find_package this step can be skipped using the NO_PACKAGE_ROOT_PATH parameter Search path from cmake cache. During a clean cmake generation, these can be only supplied by command line. Considered variables: CMAKE_LIBRARY_ARCHITECTURE CMAKE_PREFIX_PATH CMAKE_LIBRARY_PATH CMAKE_FRAMEWORK_PATH this step can be skipped using the NO_CMAKE_PATH parameter Same as step 3, but the variables are searched among system environmental variables instead this step can be skipped using the NO_CMAKE_ENVIRONMENT_PATH parameter Search paths specified by the HINTS option Search the standard system environmental paths variables considered are LIB and PATH this step can be skipped using the NO_SYSTEM_ENVIRONMENT_PATH parameter Search in system paths Considered variables: CMAKE_LIBRARY_ARCHITECTURE CMAKE_SYSTEM_PREFIX_PATH CMAKE_SYSTEM_LIBRARY_PATH CMAKE_SYSTEM_FRAMEWORK_PATH this step can be skipped using the NO_CMAKE_SYSTEM_PATH parameter Search the paths specified by the PATHS option Searching for libraries in the project dir \u00b6 Note that the project dir is not searched by default. To include in the search, use: HINTS ${PROJECT_SOURCE_DIR} . Full example on the Gurobi lib stored in <CMAKE LISTS DIR/lib/gurobi_c++.a> : find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED ) Creating a custom find script \u00b6 The structure of a simple find scripts is described in the documentation . We can either put the find script to the default location, so it will be available for all projects, or we can put it in the project directory and add that directory to the CMAKE_MODULE_PATH : list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\") The usual structure of the find script is: the comment section describing the file: #[=======================================================================[.rst: FindMOSEK ------- Finds the MOSEK library. Result Variables ^^^^^^^^^^^^^^^^ This will define the following variables: ``MOESK_FOUND`` True if the system has the MOSEK library. ``MOSEK_INCLUDE_DIRS`` Include directories needed to use MOSEK. ``MOSEK_LIBRARIES`` Libraries needed to link to MOSEK. #]=======================================================================] find commands that fils some temp variables: find_path( MOSEK_INCLUDE_DIR NAMES mosek.h PATHS \"$ENV{MOSEK_HOME}/h\" ) find_library( MOSEK_LIBRARY NAMES libmosek64.so.10.0 libfusion64.so.10.0 PATHS \"$ENV{MOSEK_HOME}/bin\" ) handeling of the result of the file commands. The standard approach is: include(FindPackageHandleStandardArgs) find_package_handle_standard_args(MOSEK FOUND_VAR MOSEK_FOUND REQUIRED_VARS MOSEK_LIBRARY MOSEK_INCLUDE_DIR ) setting the final variables: if(MOSEK_FOUND) set(MOSEK_LIBRARIES ${MOSEK_LIBRARY}) set(MOSEK_INCLUDE_DIRS ${MOSEK_INCLUDE_DIR}) endif() Downolad dependencies during configuration and build them from source \u00b6 To download and build dependencies during the configuration, we can use the FetchContent module. This way, the sources are downloaded at the configuration step, and built during the build step. Note that because of this, the usage of these dependencies is limited, we cannot, for example, run the dependency tools at configuration time. For details, see the dependency management documentation . The usual workflow is: Download the dependency using the FetchContent_Declare command cmake FetchContent_Declare( <NAME> <SPECIFICATION> DOWNLOAD_EXTRACT_TIMESTAMP ON ) The <SPECIFICATION> can be: URL <url> with and optional HASH <hash> GIT <git repository> with an optional TAG <tag> or BRANCH <branch> or COMMIT <commit> The DOWNLOAD_EXTRACT_TIMESTAMP option ensures that the timestamp of the downloaded files is preserved. This is useful when the dependency is downloaded multiple times, and we want to avoid unnecessary rebuilds. Note that this option shoud be placed before the URL option . This is because it was introduced in CMake 3.21, so if it is placed after the URL and the CMake version is lower than 3.21, the FetchContent_Declare command will fail as the option will be considered a part of the URL. Configure the dependency using the FetchContent_MakeAvailable command: cmake FetchContent_MakeAvailable(<NAME>) If the dependency is a CMake project, the FetchContent_MakeAvailable command will automatically configure the project by calling the add_subdirectory command with the path to the downloaded source code. Both the dependency source code and the build directory are stored in the build directory in _deps folder. This directory contains: <NAME>-src - the source code of the dependency <NAME>-build - the build directory of the dependency CMake Targets \u00b6 Targets define logical units of the build process. These can be: executables libraries custom targets doing all sorts of things sets of targets, i.e., aliases for building multiple targets at once Available targets are either user-defined or automatically generated by CMake. Executable targets \u00b6 The target definition is done using the add_executable command. The syntax is: add_executable(<target name> <source file 1> <source file 2> ...) The target name is used to refer to the target in other commands. The target name is also used to name the output file. The list of source files should contain all the source files that are needed to build the target. There are some automatic mechanisms that can be used to add the source files (discussed e.g. on cmake forums ), but they are not recommended. Library targets \u00b6 Library targets are defined using the add_library command. The syntax is: add_library(<target name> <type> <source file 1> <source file 2> ...) Object libraries \u00b6 Manual CMake can define object libraries , which are similar to source files, just pre-compiled. The syntax is: add_library(<object librarytarget name> OBJECT <source file 1> <source file 2> ...) Later, we can use object libraries as dependencies of other targets using target_sources command or specify them instead of source files in the add_library or add_executable commands. To specify the object library, we can use the expression $<TARGET_OBJECTS:<object library target name>> . Targets automatically generated by CMake \u00b6 listed here . Besides the user-defined targets, CMake automatically generates some targets. These are: all : alias for building all targets. Default target if the --target argument is not specified. In Visual Studio and Xcode generators, this target is called ALL_BUILD . we can exclude some targets from the all target using the EXCLUDE_FROM_ALL target property or the EXCLUDE_FROM_ALL directory property . clean target that cleans the build directory install target that installs the project. It depends on the all target. this target is only available if the CMakelists.txt file contains the install command (see installation configuration ). and some more... Set properties for a target \u00b6 To set compile options for a target, use the target_compile_definitions command. The syntax is: target_compile_definitions(<target name> <SCOPE> <definition 1> <definition 2> ...) Include directories \u00b6 Typically, we only need to include the headers for non-standard packages , as the standard cmake config packages include the headers automatically from the packege config files that are read by the find_package command. To include the headers, we need to use a inlude_directories (global), or better target_include_directories command. The difference: target specification : target_include_directories specifies the include directories for a specific target, while include_directories specifies the include directories for all targets in the current directory. mode specification : target_include_directories specifies the mode of the include directories (e.g., PUBLIC , PRIVATE , INTERFACE ), while include_directories behaves simillar to PRIVATE . Therefore, for libraries, the target_include_directories has to be used. Debugging the include directories \u00b6 If the headers are not found during the compilation of a CMake target, there are several steps we should take to debug the issue: Locate the missing header file. If the file does not exist, the package may not be installed at all. Check that the include in the source code match the header location. Sometimes, the package is moved into a subdirectory, without notification (e.g., we have to change #include <header.h> to #include <package/header.h> ). Check the include paths for the problematic target: cmake message(STATUS \"Listing include directories for DARP-benchmark target:\") get_target_property(DARP-benchmark-include-test DARP-benchmark INCLUDE_DIRECTORIES) foreach(dir ${DARP-benchmark-include-test}) message(STATUS \"dir='${dir}'\") endforeach() Inspect the global include directories \u00b6 All the global include directories are stored in the INCLUDE_DIRECTORIES property, to print them, add this to the CMakeLists.txt file: get_property(dirs DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} PROPERTY INCLUDE_DIRECTORIES) foreach(dir ${dirs}) message(STATUS \"dir='${dir}'\") endforeach() Linking configuration \u00b6 For linking, use the target_link_libraries command. The general syntax is: target_link_libraries(<target name> <library 1> <library 2> ...) We can use multiple commands for a single target: target_link_libraries(<target name> <library 1>) target_link_libraries(<target name> <library 2>) Make sure that you always link against all the libraries that are needed for the target to work! Do not rely on the linker errors, these may not appear due to library preloading, indirect linkage, advanced linker heuristics, etc. The result is that on one machine the code will work, but on another, it will fail. To find out if and how to link against a library, refer to the documentation of the library. The <library> can be: a library target name : The name of the target that is defined using the add_library command. this is the way how to link against vcpkg libraries check the usage file of the library for the correct target name a full path to the library file a library name : The name of the library without any prefix (e.g., -l ) or suffix (e.g., .a , .so , .dll ). this works for the system libraries and other libraries that are in the system path a link flag : A flag that is passed to the linker. for libraries integrated with the linker a generator expression : A generator expression that is evaluated during the CMakelists.txt configuration to one of the above options. Note that cmake does not check the validity of the supplied <library> argument! . Always check the documentation of the library to find out how to link against it. If we want to double-check that the supplied <library> is valid, we can: for the library target name, check the presence of the target: cmake if(NOT TARGET <library>) message(FATAL_ERROR \"The target <library> does not exist!\") endif() Checking the full path to a linked library \u00b6 Sometimes, it can be usefull to check the full path to a linked library. This can be done using the get_target_property command: get_target_property(LIB_PATH <library name> IMPORTED_LOCATION) message(STATUS \"LIB_PATH=${LIB_PATH}\") Fixing incorrect linking configuration in upstream projects \u00b6 Sometimes, an upstream project may incorrectly link a library as a private dependency, despite the fact that it is needed at runtime by the downstream project. We can fix it by set up the linking in our project: target_link_libraries(<upstream target> INTERFACE <library name>) Of course, the <library name> has to be known, i.e., we may need to also add the appropriate find_package command to our project. Handling runtime dependencies in the output directory \u00b6 When linking dynamically (e.g., by using dynamic toochain, which is default on Windows), the runtime dependencies must be available at runtime, otherwise the program will not run. On Windows, this is usually solved by copying the runtime dependencies to the output directory. There are several ways how to do that. when using vcpkg , the runtime dependencies are copied automatically if the VCPKG_APPLOCAL_DEPS variable is set to ON otherwise, we can copy the runtime dependencies using the add_custom_command command or we can just manually copy the runtime dependencies to the output directory Copying runtime dependencies using add_custom_command \u00b6 We can use the add_custom_command command together with the $<TARGET_RUNTIME_DLLS:<target name>> generator expression. Be careful to wrap this code by a condition that checks the generator type, as the generator expression is only available for DLL-aware generators. if(CMAKE_GENERATOR MATCHES \"Visual Studio.*\") add_custom_command( TARGET <target name> POST_BUILD COMMAND ${CMAKE_COMMAND} -E copy $<TARGET_RUNTIME_DLLS:<target name>> $<TARGET_FILE_DIR:<target name>> COMMAND_EXPAND_LISTS ) endif() Here, the COMMAND_EXPAND_LISTS is used to expand the generator expressions in the command. We use it for the $<TARGET_RUNTIME_DLLS:<target name>> generator expression that returns the list of runtime dependencies of the target. Runtime dependencies and subdirectories \u00b6 When using the add_subdirectory command, a new build directory is created for the subdirectory. However, the runtime dependencies in the parent directory are not accessible and therefore it must be duplicated in the subdirectory ( VCPKG_APPLOCAL_DEPS handles this automatically). One can think that we can solve that by using the same output directory for the parent and the subdirectory (using the build dir parameter of the add_subdirectory command ). However, this is prohibited by CMake. Installation Configuration \u00b6 Importing and Exporting Guide To enable installation, we have to provide several commands and do some adjustments in the CMakeLists.txt file. Specific steps depends on what we want to achieve. The minimal installation that installs only binaries and headers can be set up using two commands: Install binaries with the install(TARGETS... command. The basic syntax is: cmake install(TARGETS <target name 1> <target name 2>) Install headers with the install(FILES... command. The basic syntax is: cmake install(FILES <file 1> <file 2> DESTINATION <destination>) We have to install both public API headers and their internal dependencies. Usually, this means installing most of the headers. The <destination> is the directory where the files will be installed. Typically, it is ${CMAKE_INSTALL_INCLUDEDIR}/${PROJECT_NAME} for header files. We usually only want a single set of headers to be installed for both Debug and Release builds ( for vcpkg, this is required ). This can be achieved by using the CONFIGURATIONS parameter of the install command. Example: cmake install( FILES <file 1> <file 2> DESTINATION <destination> CONFIGURATIONS Release ) These commands provide the install target that builds all targets (it depends on the all target) and then installs the project, which means that it copies the binary files to the installation directory. Install CMake files \u00b6 If we want our library to be used from CMake projects, the basic configuration is not enough. We have to provide the CMake configuration files that will be used by the find_package command when searching for the dependency. Usually, CMake packages have the following cmake files: <package name>Targes.cmake - contains the targets that are installed by the package <package name>Config.cmake - contains the configuration of the package, i.e., the include directories, linking directories, and other variables that are needed to use the package. <package name>ConfigVersion.cmake - contains the version of the package and the compatibility check. Installing the Targets file \u00b6 This involves two steps: use the EXPORT parameter of the install(TARGETS... command to create a reference to installed targets that we can use further cmake install(TARGETS <target name 1> <target name 2> EXPORT <export name> ) The <export name> is the name of the export set that will be used in the CMake configuration file. Typically, it is ${PROJECT_NAME}Targets . use the install(EXPORT... command to install the cmake files cmake install(EXPORT <export name> DESTINATION <destination> NAMESPACE <namespace> ) The <export name> is the name of the export set that was used in the install(TARGETS... command The <destination> is the directory where the CMake configuration files will be installed. Typically, it is ( source ): ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME} for binaries share/cmake/${PROJECT_NAME} for header-only libraries for vcpkg : share/${PROJECT_NAME} . Otherwise, we need to mess with the CONFIG_PATH parameter of the vcpkg_cmake_config_fixup function. for using the CMAKE_INSTALL_LIBDIR variable, we have to include the GNUInstallDirs module using the include command. The <namespace> is the namespace that will be used in the CMake configuration file, and later in the target_link_libraries command of the dependent project. It is not required, but it is recommended because the namespace with :: only searches between the exported targets, preventing a possible name clash with some library installed in the system. typically, it has the form of <project name>:: The package configuration file and the version file \u00b6 official guide - package config file In order for these files to be portable, they should be generated. The appropriate functions for this are in the `CMakePackageConfigHelpers module , which has to be included. The package configuration file \u00b6 To generate the package configuration file: create the input file somewhere in the project directory with the name <package name>Config.cmake.in and fill it with the following content: ```cmake @PACKAGE_INIT@ include (\"${CMAKE_CURRENT_LIST_DIR}/ Targes.cmake\") optional content ... - The `@PACKAGE_INIT@` is a placeholder that will be replaced by the `write_basic_package_version_file` command. 1. then generate the package configuration file by ading the `configure_package_config_file` command to the `CMakeLists.txt` file: cmake configure_package_config_file( INSTALL_DESTINATION ) `` - here, the <cmake files installation path> is the same as the parameter in the install(EXPORT...` command. The version file \u00b6 official guide - version file The version file is generated using the write_basic_package_version_file command. The syntax is: write_basic_package_version_file( <version file build path> VERSION <version> COMPATIBILITY AnyNewerVersion ) The <version file build path> is the path where the version file will be generated. The <version> is the version of the package. Typically, it is the version of the project: ${<project name>Major}.${<project name>Minor}.${<project name>Patch} . Finally, to install both files, we use the install(FILES... command: install(FILES <path to package config file in the build dir> <path to version file in the build dir> DESTINATION <cmake files installation path> ) Sanitation of the public include directories \u00b6 Additionally, an extra step is needed in case we define include directories to be accessible from other projects (i.e., PUBLIC or INTERFACE mode). In this case, we have to use a special configuration for those directories so that a different path is used when the library is installed. For example, if we include the src directory using: target_include_directories(<target name> PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/src) we have to change it to: target_include_directories(<target name> PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/src> $<INSTALL_INTERFACE:include> ) Handling dependencies \u00b6 If our library depends on other libraries, the targets depending on our library may also need to link against these dependencies. The linking itself is automatically handled by CMake. However, we need to provide the variables that are needed for the in the package configuration file ( <packag name.cmake.in ). To do so, we use the find_dependency command from the `CMakeFindDependencyMacro module : @PACKAGE_INIT@ include(CMakeFindDependencyMacro) find_dependency(<dependency 1>) ... find_dependency(<dependency n>) include(\"${CMAKE_CURRENT_LIST_DIR}/<package name>Targets.cmake\") ... The name of the dependency is the name of the package that is used in the find_package command in the CMakeLists.txt file. Other parameters ( REQUIRED , CONFIG , etc.) are not needed as they are inherited from the find_package command. Specify the targets to install \u00b6 The install command has a parameter TARGETS that can specify the targets that should be installed. The syntax is: However, the TARGETS parameter only affects the installation part of the install target . In other words, it determines what files are copied to the installation directory, but it does not affect the build process. Therefore, all targets are built regardless of the TARGETS parameter . To prevent the building of some targets when installing, we have several options: exclude selected targets from the all target using the EXCLUDE_FROM_ALL target property or the EXCLUDE_FROM_ALL directory property . disable building for the install target and use a custom wrapper target that builds only the targets that should be installed and then calls the install target. Selecting the targets to build using the EXCLUDE_FROM_ALL property \u00b6 For user targets added by the add_executable or add_library command, we can use the EXCLUDE_FROM_ALL target property: add_executable(my_target EXCLUDE_FROM_ALL <source file 1> <source file 2> ...) For targets added by the FetchContent_MakeAvailable command. The situation is more complicated. The FetchContent_Declare command provides the EXCLUDE_FROM_ALL option, but it does not work as expected. Instead, we have to set the EXCLUDE_FROM_ALL property for the target after the FetchContent_MakeAvailable command: FetchContent_Declare( <declaration name> ... ) FetchContent_MakeAvailable(<declaration name>) set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_ALL TRUE) Disabling the building of the install target and using a custom wrapper target \u00b6 For this solution,we first need to disable the building of the install using: set(CMAKE_SKIP_INSTALL_ALL_DEPENDENCY true) . Then, we need to create a target that: a) builds the selected targets, b) calls the install target. CMAKE_SKIP_INSTALL_ALL_DEPENDENCY documentation Support both shared and static libraries \u00b6 Sometimes, we want the user to be able to choose between shared and static libraries when installing the package. For vcpkg, this is required. To add this support, we have to: define option for choosing the shared or static library (most likely, we do not want to install both) install the correct library target based on the option install the export header file (in both cases, as we use the same headers for both shared and static libraries) The conditional installation can look like this: if(<package name>_BUILD_SHARED_LIBS) set_target_properties(<static lib target> PROPERTIES EXCLUDE_FROM_ALL TRUE) install(TARGETS <shared lib target> EXPORT <export name>) else() set_target_properties(<shared lib target> PROPERTIES EXCLUDE_FROM_ALL TRUE) set_target_properties(<static lib target> PROPERTIES EXPORT_NAME <shared lib target>) # this aligns the name of the exported targets install(TARGETS <static lib target> EXPORT <export name>) endif() Here, the set_target_properties command is used to exclude the other target from the all target, effectively preventing it from being built during the installation. The install command is then used to install the correct target. Finally, we just copy the export header file to the installation directory: install(FILES ${CMAKE_CURRENT_BINARY_DIR}/<export header file> DESTINATION <install include dir>) Support both Debug and Release builds \u00b6 In vcpkg, the Debug and Release builds are separated automatically, so the side-by-side installation of Debug and Release builds is supported out of the box. However, for other installations, e.g., when installing the package using the CMake install command, we have to take care of this ourselves. The standard way to do that is to add a postfix to all targets so that the debug binaries can be distinguished from the release binaries and one installation does not overwrite the other. For that, set the DEBUG_POSTFIX property for each target: set_target_properties( <target name> PROPERTIES DEBUG_POSTFIX <postfix> ) Usually, the postfix is d is used for debug builds. When using this postfix, no further configuration is needed, the correct binary will be used in the client depending on its own configuration. Installation of executables \u00b6 Executables are installed just like libraries. There are two differences: we do not need to export the executables, so we skip the EXPORT parameter of the install(TARGETS... command on windows, when linking the executable to shared libraries and not using vcpkg, we have to manually copy the runtime dependencies to the installation bin directory. As this is done after build, we can use the TARGET_RUNTIME_DLLS generator expression: cmake install(FILES $<TARGET_RUNTIME_DLLS:<target name>> DESTINATION ${CMAKE_INSTALL_BINDIR}) Specific configuration for frequentlly used libraries \u00b6 for google test, we want to prevent the installation of the gtest targets. To do that, turn it off before the gtets config in the CMakeLists.txt file: ```cmake GOOGLE TEST \u00b6 do not install gtest \u00b6 set(INSTALL_GTEST OFF) include(FetchContent) FetchContent_Declare( googletest URL https://github.com/google/googletest/archive/03597a01ee50ed33e9dfd640b249b4be3799d395.zip ) For Windows: Prevent overriding the parent project's compiler/linker settings \u00b6 set(gtest_force_shared_crt ON CACHE BOOL \"\" FORCE) FetchContent_MakeAvailable(googletest) ``` Multiple CMakeLists.txt files in subdirectories \u00b6 The configuration can be modularized by splitting the CMakeLists.txt file into multiple files, each in a separate directory. Then, these files can be executed using the add_subdirectory command in the main CMake_lists.txt file or one of the CMakeLists.txt files already added by the add_subdirectory command. This has several advantages : the configuration is more organized, it is easier to find the relevant part of the configuration or reuse the configuration in other projects the configuration is more configurable, i.e., we can turn on or off the configuration of a specific part of the project. The order of execution follows the order of the add_subdirectory commands, i.e., the processing of the CMakeLists.txt containing the add_subdirectory command is paused until the added CMakeLists.txt file is processed. The syntax of the add_subdirectory command is: add_subdirectory(<source dir> [<binary dir>]) By default, the <binary dir> is the same as the <source dir> . Variables scope \u00b6 The variable scope for multiple CMakeLists.txt files is hierarchical. This means that the variables defined in the parent CMakeLists.txt file are visible in the child CMakeLists.txt file, but not vice versa. Decide based on the build configuration \u00b6 Sometimes is essential to decide based on the build configuration in the CMakeLists.txt file. However, this is not possible for the multi-configuration generators like Visual Studio or Xcode because the build configuration is not known during the CMake configuration step. However, there are measures that can be taken to achieve the result for particular tasks. For install command, we can use the CONFIGURATIONS parameter to install the files only for the selected build configuration. Example: install( FILES <file 1> <file 2> DESTINATION <destination> CONFIGURATIONS Release ) For building, there are ways to limit the build based on the build configuration for both single- and multi-configuration generators. For single configuration generators, we can use the CMAKE_BUILD_TYPE variable and simply exclude the target from the build using the EXCLUDE_FROM_ALL target property: if(CMAKE_BUILD_TYPE STREQUAL \"Debug\") set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_ALL TRUE) endif() For multi-configuration generators, this variable is not set, but we can use the EXCLUDE_FROM_DEFAULT_BUILD_DEBUG property: set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_DEFAULT_BUILD_DEBUG TRUE) To support both single- and multi-configuration generators, we have to first check whether we use a single- or multi-configuration generator using the CMAKE_CONFIGURATION_TYPES variable and then set the property accordingly: if(CMAKE_CONFIGURATION_TYPES) set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_DEFAULT_BUILD_DEBUG TRUE) else() if(CMAKE_BUILD_TYPE STREQUAL \"Debug\") set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_ALL TRUE) endif() endif() Executing external commands \u00b6 There are different ways to execute external commands in the CMakeLists.txt , the proper way depends on the time when we want to execute the command: configuration time : use the execute_process command build time : use the add_custom_command command can be run both before and after the build step using the PRE_BUILD and POST_BUILD options To both commands, we can pass the COMMAND as a list of strings, where the first string is the command to execute. Additionally, when using the add_custom_command command, we can to use the COMMAND_EXPAND_LISTS option to expand the generator expressions in the command. CMake Cache \u00b6 CMake has two types of variables: normal variables that are used just like in any other programming language and cache variables , which are configured in the first cmake run and then stored in the CMakeCache.txt file in the build directory. The CMake cache is an essential part of the CMake build system. It stores variables that are used to configure the build scripts. Cache variables can be set in the following ways: by the set command in the CMakeLists.txt file with the CACHE option by the -D command line argument: cmake -D<variable name>=<variable value> <dir> by the cmake GUI by using the -C option: cmake -C <cache file> <dir> The rule is that once a cache variable is set, it is not changed when the cmake command is run again (that is why it is called cache :) ). Moreover, the cache variables are not overwritten by the set command in the CMakeLists.txt file. In other words, the set command in the CMakeLists.txt is only used to set the default value of the variable. If the variable is already set in the cache, the set command is ignored. However, the cache variables can be still overridden from the CMakeLists.txt if the set command is used without the CACHE option (by normal variables). CMake Directory Structure \u00b6 System Find_XXX.cmake files \u00b6 The system find scripts are located in the CMake/share/cmake-<version>/Modules/ directory. Various Tasks \u00b6 Showing the generator for a configured directory \u00b6 If the configuration step is already done, we can show the generator used for the configuration by reading the CMAKE_GENERATOR variable from the CMakeCache.txt file: Get-Content CMakeCache.txt | Select-String -Pattern \"CMAKE_GENERATOR\" Display available generators \u00b6 Unfortunately, there is no way how to display the available generators on the machine [ source ]. We can only list the generators that are compatible with the system by running the cmake --help . However, typically, the default generator marked with the * is available. Determine the binary output directory \u00b6 Sometimes, we need to know where the binary files are stored. Unfortunately, there is no direct way to get this information from the CMake variables. We can only get the directory in the post-build step for a binary target using the TARGET_FILE_DIR generator expression: # prints the binary directory of the target after the build add_custom_command( TARGET <target name> POST_BUILD COMMAND ${CMAKE_COMMAND} -E echo $<TARGET_FILE_DIR:<target name>> ) Debugging CMake \u00b6 Debugging CMake using CLion CMake debugger \u00b6 documentation In CLion, we can debug the CMake configuration process by: opening the CMakeLists.txt file setting the breakpoint(s) starting the debugging process by clicking on the play button which is located on the first line of the CMakeLists.txt file on the left side of the editor window. or by clicking on the debug button in the main toolbar Getting a a complete output of the CMake configuration \u00b6 We can get the complete output of the CMake configuration by running the cmake command with the --trace option. This can be especially useful when investigating included scripts, as the debuggers usually cannot step into CMake calls. Usually, the output is very long, so it is recommended to redirect it to a file: cmake --trace <dir> *> cmake_trace.txt If we want to also expand the variables, we can use the --trace-expand option. Testing with CMake \u00b6 documentation CMake has a built-in capability for organizing and running tests. This is useful, because for testing, we usually need information about the build configuration, which is already available in CMake. To enable testing in a project, we have to: add include(CTest) to the CMakeLists.txt file, add the individual tests using the add_test command, and configure the project Then, we can run the tests using the ctest command. Adding tests \u00b6 The add_test command is used to add a test to the project. The syntax is: add_test(NAME <test name> COMMAND <command>) By default, the command is executed in the build directory. Therefore, we can directly call the project executable by using the target name. We can also modify the test (e.g.: check the output of the test) using the set_property command: set_property(TEST <test name> PROPERTY <property name> <property value>) The most useful properties are: PASS_REGULAR_EXPRESSION : a regular expression that the output of the test has to match: cmake set_property(TEST <test name> PROPERTY PASS_REGULAR_EXPRESSION \"expected output\") CTest execution \u00b6 documentation The ctest executable run the tests configured with CMake and reports the results. It can run in three modes: Run Tests mode (default): runs the tests and reports the results Build and Test mode: builds the tests and then runs them activated by the --build-and-test argument Dashboard mode: run CTest as a client of the CDash dashboard application. activated by one of the -D ( --dahboard ), -M ( --test-model ), -S ( --script ), or -SP ( --script-new-process ) arguments this mode facilitates every phase of the testing process, from updating the source code to running the tests and collecting the results if configured correctly, the results can be displayed on the CDash server dashboard Important parameters for all modes: -C <config> : specifies the configuration to use (e.g., Debug , Release , etc.) Only applies for multi-configuration generators This configuration is used by the build step if the CTEST_BUILD_COMMAND variable is not set, but also in other steps, like running the tests. Therefore, it is important to set this parameter even if we provide a custom build command, otherwise, the tests will be skipped. -V : verbose output -VV : very verbose output -O <file> : output the results to a file Run Tests mode (default) \u00b6 The default run mode expects the project to be configured and built. It simply runs the tests and reports the results. For multi-configurations generators, we have to specify the configuration using the -C argument. Build and Test mode \u00b6 The build and test mode is activated by the --build-and-test argument. Unlike the cmake command, the ctest command does not choose the generator automatically. Instead, we have to supply the generator using the --build-generator argument. Other useful arguments: --build-options : additional options for the cmake command, e.g., --toolchain <path> to specify the toolchain file Dashboard mode \u00b6 CDash documentation at cmake.org Unlike the previous two modes, the dashboard mode facilitates every phase of the testing process, namely: updating the source code from the version control system configuring the project building the project running the tests collecting the results submitting the results to the CDash server This is iteself a strong argument for using the dashboard mode but the main advantage is that in this mode, we can see the results of the process on the CDash server dashboard. However, this feature requires some extra configuration. There are two main ways to use the dashboard mode: configure the dashboard mode using the -D argument together with the dahsboard command line arguments configure the dashboard mode using a cmake script and then run the script using the -S or -SP argument Using the Dashboard mode configured by the script \u00b6 When run with the -S or -SP argument, the ctest executable runs the script that configures the dashboard mode. The -SP mode only differs in that it runs the script in a new process. The script has to manage all the dashboard mode phases. For each phase, there is a corresponding command cmake_<phase> that has to be called. The commands are: ctest_start : Initializes the dashboard mode. The only required parameter is a positional parameter mode or model, which is one of the following: Continuous : for continuous integration Nightly : for nightly builds Experimental : for local testing ctest_update : Updates the source code from version control (Git, SVN, etc.) optional phase ctest_configure : Configures the project ctest_build : Builds the project optional phase ctest_test : Runs the tests ctest_coverage : Collects the coverage information optional phase ctest_memcheck : Runs the memory check optional phase ctest_submit : Submits the results to the CDash server The above commands depend on some cmake variables (some of them can be replaced by additional arguments of the command): CTEST_SOURCE_DIRECTORY : the source directory of the project (the directory where the CMakeLists.txt file is located) CTEST_BINARY_DIRECTORY : the build directory of the project CTEST_CONFIGURE_COMMAND : the command that configures the project. Typically cmake command with some arguments. CTEST_BUILD_COMMAND : the command that builds the project. Typically cmake --build command with some arguments. Configuring the Dashboard \u00b6 To see the results of the process in a nice GUI, we need a project on a CDash server. We can either use the public CDash server or set up our own server. To use the public CDash server, we have to: create an account on the CDash server create a project on the server download the CTestConfig.cmake file from the project page and put it in the project directory (the directory where the CMakeLists.txt file is located) Troubleshooting \u00b6 Problems \u00b6 Warning! <name> library version mismatched error \u00b6 This error typically occures when the library <name> used during the build is different from the library used at runtime when running the tests. This can happen due to following scenario: The library relies on the PATH variable to find the library at runtime, but the path used during the build is specified manually. There is another library with the same name earlier in the PATH variable, and the version differs. The solution is: Identify the path used incorrectly at runtime Move the problematic path after the correct path in the PATH variable The identification step can be hard here, as the ctest does not report the real path to the library used at runtime, but the path where this library was compiled. In other words, the Installation point under the General Information is incorrect. The identification of the real path to the problematic library can be done as follows: build the tests manually using CMake run a single test in the debugger and break open the Process Explorer View -> Lower Pane View -> DLLs click on the test process In the Lower Pane View, there is a list of all DLLs loaded by the test process. Find the problematic library and check the path to it. Test fixtures \u00b6 In CMake, there is no dirrect suppor for fixtures as we know them from other testing frameworks. Instead, we define the setup and teardown code as separate tests and then, we use a set_tests_properties command to set up the dependencies between the tests. Example: add_test(NAME setup COMMAND <setup command>) add_test(NAME test1 COMMAND <test 1 command>) add_test(NAME test2 COMMAND <test 2 command>) add_test(NAME teardown COMMAND <teardown command>) set_tests_properties(test1, test2 PROPERTIES FIXTURES_REQUIRED my_test_suite) set_tests_properties(setup PROPERTIES FIXTURES_SETUP my_test_suite) set_tests_properties(teardown PROPERTIES FIXTURES_CLEANUP my_test_suite) Running tests not related to any CMake project \u00b6 There is no way how to run tests that are not related to any CMake project directly using the ctest command, and we cannot add such tests in the ctest starter script. The minimal solution is to: create a dummy CMake project that only contains the tests (no targets, etc.) skip the build step in the ctest starter script run the tests starter normally using the ctest command","title":"CMake Manual"},{"location":"Programming/C%2B%2B/CMake%20Manual/#introduction","text":"CMake is a cross-platform build system generator that generates build scripts for various build systems (e.g., make, ninja, Visual Studio, Xcode) a name of the language used to write the configuration files for the build system generator and other related scripts. CMake can run in two modes: project mode : is the standard mode. This mode is used when the project is configured using the cmake command. Therefore, the CMakeLists.txt file is executed in this mode. script mode : is used when the cmake command is run with the -P option. In this mode, the CMakeLists.txt file is not executed, but the script specified by the -P option is executed. Main resources: CMake documentation CMake tutorial","title":"Introduction"},{"location":"Programming/C%2B%2B/CMake%20Manual/#generators","text":"CMake not only supports multiple platforms but also multiple build systems on each platform. These build systems are called generators . To get a list of available generators, run: cmake -E capabilities","title":"Generators"},{"location":"Programming/C%2B%2B/CMake%20Manual/#the-cmake-command","text":"documentation","title":"The cmake command"},{"location":"Programming/C%2B%2B/CMake%20Manual/#configuration-generating-build-scripts","text":"General syntax is: cmake <source dir> Here <source dir> is the CMakeLists.txt directory. The build scripts are build in current directory. We can set any cache variable using the -D argument. Example: cmake <source dir> -D <variable name>=<variable value> # or equivalently cmake <source dir> -D<variable name>=<variable value>","title":"Configuration: Generating Build scripts"},{"location":"Programming/C%2B%2B/CMake%20Manual/#toolchain-file","text":"To work with package managers, a link to toolchain file has to be provided as an argument. For vcpkg , the argument is as follows: # new version cmake <dir> --toolchain <vcpkg location>/scripts/buildsystems/vcpkg.cmake # old version cmake <dir> -DCMAKE_TOOLCHAIN_FILE=<vcpkg location>/scripts/buildsystems/vcpkg.cmake Note that the toolchain is only loaded at the beginnning of the generation process. Once you forgot it, you need to delete the build scripts diectory content to make this argument work for subsequent cmake commands.","title":"Toolchain file"},{"location":"Programming/C%2B%2B/CMake%20Manual/#other-usefull-arguments","text":"-B <build dir> to specify the build directory. By default, the build directory is the current directory. -LH to see cmake nonadvanced variables together with the description. -LHA to see also the advanced variables. Note that this prints only cached variables, to print all variables, we have to edit the CmakeLists.txt. --graphviz=<file name> to generate a graphviz file of the dependencies. Note that only visible CMake dependencies are included","title":"Other Usefull arguments"},{"location":"Programming/C%2B%2B/CMake%20Manual/#legacy-arguments","text":"-H : to specify the source directory (where the CMakeLists.txt file is located). Now it is specified as the positional argument or using -S .","title":"Legacy arguments"},{"location":"Programming/C%2B%2B/CMake%20Manual/#building","text":"For building, use: cmake --build <build dir> where build dir is the directory containing the build scripts ( CmakeFiles folder). To list the build options: cmake -L","title":"Building"},{"location":"Programming/C%2B%2B/CMake%20Manual/#specify-the-target","text":"By default, all targets are built. We can use the --target to specify a single target: cmake --build . --target <TARGET NAME> There is also a special target all that builds all targets, which is equivalent to not specifying the --target argument.","title":"Specify the target"},{"location":"Programming/C%2B%2B/CMake%20Manual/#specify-the-build-type-debug-release","text":"In CMake, we use a specific build type string instead of compiler and linker flags: Debug - Debug build Release - Release build RelWithDebInfo - Release build with debug information MinSizeRel - Release build with minimal size Unfortunately, the way how the build type should be specified depends on the build system: Single-configuration systems (GCC, Clang, MinGW) Multi-configuration systems (MSVC)","title":"Specify the build type (Debug, Release)"},{"location":"Programming/C%2B%2B/CMake%20Manual/#single-configuration-systems","text":"Single configuration systems have the compiler flags hardcoded in the build scripts. Therefore, we need to specify the build type for CMake when we generate the build scripts: cmake ../ -DCMAKE_BUILD_TYPE=Release By default, the build type is an empty string . This means that no extra flags are added to the compiler and linker so the compiler and linker run with their default settings. Interesting info can be found in this SO question .","title":"Single-configuration systems"},{"location":"Programming/C%2B%2B/CMake%20Manual/#multi-configuration-systems","text":"In multi-configuration systems, the -DCMAKE_BUILD_TYPE parameter is ignored, because the build configuration is supposed to be determined when building the code (i.e., same build scripts for debug and for release). Therefore, we omit it, and instead specify the --config parameter when building the code: cmake --build . --config Release","title":"Multi-configuration systems"},{"location":"Programming/C%2B%2B/CMake%20Manual/#rebuild-or-clean-the-build-binaries","text":"To rebuild, we use the --clean-first argument: cmake --build . --clean-first If we want just to clean the build binaries without building, we use the clean target: cmake --build . --target clean","title":"Rebuild or Clean the build binaries"},{"location":"Programming/C%2B%2B/CMake%20Manual/#install","text":"To be able to install the project, it needs to be configured to do so. For this, check the installation configuration . To install the project, run: cmake --install <build dir> Note that the project needs to be built first. If it is not, we can build and install in one step using the --build with the --target install argument: cmake --build <build dir> --target install Usually, we want to use a different directory when testing the installation. To do that, we need to configure the project with the CMAKE_INSTALL_PREFIX variable. Example: cmake -DCMAKE_INSTALL_PREFIX=<test install dir> <source dir> A note for Windows installations : The default CMAKE_INSTALL_PREFIX is C:/Program Files (x86)/<project name> , even if the project is a 64-bit project. To override this, configure the project with the following argument: cmake <other arguments> -A x64","title":"Install"},{"location":"Programming/C%2B%2B/CMake%20Manual/#cmake-command-line-tools","text":"documentation Apart from standard commands listed in previous sections, CMake provides several command-line tools that are not directly related to the build process. These tools wrap the system commands so that we are able to use them in a cross-platform way. To run these tools, execute: cmake -E <tool name> <arguments> The most useful tools are: copy - copy files and directories capabilities - print the properties of the system related to the build process","title":"CMake command-line tools"},{"location":"Programming/C%2B%2B/CMake%20Manual/#copy-tool","text":"The copy tool has two signatures: copy <source> <destination> copy -t <destination> <source> ( only available in CMake 3.26 and later ) Here, <source> can be a directory, a file, or a list of files. The <destination> can be a directory or a file.","title":"Copy tool"},{"location":"Programming/C%2B%2B/CMake%20Manual/#execute-a-cmake-script","text":"To execute a CMake script, use the cmake -P <script> command.","title":"Execute a CMake script"},{"location":"Programming/C%2B%2B/CMake%20Manual/#syntax","text":"","title":"Syntax"},{"location":"Programming/C%2B%2B/CMake%20Manual/#variables","text":"In order to use a variable in the CMakeLists.txt file, we have to use the ${} syntax: message(STATUS \"dir=${dir}\") In conditions, we can use the variable using its name: if(DEFINED <name>) ... Variables are set using the set command: set(<variable name> <variable value>)","title":"Variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#variable-types","text":"CMake has two types of variables: string variables : the most common type of variable. A string variable is created if the <variable value> is a single word or a quoted string. Example: cmake set(dir \"C:/Program Files\") list variables : a list variable is created if the <variable value> is a list of words. Example: cmake set(dirs \"C:/Program Files\" \"C:/Program Files (x86)\")","title":"Variable types"},{"location":"Programming/C%2B%2B/CMake%20Manual/#enviromental-variables","text":"We can use environmental variables using the ENV variable: if(DEFINED ENV{<name>}) ... Be aware that in string, we use only one pair of curly braces (see variable references manual ): message(STATUS \"dir=$ENV{dir}\")","title":"Enviromental variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#built-in-variables","text":"documentation There are some variable generated by default by CMake. Howver, these differ between the script and config modes.","title":"Built-in variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#variables-available-in-both-modes","text":"CMAKE_SOURCE_DIR : the directory where the top level CMakeLists.txt file is located. This variable is rarely the right one, because our project may be used as a subproject in another project. in the script mode, this variable is set to the directory where the script is located. CMAKE_CURRENT_SOURCE_DIR : the directory where the currently processed CMakeLists.txt file is located. in the script mode, this variable is set to the directory where the script is located. CMAKE_BINARY_DIR : the top level build directory. Most of the time, it is appropriate to use CMAKE_CURRENT_BINARY_DIR instead of this variable. in the script mode, this variable is set to the directory where the script is located. CMAKE_CURRENT_BINARY_DIR : the directory where the build scripts for the current target are located and where the build process is executed. For some generators, this is also the directory where the binaries are stored. in the script mode, this variable is set to the directory where the script is located.","title":"Variables available in both modes"},{"location":"Programming/C%2B%2B/CMake%20Manual/#config-mode-variables","text":"CMAKE_CURRENT_LIST_DIR : the directory where the currently processed CMakeLists.txt file is located. PROJECT_SOURCE_DIR : the nearest directory up in the directory tree where the CMakeLists.txt with the project command is located. CMAKE_<LANG>_COMPILER_ID : the compiler ID for the language <LANG> . CMAKE_<LANG>_COMPILER_VERSION : the compiler version for the language <LANG> . CMAKE_PROJECT_NAME : the name of the top level project. Most of the time, it is appropriate to use PROJECT_NAME instead of this variable. PROJECT_NAME : the name of the project. MSVC : set to TRUE if the compiler is MSVC.","title":"Config mode variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#script-mode-variables","text":"CMAKE_CURRENT_LIST_DIR : the directory where the currently processed script is located","title":"Script mode variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#unix-installation-directories","text":"There are also variables for installation directories typical for Unix systems. Touse them, we have to include the GNUInstallDirs module . The variables have two formats: CMAKE_INSTALL_<dir> : the directory relative to the installation prefix. These variables have to be used in the install command and other commands that use the installation prefix. CMAKE_INSTALL_FULL_<dir> : the full path to the directory. Notable variables are: BINDIR : the directory for executables ( bin ) LIBDIR : the directory for libraries ( lib ) INCLUDEDIR : the directory for headers ( include )","title":"Unix installation directories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#list-variables","text":"List variables are defined similarly to scalar variables using the set command: set(<name> <value 1> <value 2> ...) Then, we can use the list variable in: commands that accept lists (e.g., add_executable , add_library , target_link_libraries ) for loops","title":"List variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#print-all-variables","text":"To print all variables, the following function can be used: function(dump_cmake_variables) if (ARGV0) message(STATUS \"Printing variables matching '${ARGV0}'\") else() message(STATUS \"Printing all variables\") endif() get_cmake_property(_variableNames VARIABLES) list (SORT _variableNames) foreach (_variableName ${_variableNames}) if (ARGV0) unset(MATCHED) string(REGEX MATCH ${ARGV0} MATCHED ${_variableName}) if (NOT MATCHED) continue() endif() endif() message(STATUS \"${_variableName}=${${_variableName}}\") endforeach() message(STATUS \"Printing variables - END\") endfunction() To print all variables related to HDF5 lib, call dump_cmake_variables(HDF) after the find_package call.","title":"Print all variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#the-option-command","text":"The option command is used to define a boolean variable that can: be set by the user using the -D argument when running the cmake command, have a default value, have a description that will be printed when the variable is set, and set a cache variable (see CMake cache ). The syntax is: option(<option name> <option description> <default value>) The behavior of the option command is as follows: If variable is already set (either a cache variable or a normal variable), the option command is ignored. Otherwise, a cache variable is created if we are in the project mode, and a normal variable is created if we are in the script mode.","title":"The option command"},{"location":"Programming/C%2B%2B/CMake%20Manual/#control-structures","text":"","title":"Control structures"},{"location":"Programming/C%2B%2B/CMake%20Manual/#if","text":"The if command has the following syntax: if(<condition>) ... elseif(<condition>) ... else() ... endif() The condition can be: a variable an expression Each expression can use some of the supported operators: logical operators: AND , OR , NOT comparison operators: EQUAL , LESS , GREATER , LESS_EQUAL , GREATER_EQUAL , STREQUAL , STRLESS , STRGREATER , STRLESS_EQUAL , STRGREATER_EQUAL file operators: EXISTS , IS_DIRECTORY , IS_REGULAR_FILE , IS_SYMLINK , IS_ABSOLUTE , IS_RELATIVE , IS_NEWER_THAN , IS_OLDER_THAN string operators: MATCHES , LESS , GREATER , LESS_EQUAL , GREATER_EQUAL , STREQUAL , STRLESS , STRGREATER , STRLESS_EQUAL , STRGREATER_EQUAL The MATCHES operator requires a regex, not a simplified filesystem filter pattern. See the regex documentation for more. version operators: VERSION_EQUAL , VERSION_LESS , VERSION_GREATER , VERSION_LESS_EQUAL , VERSION_GREATER_EQUAL these are ment to be used with version string variables created by the find_package command: cmake find_package(<package name> CONFIG REQUIRED) if(<package name>_VERSION VERSION_LESS <version>) ... endif() and more... For the full list of operators, see the if command documentation .","title":"if"},{"location":"Programming/C%2B%2B/CMake%20Manual/#foreach","text":"The foreach command has the following syntax: foreach(<variable name> <items>) ... endforeach()","title":"foreach"},{"location":"Programming/C%2B%2B/CMake%20Manual/#generator-expressions","text":"Manual Generator expressions are a very useful tool to control the build process based on the build type, compiler type, or similar properties. CMake use them to generate mutliple build scripts from a single CMakeLists.txt file. The syntax for a basic condition expression is: \"$<$<condition>:<this will be printed if condition is satisfied>>\" Unlike, variables, the generator expressions are evaluated during the build process, not during the configuration process . Therefore, they cannot be dumped during the configuration process, and they cannot be used in the if command. However, they can be still used in variables and commands, if the evaluation is not needed during the configuration process. Notable variable expressions: $<TARGET_FILE_DIR:<target name>> - the directory where the target will be built $<TARGET_RUNTIME_DLLS:<target name>> - the list of runtime dependencies of the target note that this property is only available (and necessary) for MSBuild generator and it is only available in the POST_BUILD phase.","title":"Generator expressions"},{"location":"Programming/C%2B%2B/CMake%20Manual/#evaluating-generator-expressions-during-configuration","text":"In case we need to see the evaluated generator expressions during cmake configuration, we can try to cheat using the following command: file(GENERATE OUTPUT <filename> CONTENT <string-with-generator-expression>) This way, we receive the evaluated value of the generator expression in the file for one of the build configurations.","title":"Evaluating generator expressions during configuration"},{"location":"Programming/C%2B%2B/CMake%20Manual/#file-operations","text":"To perform file operations, use the file command. The most useful subcommands are: MAKE_DIRECTORY <directory> - create a directory RENAME <from> <to> - renames/moves a file or directory. The <from> must exist, and the parent directory of <to> must exist. REMOVE <file> - remove a file REMOVE_RECURSE <directory> - remove a directory and all its content","title":"File operations"},{"location":"Programming/C%2B%2B/CMake%20Manual/#path-operations","text":"Path operations are performed using the cmake_path command. The sytax for this command varies based on the subcommand.","title":"Path operations"},{"location":"Programming/C%2B%2B/CMake%20Manual/#path-decomposition","text":"The syntax for the path decomposition is: cmake_path(GET <path> <path part> <output variable>) Here, <path part> can be: PARENT_PATH - the parent directory of the path","title":"Path decomposition"},{"location":"Programming/C%2B%2B/CMake%20Manual/#list-operations","text":"For list operations, use the list command. The most useful subcommands are: APPEND - append an element to the list: list(APPEND <list name> <elements>) JOIN - join the list elements into a string: list(JOIN <list name> <separator> <output variable>)","title":"List operations"},{"location":"Programming/C%2B%2B/CMake%20Manual/#functions","text":"Functions are defined using the function command. The syntax is: function(<function name> <argument 1> <argument 2> ...) ... endfunction() This way, we have a function with simple positional arguments. These arguments can be used in the function body as variables: function(print_arguments arg1 arg2) message(STATUS \"arg1=${arg1}\") message(STATUS \"arg2=${arg2}\") endfunction() To call the function, use the following syntax: print_arguments(\"value 1\" \"value 2\") More resources: https://hsf-training.github.io/hsf-training-cmake-webpage/11-functions/index.html","title":"Functions"},{"location":"Programming/C%2B%2B/CMake%20Manual/#named-arguments","text":"We can notice that a typical cmake function has named arguments, e.g., add_custom_command(TARGET <target name> POST_BUILD COMMAND <command>) . To achieve this, we can use the cmake_parse_arguments command. The syntax is: function(<function name>) cmake_parse_arguments( PARSE_ARGV <positional args count> <variable prefix> <options> <one_value_keywords> <multi_value_keywords> ) Here: <positional args count> is the number of positional arguments that are skipped by the cmake_parse_arguments command, <variable prefix> is the prefix for variables created from named arguments (the name will be <variable prefix>_<variable_name> ), and <options> , <one_value_keywords> , and <multi_value_keywords> are the lists of named arguments of each type. the list have to be specified as a string divided by a semicolon. The <options> are the arguments that can be either present or not, the <one_value_keywords> are the arguments that have a single value, and the <multi_value_keywords> are the arguments that have multiple values.","title":"Named arguments"},{"location":"Programming/C%2B%2B/CMake%20Manual/#default-values-for-arguments","text":"There is no specific syntax for default values for arguments. We can achieve this, for example, by using the if command: if(NOT DEFINED <variable>) set(<variable> <default value>) endif()","title":"Default values for arguments"},{"location":"Programming/C%2B%2B/CMake%20Manual/#return-values","text":"There is a return statement in CMake, but in general, the value is returned by setting a variable with a parent scope: function(return_value) set(<return val name> <value> PARENT_SCOPE) endfunction() If we want to determine the return variable name by the caller, we have to pass the variable name as an argument: function(return_value return_var_name) set(${return_var_name} <value> PARENT_SCOPE) endfunction()","title":"Return values"},{"location":"Programming/C%2B%2B/CMake%20Manual/#useful-functions","text":"","title":"Useful functions"},{"location":"Programming/C%2B%2B/CMake%20Manual/#message","text":"The message command is used to print messages during the configuration process. The syntax is: message(<mode> <message>) The <mode> can be: STATUS - the message is printed as a status message WARNING - the message is printed as a warning AUTHOR_WARNING - the message is printed as an author warning SEND_ERROR - the message is printed as an error and the configuration process is stopped FATAL_ERROR - the message is printed as a fatal error and the configuration process is stopped DEPRECATION - the message is printed as a deprecation warning","title":"message"},{"location":"Programming/C%2B%2B/CMake%20Manual/#execute_process","text":"The execute_process command is used to execute an external command. The syntax is: execute_process(COMMAND <command> [OPTIONS]) Important options: RESULT_VARIABLE <variable name> : the variable that will store the result (exit code) of the command.","title":"execute_process"},{"location":"Programming/C%2B%2B/CMake%20Manual/#find_program","text":"The find_program command is used to find an executable in the system. It's adventages over providing our own path/logic is: it is cross-platform - it automatically searches for the executable with the correct extension for the current platform it can be configured to raise an error if the executable is not found it can automatically search for the executable in the system paths The syntax is: find_program(<variable name> <executable name> [OTHER_ARGUMENTS]) The full path to the executable is stored in the <variable name> variable. The OTHER_ARGUMENTS are: REQUIRED : if the executable is not found, the configuration process is stopped HINTS <path> , PATHS <path> : the path where the executable should be searched. HINTS and PATHS are equivalent exept the priority: HINTS are searched before the standard paths, PATHS are searched at the end with the lowest priority.","title":"find_program"},{"location":"Programming/C%2B%2B/CMake%20Manual/#cmakeliststxt","text":"The CMakeLists.txt file is the main configuration file for any CMake project. This file is executed during the configuration step (when the cmake command is run without arguments specifying another step). It contains commands written in the CMake language that are used to configure the build process. The typical structure of the CMakeLists.txt file is as follows: Top section contains project wide setting like name, minimum cmake version, and the language specification. Targets sections containing: the target definition together with sources used target includes target linking","title":"CMakeLists.txt"},{"location":"Programming/C%2B%2B/CMake%20Manual/#typical-top-section-content","text":"The typical content of the top section is: minimum cmake version: cmake_minimum_required(VERSION <version>) project name and version: project(<name> [VERSION <version>]) language specification: enable_language(<language>) cmake variables setup, e.g.: set(CMAKE_CXX_STANDARD <version>) compile options: add_compile_options(<option 1> <option 2> ...) cmake module inclusion: include(<module name>)","title":"Typical Top section content"},{"location":"Programming/C%2B%2B/CMake%20Manual/#language-standards","text":"The language standard is set using the set command together with the CMAKE_<LANG>_STANDARD variable. Example: set(CMAKE_CXX_STANDARD 17) This way, the standard is set for all targets and the compiler should be configured for that standard. However, if the compiler does not support the standard, the build script generation continues and the failure will appear later during the compilation. To avoid that, we can make the standard a requirement using the set command together with the CMAKE_<LANG>_STANDARD_REQUIRED variable. Example: set(CMAKE_CXX_STANDARD_REQUIRED ON) However, the required standard is not always correctly supported by the compiler (e.g., GCC up to version 13 does not support C++20). Therefore, we need to specify the minimum version for these compilers: if(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 13.0.0) message(FATAL_ERROR \"GCC version must be at least 13.0.0!\") endif()","title":"Language standards"},{"location":"Programming/C%2B%2B/CMake%20Manual/#set-the-runtime-library-type-for-msvc","text":"In MSVC, it is crucial that both the target and all its dependencies are compiled with the same (standard) runtime library type. To set the library type for the target in CMake, we use the CMKAE_MSVC_RUNTIME_LIBRARY variable. The possible values are: MultiThreadedDLL - the dynamic library (default in Release mode) MultiThreadedDebugDLL - the dynamic library with debug information (default in Debug mode) MultiThreaded - the static library MultiThreadedDebug - the static library with debug information By default, the dynamic library is used. To set the static library, use: set(CMAKE_MSVC_RUNTIME_LIBRARY \"MultiThreaded$<$<CONFIG:Debug>:Debug>\") This way, the static library with debug information is used in the Debug mode, and the static library is used in the Release mode. Note that the CMAKE_MSVC_RUNTIME_LIBRARY variable was introduced in CMake 3.15. Therefore, you have to set cmake_minimum_required(VERSION 3.15) in the CMakeLists.txt file, or set the CMP0091 policy to NEW using the cmake_policy command.","title":"Set the runtime library type for MSVC"},{"location":"Programming/C%2B%2B/CMake%20Manual/#compile-options","text":"Most of the compile options are now sets automatically based on the declarations in the CMakeLists.txt file. However, some notable exceptions exists. To set such options, we have to use the add_compile_options command: add_compile_options(<option 1> <option 2> ...)","title":"Compile options"},{"location":"Programming/C%2B%2B/CMake%20Manual/#msvc","text":"/permissive- to enable the strictest mode of the compiler","title":"MSVC"},{"location":"Programming/C%2B%2B/CMake%20Manual/#gcc","text":"-pedantic-errors to report all cases where non-standard GCC extension is used and treat them as errors","title":"GCC"},{"location":"Programming/C%2B%2B/CMake%20Manual/#linker-options","text":"Linker options can be set with add_link_options command. Example: add_link_options(\"/STACK: 10000000\")","title":"Linker Options"},{"location":"Programming/C%2B%2B/CMake%20Manual/#dependency-management","text":"There are many ways how to manage dependencies in CMake, for complete overview, see the documentation . Although it is possible to hard-code the paths for includes and linking, it is usually better to initialize the paths automatically using a rich set of commands cmake offers. It has the following advatages: Hardcoding the paths is error-prone, while cmake commands usually deliver correct paths It boost the productivity as we do not have to investigate where each library is installed The resulting CMakeLists.txt file is more portable And most importantly, potential errors concerning missing libraries are reported prior to the compilation/linking . Most of the libraries have CMake support, so their CMake variables can be initialized simply by: calling the find_package command described below, or, if it is a simple header-only library using the find_path command For packages without the CMake support, we have to use lower-level cmake commands like find_path or find_libraries . For convinience, we can put these command to our own Find<name> script that can be used by multiple project or even shared.","title":"Dependency management"},{"location":"Programming/C%2B%2B/CMake%20Manual/#how-to-recognize-a-cmake-package","text":"CMake packages have either: their own cmake config (cmake-aware libs usually installed through the package manager like vcpkg ) or they have a Find<package name> script created by someone else that heuristically search for the packege (The default location for these scripts is CMake/share/cmake-<version>/Modules ).","title":"How to recognize a CMake package?"},{"location":"Programming/C%2B%2B/CMake%20Manual/#standard-way-find_package","text":"The find_package command is the primary command for dependencies. It tries to find the correct variables for a library. The command sets: the <PackageName>_FOUND variable to TRUE or 1 if the package is found include paths linking paths platform/toolchain specific enviromental variables There are two modes of operation for the command module mode , which uses the Find<library name> cmake scripts, typically provided not by the library developers, but somebody else who wants the libraries to be accessible by CMake. All modules provided by CMake itself are listed in the documentation . config mode which uses CMake scripts with name <PackageName>Config.cmake or <lowercasePackageName>-config.cmake provided by the developers of the package. They are typically distributed with the source code and downloaded by the package manager. To find out which of the operation modes is used, the find_package command uses the following logic: if the MODULE parameter is used, the module mode is used if the CONFIG or No_MODULE parameter is used, the config mode is used if some parameters from the full (advanced) signature of the find_package command are used (e.g.: NAMES ), the config mode is used otherwise, by default, the module mode is used with the fallback to the config mode if the module is not found","title":"Standard way: find_package"},{"location":"Programming/C%2B%2B/CMake%20Manual/#config-mode","text":"Config packages are CMake modules that were created as cmake projects by their developers. They are therefore naturally integrated into Cmake. The configuration files are executed as follows: Package version file: <package name>-config-version.cmake or <package name>ConfigVersion.cmake . This file handles the version compatibility, i.e., it ensures that the installed version of the package is compatible with the version requested in the find_package command. Package configuration file: <package name>-config.cmake or <package name>Config.cmake . The process of searching for these files is very complex. For the full description, see the documentation . The most important steps are: Search the directory specified by the CMAKE_FIND_PACKAGE_REDIRECT_DIR variable. Typically, this variable is set to <build dir>/CMakeFiles/pkgRedirects . Search specified subdirectories of a <prefix path> . Multiple <prefix path> variables are considered in the following order: Package-specific prefix paths, in the following order: <PackageName>_ROOT CMake variable <PACKAGENAME>_ROOT} CMake variable <PackageName>_ROOT environment variable <PACKAGENAME>_ROOT} environment variable Prefix paths specified in CMake cache variables, in the following order: CMAKE_PREFIX_PATH CMake variable CMAKE_FRAMEWORK_PATH CMake variable CMAKE_APPBUNDLE_PATH CMake variable Prefix paths specified in CMake environment variables, in the following order: <PackageName>_DIR CMake variable CMAKE_PREFIX_PATH environment variable CMAKE_FRAMEWORK_PATH environment variable CMAKE_APPBUNDLE_PATH environment variable And much more...","title":"Config mode"},{"location":"Programming/C%2B%2B/CMake%20Manual/#module-mode","text":"Module packages are packages that are not cmake projects themselves, but are hooked into cmake using custom find module scrips. These scripts are automatically executed by find_package . The find module script is named Find<package name>.cmake . The find_package command searches for these scripts in: the CMAKE_MODULE_PATH directories, and then int the CMake installation, e.g.: CMake/share/cmake-3.22/Modules/Find<package name>.cmake .","title":"Module mode"},{"location":"Programming/C%2B%2B/CMake%20Manual/#searching-for-include-directories-with-find_path","text":"The find_path command is intended to find the path (e.g., an include directory). It is either used for non-CMake packages or for header-only libraries. A simple syntax is: find_path( <var name> NAMES <file names> ) Here: <var name> is the name of the resulting variable <file names> are all possible file names split by space. At least one of the files needs to be present in a path for it to be considered to be the found path. By default, only some default paths are considered. To consider other paths, we have some options: PATHS <paths> , where <paths> are full candidate paths split by space PATH_SUFFIXES <relative paths> , where <relative paths> are relative paths that will be appended to all paths considered. Important parameters: REQUIRED : if the path is not found, the configuration process is stopped. Similar to the REQUIRED parameter of the find_package command.","title":"Searching for include directories with find_path"},{"location":"Programming/C%2B%2B/CMake%20Manual/#versions","text":"We may specify the version of package by using the VERSION parameter. The syntax is: VERSION <minimum version> VERSION <minimum version>...<maximum version> The version here is the cmake version used in the cmake version file (i.e., <package name>-config-version.cmake or <package name>ConfigVersion.cmake ) of the package. Typically, this is the project version of the package, specified in the project command.","title":"Versions"},{"location":"Programming/C%2B%2B/CMake%20Manual/#low-level-command-find_library","text":"The find_library command is used to populate a variable with a result of a specific file search optimized for libraries. The search algorithm works as follows: ? Search package paths order: <CurrentPackage>_ROOT , ENV{<CurrentPackage>_ROOT} , <ParentPackage>_ROOT , ENV{<ParentPackage>_ROOT} this only happens if the find_library command is called from within a find_<module> or find_package this step can be skipped using the NO_PACKAGE_ROOT_PATH parameter Search path from cmake cache. During a clean cmake generation, these can be only supplied by command line. Considered variables: CMAKE_LIBRARY_ARCHITECTURE CMAKE_PREFIX_PATH CMAKE_LIBRARY_PATH CMAKE_FRAMEWORK_PATH this step can be skipped using the NO_CMAKE_PATH parameter Same as step 3, but the variables are searched among system environmental variables instead this step can be skipped using the NO_CMAKE_ENVIRONMENT_PATH parameter Search paths specified by the HINTS option Search the standard system environmental paths variables considered are LIB and PATH this step can be skipped using the NO_SYSTEM_ENVIRONMENT_PATH parameter Search in system paths Considered variables: CMAKE_LIBRARY_ARCHITECTURE CMAKE_SYSTEM_PREFIX_PATH CMAKE_SYSTEM_LIBRARY_PATH CMAKE_SYSTEM_FRAMEWORK_PATH this step can be skipped using the NO_CMAKE_SYSTEM_PATH parameter Search the paths specified by the PATHS option","title":"Low level command: find_library"},{"location":"Programming/C%2B%2B/CMake%20Manual/#searching-for-libraries-in-the-project-dir","text":"Note that the project dir is not searched by default. To include in the search, use: HINTS ${PROJECT_SOURCE_DIR} . Full example on the Gurobi lib stored in <CMAKE LISTS DIR/lib/gurobi_c++.a> : find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED )","title":"Searching for libraries in the project dir"},{"location":"Programming/C%2B%2B/CMake%20Manual/#creating-a-custom-find-script","text":"The structure of a simple find scripts is described in the documentation . We can either put the find script to the default location, so it will be available for all projects, or we can put it in the project directory and add that directory to the CMAKE_MODULE_PATH : list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\") The usual structure of the find script is: the comment section describing the file: #[=======================================================================[.rst: FindMOSEK ------- Finds the MOSEK library. Result Variables ^^^^^^^^^^^^^^^^ This will define the following variables: ``MOESK_FOUND`` True if the system has the MOSEK library. ``MOSEK_INCLUDE_DIRS`` Include directories needed to use MOSEK. ``MOSEK_LIBRARIES`` Libraries needed to link to MOSEK. #]=======================================================================] find commands that fils some temp variables: find_path( MOSEK_INCLUDE_DIR NAMES mosek.h PATHS \"$ENV{MOSEK_HOME}/h\" ) find_library( MOSEK_LIBRARY NAMES libmosek64.so.10.0 libfusion64.so.10.0 PATHS \"$ENV{MOSEK_HOME}/bin\" ) handeling of the result of the file commands. The standard approach is: include(FindPackageHandleStandardArgs) find_package_handle_standard_args(MOSEK FOUND_VAR MOSEK_FOUND REQUIRED_VARS MOSEK_LIBRARY MOSEK_INCLUDE_DIR ) setting the final variables: if(MOSEK_FOUND) set(MOSEK_LIBRARIES ${MOSEK_LIBRARY}) set(MOSEK_INCLUDE_DIRS ${MOSEK_INCLUDE_DIR}) endif()","title":"Creating a custom find script"},{"location":"Programming/C%2B%2B/CMake%20Manual/#downolad-dependencies-during-configuration-and-build-them-from-source","text":"To download and build dependencies during the configuration, we can use the FetchContent module. This way, the sources are downloaded at the configuration step, and built during the build step. Note that because of this, the usage of these dependencies is limited, we cannot, for example, run the dependency tools at configuration time. For details, see the dependency management documentation . The usual workflow is: Download the dependency using the FetchContent_Declare command cmake FetchContent_Declare( <NAME> <SPECIFICATION> DOWNLOAD_EXTRACT_TIMESTAMP ON ) The <SPECIFICATION> can be: URL <url> with and optional HASH <hash> GIT <git repository> with an optional TAG <tag> or BRANCH <branch> or COMMIT <commit> The DOWNLOAD_EXTRACT_TIMESTAMP option ensures that the timestamp of the downloaded files is preserved. This is useful when the dependency is downloaded multiple times, and we want to avoid unnecessary rebuilds. Note that this option shoud be placed before the URL option . This is because it was introduced in CMake 3.21, so if it is placed after the URL and the CMake version is lower than 3.21, the FetchContent_Declare command will fail as the option will be considered a part of the URL. Configure the dependency using the FetchContent_MakeAvailable command: cmake FetchContent_MakeAvailable(<NAME>) If the dependency is a CMake project, the FetchContent_MakeAvailable command will automatically configure the project by calling the add_subdirectory command with the path to the downloaded source code. Both the dependency source code and the build directory are stored in the build directory in _deps folder. This directory contains: <NAME>-src - the source code of the dependency <NAME>-build - the build directory of the dependency","title":"Downolad dependencies during configuration and build them from source"},{"location":"Programming/C%2B%2B/CMake%20Manual/#cmake-targets","text":"Targets define logical units of the build process. These can be: executables libraries custom targets doing all sorts of things sets of targets, i.e., aliases for building multiple targets at once Available targets are either user-defined or automatically generated by CMake.","title":"CMake Targets"},{"location":"Programming/C%2B%2B/CMake%20Manual/#executable-targets","text":"The target definition is done using the add_executable command. The syntax is: add_executable(<target name> <source file 1> <source file 2> ...) The target name is used to refer to the target in other commands. The target name is also used to name the output file. The list of source files should contain all the source files that are needed to build the target. There are some automatic mechanisms that can be used to add the source files (discussed e.g. on cmake forums ), but they are not recommended.","title":"Executable targets"},{"location":"Programming/C%2B%2B/CMake%20Manual/#library-targets","text":"Library targets are defined using the add_library command. The syntax is: add_library(<target name> <type> <source file 1> <source file 2> ...)","title":"Library targets"},{"location":"Programming/C%2B%2B/CMake%20Manual/#object-libraries","text":"Manual CMake can define object libraries , which are similar to source files, just pre-compiled. The syntax is: add_library(<object librarytarget name> OBJECT <source file 1> <source file 2> ...) Later, we can use object libraries as dependencies of other targets using target_sources command or specify them instead of source files in the add_library or add_executable commands. To specify the object library, we can use the expression $<TARGET_OBJECTS:<object library target name>> .","title":"Object libraries"},{"location":"Programming/C%2B%2B/CMake%20Manual/#targets-automatically-generated-by-cmake","text":"listed here . Besides the user-defined targets, CMake automatically generates some targets. These are: all : alias for building all targets. Default target if the --target argument is not specified. In Visual Studio and Xcode generators, this target is called ALL_BUILD . we can exclude some targets from the all target using the EXCLUDE_FROM_ALL target property or the EXCLUDE_FROM_ALL directory property . clean target that cleans the build directory install target that installs the project. It depends on the all target. this target is only available if the CMakelists.txt file contains the install command (see installation configuration ). and some more...","title":"Targets automatically generated by CMake"},{"location":"Programming/C%2B%2B/CMake%20Manual/#set-properties-for-a-target","text":"To set compile options for a target, use the target_compile_definitions command. The syntax is: target_compile_definitions(<target name> <SCOPE> <definition 1> <definition 2> ...)","title":"Set properties for a target"},{"location":"Programming/C%2B%2B/CMake%20Manual/#include-directories","text":"Typically, we only need to include the headers for non-standard packages , as the standard cmake config packages include the headers automatically from the packege config files that are read by the find_package command. To include the headers, we need to use a inlude_directories (global), or better target_include_directories command. The difference: target specification : target_include_directories specifies the include directories for a specific target, while include_directories specifies the include directories for all targets in the current directory. mode specification : target_include_directories specifies the mode of the include directories (e.g., PUBLIC , PRIVATE , INTERFACE ), while include_directories behaves simillar to PRIVATE . Therefore, for libraries, the target_include_directories has to be used.","title":"Include directories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#debugging-the-include-directories","text":"If the headers are not found during the compilation of a CMake target, there are several steps we should take to debug the issue: Locate the missing header file. If the file does not exist, the package may not be installed at all. Check that the include in the source code match the header location. Sometimes, the package is moved into a subdirectory, without notification (e.g., we have to change #include <header.h> to #include <package/header.h> ). Check the include paths for the problematic target: cmake message(STATUS \"Listing include directories for DARP-benchmark target:\") get_target_property(DARP-benchmark-include-test DARP-benchmark INCLUDE_DIRECTORIES) foreach(dir ${DARP-benchmark-include-test}) message(STATUS \"dir='${dir}'\") endforeach()","title":"Debugging the include directories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#inspect-the-global-include-directories","text":"All the global include directories are stored in the INCLUDE_DIRECTORIES property, to print them, add this to the CMakeLists.txt file: get_property(dirs DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} PROPERTY INCLUDE_DIRECTORIES) foreach(dir ${dirs}) message(STATUS \"dir='${dir}'\") endforeach()","title":"Inspect the global include directories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#linking-configuration","text":"For linking, use the target_link_libraries command. The general syntax is: target_link_libraries(<target name> <library 1> <library 2> ...) We can use multiple commands for a single target: target_link_libraries(<target name> <library 1>) target_link_libraries(<target name> <library 2>) Make sure that you always link against all the libraries that are needed for the target to work! Do not rely on the linker errors, these may not appear due to library preloading, indirect linkage, advanced linker heuristics, etc. The result is that on one machine the code will work, but on another, it will fail. To find out if and how to link against a library, refer to the documentation of the library. The <library> can be: a library target name : The name of the target that is defined using the add_library command. this is the way how to link against vcpkg libraries check the usage file of the library for the correct target name a full path to the library file a library name : The name of the library without any prefix (e.g., -l ) or suffix (e.g., .a , .so , .dll ). this works for the system libraries and other libraries that are in the system path a link flag : A flag that is passed to the linker. for libraries integrated with the linker a generator expression : A generator expression that is evaluated during the CMakelists.txt configuration to one of the above options. Note that cmake does not check the validity of the supplied <library> argument! . Always check the documentation of the library to find out how to link against it. If we want to double-check that the supplied <library> is valid, we can: for the library target name, check the presence of the target: cmake if(NOT TARGET <library>) message(FATAL_ERROR \"The target <library> does not exist!\") endif()","title":"Linking configuration"},{"location":"Programming/C%2B%2B/CMake%20Manual/#checking-the-full-path-to-a-linked-library","text":"Sometimes, it can be usefull to check the full path to a linked library. This can be done using the get_target_property command: get_target_property(LIB_PATH <library name> IMPORTED_LOCATION) message(STATUS \"LIB_PATH=${LIB_PATH}\")","title":"Checking the full path to a linked library"},{"location":"Programming/C%2B%2B/CMake%20Manual/#fixing-incorrect-linking-configuration-in-upstream-projects","text":"Sometimes, an upstream project may incorrectly link a library as a private dependency, despite the fact that it is needed at runtime by the downstream project. We can fix it by set up the linking in our project: target_link_libraries(<upstream target> INTERFACE <library name>) Of course, the <library name> has to be known, i.e., we may need to also add the appropriate find_package command to our project.","title":"Fixing incorrect linking configuration in upstream projects"},{"location":"Programming/C%2B%2B/CMake%20Manual/#handling-runtime-dependencies-in-the-output-directory","text":"When linking dynamically (e.g., by using dynamic toochain, which is default on Windows), the runtime dependencies must be available at runtime, otherwise the program will not run. On Windows, this is usually solved by copying the runtime dependencies to the output directory. There are several ways how to do that. when using vcpkg , the runtime dependencies are copied automatically if the VCPKG_APPLOCAL_DEPS variable is set to ON otherwise, we can copy the runtime dependencies using the add_custom_command command or we can just manually copy the runtime dependencies to the output directory","title":"Handling runtime dependencies in the output directory"},{"location":"Programming/C%2B%2B/CMake%20Manual/#copying-runtime-dependencies-using-add_custom_command","text":"We can use the add_custom_command command together with the $<TARGET_RUNTIME_DLLS:<target name>> generator expression. Be careful to wrap this code by a condition that checks the generator type, as the generator expression is only available for DLL-aware generators. if(CMAKE_GENERATOR MATCHES \"Visual Studio.*\") add_custom_command( TARGET <target name> POST_BUILD COMMAND ${CMAKE_COMMAND} -E copy $<TARGET_RUNTIME_DLLS:<target name>> $<TARGET_FILE_DIR:<target name>> COMMAND_EXPAND_LISTS ) endif() Here, the COMMAND_EXPAND_LISTS is used to expand the generator expressions in the command. We use it for the $<TARGET_RUNTIME_DLLS:<target name>> generator expression that returns the list of runtime dependencies of the target.","title":"Copying runtime dependencies using add_custom_command"},{"location":"Programming/C%2B%2B/CMake%20Manual/#runtime-dependencies-and-subdirectories","text":"When using the add_subdirectory command, a new build directory is created for the subdirectory. However, the runtime dependencies in the parent directory are not accessible and therefore it must be duplicated in the subdirectory ( VCPKG_APPLOCAL_DEPS handles this automatically). One can think that we can solve that by using the same output directory for the parent and the subdirectory (using the build dir parameter of the add_subdirectory command ). However, this is prohibited by CMake.","title":"Runtime dependencies and subdirectories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#installation-configuration","text":"Importing and Exporting Guide To enable installation, we have to provide several commands and do some adjustments in the CMakeLists.txt file. Specific steps depends on what we want to achieve. The minimal installation that installs only binaries and headers can be set up using two commands: Install binaries with the install(TARGETS... command. The basic syntax is: cmake install(TARGETS <target name 1> <target name 2>) Install headers with the install(FILES... command. The basic syntax is: cmake install(FILES <file 1> <file 2> DESTINATION <destination>) We have to install both public API headers and their internal dependencies. Usually, this means installing most of the headers. The <destination> is the directory where the files will be installed. Typically, it is ${CMAKE_INSTALL_INCLUDEDIR}/${PROJECT_NAME} for header files. We usually only want a single set of headers to be installed for both Debug and Release builds ( for vcpkg, this is required ). This can be achieved by using the CONFIGURATIONS parameter of the install command. Example: cmake install( FILES <file 1> <file 2> DESTINATION <destination> CONFIGURATIONS Release ) These commands provide the install target that builds all targets (it depends on the all target) and then installs the project, which means that it copies the binary files to the installation directory.","title":"Installation Configuration"},{"location":"Programming/C%2B%2B/CMake%20Manual/#install-cmake-files","text":"If we want our library to be used from CMake projects, the basic configuration is not enough. We have to provide the CMake configuration files that will be used by the find_package command when searching for the dependency. Usually, CMake packages have the following cmake files: <package name>Targes.cmake - contains the targets that are installed by the package <package name>Config.cmake - contains the configuration of the package, i.e., the include directories, linking directories, and other variables that are needed to use the package. <package name>ConfigVersion.cmake - contains the version of the package and the compatibility check.","title":"Install CMake files"},{"location":"Programming/C%2B%2B/CMake%20Manual/#installing-the-targets-file","text":"This involves two steps: use the EXPORT parameter of the install(TARGETS... command to create a reference to installed targets that we can use further cmake install(TARGETS <target name 1> <target name 2> EXPORT <export name> ) The <export name> is the name of the export set that will be used in the CMake configuration file. Typically, it is ${PROJECT_NAME}Targets . use the install(EXPORT... command to install the cmake files cmake install(EXPORT <export name> DESTINATION <destination> NAMESPACE <namespace> ) The <export name> is the name of the export set that was used in the install(TARGETS... command The <destination> is the directory where the CMake configuration files will be installed. Typically, it is ( source ): ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME} for binaries share/cmake/${PROJECT_NAME} for header-only libraries for vcpkg : share/${PROJECT_NAME} . Otherwise, we need to mess with the CONFIG_PATH parameter of the vcpkg_cmake_config_fixup function. for using the CMAKE_INSTALL_LIBDIR variable, we have to include the GNUInstallDirs module using the include command. The <namespace> is the namespace that will be used in the CMake configuration file, and later in the target_link_libraries command of the dependent project. It is not required, but it is recommended because the namespace with :: only searches between the exported targets, preventing a possible name clash with some library installed in the system. typically, it has the form of <project name>::","title":"Installing the Targets file"},{"location":"Programming/C%2B%2B/CMake%20Manual/#the-package-configuration-file-and-the-version-file","text":"official guide - package config file In order for these files to be portable, they should be generated. The appropriate functions for this are in the `CMakePackageConfigHelpers module , which has to be included.","title":"The package configuration file and the version file"},{"location":"Programming/C%2B%2B/CMake%20Manual/#the-package-configuration-file","text":"To generate the package configuration file: create the input file somewhere in the project directory with the name <package name>Config.cmake.in and fill it with the following content: ```cmake @PACKAGE_INIT@ include (\"${CMAKE_CURRENT_LIST_DIR}/ Targes.cmake\") optional content ... - The `@PACKAGE_INIT@` is a placeholder that will be replaced by the `write_basic_package_version_file` command. 1. then generate the package configuration file by ading the `configure_package_config_file` command to the `CMakeLists.txt` file: cmake configure_package_config_file( INSTALL_DESTINATION ) `` - here, the <cmake files installation path> is the same as the parameter in the install(EXPORT...` command.","title":"The package configuration file"},{"location":"Programming/C%2B%2B/CMake%20Manual/#the-version-file","text":"official guide - version file The version file is generated using the write_basic_package_version_file command. The syntax is: write_basic_package_version_file( <version file build path> VERSION <version> COMPATIBILITY AnyNewerVersion ) The <version file build path> is the path where the version file will be generated. The <version> is the version of the package. Typically, it is the version of the project: ${<project name>Major}.${<project name>Minor}.${<project name>Patch} . Finally, to install both files, we use the install(FILES... command: install(FILES <path to package config file in the build dir> <path to version file in the build dir> DESTINATION <cmake files installation path> )","title":"The version file"},{"location":"Programming/C%2B%2B/CMake%20Manual/#sanitation-of-the-public-include-directories","text":"Additionally, an extra step is needed in case we define include directories to be accessible from other projects (i.e., PUBLIC or INTERFACE mode). In this case, we have to use a special configuration for those directories so that a different path is used when the library is installed. For example, if we include the src directory using: target_include_directories(<target name> PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/src) we have to change it to: target_include_directories(<target name> PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/src> $<INSTALL_INTERFACE:include> )","title":"Sanitation of the public include directories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#handling-dependencies","text":"If our library depends on other libraries, the targets depending on our library may also need to link against these dependencies. The linking itself is automatically handled by CMake. However, we need to provide the variables that are needed for the in the package configuration file ( <packag name.cmake.in ). To do so, we use the find_dependency command from the `CMakeFindDependencyMacro module : @PACKAGE_INIT@ include(CMakeFindDependencyMacro) find_dependency(<dependency 1>) ... find_dependency(<dependency n>) include(\"${CMAKE_CURRENT_LIST_DIR}/<package name>Targets.cmake\") ... The name of the dependency is the name of the package that is used in the find_package command in the CMakeLists.txt file. Other parameters ( REQUIRED , CONFIG , etc.) are not needed as they are inherited from the find_package command.","title":"Handling dependencies"},{"location":"Programming/C%2B%2B/CMake%20Manual/#specify-the-targets-to-install","text":"The install command has a parameter TARGETS that can specify the targets that should be installed. The syntax is: However, the TARGETS parameter only affects the installation part of the install target . In other words, it determines what files are copied to the installation directory, but it does not affect the build process. Therefore, all targets are built regardless of the TARGETS parameter . To prevent the building of some targets when installing, we have several options: exclude selected targets from the all target using the EXCLUDE_FROM_ALL target property or the EXCLUDE_FROM_ALL directory property . disable building for the install target and use a custom wrapper target that builds only the targets that should be installed and then calls the install target.","title":"Specify the targets to install"},{"location":"Programming/C%2B%2B/CMake%20Manual/#selecting-the-targets-to-build-using-the-exclude_from_all-property","text":"For user targets added by the add_executable or add_library command, we can use the EXCLUDE_FROM_ALL target property: add_executable(my_target EXCLUDE_FROM_ALL <source file 1> <source file 2> ...) For targets added by the FetchContent_MakeAvailable command. The situation is more complicated. The FetchContent_Declare command provides the EXCLUDE_FROM_ALL option, but it does not work as expected. Instead, we have to set the EXCLUDE_FROM_ALL property for the target after the FetchContent_MakeAvailable command: FetchContent_Declare( <declaration name> ... ) FetchContent_MakeAvailable(<declaration name>) set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_ALL TRUE)","title":"Selecting the targets to build using the EXCLUDE_FROM_ALL property"},{"location":"Programming/C%2B%2B/CMake%20Manual/#disabling-the-building-of-the-install-target-and-using-a-custom-wrapper-target","text":"For this solution,we first need to disable the building of the install using: set(CMAKE_SKIP_INSTALL_ALL_DEPENDENCY true) . Then, we need to create a target that: a) builds the selected targets, b) calls the install target. CMAKE_SKIP_INSTALL_ALL_DEPENDENCY documentation","title":"Disabling the building of the install target and using a custom wrapper target"},{"location":"Programming/C%2B%2B/CMake%20Manual/#support-both-shared-and-static-libraries","text":"Sometimes, we want the user to be able to choose between shared and static libraries when installing the package. For vcpkg, this is required. To add this support, we have to: define option for choosing the shared or static library (most likely, we do not want to install both) install the correct library target based on the option install the export header file (in both cases, as we use the same headers for both shared and static libraries) The conditional installation can look like this: if(<package name>_BUILD_SHARED_LIBS) set_target_properties(<static lib target> PROPERTIES EXCLUDE_FROM_ALL TRUE) install(TARGETS <shared lib target> EXPORT <export name>) else() set_target_properties(<shared lib target> PROPERTIES EXCLUDE_FROM_ALL TRUE) set_target_properties(<static lib target> PROPERTIES EXPORT_NAME <shared lib target>) # this aligns the name of the exported targets install(TARGETS <static lib target> EXPORT <export name>) endif() Here, the set_target_properties command is used to exclude the other target from the all target, effectively preventing it from being built during the installation. The install command is then used to install the correct target. Finally, we just copy the export header file to the installation directory: install(FILES ${CMAKE_CURRENT_BINARY_DIR}/<export header file> DESTINATION <install include dir>)","title":"Support both shared and static libraries"},{"location":"Programming/C%2B%2B/CMake%20Manual/#support-both-debug-and-release-builds","text":"In vcpkg, the Debug and Release builds are separated automatically, so the side-by-side installation of Debug and Release builds is supported out of the box. However, for other installations, e.g., when installing the package using the CMake install command, we have to take care of this ourselves. The standard way to do that is to add a postfix to all targets so that the debug binaries can be distinguished from the release binaries and one installation does not overwrite the other. For that, set the DEBUG_POSTFIX property for each target: set_target_properties( <target name> PROPERTIES DEBUG_POSTFIX <postfix> ) Usually, the postfix is d is used for debug builds. When using this postfix, no further configuration is needed, the correct binary will be used in the client depending on its own configuration.","title":"Support both Debug and Release builds"},{"location":"Programming/C%2B%2B/CMake%20Manual/#installation-of-executables","text":"Executables are installed just like libraries. There are two differences: we do not need to export the executables, so we skip the EXPORT parameter of the install(TARGETS... command on windows, when linking the executable to shared libraries and not using vcpkg, we have to manually copy the runtime dependencies to the installation bin directory. As this is done after build, we can use the TARGET_RUNTIME_DLLS generator expression: cmake install(FILES $<TARGET_RUNTIME_DLLS:<target name>> DESTINATION ${CMAKE_INSTALL_BINDIR})","title":"Installation of executables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#specific-configuration-for-frequentlly-used-libraries","text":"for google test, we want to prevent the installation of the gtest targets. To do that, turn it off before the gtets config in the CMakeLists.txt file: ```cmake","title":"Specific configuration for frequentlly used libraries"},{"location":"Programming/C%2B%2B/CMake%20Manual/#google-test","text":"","title":"GOOGLE TEST"},{"location":"Programming/C%2B%2B/CMake%20Manual/#do-not-install-gtest","text":"set(INSTALL_GTEST OFF) include(FetchContent) FetchContent_Declare( googletest URL https://github.com/google/googletest/archive/03597a01ee50ed33e9dfd640b249b4be3799d395.zip )","title":"do not install gtest"},{"location":"Programming/C%2B%2B/CMake%20Manual/#for-windows-prevent-overriding-the-parent-projects-compilerlinker-settings","text":"set(gtest_force_shared_crt ON CACHE BOOL \"\" FORCE) FetchContent_MakeAvailable(googletest) ```","title":"For Windows: Prevent overriding the parent project's compiler/linker settings"},{"location":"Programming/C%2B%2B/CMake%20Manual/#multiple-cmakeliststxt-files-in-subdirectories","text":"The configuration can be modularized by splitting the CMakeLists.txt file into multiple files, each in a separate directory. Then, these files can be executed using the add_subdirectory command in the main CMake_lists.txt file or one of the CMakeLists.txt files already added by the add_subdirectory command. This has several advantages : the configuration is more organized, it is easier to find the relevant part of the configuration or reuse the configuration in other projects the configuration is more configurable, i.e., we can turn on or off the configuration of a specific part of the project. The order of execution follows the order of the add_subdirectory commands, i.e., the processing of the CMakeLists.txt containing the add_subdirectory command is paused until the added CMakeLists.txt file is processed. The syntax of the add_subdirectory command is: add_subdirectory(<source dir> [<binary dir>]) By default, the <binary dir> is the same as the <source dir> .","title":"Multiple CMakeLists.txt files in subdirectories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#variables-scope","text":"The variable scope for multiple CMakeLists.txt files is hierarchical. This means that the variables defined in the parent CMakeLists.txt file are visible in the child CMakeLists.txt file, but not vice versa.","title":"Variables scope"},{"location":"Programming/C%2B%2B/CMake%20Manual/#decide-based-on-the-build-configuration","text":"Sometimes is essential to decide based on the build configuration in the CMakeLists.txt file. However, this is not possible for the multi-configuration generators like Visual Studio or Xcode because the build configuration is not known during the CMake configuration step. However, there are measures that can be taken to achieve the result for particular tasks. For install command, we can use the CONFIGURATIONS parameter to install the files only for the selected build configuration. Example: install( FILES <file 1> <file 2> DESTINATION <destination> CONFIGURATIONS Release ) For building, there are ways to limit the build based on the build configuration for both single- and multi-configuration generators. For single configuration generators, we can use the CMAKE_BUILD_TYPE variable and simply exclude the target from the build using the EXCLUDE_FROM_ALL target property: if(CMAKE_BUILD_TYPE STREQUAL \"Debug\") set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_ALL TRUE) endif() For multi-configuration generators, this variable is not set, but we can use the EXCLUDE_FROM_DEFAULT_BUILD_DEBUG property: set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_DEFAULT_BUILD_DEBUG TRUE) To support both single- and multi-configuration generators, we have to first check whether we use a single- or multi-configuration generator using the CMAKE_CONFIGURATION_TYPES variable and then set the property accordingly: if(CMAKE_CONFIGURATION_TYPES) set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_DEFAULT_BUILD_DEBUG TRUE) else() if(CMAKE_BUILD_TYPE STREQUAL \"Debug\") set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_ALL TRUE) endif() endif()","title":"Decide based on the build configuration"},{"location":"Programming/C%2B%2B/CMake%20Manual/#executing-external-commands","text":"There are different ways to execute external commands in the CMakeLists.txt , the proper way depends on the time when we want to execute the command: configuration time : use the execute_process command build time : use the add_custom_command command can be run both before and after the build step using the PRE_BUILD and POST_BUILD options To both commands, we can pass the COMMAND as a list of strings, where the first string is the command to execute. Additionally, when using the add_custom_command command, we can to use the COMMAND_EXPAND_LISTS option to expand the generator expressions in the command.","title":"Executing external commands"},{"location":"Programming/C%2B%2B/CMake%20Manual/#cmake-cache","text":"CMake has two types of variables: normal variables that are used just like in any other programming language and cache variables , which are configured in the first cmake run and then stored in the CMakeCache.txt file in the build directory. The CMake cache is an essential part of the CMake build system. It stores variables that are used to configure the build scripts. Cache variables can be set in the following ways: by the set command in the CMakeLists.txt file with the CACHE option by the -D command line argument: cmake -D<variable name>=<variable value> <dir> by the cmake GUI by using the -C option: cmake -C <cache file> <dir> The rule is that once a cache variable is set, it is not changed when the cmake command is run again (that is why it is called cache :) ). Moreover, the cache variables are not overwritten by the set command in the CMakeLists.txt file. In other words, the set command in the CMakeLists.txt is only used to set the default value of the variable. If the variable is already set in the cache, the set command is ignored. However, the cache variables can be still overridden from the CMakeLists.txt if the set command is used without the CACHE option (by normal variables).","title":"CMake Cache"},{"location":"Programming/C%2B%2B/CMake%20Manual/#cmake-directory-structure","text":"","title":"CMake Directory Structure"},{"location":"Programming/C%2B%2B/CMake%20Manual/#system-find_xxxcmake-files","text":"The system find scripts are located in the CMake/share/cmake-<version>/Modules/ directory.","title":"System Find_XXX.cmake files"},{"location":"Programming/C%2B%2B/CMake%20Manual/#various-tasks","text":"","title":"Various Tasks"},{"location":"Programming/C%2B%2B/CMake%20Manual/#showing-the-generator-for-a-configured-directory","text":"If the configuration step is already done, we can show the generator used for the configuration by reading the CMAKE_GENERATOR variable from the CMakeCache.txt file: Get-Content CMakeCache.txt | Select-String -Pattern \"CMAKE_GENERATOR\"","title":"Showing the generator for a configured directory"},{"location":"Programming/C%2B%2B/CMake%20Manual/#display-available-generators","text":"Unfortunately, there is no way how to display the available generators on the machine [ source ]. We can only list the generators that are compatible with the system by running the cmake --help . However, typically, the default generator marked with the * is available.","title":"Display available generators"},{"location":"Programming/C%2B%2B/CMake%20Manual/#determine-the-binary-output-directory","text":"Sometimes, we need to know where the binary files are stored. Unfortunately, there is no direct way to get this information from the CMake variables. We can only get the directory in the post-build step for a binary target using the TARGET_FILE_DIR generator expression: # prints the binary directory of the target after the build add_custom_command( TARGET <target name> POST_BUILD COMMAND ${CMAKE_COMMAND} -E echo $<TARGET_FILE_DIR:<target name>> )","title":"Determine the binary output directory"},{"location":"Programming/C%2B%2B/CMake%20Manual/#debugging-cmake","text":"","title":"Debugging CMake"},{"location":"Programming/C%2B%2B/CMake%20Manual/#debugging-cmake-using-clion-cmake-debugger","text":"documentation In CLion, we can debug the CMake configuration process by: opening the CMakeLists.txt file setting the breakpoint(s) starting the debugging process by clicking on the play button which is located on the first line of the CMakeLists.txt file on the left side of the editor window. or by clicking on the debug button in the main toolbar","title":"Debugging CMake using CLion CMake debugger"},{"location":"Programming/C%2B%2B/CMake%20Manual/#getting-a-a-complete-output-of-the-cmake-configuration","text":"We can get the complete output of the CMake configuration by running the cmake command with the --trace option. This can be especially useful when investigating included scripts, as the debuggers usually cannot step into CMake calls. Usually, the output is very long, so it is recommended to redirect it to a file: cmake --trace <dir> *> cmake_trace.txt If we want to also expand the variables, we can use the --trace-expand option.","title":"Getting a a complete output of the CMake configuration"},{"location":"Programming/C%2B%2B/CMake%20Manual/#testing-with-cmake","text":"documentation CMake has a built-in capability for organizing and running tests. This is useful, because for testing, we usually need information about the build configuration, which is already available in CMake. To enable testing in a project, we have to: add include(CTest) to the CMakeLists.txt file, add the individual tests using the add_test command, and configure the project Then, we can run the tests using the ctest command.","title":"Testing with CMake"},{"location":"Programming/C%2B%2B/CMake%20Manual/#adding-tests","text":"The add_test command is used to add a test to the project. The syntax is: add_test(NAME <test name> COMMAND <command>) By default, the command is executed in the build directory. Therefore, we can directly call the project executable by using the target name. We can also modify the test (e.g.: check the output of the test) using the set_property command: set_property(TEST <test name> PROPERTY <property name> <property value>) The most useful properties are: PASS_REGULAR_EXPRESSION : a regular expression that the output of the test has to match: cmake set_property(TEST <test name> PROPERTY PASS_REGULAR_EXPRESSION \"expected output\")","title":"Adding tests"},{"location":"Programming/C%2B%2B/CMake%20Manual/#ctest-execution","text":"documentation The ctest executable run the tests configured with CMake and reports the results. It can run in three modes: Run Tests mode (default): runs the tests and reports the results Build and Test mode: builds the tests and then runs them activated by the --build-and-test argument Dashboard mode: run CTest as a client of the CDash dashboard application. activated by one of the -D ( --dahboard ), -M ( --test-model ), -S ( --script ), or -SP ( --script-new-process ) arguments this mode facilitates every phase of the testing process, from updating the source code to running the tests and collecting the results if configured correctly, the results can be displayed on the CDash server dashboard Important parameters for all modes: -C <config> : specifies the configuration to use (e.g., Debug , Release , etc.) Only applies for multi-configuration generators This configuration is used by the build step if the CTEST_BUILD_COMMAND variable is not set, but also in other steps, like running the tests. Therefore, it is important to set this parameter even if we provide a custom build command, otherwise, the tests will be skipped. -V : verbose output -VV : very verbose output -O <file> : output the results to a file","title":"CTest execution"},{"location":"Programming/C%2B%2B/CMake%20Manual/#run-tests-mode-default","text":"The default run mode expects the project to be configured and built. It simply runs the tests and reports the results. For multi-configurations generators, we have to specify the configuration using the -C argument.","title":"Run Tests mode (default)"},{"location":"Programming/C%2B%2B/CMake%20Manual/#build-and-test-mode","text":"The build and test mode is activated by the --build-and-test argument. Unlike the cmake command, the ctest command does not choose the generator automatically. Instead, we have to supply the generator using the --build-generator argument. Other useful arguments: --build-options : additional options for the cmake command, e.g., --toolchain <path> to specify the toolchain file","title":"Build and Test mode"},{"location":"Programming/C%2B%2B/CMake%20Manual/#dashboard-mode","text":"CDash documentation at cmake.org Unlike the previous two modes, the dashboard mode facilitates every phase of the testing process, namely: updating the source code from the version control system configuring the project building the project running the tests collecting the results submitting the results to the CDash server This is iteself a strong argument for using the dashboard mode but the main advantage is that in this mode, we can see the results of the process on the CDash server dashboard. However, this feature requires some extra configuration. There are two main ways to use the dashboard mode: configure the dashboard mode using the -D argument together with the dahsboard command line arguments configure the dashboard mode using a cmake script and then run the script using the -S or -SP argument","title":"Dashboard mode"},{"location":"Programming/C%2B%2B/CMake%20Manual/#using-the-dashboard-mode-configured-by-the-script","text":"When run with the -S or -SP argument, the ctest executable runs the script that configures the dashboard mode. The -SP mode only differs in that it runs the script in a new process. The script has to manage all the dashboard mode phases. For each phase, there is a corresponding command cmake_<phase> that has to be called. The commands are: ctest_start : Initializes the dashboard mode. The only required parameter is a positional parameter mode or model, which is one of the following: Continuous : for continuous integration Nightly : for nightly builds Experimental : for local testing ctest_update : Updates the source code from version control (Git, SVN, etc.) optional phase ctest_configure : Configures the project ctest_build : Builds the project optional phase ctest_test : Runs the tests ctest_coverage : Collects the coverage information optional phase ctest_memcheck : Runs the memory check optional phase ctest_submit : Submits the results to the CDash server The above commands depend on some cmake variables (some of them can be replaced by additional arguments of the command): CTEST_SOURCE_DIRECTORY : the source directory of the project (the directory where the CMakeLists.txt file is located) CTEST_BINARY_DIRECTORY : the build directory of the project CTEST_CONFIGURE_COMMAND : the command that configures the project. Typically cmake command with some arguments. CTEST_BUILD_COMMAND : the command that builds the project. Typically cmake --build command with some arguments.","title":"Using the Dashboard mode configured by the script"},{"location":"Programming/C%2B%2B/CMake%20Manual/#configuring-the-dashboard","text":"To see the results of the process in a nice GUI, we need a project on a CDash server. We can either use the public CDash server or set up our own server. To use the public CDash server, we have to: create an account on the CDash server create a project on the server download the CTestConfig.cmake file from the project page and put it in the project directory (the directory where the CMakeLists.txt file is located)","title":"Configuring the Dashboard"},{"location":"Programming/C%2B%2B/CMake%20Manual/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Programming/C%2B%2B/CMake%20Manual/#problems","text":"","title":"Problems"},{"location":"Programming/C%2B%2B/CMake%20Manual/#warning-name-library-version-mismatched-error","text":"This error typically occures when the library <name> used during the build is different from the library used at runtime when running the tests. This can happen due to following scenario: The library relies on the PATH variable to find the library at runtime, but the path used during the build is specified manually. There is another library with the same name earlier in the PATH variable, and the version differs. The solution is: Identify the path used incorrectly at runtime Move the problematic path after the correct path in the PATH variable The identification step can be hard here, as the ctest does not report the real path to the library used at runtime, but the path where this library was compiled. In other words, the Installation point under the General Information is incorrect. The identification of the real path to the problematic library can be done as follows: build the tests manually using CMake run a single test in the debugger and break open the Process Explorer View -> Lower Pane View -> DLLs click on the test process In the Lower Pane View, there is a list of all DLLs loaded by the test process. Find the problematic library and check the path to it.","title":"Warning! &lt;name&gt; library version mismatched error"},{"location":"Programming/C%2B%2B/CMake%20Manual/#test-fixtures","text":"In CMake, there is no dirrect suppor for fixtures as we know them from other testing frameworks. Instead, we define the setup and teardown code as separate tests and then, we use a set_tests_properties command to set up the dependencies between the tests. Example: add_test(NAME setup COMMAND <setup command>) add_test(NAME test1 COMMAND <test 1 command>) add_test(NAME test2 COMMAND <test 2 command>) add_test(NAME teardown COMMAND <teardown command>) set_tests_properties(test1, test2 PROPERTIES FIXTURES_REQUIRED my_test_suite) set_tests_properties(setup PROPERTIES FIXTURES_SETUP my_test_suite) set_tests_properties(teardown PROPERTIES FIXTURES_CLEANUP my_test_suite)","title":"Test fixtures"},{"location":"Programming/C%2B%2B/CMake%20Manual/#running-tests-not-related-to-any-cmake-project","text":"There is no way how to run tests that are not related to any CMake project directly using the ctest command, and we cannot add such tests in the ctest starter script. The minimal solution is to: create a dummy CMake project that only contains the tests (no targets, etc.) skip the build step in the ctest starter script run the tests starter normally using the ctest command","title":"Running tests not related to any CMake project"},{"location":"Programming/C%2B%2B/Google%20Test/","text":"Assertions \u00b6 To determine whether a test should pass or fail, we use assertion macros. There are two main types of assertions: ASSERT_* - generates a fatal failure when it fails, aborting the current function EXPECT_* - generates a nonfatal failure, allowing the function to continue running Basic Assertions \u00b6 _EQ(computed, expected) - tests that expected == actual . == must be defined for the type of the arguments.","title":"Google Test"},{"location":"Programming/C%2B%2B/Google%20Test/#assertions","text":"To determine whether a test should pass or fail, we use assertion macros. There are two main types of assertions: ASSERT_* - generates a fatal failure when it fails, aborting the current function EXPECT_* - generates a nonfatal failure, allowing the function to continue running","title":"Assertions"},{"location":"Programming/C%2B%2B/Google%20Test/#basic-assertions","text":"_EQ(computed, expected) - tests that expected == actual . == must be defined for the type of the arguments.","title":"Basic Assertions"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/","text":"Introduction \u00b6 Vcpkg is a package manager for C++ libraries it serves as a developement package manager rather than a system package manager. Vcpkg can work in two modes: Classic mode : vcpkg is installed centrally. This mode is useful for development and testing. Manifest mode : vcpkg is installed in the project directory. This mode is useful for deployment. To install vcpkg: clone the repo to the desired location (project directory for manifest mode, any directory for classic mode) run the bootstrap script ( bootstrap-vcpkg.bat on Windows, bootstrap-vcpkg.sh on Linux) for classic mode, add the vcpkg directory to PATH , so the program can be run from anywhere Beware that to run it with sudo on linux, it is not that easy . Basic commands \u00b6 Install a package \u00b6 To install a package, use the install <package name> command. Important options are: --triplet <triplet> : the target triplet. --binarysource <binarysource> : the source of the binary packages. clear : do not use binary cache Remove a package \u00b6 To remove a package, use the remove <package name> command. Important options are: --triplet <triplet> : the target triplet. Search for a package \u00b6 To search for a package, use the search <package name> command. List installed packages \u00b6 To list the installed packages, use the list command. The first positional argument is the triplet filter, e.g., list x64-windows lists only the packages for the x64-windows triplet. CMake Integration \u00b6 documentation By default, CMake does not see the vcpkg. To set up the appropriate enviroment variables, paths, etc., we need to run cmake commands with path to cmake toolchain file: vcpkg/scripts/buildsystems/vcpkg.cmake . See the IDE and command line section for the detailed instructions how to execute cmake with the path to the vcpkg toolchain file. The toolchain file is executed early on, so it is safe to assume that the environment will be correctly set up before the commands in yor cmake script. Directory Structure \u00b6 vcpkg has the following directory structure: buildtrees : contains the build directories for each installed package. Each build directory contains the build logs. installed : contains the installed packages. It has subdirectories for each triplet. Each triplet directory is than divided into folloeing subdirectories: bin : contains the shared libraries debug : contains the debug version of everything in a similar structure as the triplet directory examples : contains example binaries include : contains the header files lib : contains the static libraries share : contains the cmake scripts and other files needed for the integration of the package into a cmake project tools : contains the executables installed with vcpkg packages ports : Contains the package information for each package from the official vcpkg list. There is no special way how to update just the port dir, so update the whole vcpkg by git pull in case you need to update the list of available packages. scripts : various scripts toolchains : cmake files that configure the toolchains. There is a special file for each platform (windows, linux, etc.) triplets : contains the triplet files. Modules \u00b6 Vcpkg has it s own find_package macro in the toolchain file. It executes the script: vcpkg/installed/<tripplet>/share/<package name>/vcpkg-cmake-wrapper.cmake , if exists. Then, it executes the cmake scripts in that directory using the standard find_package , like a cmake config package. Triplets \u00b6 documentation Vcpkg supports installing packages built for multiple platforms and compilers in the same vcpkg installation. To do this, vcpkg uses the concept of triplets . A triplet is definition of target environment. Usually, the triplet defines three things: the target platform (e.g., x64, arm) the target operating system (e.g., windows, linux) the target compiler (e.g., msvc, gcc) The triplet definition is stored in the triplet file in the <vcpkg root>/triplets directory. Changing the default triplet \u00b6 To change the default triplet, add a new system variable VCPKG_DEFAULT_TRIPLET , so your default library version installed with vcpkg will be x64 (like our builds), set it to: x64-linux for Linux Compilers x64-windows for MSVC x64-MinGW for MinGW Using a custom triplet \u00b6 If you need to test a specific system environment with vcpkg, you can use a custom triplet. Typically, you can create such a triplet by copying an existing one and modifying it. Typically, you just modify the triplet variables in the file. To use the custom triplet add two arguments to the vcpkg command: --triplet <triplet name> : the name of the custom triplet --overlay-triplets=<path to the directory containing the custom triplet> : the path to the directory containing the custom triplet file To change the compiler , it is a little bit more complicated, as there is no triplet variable for the compiler. Instead, we need to provide a custom toolchain: copy an existing toolchain file from the <vcpkg root>/scripts/toolchains . modify the toolchain file to use the desired compiler, e.g., by setting the CMAKE_CXX_COMPILER variable. in the custom triplet file, set the VCPKG_CHAINLOAD_TOOLCHAIN_FILE variable to point to the custom toolchain file. Update \u00b6 git pull bootstrap vcpkg again Windows: bootstrap-vcpkg.bat Linux: bootstrap-vcpkg.sh Update package \u00b6 Update vcpkg vcpkg update to get a list of available updates vcpkg upgrade --no-dry-run to actually perform the upgrade All packages are upgraded by default. To upgrade just one package, supply the name of the package (e.g., zlib:x64-windows) as an argument to upgrade command. Upgrade packages matching a pattern \u00b6 For libraries that are divided into many interdependent packages (like boost), it is useful to upgrade all packages that match a pattern at once. Unfortunately, the upgrade command does not support the pattern matching. The following command can be used to upgrade all packages that match a pattern in PowerShell: vcpkg update | sls -pattern \"boost-\\w+\" | foreach-object { vcpkg upgrade $_.Matches.Value --no-dry-run } Package Features \u00b6 Some libraries have optional features, which are not installed by default, but we can install them explicitely. For example llvm . After vcpkg install llvm and typing vcpkg search llvm : llvm 10.0.0#6 The LLVM Compiler Infrastructure llvm[clang] Build C Language Family Front-end. llvm[clang-tools-extra] Build Clang tools. ... llvm[target-all] Build with all backends. llvm[target-amdgpu] Build with AMDGPU backend. llvm[target-arm] Build with ARM backend. Above, we can see that there are a lot of optional targets. To install the the arm target, for example, we can use vcpkg install llvm[target-arm] . Sometimes, a new build of the main package is required, in that case, we need to type vcpkg install llvm[target-arm] --recurse . Package Versions \u00b6 In classic mode, there is no way how to control the version of a package. For that, we have to use the manifest mode Making vcpkg available for all CMake projects \u00b6 The cmake has no mechanism to set a default toolchain, so we cannot configure it to use vcpkg by default. The only way is to use a Shim for cmake, that calls cmake with the toolchain argument set to the vcpkg toolchain file. Such schim can be found in the cpp dev support repository . The shim will work in most contexts, but it is still ignored in CLion, as it has its own mechanism for finding the cmake executable. In Clion, we need to set up the path to shim as a cmake executable in the toolchain settings. Integrate your library to vcpkg \u00b6 For complete integration of your library to vcpkg, the following steps are needed: Configure and test the CMake installation Crate the port and test it locally ( vcpkg installation ) Submit the port to the vcpkg repository ( publishing ) Resources: decovar tutorial Create the Port \u00b6 The official guide for packageing Maintainer guide missing details from other guides contains the list of deprecated functions Vcpkg works with ports which are special directories containing all files describing a C++ package. The usuall process is: The usual port contain these files: portfile.cmake : the main file containing the calls to cmake functions that install the package vcpkg.json : metadata file containing the package name, version, dependencies, etc. usage : a file containing the usage instructions for the package. These instructions are displayed at the end of the installation process. example A simple portfile.cmake can look like this: # download the source code vcpkg_from_github( OUT_SOURCE_PATH SOURCE_PATH REPO <reo owner>/<repo name> REF <tag name> SHA512 <hash of the files> HEAD_REF <branch name> ) # configure the source code vcpkg_cmake_configure( SOURCE_PATH <path to source dir> ) # build the source code and install it vcpkg_cmake_install() # fix the cmake generaed files for vcpkg vcpkg_cmake_config_fixup(PACKAGE_NAME <package name>) # install the license vcpkg_install_copyright(FILE_LIST \"${SOURCE_PATH}/LICENSE.txt\") # install the usage file file(INSTALL \"${CMAKE_CURRENT_LIST_DIR}/usage\" DESTINATION \"${CURRENT_PACKAGES_DIR}/share/${PORT}\") Explanation: vcpkg_from_github : downloads the source code from the github repository the <path to source dir> is the directory where the CMakeLists.txt file is located. It is usually the directory where the source code is downloaded, so we can set it to ${SOURCE_PATH} the <hash of the files> can be easily obtained by: setting the ` to 0 running the vcpkg install <port name> copying the hash from the error message vcpkg_cmake_configure : configures the source code using cmake (wraps the cmake command) vcpkg_cmake_install : builds and installs the source code (wraps the cmake --build . --target install command) the majority of code is in the subroutine vcpkg_cmake_build if we need some libraries installed with vcpkg at runtime during the build of the package, we need to use the ADD_BIN_TO_PATH option in the vcpkg_cmake_install function . This is needed as the automatic dll copy to the output dir ( VCPKG_APPLOCAL_DEPS ) is disabelled by the vcpkg_cmake_build function. This option solve the problem by prepending the PATH environment variable with the path to the vcpkg installed libraries ( <vcpkg root>/installed/<triplet>/bin for release and <vcpkg root>/installed/<triplet>/debug/bin for debug). vcpkg_cmake_config_fixup : fixes the cmake generated files for vcpkg. This is needed because the cmake generated files are not compatible with vcpkg. The function fixes the CMakeConfig.cmake and CMakeConfigVersion.cmake files. the <package name> is the name of the package, usually the same as the port name vcpkg_install_copyright installs the license files listed in the FILE_LIST argument to share/<port name>/copyright file. The copyright file is obligatory for the package to be accepted to the vcpkg repository. The vcpkg.json file can look like this: { { \"name\": \"fconfig\", \"version-string\": \"0.1.0\", \"description\": \"C++ implementation of the fconfig configuration system\", \"homepage\": \"https://github.com/F-I-D-O/Future-Config\", \"license\": \"MIT\", \"dependencies\": [ { \"name\" : \"vcpkg-cmake\", \"host\" : true }, \"yaml-cpp\", \"spdlog\", \"inja\" ] } } Here: the license key is obligatory and has to match the license file of the package The dependencies with the host key set to true are the dependencies that are required for the build, but not for the runtime. Variables and Functions available in the portfile.cmake \u00b6 The variables and functions available in the portfile.cmake are described in the create command documentation . The most important variables are: CURRENT_PACKAGES_DIR : the directory where the package is installed: <vcpkg root>/installed/<triplet>/<port name> Installation \u00b6 To install the port locally, run: vcpkg install <port name> For this command to work, the port has to be located in <vcpkg root>/ports/<port name> . If we want to install the port from an alternative location, we can use the --overlay-ports option. For example, if we have the port stored in the C:/custom_ports/our_new_port directory, we can install it by: vcpkg install our_new_port --overlay-ports=C:/custom_ports If the port installation is failing and the reason is not clear from stdout, check the logs located in <vcpkg root>/buildtrees/<port name>/ Reinistallation after changes \u00b6 During testing, we can reach a scenario where a) we successfully installed the port, b) we need to make some changes. In this case, we need to reinstall the port. However, it is not completely straightforward due to binary caching . The following steps are needed to reinstall the port: uninstall the port: vcpkg remove <port name> disable the binary cache by setting the VCPKG_BINARY_SOURCES environment variable to clear in PowerShell: $env:VCPKG_BINARY_SOURCES = \"clear\" in bash: export VCPKG_BINARY_SOURCES=clear if setting the environment variable does not work (WSL), we can specify the --binarysource=clear option in the next step install the port again: vcpkg install <port name> Executable installation \u00b6 In general vcpgk does not allow to install executables, as it is a dependency manager rather than a package manager for OS. However, it is possible to install executables that are intedned to be used as tools (to the installed/<triplet>/tools directory) used in the build process. To do so, you have to add the vcpgk_copy_tools call to the portfile.cmake file: vcpkg_copy_tools( TOOL_NAMES <tool target name> AUTO_CLEAN ) The AUTO_CLEAN option ensures that the tools are deleted from the bin directory. Without it the tools will be kept in the bin directory, resulting in warnings and non-complicance with the vcpkg rules. The vcpgk_copy_tools function also automatically copies the runtime dependencies of the tools to the tools directory. Executing installed tools from cmake \u00b6 The installed tools can be executed from cmake using cmake comands specified in the CMake manual . To specify the path to the tools directory, use the VCPKG_INSTALLED_DIR and VCPKG_TARGET_TRIPLET variables: execute_process( COMMAND ${VCPKG_INSTALLED_DIR}/${VCPKG_TARGET_TRIPLET}/tools/${PROJECT_NAME}/<tool name> ) Publishing \u00b6 official guide Before publishing the port, we should check for the following: all dependencies in CMakelists.txt are required ( find_package(<package name> REQUIRED) ) and listed in the vcpkg.json file in the dependencies array the port follows the maintainer guide , especially: the port name does not clash with existing packages (check at repology ) the port should work for both Windows and Linux and on both platforms, the port should support both static and dynamic linking. the PR checklist is followed Then, the submission process is as follows (The emphezised steps are not needed in case of fixing a failed release, i.e., when the release was rejected by vcpkg): create a fork of the vcpkg repository commit and push the changes to the project repository if not done yet create or replace a release in the projects GitHub repository update the verison in the vcpkg_from_github call in portfile.cmake update the version in the vcpkg.json file pull from the vcpgk fork repository copy the port ( portfile.cmake , vcpkg.json and usage ) to the vcpkg repository remove the local SOURCE_PATH overrides and uncomment the vcpkg_from_github call in portfile, assign the correct hash to the vcpkg_from_github call test the port installation locally without the --overlay-ports option format the vcpkg.json file using the vcpkg format-manifest <path to the vcpkg.json file> command create a new branch for the package commit the changes to the package branch in the vcpkg repository update the port version using the vcpkg x-add-version <port name> command commit again to the package branch in the vcpkg repository push the branch to the forked vcpkg repository open the forked repository in the browser and create a new pull request to the main vcpkg repository","title":"vcpkg Manual"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#introduction","text":"Vcpkg is a package manager for C++ libraries it serves as a developement package manager rather than a system package manager. Vcpkg can work in two modes: Classic mode : vcpkg is installed centrally. This mode is useful for development and testing. Manifest mode : vcpkg is installed in the project directory. This mode is useful for deployment. To install vcpkg: clone the repo to the desired location (project directory for manifest mode, any directory for classic mode) run the bootstrap script ( bootstrap-vcpkg.bat on Windows, bootstrap-vcpkg.sh on Linux) for classic mode, add the vcpkg directory to PATH , so the program can be run from anywhere Beware that to run it with sudo on linux, it is not that easy .","title":"Introduction"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#basic-commands","text":"","title":"Basic commands"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#install-a-package","text":"To install a package, use the install <package name> command. Important options are: --triplet <triplet> : the target triplet. --binarysource <binarysource> : the source of the binary packages. clear : do not use binary cache","title":"Install a package"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#remove-a-package","text":"To remove a package, use the remove <package name> command. Important options are: --triplet <triplet> : the target triplet.","title":"Remove a package"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#search-for-a-package","text":"To search for a package, use the search <package name> command.","title":"Search for a package"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#list-installed-packages","text":"To list the installed packages, use the list command. The first positional argument is the triplet filter, e.g., list x64-windows lists only the packages for the x64-windows triplet.","title":"List installed packages"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#cmake-integration","text":"documentation By default, CMake does not see the vcpkg. To set up the appropriate enviroment variables, paths, etc., we need to run cmake commands with path to cmake toolchain file: vcpkg/scripts/buildsystems/vcpkg.cmake . See the IDE and command line section for the detailed instructions how to execute cmake with the path to the vcpkg toolchain file. The toolchain file is executed early on, so it is safe to assume that the environment will be correctly set up before the commands in yor cmake script.","title":"CMake Integration"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#directory-structure","text":"vcpkg has the following directory structure: buildtrees : contains the build directories for each installed package. Each build directory contains the build logs. installed : contains the installed packages. It has subdirectories for each triplet. Each triplet directory is than divided into folloeing subdirectories: bin : contains the shared libraries debug : contains the debug version of everything in a similar structure as the triplet directory examples : contains example binaries include : contains the header files lib : contains the static libraries share : contains the cmake scripts and other files needed for the integration of the package into a cmake project tools : contains the executables installed with vcpkg packages ports : Contains the package information for each package from the official vcpkg list. There is no special way how to update just the port dir, so update the whole vcpkg by git pull in case you need to update the list of available packages. scripts : various scripts toolchains : cmake files that configure the toolchains. There is a special file for each platform (windows, linux, etc.) triplets : contains the triplet files.","title":"Directory Structure"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#modules","text":"Vcpkg has it s own find_package macro in the toolchain file. It executes the script: vcpkg/installed/<tripplet>/share/<package name>/vcpkg-cmake-wrapper.cmake , if exists. Then, it executes the cmake scripts in that directory using the standard find_package , like a cmake config package.","title":"Modules"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#triplets","text":"documentation Vcpkg supports installing packages built for multiple platforms and compilers in the same vcpkg installation. To do this, vcpkg uses the concept of triplets . A triplet is definition of target environment. Usually, the triplet defines three things: the target platform (e.g., x64, arm) the target operating system (e.g., windows, linux) the target compiler (e.g., msvc, gcc) The triplet definition is stored in the triplet file in the <vcpkg root>/triplets directory.","title":"Triplets"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#changing-the-default-triplet","text":"To change the default triplet, add a new system variable VCPKG_DEFAULT_TRIPLET , so your default library version installed with vcpkg will be x64 (like our builds), set it to: x64-linux for Linux Compilers x64-windows for MSVC x64-MinGW for MinGW","title":"Changing the default triplet"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#using-a-custom-triplet","text":"If you need to test a specific system environment with vcpkg, you can use a custom triplet. Typically, you can create such a triplet by copying an existing one and modifying it. Typically, you just modify the triplet variables in the file. To use the custom triplet add two arguments to the vcpkg command: --triplet <triplet name> : the name of the custom triplet --overlay-triplets=<path to the directory containing the custom triplet> : the path to the directory containing the custom triplet file To change the compiler , it is a little bit more complicated, as there is no triplet variable for the compiler. Instead, we need to provide a custom toolchain: copy an existing toolchain file from the <vcpkg root>/scripts/toolchains . modify the toolchain file to use the desired compiler, e.g., by setting the CMAKE_CXX_COMPILER variable. in the custom triplet file, set the VCPKG_CHAINLOAD_TOOLCHAIN_FILE variable to point to the custom toolchain file.","title":"Using a custom triplet"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#update","text":"git pull bootstrap vcpkg again Windows: bootstrap-vcpkg.bat Linux: bootstrap-vcpkg.sh","title":"Update"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#update-package","text":"Update vcpkg vcpkg update to get a list of available updates vcpkg upgrade --no-dry-run to actually perform the upgrade All packages are upgraded by default. To upgrade just one package, supply the name of the package (e.g., zlib:x64-windows) as an argument to upgrade command.","title":"Update package"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#upgrade-packages-matching-a-pattern","text":"For libraries that are divided into many interdependent packages (like boost), it is useful to upgrade all packages that match a pattern at once. Unfortunately, the upgrade command does not support the pattern matching. The following command can be used to upgrade all packages that match a pattern in PowerShell: vcpkg update | sls -pattern \"boost-\\w+\" | foreach-object { vcpkg upgrade $_.Matches.Value --no-dry-run }","title":"Upgrade packages matching a pattern"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#package-features","text":"Some libraries have optional features, which are not installed by default, but we can install them explicitely. For example llvm . After vcpkg install llvm and typing vcpkg search llvm : llvm 10.0.0#6 The LLVM Compiler Infrastructure llvm[clang] Build C Language Family Front-end. llvm[clang-tools-extra] Build Clang tools. ... llvm[target-all] Build with all backends. llvm[target-amdgpu] Build with AMDGPU backend. llvm[target-arm] Build with ARM backend. Above, we can see that there are a lot of optional targets. To install the the arm target, for example, we can use vcpkg install llvm[target-arm] . Sometimes, a new build of the main package is required, in that case, we need to type vcpkg install llvm[target-arm] --recurse .","title":"Package Features"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#package-versions","text":"In classic mode, there is no way how to control the version of a package. For that, we have to use the manifest mode","title":"Package Versions"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#making-vcpkg-available-for-all-cmake-projects","text":"The cmake has no mechanism to set a default toolchain, so we cannot configure it to use vcpkg by default. The only way is to use a Shim for cmake, that calls cmake with the toolchain argument set to the vcpkg toolchain file. Such schim can be found in the cpp dev support repository . The shim will work in most contexts, but it is still ignored in CLion, as it has its own mechanism for finding the cmake executable. In Clion, we need to set up the path to shim as a cmake executable in the toolchain settings.","title":"Making vcpkg available for all CMake projects"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#integrate-your-library-to-vcpkg","text":"For complete integration of your library to vcpkg, the following steps are needed: Configure and test the CMake installation Crate the port and test it locally ( vcpkg installation ) Submit the port to the vcpkg repository ( publishing ) Resources: decovar tutorial","title":"Integrate your library to vcpkg"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#create-the-port","text":"The official guide for packageing Maintainer guide missing details from other guides contains the list of deprecated functions Vcpkg works with ports which are special directories containing all files describing a C++ package. The usuall process is: The usual port contain these files: portfile.cmake : the main file containing the calls to cmake functions that install the package vcpkg.json : metadata file containing the package name, version, dependencies, etc. usage : a file containing the usage instructions for the package. These instructions are displayed at the end of the installation process. example A simple portfile.cmake can look like this: # download the source code vcpkg_from_github( OUT_SOURCE_PATH SOURCE_PATH REPO <reo owner>/<repo name> REF <tag name> SHA512 <hash of the files> HEAD_REF <branch name> ) # configure the source code vcpkg_cmake_configure( SOURCE_PATH <path to source dir> ) # build the source code and install it vcpkg_cmake_install() # fix the cmake generaed files for vcpkg vcpkg_cmake_config_fixup(PACKAGE_NAME <package name>) # install the license vcpkg_install_copyright(FILE_LIST \"${SOURCE_PATH}/LICENSE.txt\") # install the usage file file(INSTALL \"${CMAKE_CURRENT_LIST_DIR}/usage\" DESTINATION \"${CURRENT_PACKAGES_DIR}/share/${PORT}\") Explanation: vcpkg_from_github : downloads the source code from the github repository the <path to source dir> is the directory where the CMakeLists.txt file is located. It is usually the directory where the source code is downloaded, so we can set it to ${SOURCE_PATH} the <hash of the files> can be easily obtained by: setting the ` to 0 running the vcpkg install <port name> copying the hash from the error message vcpkg_cmake_configure : configures the source code using cmake (wraps the cmake command) vcpkg_cmake_install : builds and installs the source code (wraps the cmake --build . --target install command) the majority of code is in the subroutine vcpkg_cmake_build if we need some libraries installed with vcpkg at runtime during the build of the package, we need to use the ADD_BIN_TO_PATH option in the vcpkg_cmake_install function . This is needed as the automatic dll copy to the output dir ( VCPKG_APPLOCAL_DEPS ) is disabelled by the vcpkg_cmake_build function. This option solve the problem by prepending the PATH environment variable with the path to the vcpkg installed libraries ( <vcpkg root>/installed/<triplet>/bin for release and <vcpkg root>/installed/<triplet>/debug/bin for debug). vcpkg_cmake_config_fixup : fixes the cmake generated files for vcpkg. This is needed because the cmake generated files are not compatible with vcpkg. The function fixes the CMakeConfig.cmake and CMakeConfigVersion.cmake files. the <package name> is the name of the package, usually the same as the port name vcpkg_install_copyright installs the license files listed in the FILE_LIST argument to share/<port name>/copyright file. The copyright file is obligatory for the package to be accepted to the vcpkg repository. The vcpkg.json file can look like this: { { \"name\": \"fconfig\", \"version-string\": \"0.1.0\", \"description\": \"C++ implementation of the fconfig configuration system\", \"homepage\": \"https://github.com/F-I-D-O/Future-Config\", \"license\": \"MIT\", \"dependencies\": [ { \"name\" : \"vcpkg-cmake\", \"host\" : true }, \"yaml-cpp\", \"spdlog\", \"inja\" ] } } Here: the license key is obligatory and has to match the license file of the package The dependencies with the host key set to true are the dependencies that are required for the build, but not for the runtime.","title":"Create the Port"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#variables-and-functions-available-in-the-portfilecmake","text":"The variables and functions available in the portfile.cmake are described in the create command documentation . The most important variables are: CURRENT_PACKAGES_DIR : the directory where the package is installed: <vcpkg root>/installed/<triplet>/<port name>","title":"Variables and Functions available in the portfile.cmake"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#installation","text":"To install the port locally, run: vcpkg install <port name> For this command to work, the port has to be located in <vcpkg root>/ports/<port name> . If we want to install the port from an alternative location, we can use the --overlay-ports option. For example, if we have the port stored in the C:/custom_ports/our_new_port directory, we can install it by: vcpkg install our_new_port --overlay-ports=C:/custom_ports If the port installation is failing and the reason is not clear from stdout, check the logs located in <vcpkg root>/buildtrees/<port name>/","title":"Installation"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#reinistallation-after-changes","text":"During testing, we can reach a scenario where a) we successfully installed the port, b) we need to make some changes. In this case, we need to reinstall the port. However, it is not completely straightforward due to binary caching . The following steps are needed to reinstall the port: uninstall the port: vcpkg remove <port name> disable the binary cache by setting the VCPKG_BINARY_SOURCES environment variable to clear in PowerShell: $env:VCPKG_BINARY_SOURCES = \"clear\" in bash: export VCPKG_BINARY_SOURCES=clear if setting the environment variable does not work (WSL), we can specify the --binarysource=clear option in the next step install the port again: vcpkg install <port name>","title":"Reinistallation after changes"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#executable-installation","text":"In general vcpgk does not allow to install executables, as it is a dependency manager rather than a package manager for OS. However, it is possible to install executables that are intedned to be used as tools (to the installed/<triplet>/tools directory) used in the build process. To do so, you have to add the vcpgk_copy_tools call to the portfile.cmake file: vcpkg_copy_tools( TOOL_NAMES <tool target name> AUTO_CLEAN ) The AUTO_CLEAN option ensures that the tools are deleted from the bin directory. Without it the tools will be kept in the bin directory, resulting in warnings and non-complicance with the vcpkg rules. The vcpgk_copy_tools function also automatically copies the runtime dependencies of the tools to the tools directory.","title":"Executable installation"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#executing-installed-tools-from-cmake","text":"The installed tools can be executed from cmake using cmake comands specified in the CMake manual . To specify the path to the tools directory, use the VCPKG_INSTALLED_DIR and VCPKG_TARGET_TRIPLET variables: execute_process( COMMAND ${VCPKG_INSTALLED_DIR}/${VCPKG_TARGET_TRIPLET}/tools/${PROJECT_NAME}/<tool name> )","title":"Executing installed tools from cmake"},{"location":"Programming/C%2B%2B/vcpkg%20Manual/#publishing","text":"official guide Before publishing the port, we should check for the following: all dependencies in CMakelists.txt are required ( find_package(<package name> REQUIRED) ) and listed in the vcpkg.json file in the dependencies array the port follows the maintainer guide , especially: the port name does not clash with existing packages (check at repology ) the port should work for both Windows and Linux and on both platforms, the port should support both static and dynamic linking. the PR checklist is followed Then, the submission process is as follows (The emphezised steps are not needed in case of fixing a failed release, i.e., when the release was rejected by vcpkg): create a fork of the vcpkg repository commit and push the changes to the project repository if not done yet create or replace a release in the projects GitHub repository update the verison in the vcpkg_from_github call in portfile.cmake update the version in the vcpkg.json file pull from the vcpgk fork repository copy the port ( portfile.cmake , vcpkg.json and usage ) to the vcpkg repository remove the local SOURCE_PATH overrides and uncomment the vcpkg_from_github call in portfile, assign the correct hash to the vcpkg_from_github call test the port installation locally without the --overlay-ports option format the vcpkg.json file using the vcpkg format-manifest <path to the vcpkg.json file> command create a new branch for the package commit the changes to the package branch in the vcpkg repository update the port version using the vcpkg x-add-version <port name> command commit again to the package branch in the vcpkg repository push the branch to the forked vcpkg repository open the forked repository in the browser and create a new pull request to the main vcpkg repository","title":"Publishing"},{"location":"Programming/Cloud%20computing/Lmod/","text":"Lmod is a module system that simplify the package management on the Linux cloud computing clusters. It is used on the RCI cluster. home user guide RCI cluster documentation module ( ml ) command \u00b6 The module performs different operations based on its first argument. The default operation (if the first/main argument is omited) is load . load <module name> : loads the module. Specific version can be used by adding / and the version number. If the version is not specified, the latest version is loaded. list : lists all loaded modules","title":"Lmod"},{"location":"Programming/Cloud%20computing/Lmod/#module-ml-command","text":"The module performs different operations based on its first argument. The default operation (if the first/main argument is omited) is load . load <module name> : loads the module. Specific version can be used by adding / and the version number. If the version is not specified, the latest version is loaded. list : lists all loaded modules","title":"module (ml) command"},{"location":"Programming/Cloud%20computing/RCI%20cluster/","text":"General \u00b6 Form to get access to the RCI cluster Official instructions Storage manual and description of the partitions Usual command Usage \u00b6 You can watch your jobs with squeue -u username . You test/debug your program when running it with srun command you usually don\u2019t have to allocate resources when testing To start an interactive shell, run: srun -p cpufast --pty bash -i Set your main script file (sh) executable via chmod +x <filename> command Test your script in console Cancel the job: scancel <JOB ID> You run your job with sbatch command with allocated resources Example sbatch: sbatch --mem=30G -t 60 -n30 -o /home/fiedlda1/Amodsim/log/ih.log /home/fiedlda1/Amodsim/rci_launchers/ih.sh How to clone projects \u00b6 Usually, you need to clone some of the SUM projects to start working on the RCI cluster. To do that: Copy your key to ~/.ssh/ Set file permissions to your ssh key safely Modify ~/.ssh/config IdentityFile to point to your key Clone your project Specifics for java projects \u00b6 Clone project on RCI cluster Download binary maven from: http://maven.apache.org/download.cgi and export it to your home folder on the RCI cluster. Prepare your bash script Add #!/bin/bash to the first line Change the environment variable PATH for your maven location with this command: PATH=$PATH:/home/$USER/apache-maven-3.6.1/bin/ Load all required software via ml command, definitely ml Java Build, compile, run your project via mvn commands Set your file executable via chmod +x filename command Example run command: mvn exec:exec -Dexec.executable=java -Dexec.args=\u2019-classpath %classpath -Xmx30g cz.cvut.fel.aic.amodsim.OnDemandVehiclesSimulation /home/kholkolg/amod-to-agentpolis/local_config_files/olga_vga_RCI.cfg\u2019 -Dfile.encoding=UTF-8 Example - Bash script for amodsim project: Run your script with srun, sbatch etc. commands, I recommend first use srun, to check everything is set up ok and then use sbatch command, because if computational nodes are busy, your job will be added to the queue and you can do other work. Specifics for python projects \u00b6 First load the appropriate version of python, e. G.: ml Python/3.6.6-foss-2018b You can\u2019t just install the packages with sudo , you have to install them to the user space instead: Run pip install --user packagename Specifics for C++ projects \u00b6 Workflow options \u00b6 As linux binaries are usually not portable. They are not compatible with older linux versions due to the infamous glibc incompatibility. There are three solutions to this problem: Method Setup Program Upgrade Compile the code on the RCI Setup the compilation on RCI. Copy the source code to RCI and recompile after every change Use a Singularity container learn with singularity, create the container Generate new container and copy it to the RCI Build a compatible binary using a modified toolchain learn with a toolchain generator, configure and generate the right toolchain Copy the updated binary Building on RCI \u00b6 In general the workflow is the same as on a local machine. The difference is that we do not have root access, so for all needed tools, we have to either load them via ml command, or, if not available, install them in the user space. Typically, we need to load: git: ml git GCC: ml GCC CMake: ml CMake Specific for projects with gurobi \u00b6 Load Gurobi with ml Gurobi or ml Gurobi/8.1.1-foss-2018b-Python-3.6.6 for a specific version Be aware that this operation can reload other packages Gurobi and Java \u00b6 It is necessary to install Gurobi to maven: mvn install:install-file -Dfile=/mnt/appl/software/Gurobi/9.0.3-GCCcore-8.3.0-Python-3.7.4/lib/gurobi.jar -DgroupId=com.gurobi -DartifactId=gurobi -Dversion=1.0 -Dpackaging=jar Gurobi and C++ \u00b6 As RCI use Linux as OS, we need to compile the Gurobi C++ libs with the same compiler as the one we use for compilation of our code (see C++ Workflow for more details). Note that this is necessary even if the Gurobi seems to be compiled with the same copiler we use for compilation . Unlike in Linux installation we controll, we cannot build the C++ lib in the Gurobi installation folder. To make the Linking work, foloow these steps: copy the src dir from the RCI Gurobi module located at mnt/appl/software/Gurobi/<desired version> to our home run make located in src/build copy the libgurobi_c++.a to the lib subfolder of your project configure the searching for the C++ lib in FindGUROBI.cmake file: ```cmake Find the Gurobi C library \u00b6 find_library(GUROBI_LIBRARY NAMES gurobi HINTS ${GUROBI_DIR} $ENV{GUROBI_HOME} PATH_SUFFIXES lib ) Find the Gurobi C++ library \u00b6 find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED ) Find the Gurobi C++ debug library. We don't need it on RCI, but it may be required in CMakelists.txt. Therefore, we supply the release version as a debug version. \u00b6 set(GUROBI_CXX_DEBUG_LIBRARY ${GUROBI_CXX_LIBRARY}) ``` if the CMake cache is already generated, delete it. Generate the CMake cache and build the project Commands \u00b6 For command description, see Slurm manual. Main params --mem=<required memory> <required memory> is in megabytes by default, for gigabytes, we need to add G -t <time> Time in minutes Task Explanations \u00b6 srun --pty bash -i - --pty runs the first task and close output and error stream for everything except the first task - bash : what we want to run - -i Input setting, here followed by no param indicating that the input stream is closed osed Access the RCI cluster using JetBrains Gateway \u00b6 When connecting to the RCI cluster, we have to use the login2 or login3 nodes. The login1 node has an outdated version of glibc which is not compatible with the JetBrains Gateway. Resource limits \u00b6 The resource limits are described in the RCI cluster documentation . However, the limits: do not cover the amd partitions, and there is no information how to determine the role for a user. Therefore, to know exactly the limits, it is best to use the sacctmgr show command . Installing Linux packages \u00b6 Without root access, we have cannot install packages using the package manager. However, we can manually install them to home. Steps: Locate the repository of the package that contains the packages for Red Hat Linux (rpm) Download the package Extract the package using rpm2cpio pv-*.rpm | cpio -idmv Move the extracted files to the desired location in the home folder Add the bin folder of the package to the PATH environment variable","title":"RCI cluster"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#general","text":"Form to get access to the RCI cluster Official instructions Storage manual and description of the partitions","title":"General"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#usual-command-usage","text":"You can watch your jobs with squeue -u username . You test/debug your program when running it with srun command you usually don\u2019t have to allocate resources when testing To start an interactive shell, run: srun -p cpufast --pty bash -i Set your main script file (sh) executable via chmod +x <filename> command Test your script in console Cancel the job: scancel <JOB ID> You run your job with sbatch command with allocated resources Example sbatch: sbatch --mem=30G -t 60 -n30 -o /home/fiedlda1/Amodsim/log/ih.log /home/fiedlda1/Amodsim/rci_launchers/ih.sh","title":"Usual command Usage"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#how-to-clone-projects","text":"Usually, you need to clone some of the SUM projects to start working on the RCI cluster. To do that: Copy your key to ~/.ssh/ Set file permissions to your ssh key safely Modify ~/.ssh/config IdentityFile to point to your key Clone your project","title":"How to clone projects"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#specifics-for-java-projects","text":"Clone project on RCI cluster Download binary maven from: http://maven.apache.org/download.cgi and export it to your home folder on the RCI cluster. Prepare your bash script Add #!/bin/bash to the first line Change the environment variable PATH for your maven location with this command: PATH=$PATH:/home/$USER/apache-maven-3.6.1/bin/ Load all required software via ml command, definitely ml Java Build, compile, run your project via mvn commands Set your file executable via chmod +x filename command Example run command: mvn exec:exec -Dexec.executable=java -Dexec.args=\u2019-classpath %classpath -Xmx30g cz.cvut.fel.aic.amodsim.OnDemandVehiclesSimulation /home/kholkolg/amod-to-agentpolis/local_config_files/olga_vga_RCI.cfg\u2019 -Dfile.encoding=UTF-8 Example - Bash script for amodsim project: Run your script with srun, sbatch etc. commands, I recommend first use srun, to check everything is set up ok and then use sbatch command, because if computational nodes are busy, your job will be added to the queue and you can do other work.","title":"Specifics for java projects"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#specifics-for-python-projects","text":"First load the appropriate version of python, e. G.: ml Python/3.6.6-foss-2018b You can\u2019t just install the packages with sudo , you have to install them to the user space instead: Run pip install --user packagename","title":"Specifics for python projects"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#specifics-for-c-projects","text":"","title":"Specifics for C++ projects"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#workflow-options","text":"As linux binaries are usually not portable. They are not compatible with older linux versions due to the infamous glibc incompatibility. There are three solutions to this problem: Method Setup Program Upgrade Compile the code on the RCI Setup the compilation on RCI. Copy the source code to RCI and recompile after every change Use a Singularity container learn with singularity, create the container Generate new container and copy it to the RCI Build a compatible binary using a modified toolchain learn with a toolchain generator, configure and generate the right toolchain Copy the updated binary","title":"Workflow options"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#building-on-rci","text":"In general the workflow is the same as on a local machine. The difference is that we do not have root access, so for all needed tools, we have to either load them via ml command, or, if not available, install them in the user space. Typically, we need to load: git: ml git GCC: ml GCC CMake: ml CMake","title":"Building on RCI"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#specific-for-projects-with-gurobi","text":"Load Gurobi with ml Gurobi or ml Gurobi/8.1.1-foss-2018b-Python-3.6.6 for a specific version Be aware that this operation can reload other packages","title":"Specific for projects with gurobi"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#gurobi-and-java","text":"It is necessary to install Gurobi to maven: mvn install:install-file -Dfile=/mnt/appl/software/Gurobi/9.0.3-GCCcore-8.3.0-Python-3.7.4/lib/gurobi.jar -DgroupId=com.gurobi -DartifactId=gurobi -Dversion=1.0 -Dpackaging=jar","title":"Gurobi and Java"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#gurobi-and-c","text":"As RCI use Linux as OS, we need to compile the Gurobi C++ libs with the same compiler as the one we use for compilation of our code (see C++ Workflow for more details). Note that this is necessary even if the Gurobi seems to be compiled with the same copiler we use for compilation . Unlike in Linux installation we controll, we cannot build the C++ lib in the Gurobi installation folder. To make the Linking work, foloow these steps: copy the src dir from the RCI Gurobi module located at mnt/appl/software/Gurobi/<desired version> to our home run make located in src/build copy the libgurobi_c++.a to the lib subfolder of your project configure the searching for the C++ lib in FindGUROBI.cmake file: ```cmake","title":"Gurobi and C++"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#find-the-gurobi-c-library","text":"find_library(GUROBI_LIBRARY NAMES gurobi HINTS ${GUROBI_DIR} $ENV{GUROBI_HOME} PATH_SUFFIXES lib )","title":"Find the Gurobi C library"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#find-the-gurobi-c-library_1","text":"find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED )","title":"Find the Gurobi C++ library"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#find-the-gurobi-c-debug-library-we-dont-need-it-on-rci-but-it-may-be-required-in-cmakeliststxt-therefore-we-supply-the-release-version-as-a-debug-version","text":"set(GUROBI_CXX_DEBUG_LIBRARY ${GUROBI_CXX_LIBRARY}) ``` if the CMake cache is already generated, delete it. Generate the CMake cache and build the project","title":"Find the Gurobi C++ debug library. We don't need it on RCI, but it may be required in CMakelists.txt. Therefore, we supply the release version as a debug version."},{"location":"Programming/Cloud%20computing/RCI%20cluster/#commands","text":"For command description, see Slurm manual. Main params --mem=<required memory> <required memory> is in megabytes by default, for gigabytes, we need to add G -t <time> Time in minutes","title":"Commands"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#task-explanations","text":"srun --pty bash -i - --pty runs the first task and close output and error stream for everything except the first task - bash : what we want to run - -i Input setting, here followed by no param indicating that the input stream is closed osed","title":"Task Explanations"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#access-the-rci-cluster-using-jetbrains-gateway","text":"When connecting to the RCI cluster, we have to use the login2 or login3 nodes. The login1 node has an outdated version of glibc which is not compatible with the JetBrains Gateway.","title":"Access the RCI cluster using JetBrains Gateway"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#resource-limits","text":"The resource limits are described in the RCI cluster documentation . However, the limits: do not cover the amd partitions, and there is no information how to determine the role for a user. Therefore, to know exactly the limits, it is best to use the sacctmgr show command .","title":"Resource limits"},{"location":"Programming/Cloud%20computing/RCI%20cluster/#installing-linux-packages","text":"Without root access, we have cannot install packages using the package manager. However, we can manually install them to home. Steps: Locate the repository of the package that contains the packages for Red Hat Linux (rpm) Download the package Extract the package using rpm2cpio pv-*.rpm | cpio -idmv Move the extracted files to the desired location in the home folder Add the bin folder of the package to the PATH environment variable","title":"Installing Linux packages"},{"location":"Programming/Cloud%20computing/Slurm/","text":"Introduction and basic terms \u00b6 Slurm is a job scheduler for HPC clusters. It is used to manage the resources and to run the tasks. Because it uses a different terminology than typical users are used to, we start with a short introduction to the basic terms: Association : a record of a Slurm user. It consists of four fields: User : the Linux user name Cluster : the name of the HPC cluster Partition : the name of the partition on the cluster Account : the bank account of the user used to schedule the jobs. TRES (Trackable Resource) : a resource that can be tracked by Slurm. It is used to limit the resources that a job can use. Most important TRES are: cpu: number of CPUs GRES (Generic Resource) : other computing resources, e.g., GPUs. Commands \u00b6 srun \u00b6 Run the task in the current shell in blocking mode, i.e., the console will be blocked till the task finishes. This command is only useful if we expect that the resources will be available immediatelly and the task will finish quickly. Otherwise, we should use sbatch . Params: --pty runs the in terminal mode. Output and error streams are closed for everything except the first task. -i Input setting. If followed by no param, it indicates that the input stream is closed. sbatch \u00b6 Request the execution of a task, with the required resources specified as sbatch parameters. The plain call with all resources defaulted is: sbatch <bash script> Note that the <bash script> here realy needs to be a bash script, it cannot be an arbitrary command or executable. Important parameters: -n, --ntasks : maximum number of tasks/threads that will be allocated by the job default is one task per node -N, --nodes : number of allocated nodes. default: minimum nodes that are needed to allocate resources according to other parameters (e.g., --ntasks , --mem ). --mem maximum memory that will be allocated by the job. The suffix G stands for gigabytes, by default, it uses megabytes. Example: --mem=40G . -t, --time : time limit. possible formats are <minutes> , <minutes:seconds> , <hours:minutes:seconds> , <days-hours> , <days-hours:minutes> , and <days-hours:minutes:seconds> . default: partition time limit -p , --partition= : partition name -o , --output= : job's output file name. The default name is slurm-<JOB ID>.out squeue \u00b6 --me filter just my jobs -u <username> filter just a specific user --start print the expected start time and the nodes planed to run the task -w --nodelist filter jobs running (but not planned) on specific nodes. The format for nodelist is <name>[<range>] , e.g., n[05-06] . sinfo \u00b6 Prints information about the computer cluster. scancel \u00b6 The scancel command cancels the execution of a job specified by the ID (firsta argument). To instead cancel jobs by name, use the --name option. Note however, that full name has to be specified and no wildcards are allowed . To cancel all jobs with a certain name, we have to mess with various linux commands instead: squeue --me | awk '/smod_cha/ {print $1}' | xargs scancel sacctmgr \u00b6 The saccmgr command is for viewing and modifying Slurm account information. The most important command for users is show (or list , which is equivalent). sacctmgr show ( sacctmgr list ) \u00b6 The show <entity> subcommand is used to display information about Slurm entities . Based on the <entity> argument, it shows different information. If we want to display just some columns , we can use the format argument: sacctmgr show associations format=Cluster,Account,User,QOS If we want to filter rows , we can use the specifications arguments that differ for each entity: sacctmgr show associations Users=user1 The most important entities are: associations : associations between users and accounts, quality of service (QOS), etc. The important columns are: Cluster : the name of the cluster Account : the bank account User : the Linux user name QOS : the quality of service that is in effect for the association The important specifications are: Users : display associations for a specific user qos : quality of service: limits and priorities for each group-queue combination The important columns are: Name : the name of the QOS Priority : the priority of the QOS Preempt : list of QOS names that can be preempted by this QOS MaxTRES : maximum TRES each job can use MaxTRESPerUser : maximum TRES each user can use, specified for each TRES separately cpu : maximum number of CPUs MaxJobsPU : maximum number of running jobs per user MaxSubmitPerUser : maximum number of jobs that can be submitted by a user tres : Trackable Resources Determining why the job was killed \u00b6 Usually, the error message is at the end of the output file. Message meaning: Detected 1 oom_kill event in ... : oom stands for out of memory. The job was killed because it exceeded the memory limit.","title":"Slurm"},{"location":"Programming/Cloud%20computing/Slurm/#introduction-and-basic-terms","text":"Slurm is a job scheduler for HPC clusters. It is used to manage the resources and to run the tasks. Because it uses a different terminology than typical users are used to, we start with a short introduction to the basic terms: Association : a record of a Slurm user. It consists of four fields: User : the Linux user name Cluster : the name of the HPC cluster Partition : the name of the partition on the cluster Account : the bank account of the user used to schedule the jobs. TRES (Trackable Resource) : a resource that can be tracked by Slurm. It is used to limit the resources that a job can use. Most important TRES are: cpu: number of CPUs GRES (Generic Resource) : other computing resources, e.g., GPUs.","title":"Introduction and basic terms"},{"location":"Programming/Cloud%20computing/Slurm/#commands","text":"","title":"Commands"},{"location":"Programming/Cloud%20computing/Slurm/#srun","text":"Run the task in the current shell in blocking mode, i.e., the console will be blocked till the task finishes. This command is only useful if we expect that the resources will be available immediatelly and the task will finish quickly. Otherwise, we should use sbatch . Params: --pty runs the in terminal mode. Output and error streams are closed for everything except the first task. -i Input setting. If followed by no param, it indicates that the input stream is closed.","title":"srun"},{"location":"Programming/Cloud%20computing/Slurm/#sbatch","text":"Request the execution of a task, with the required resources specified as sbatch parameters. The plain call with all resources defaulted is: sbatch <bash script> Note that the <bash script> here realy needs to be a bash script, it cannot be an arbitrary command or executable. Important parameters: -n, --ntasks : maximum number of tasks/threads that will be allocated by the job default is one task per node -N, --nodes : number of allocated nodes. default: minimum nodes that are needed to allocate resources according to other parameters (e.g., --ntasks , --mem ). --mem maximum memory that will be allocated by the job. The suffix G stands for gigabytes, by default, it uses megabytes. Example: --mem=40G . -t, --time : time limit. possible formats are <minutes> , <minutes:seconds> , <hours:minutes:seconds> , <days-hours> , <days-hours:minutes> , and <days-hours:minutes:seconds> . default: partition time limit -p , --partition= : partition name -o , --output= : job's output file name. The default name is slurm-<JOB ID>.out","title":"sbatch"},{"location":"Programming/Cloud%20computing/Slurm/#squeue","text":"--me filter just my jobs -u <username> filter just a specific user --start print the expected start time and the nodes planed to run the task -w --nodelist filter jobs running (but not planned) on specific nodes. The format for nodelist is <name>[<range>] , e.g., n[05-06] .","title":"squeue"},{"location":"Programming/Cloud%20computing/Slurm/#sinfo","text":"Prints information about the computer cluster.","title":"sinfo"},{"location":"Programming/Cloud%20computing/Slurm/#scancel","text":"The scancel command cancels the execution of a job specified by the ID (firsta argument). To instead cancel jobs by name, use the --name option. Note however, that full name has to be specified and no wildcards are allowed . To cancel all jobs with a certain name, we have to mess with various linux commands instead: squeue --me | awk '/smod_cha/ {print $1}' | xargs scancel","title":"scancel"},{"location":"Programming/Cloud%20computing/Slurm/#sacctmgr","text":"The saccmgr command is for viewing and modifying Slurm account information. The most important command for users is show (or list , which is equivalent).","title":"sacctmgr"},{"location":"Programming/Cloud%20computing/Slurm/#sacctmgr-show-sacctmgr-list","text":"The show <entity> subcommand is used to display information about Slurm entities . Based on the <entity> argument, it shows different information. If we want to display just some columns , we can use the format argument: sacctmgr show associations format=Cluster,Account,User,QOS If we want to filter rows , we can use the specifications arguments that differ for each entity: sacctmgr show associations Users=user1 The most important entities are: associations : associations between users and accounts, quality of service (QOS), etc. The important columns are: Cluster : the name of the cluster Account : the bank account User : the Linux user name QOS : the quality of service that is in effect for the association The important specifications are: Users : display associations for a specific user qos : quality of service: limits and priorities for each group-queue combination The important columns are: Name : the name of the QOS Priority : the priority of the QOS Preempt : list of QOS names that can be preempted by this QOS MaxTRES : maximum TRES each job can use MaxTRESPerUser : maximum TRES each user can use, specified for each TRES separately cpu : maximum number of CPUs MaxJobsPU : maximum number of running jobs per user MaxSubmitPerUser : maximum number of jobs that can be submitted by a user tres : Trackable Resources","title":"sacctmgr show (sacctmgr list)"},{"location":"Programming/Cloud%20computing/Slurm/#determining-why-the-job-was-killed","text":"Usually, the error message is at the end of the output file. Message meaning: Detected 1 oom_kill event in ... : oom stands for out of memory. The job was killed because it exceeded the memory limit.","title":"Determining why the job was killed"},{"location":"Programming/Database/Database%20Workflow/","text":"DataGrip \u00b6 Import Formats \u00b6 DataGrip can handle imports only from separator baset files (csv, tsv). View Geometry \u00b6 In the reuslt window/tab, click on the gear wheel -> Show GeoView . However, the geoviewer has a fixed WGS84 projection, so you have to project the result to this projection first . Create a spatial Index \u00b6 There is currently no GUI tool for that in DataGrip. Just add a normal index and modify the auto generated statement by changing <column> to USING GIST(<column>) at the end of the statement. Filter out store procedures \u00b6 right-click on routines click open table sort by type Creating functions and procedures \u00b6 There is no UI available currently, use Navicat or console Duplicate table \u00b6 Drag the table in the database explorer and drop it to the location you want it to copy to. Known issues and workarounds \u00b6 Cannot delete a database due to DataGrip's own connections \u00b6 Before deleting a database we need to close all DataGrip sessions connected to the database. We can do that in the sessions window. DataGrip displays objects that were deleted \u00b6 Sometimes, DataGrip displays objects that were deleted. Additionally, it it displays errors when trying to refresh the view. Solution: right-click on database connection (root object in the database explorer) click on Diagnostics -> Force refresh Navicat \u00b6 Cannot connect to db \u00b6 Symptoms: cant connect to db server: Could not connect after editing the connection and trying to save it (ok button): connection is being used Try: close navicat open navicat, edit connection click test connection click ok, and start the connection by double click PgAdmin \u00b6 The best way to install the PgAdmin is to use the EDB PostgreSQL installer and uncheck the database installation during the installation configuration. This way, we also install useful tools like psql Diagrams \u00b6 To create diagram from an existing database: right click on the database -> Generate ERD Visual Studio Code \u00b6 A limited support for PostgreSQL is available as an extension in VS Code. Limitations: syntax highlighting works only for basic SQL commands, no support for, e.g., PL/pgSQL. [source] outline view does not work for SQL files PostgreSQL \u00b6 As a first step, it is always good to know which clusters are installed and running. To show this information, use the pg_lsclusters command. Starting, stopping, and restarting the cluster \u00b6 Sometimes it is needed to restart the cluster. There are two commands: pg_ctl restart : restarts the cluster pg_ctl reload : reloads the configuration of the cluster Always check which one is needed in each case. For both commands, the path to the data directory of the cluster is needed. We can specify it in two ways: using the -D parameter of the pg_ctl command setting the PGDATA environment variable Installing extensions to the cluster \u00b6 Appart from installing extensions to the database using the CREATE EXTENSION command, we need to install them to the cluster first (if not installed already/by default). The installation process differs between different extensions: PostGIS and PgRouting : Install it using the stack builder application included in the PostgreSQL installation. PgTAP : Installation is described in the PostgreSQL Manual . Monitoring activity \u00b6 To monitor the activity on Linux, we can use the pg_activity : sudo -u postgres pg_activity -U postgres For a detailed monitoring on all platforms, we can use the Cumulative Statistics System . It contains collections of statistics that can be accessed similarly to tables. The difference is that these collections are server-wide and can be accessed from any database or scheme. Important collections: pg_stat_activity : contains information about the current activity on the server pg_stat_activity \u00b6 The pg_stat_activity collection contains information about the current activity on the server. Some activities belong to background processes it is therefore best to query the collection like: SELECT * FROM pg_stat_activity WHERE state IS NOT NULL; Kill a hanging query \u00b6 To kill the query, run: SELECT pg_cancel_backend(<PID>) The PID can be obtained from the database activity monitoring tool . Creating new user \u00b6 For creating a new user, we can use the createuser command. Important parameters: -P or --pwprompt : prompt for password. If not used, the user will be created without a password. Deleting database \u00b6 To delete a database, we can use the dropdb command: dropdb <db name> Granting privileges \u00b6 Grant privileges for a database \u00b6 To give privileges for creating new tables and other objects in a database: GRANT ALL PRIVILEGES ON DATABASE <db name> TO <user name>; To give privileges for existing objects, it is best to change the owner of the objects. Grant privileges for database schema \u00b6 To grant all privileges for a database schema to a user, we can use the following command: GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO <user name>; To grant only the SELECT privilege, use: GRANT SELECT ON ALL TABLES IN SCHEMA public TO <user name>; Upgrading the database cluster \u00b6 With PostgreSQL version 9.3 and later, we can upgrade easily even between major versions using the pg_upgrade command. We can even skip versions, e.g., upgrade from 12 to 16. The process is described in the pg_upgrade manual, however, usually, most of the steps are not necessary as they apply only to very specific cases. On top of that, some aspects important for the upgrade are not mentioned in the manual. The steps for a typical ubuntu/debian installation are: stop the new cluster using systemctl stop postgresql@<version>-main run the pg_upgrade command with --check to check the compatibility stop the old cluster using systemctl stop postgresql run the pg_upgrade command without --check to perform the upgrade apply all actions recommended by the pg_upgrade output change the port in the /etc/postgresql/<version>/main/postgresql.conf file of the new cluster to the original port (usually 5432) start the new cluster using systemctl start postgresql@<version>-main an check if the connection works The standard pg_upgrade command looks like this: sudo -u postgres pg_upgrade --link -j <number of cores> -b <old bin dir> -d <old data dir> -D <new data dir> -o 'config_file=<old conf file>' -O 'config_file=<new conf file>' Description: --link : links the old data directory to the new one instead of copying the data. Fastest migration method. <old bin dir> : The bin directory of the old cluster. usually /usr/lib/postgresql/<old version>/bin <old/new data dir> : The data directory of the cluster. usually /var/lib/postgresql/<old/new version>/main . Can be found using pg_lsclusters <old/new conf file> : The path to the postgresql.conf file. usually /etc/postgresql/<old/new version>/main/postgresql.conf Upgrading extensions \u00b6 Some PostgreSQL extensions uses separate libraries. These are installed fro each version of the PostgreSQL server separately. If a library is not foun on the new cluster, it is detected by the pg_upgrade command automaticly. In that case, you have to install the library according to the instructions of the library provider. Managing access to the database \u00b6 To manage access to the database, we can use the pg_hba.conf file. This file is located in the data directory of the PostgreSQL installation. Unfortunately, the location of the data directory is not standardized, and the variants are many. However, there is a remedy, just execute the following SQL command: SHOW hba_file Documentation Lost Password to the Postgres Server \u00b6 The password for the db superuser is stored in db postgres . In order to log there and change it, the whole authentification has to be turned off, and then we can proceed with changing the password. Steps: find the pg_hba.conf file usually located in C:\\Program Files\\PostgreSQL\\13\\data backup the file and replace all occurances of scram-sha-256 in the file with trust restart the posgreSQL service in the Windows service management, there should be a service for postgresql running change the password for the superuser psql -U postgres ALTER USER postgres WITH password 'yourpassword'; (do not forget the semicolon at the end!) restore the pg_hba.conf file from backup restart the postgreSQL service again test if the new password works Configuration \u00b6 Documentation PostgreSQL server can be configured using parameters . The parameters itself can be set in multiple ways: default values are set in the configuration file stored in the <postgres data dir>/postgresql.conf . the values can be set at runtime using SQL the values can be set at runtime using shell commands Configuration with postgresql.conf \u00b6 all the properties are there, we typically just need to uncomment them. Be awere of these syntactic rules: the # character is used for comments for directonary paths, use / as the path separator, never use \\ wrap paths in single quotes, not double quotes Getting and setting parameters at runtime using SQL \u00b6 to get the value of a parameter, we can use the SHOW command or the current_setting function: SHOW <parameter name>; SELECT current_setting('<parameter name>'); To set the value of a parameter, we can use the SET command or the set_config function: SET <parameter name> TO <value>; SELECT set_config('<parameter name>', '<value>', false); If the third parameter of the set_config function is set to true , the value is set for the current transaction only. Logging \u00b6 Documentation The logs are stored in the <postgres data dir>/log directory. By default, only errors are logged. To logg statements, we need to change the log_statement parameter. Valid values are: none : no statements are logged ddl : only DDL statements are logged mod : only statements that modify the database are logged all : all statements are logged Language settings \u00b6 By default, language is set during installation according to the system locale (not to be confused with the system language). To change the language, run: ALTER SYSTEM SET lc_messages = 'en_US.UTF-8'; After that, we need to: reload the configuration: SELECT pg_reload_conf(); if we are connected to the database, we need to reconnect to get the new language Garbage collection and optimization \u00b6 There is a shared command for both garbage collection (vacuum) and optimization (analyze) of the database. To execute it from the command line, use the vacuumdb` command.","title":"Database Workflow"},{"location":"Programming/Database/Database%20Workflow/#datagrip","text":"","title":"DataGrip"},{"location":"Programming/Database/Database%20Workflow/#import-formats","text":"DataGrip can handle imports only from separator baset files (csv, tsv).","title":"Import Formats"},{"location":"Programming/Database/Database%20Workflow/#view-geometry","text":"In the reuslt window/tab, click on the gear wheel -> Show GeoView . However, the geoviewer has a fixed WGS84 projection, so you have to project the result to this projection first .","title":"View Geometry"},{"location":"Programming/Database/Database%20Workflow/#create-a-spatial-index","text":"There is currently no GUI tool for that in DataGrip. Just add a normal index and modify the auto generated statement by changing <column> to USING GIST(<column>) at the end of the statement.","title":"Create a spatial Index"},{"location":"Programming/Database/Database%20Workflow/#filter-out-store-procedures","text":"right-click on routines click open table sort by type","title":"Filter out store procedures"},{"location":"Programming/Database/Database%20Workflow/#creating-functions-and-procedures","text":"There is no UI available currently, use Navicat or console","title":"Creating functions and procedures"},{"location":"Programming/Database/Database%20Workflow/#duplicate-table","text":"Drag the table in the database explorer and drop it to the location you want it to copy to.","title":"Duplicate table"},{"location":"Programming/Database/Database%20Workflow/#known-issues-and-workarounds","text":"","title":"Known issues and workarounds"},{"location":"Programming/Database/Database%20Workflow/#cannot-delete-a-database-due-to-datagrips-own-connections","text":"Before deleting a database we need to close all DataGrip sessions connected to the database. We can do that in the sessions window.","title":"Cannot delete a database due to DataGrip's own connections"},{"location":"Programming/Database/Database%20Workflow/#datagrip-displays-objects-that-were-deleted","text":"Sometimes, DataGrip displays objects that were deleted. Additionally, it it displays errors when trying to refresh the view. Solution: right-click on database connection (root object in the database explorer) click on Diagnostics -> Force refresh","title":"DataGrip displays objects that were deleted"},{"location":"Programming/Database/Database%20Workflow/#navicat","text":"","title":"Navicat"},{"location":"Programming/Database/Database%20Workflow/#cannot-connect-to-db","text":"Symptoms: cant connect to db server: Could not connect after editing the connection and trying to save it (ok button): connection is being used Try: close navicat open navicat, edit connection click test connection click ok, and start the connection by double click","title":"Cannot connect to db"},{"location":"Programming/Database/Database%20Workflow/#pgadmin","text":"The best way to install the PgAdmin is to use the EDB PostgreSQL installer and uncheck the database installation during the installation configuration. This way, we also install useful tools like psql","title":"PgAdmin"},{"location":"Programming/Database/Database%20Workflow/#diagrams","text":"To create diagram from an existing database: right click on the database -> Generate ERD","title":"Diagrams"},{"location":"Programming/Database/Database%20Workflow/#visual-studio-code","text":"A limited support for PostgreSQL is available as an extension in VS Code. Limitations: syntax highlighting works only for basic SQL commands, no support for, e.g., PL/pgSQL. [source] outline view does not work for SQL files","title":"Visual Studio Code"},{"location":"Programming/Database/Database%20Workflow/#postgresql","text":"As a first step, it is always good to know which clusters are installed and running. To show this information, use the pg_lsclusters command.","title":"PostgreSQL"},{"location":"Programming/Database/Database%20Workflow/#starting-stopping-and-restarting-the-cluster","text":"Sometimes it is needed to restart the cluster. There are two commands: pg_ctl restart : restarts the cluster pg_ctl reload : reloads the configuration of the cluster Always check which one is needed in each case. For both commands, the path to the data directory of the cluster is needed. We can specify it in two ways: using the -D parameter of the pg_ctl command setting the PGDATA environment variable","title":"Starting, stopping, and restarting the cluster"},{"location":"Programming/Database/Database%20Workflow/#installing-extensions-to-the-cluster","text":"Appart from installing extensions to the database using the CREATE EXTENSION command, we need to install them to the cluster first (if not installed already/by default). The installation process differs between different extensions: PostGIS and PgRouting : Install it using the stack builder application included in the PostgreSQL installation. PgTAP : Installation is described in the PostgreSQL Manual .","title":"Installing extensions to the cluster"},{"location":"Programming/Database/Database%20Workflow/#monitoring-activity","text":"To monitor the activity on Linux, we can use the pg_activity : sudo -u postgres pg_activity -U postgres For a detailed monitoring on all platforms, we can use the Cumulative Statistics System . It contains collections of statistics that can be accessed similarly to tables. The difference is that these collections are server-wide and can be accessed from any database or scheme. Important collections: pg_stat_activity : contains information about the current activity on the server","title":"Monitoring activity"},{"location":"Programming/Database/Database%20Workflow/#pg_stat_activity","text":"The pg_stat_activity collection contains information about the current activity on the server. Some activities belong to background processes it is therefore best to query the collection like: SELECT * FROM pg_stat_activity WHERE state IS NOT NULL;","title":"pg_stat_activity"},{"location":"Programming/Database/Database%20Workflow/#kill-a-hanging-query","text":"To kill the query, run: SELECT pg_cancel_backend(<PID>) The PID can be obtained from the database activity monitoring tool .","title":"Kill a hanging query"},{"location":"Programming/Database/Database%20Workflow/#creating-new-user","text":"For creating a new user, we can use the createuser command. Important parameters: -P or --pwprompt : prompt for password. If not used, the user will be created without a password.","title":"Creating new user"},{"location":"Programming/Database/Database%20Workflow/#deleting-database","text":"To delete a database, we can use the dropdb command: dropdb <db name>","title":"Deleting database"},{"location":"Programming/Database/Database%20Workflow/#granting-privileges","text":"","title":"Granting privileges"},{"location":"Programming/Database/Database%20Workflow/#grant-privileges-for-a-database","text":"To give privileges for creating new tables and other objects in a database: GRANT ALL PRIVILEGES ON DATABASE <db name> TO <user name>; To give privileges for existing objects, it is best to change the owner of the objects.","title":"Grant privileges for a database"},{"location":"Programming/Database/Database%20Workflow/#grant-privileges-for-database-schema","text":"To grant all privileges for a database schema to a user, we can use the following command: GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO <user name>; To grant only the SELECT privilege, use: GRANT SELECT ON ALL TABLES IN SCHEMA public TO <user name>;","title":"Grant privileges for database schema"},{"location":"Programming/Database/Database%20Workflow/#upgrading-the-database-cluster","text":"With PostgreSQL version 9.3 and later, we can upgrade easily even between major versions using the pg_upgrade command. We can even skip versions, e.g., upgrade from 12 to 16. The process is described in the pg_upgrade manual, however, usually, most of the steps are not necessary as they apply only to very specific cases. On top of that, some aspects important for the upgrade are not mentioned in the manual. The steps for a typical ubuntu/debian installation are: stop the new cluster using systemctl stop postgresql@<version>-main run the pg_upgrade command with --check to check the compatibility stop the old cluster using systemctl stop postgresql run the pg_upgrade command without --check to perform the upgrade apply all actions recommended by the pg_upgrade output change the port in the /etc/postgresql/<version>/main/postgresql.conf file of the new cluster to the original port (usually 5432) start the new cluster using systemctl start postgresql@<version>-main an check if the connection works The standard pg_upgrade command looks like this: sudo -u postgres pg_upgrade --link -j <number of cores> -b <old bin dir> -d <old data dir> -D <new data dir> -o 'config_file=<old conf file>' -O 'config_file=<new conf file>' Description: --link : links the old data directory to the new one instead of copying the data. Fastest migration method. <old bin dir> : The bin directory of the old cluster. usually /usr/lib/postgresql/<old version>/bin <old/new data dir> : The data directory of the cluster. usually /var/lib/postgresql/<old/new version>/main . Can be found using pg_lsclusters <old/new conf file> : The path to the postgresql.conf file. usually /etc/postgresql/<old/new version>/main/postgresql.conf","title":"Upgrading the database cluster"},{"location":"Programming/Database/Database%20Workflow/#upgrading-extensions","text":"Some PostgreSQL extensions uses separate libraries. These are installed fro each version of the PostgreSQL server separately. If a library is not foun on the new cluster, it is detected by the pg_upgrade command automaticly. In that case, you have to install the library according to the instructions of the library provider.","title":"Upgrading extensions"},{"location":"Programming/Database/Database%20Workflow/#managing-access-to-the-database","text":"To manage access to the database, we can use the pg_hba.conf file. This file is located in the data directory of the PostgreSQL installation. Unfortunately, the location of the data directory is not standardized, and the variants are many. However, there is a remedy, just execute the following SQL command: SHOW hba_file Documentation","title":"Managing access to the database"},{"location":"Programming/Database/Database%20Workflow/#lost-password-to-the-postgres-server","text":"The password for the db superuser is stored in db postgres . In order to log there and change it, the whole authentification has to be turned off, and then we can proceed with changing the password. Steps: find the pg_hba.conf file usually located in C:\\Program Files\\PostgreSQL\\13\\data backup the file and replace all occurances of scram-sha-256 in the file with trust restart the posgreSQL service in the Windows service management, there should be a service for postgresql running change the password for the superuser psql -U postgres ALTER USER postgres WITH password 'yourpassword'; (do not forget the semicolon at the end!) restore the pg_hba.conf file from backup restart the postgreSQL service again test if the new password works","title":"Lost Password to the Postgres Server"},{"location":"Programming/Database/Database%20Workflow/#configuration","text":"Documentation PostgreSQL server can be configured using parameters . The parameters itself can be set in multiple ways: default values are set in the configuration file stored in the <postgres data dir>/postgresql.conf . the values can be set at runtime using SQL the values can be set at runtime using shell commands","title":"Configuration"},{"location":"Programming/Database/Database%20Workflow/#configuration-with-postgresqlconf","text":"all the properties are there, we typically just need to uncomment them. Be awere of these syntactic rules: the # character is used for comments for directonary paths, use / as the path separator, never use \\ wrap paths in single quotes, not double quotes","title":"Configuration with postgresql.conf"},{"location":"Programming/Database/Database%20Workflow/#getting-and-setting-parameters-at-runtime-using-sql","text":"to get the value of a parameter, we can use the SHOW command or the current_setting function: SHOW <parameter name>; SELECT current_setting('<parameter name>'); To set the value of a parameter, we can use the SET command or the set_config function: SET <parameter name> TO <value>; SELECT set_config('<parameter name>', '<value>', false); If the third parameter of the set_config function is set to true , the value is set for the current transaction only.","title":"Getting and setting parameters at runtime using SQL"},{"location":"Programming/Database/Database%20Workflow/#logging","text":"Documentation The logs are stored in the <postgres data dir>/log directory. By default, only errors are logged. To logg statements, we need to change the log_statement parameter. Valid values are: none : no statements are logged ddl : only DDL statements are logged mod : only statements that modify the database are logged all : all statements are logged","title":"Logging"},{"location":"Programming/Database/Database%20Workflow/#language-settings","text":"By default, language is set during installation according to the system locale (not to be confused with the system language). To change the language, run: ALTER SYSTEM SET lc_messages = 'en_US.UTF-8'; After that, we need to: reload the configuration: SELECT pg_reload_conf(); if we are connected to the database, we need to reconnect to get the new language","title":"Language settings"},{"location":"Programming/Database/Database%20Workflow/#garbage-collection-and-optimization","text":"There is a shared command for both garbage collection (vacuum) and optimization (analyze) of the database. To execute it from the command line, use the vacuumdb` command.","title":"Garbage collection and optimization"},{"location":"Programming/Database/PostgreSQL%20Debugging%20and%20Profiling/","text":"Interpreting Error Messages \u00b6 If there is an error in a function, the line number of the error refers to the line number in the function body (from the opening quotes), not including the function declaration. Unsafe query: Update statement without where clause updates all table rows \u00b6 This is actually a warning raised by DataGrip, not a real error. We can safely ignore it, or disable the warning in Settings > Database > Query Execution , just uncheck Show warnings before running potentially unsafe queries . Debugging \u00b6 There is a debugger included in PostgreSQL. However its support in IDEs is limited: DataGrip: No built-in support. there is a plugin but it is outdated. pgAdmin: There is a debugger . Debugging with pgAdmin \u00b6 oficial documentation First, it is necessary to activate the debugger on the server: Open the postgresql.conf file Set the shared_preload_libraries parameter to <PostgreSQL installation directory>/lib/plugin_debugger Restart the server Then, the debugger extension must be installed on target database: CREATE EXTENSION pldbgapi; Now, we can debug a function: Right-click on the function in the pgAdmin tree and select Debugging > Set Breakpoint Call the function, the debugger will stop at the breakpoint","title":"PostgreSQL Debugging and Profiling"},{"location":"Programming/Database/PostgreSQL%20Debugging%20and%20Profiling/#interpreting-error-messages","text":"If there is an error in a function, the line number of the error refers to the line number in the function body (from the opening quotes), not including the function declaration.","title":"Interpreting Error Messages"},{"location":"Programming/Database/PostgreSQL%20Debugging%20and%20Profiling/#unsafe-query-update-statement-without-where-clause-updates-all-table-rows","text":"This is actually a warning raised by DataGrip, not a real error. We can safely ignore it, or disable the warning in Settings > Database > Query Execution , just uncheck Show warnings before running potentially unsafe queries .","title":"Unsafe query: Update statement without where clause updates all table rows"},{"location":"Programming/Database/PostgreSQL%20Debugging%20and%20Profiling/#debugging","text":"There is a debugger included in PostgreSQL. However its support in IDEs is limited: DataGrip: No built-in support. there is a plugin but it is outdated. pgAdmin: There is a debugger .","title":"Debugging"},{"location":"Programming/Database/PostgreSQL%20Debugging%20and%20Profiling/#debugging-with-pgadmin","text":"oficial documentation First, it is necessary to activate the debugger on the server: Open the postgresql.conf file Set the shared_preload_libraries parameter to <PostgreSQL installation directory>/lib/plugin_debugger Restart the server Then, the debugger extension must be installed on target database: CREATE EXTENSION pldbgapi; Now, we can debug a function: Right-click on the function in the pgAdmin tree and select Debugging > Set Breakpoint Call the function, the debugger will stop at the breakpoint","title":"Debugging with pgAdmin"},{"location":"Programming/Database/PostgreSQL%20Manual/","text":"fido7382 Data types \u00b6 official documentation Date \u00b6 official documentation date : for dates time for time timestmp for both date and time interval Select a part of date/time/timestamp \u00b6 If we want just a part of a date, time, or timestamp, we can use the extract function. Example: SELECT extract(hour FROM <date column name>) FROM... Other parts can be extracted too. To extract day of week , we can use isodow (assigns 1 to Monday and 7 to Sunday). Auto incrementing columns \u00b6 In PostgreSQL, sequences are used for auto-incrementing columns. When you are creating a new db table or adding a new column, the process of creating a new sequence can be automated by choosing an identity or a serial column type. When updating an aexisting column, a manual intervention is required: change the column to some numerical datatype create the sequence: SQL CREATE SEQUENCE <SEQUENCE NAME> OWNED BY <TABLE NAME>.<COLUMN NAME>; adjust the value of the sequence: SQL SELECT setval(pg_get_serial_sequence('<TABLE NAME>', '<COLUMN NAME>'), max(<COLUMN NAME>)) FROM <TABLE NAME>; set the column to be incremented by the sequence: SQL ALTER TABLE <TABLE NAME> ALTER COLUMN <COLUMN NAME> SET DEFAULT nextval('<SEQUENCE NAME>'); Strings \u00b6 official documentation The most important string type is text , it is a variable unlimited length string, like we know it from programming languages. Other types are: name : An internal type for database object names. It is limited to 63 characters. There are many string function available, For all functions, check the documentation . Formatting strings \u00b6 For formatting strings, we can use the format . Unlike in other languages, there are different types: %I : SQL identifier. This one is used for, e.g., table names. The string is quoted if necessary. %L : SQL Literal. Automatically quotes the string if it is not NULL . %s : String. For everything else. Arrays \u00b6 arrays array functions and operators Arrays are declared as <type>[] , e.g., integer[] . The type can be any type, including composite types. Array literals are declared using we use single quatation and curly brackets, e.g., '{1, 2, 3}' . For the same purpose, we can use the ARRAY constructtor sytax: ARRAY[1, 2, 3] . If we use the ARRAY constructor with an empty array, we have to specify the type of the array, e.g., ARRAY[]::integer[] . To compute array length , use array_length(<array>, 1) where 1 stands for the first dimension. Note that most of the array functions return NULL if the array is empty. To check that some value match at least some member of the array, we use ANY : SELECT ... FROM tab WHERE tab.a = ANY(<array>) Working with the array members individualy \u00b6 For using the members of an array in the SELECT or JOIN , we have to first split the array using the unnest function. This function transforms the result set to a form where there is a separate row for each member of the array (a kind of inverse operation to group by). If we want to also keep the array index, we can use the WITH ORDINALITY expression, as shown in the manual or on SO . Converting a result set (query result) to an array \u00b6 There are two ways to convert a result set to array in PostgreSQL: If we want to convert the whole result set to a single array, we use the array constructor : SQL SELECT array(<query>); If the query aggregates the rows and we want to create an array for each group, we use the array_agg function (see aggregate functions ): SQL SELECT array_agg(<column name>) FROM <table name>; note that counterintuitively, the array_agg function returns NULL if the result set is empty, not an empty array. Creating an array to a result set \u00b6 The opposite operation to the section above is to convert an array to a result set. This can be done using the unnest function: SELECT unnest(<array>) AS <column name>; hstore \u00b6 A specific feature of PostgreSQL is the hstore column type. It enables to store structured data in a single column. It can be used to dump variables that we do not plan to utilize in the database (i.e., in the SELECT, JOIN statements) frequently. This data type requires the hstore extension to be enabled in the database. To enable it, use the following command: CREATE EXTENSION hstore; When we, exceptionally, want to access a variable from a hstore column, we can use the following syntax: SELECT <COLUMN NAME>-><VARIABLE NAME> AS ... Schemas \u00b6 official documentation Schemas in PostgreSQL are implemented as namespaces according to the SQL standard. They are intended to organize the database objects into logical groups. Unfortunately, they cannot be nested, so they can hardly be used as a replacement for namespaces/packages/modules in programming languages. Search path \u00b6 official documentation Usually, we does not have to qualify the table name with the schema name, as the schema name is in the search path . By default the search path is set to \"$user\", public , which means that the tables are first searched in the schema with the same name as the user, and then in the public schema. To show the current search path, run: SHOW search_path; To change the search path, run: SET search_path TO <schema name list>; Creating objects (tables, indexes, etc.) \u00b6 The syntax for creating objects is mostly the same as in the SQL standard: CREATE <object type> <object name> <object parameters> IF NOT EXISTS modifier \u00b6 An important modifier is the IF NOT EXISTS clause that prevents errors when the object already exists. This is very useful if we want to update the database schema that is in the development phase with some automated script without the need to drop the database every time. However, this modifier is not available for all objects. It is available for: TABLE INDEX SEQUENCE VIEW MATERIALIZED VIEW PROCEDURE FUNCTION TRIGGER SCHEMA However, it is not available for: DATABASE USER CONSTRAINT For these objects, we have to use some workaround. There are three options in general: check if the object exists before creating it ( SO example with Constraint ) delete the object before creating it catch the error and ignore it ( SO example with Constraint ) CREATE TABLE AS \u00b6 documentation The CREATE TABLE AS syntax is mostly equivalent to the SQL standard. An addition is the WITH NO DATA which we can put after the SELECT statement: CREATE TABLE <table name> AS SELECT <select statement> WITH NO DATA This way, we use the result of the SELECT statement to create the table structure, but the table is empty. Procedures and functions \u00b6 To store a set of SQL commands for later use, PostgreSQL provides two options: procedures and functions. Both are similar and can use SQL, PL/pgSQL, or other languages supported by PostgreSQL. The key differences are: functions return a value, while procedures do not. However, procedures can return a value using an OUT parameter. functions can be called in SQL queries, while procedures require a separate CALL statement. functions: ... SELECT <function name>(<function arguments>) ... procedures: CALL <procedure name>(<procedure arguments>); procedures can manage transactions, while functions cannot. Syntax \u00b6 Both functions and procedures have the following syntax: CREATE OR REPLACE <FUNCTION/PROCEDURE> <name> (<parameters>) <return definition - only for functions> LANGUAGE <language name> AS <body> where <body> can be both a delimited string: AS $$ <function content> $$ OR an active SQL body, if we use SQL language (since PostgreSQL 14): BEGIN ATOMIC <sql statements> END There are some differences between those syntaxes (e.g., the second one works only for SQL and is evaluated/checked for validity at the time of creation), but in most cases, they are interchangable. The <function content> depends on the language used: for SQL, it is a set of SQL statements for PL/pgSQL, it is a block . See the functions and procedures section of the PL/pgSQL chapter. Functions \u00b6 To call a function, we use the SELECT statement: SELECT * FROM <function signature> To create a function, we use the CREATE FUNCTION statement . Unlike for procedures, we need to specify a return type for function, either as an OUT / INOUT parameter, or using the RETURNS clause. To return tabular data, we use TABLE or SETOF <row type> return type: RETURNS TABLE(<param 1 name> <param 1 type>, ..., <param n name> <param n type>) RETURNS SETOF <row type> Note that TABLE and SETOF types can only be used as a return type of a function. Procedures \u00b6 To exectue a stored procedure, use: CALL <procedure name>(<procedure arguments>); To create a procedure, use the CREATE PROCEDURE command . The syntax is as follows: CREATE PROCEDURE <name> (<params>) LANGUAGE <language name> <procedure body> Function and procedure parameters \u00b6 The definition of parameters of functions and procedures are composed of three parts: mode: IN , OUT , or INOUT (optional, default is IN ) name: the name of the parameter (optional, default for OUT parameters generated) type: the type of the parameter Unlike in programing languages, there is no implicit type cast of the program arguments, including literals. Therefore, we need to cast all parameters explicitely, as in the following example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments(target_area_id smallint) ... CALL compute_speeds_for_segments(1::smallint); Deciding the language \u00b6 For simple statements, we can use the SQL language. We need the PL/pgSQL if: we need to use variables or control statements specific to PL/pgSQL we need to use temporary tables, as the SQL language fails to recognize them if they are created inside the function/procedure Conditional filters based on the value of a variable or input parameter \u00b6 To add some filter to the WHERE or ON clause of a query based on the value of a variable or input parameter, we can use the following technique: set the variable or input parameter to NULL if we do not want to apply the filter in the filter test for disjunction of NULL or the filter condition Example: SELECT ... FROM ... WHERE param IS NULL OR some_column = param Organizing functions and procedures \u00b6 A natural task that emerge if there is a lot of functions and procedures is to organize them into packages. Unfortunately, PostgreSQL does not support any such feature. The closest thing is a schema, however, schemas are not suitable for this purpose, as they cannot be nested. Temporary tables \u00b6 To use a result set efficiently in a function or procedure, we often use temporary tables. Unlike in other relational database systems, in PostgreSQl, the lifetime of a temporary table is bound to a session. Therefoe, if we call a function that creates a tempoary table multiple times in a single session, we encounter an error, because the table already exists. To tackle this problem, we need to delete all temporary tables manually. Luckily, there is a special DISCARD command that can be used to dtop all temporary tables at once: DISCARD TEMPORARY; DO command \u00b6 The DO command can be used to execude an anonymus code block in any of the languages suported by PostgreSQL. It behaves like a function with no parameters and no return value. Syntax: DO [LANGUAGE <lang name>] <code> The default language is plpgsql . Example: DO $$ BEGIN RAISE NOTICE 'Hello world'; END $$ Window functions \u00b6 PostgreSQL supports an extended syntax for window functions . We can use it for example to retrieve the value of a column that has maximum value in another column, as demonstrated in an SO answer . Return data from INSERT , UPDATE , and DELETE statements \u00b6 In PostgreSQL, the INSERT , UPDATE , and DELETE statements can return data with the RETURNING clause. The syntax is as follows: INSERT INTO <table name> ... RETURNING <column name> This is especially useful when inserting data into a table with an auto-incrementing column, as we can retrieve the value of the column after the insert. Various specific tasks \u00b6 Selecting rows for deletion based on data from another table \u00b6 If we want to delete rows from a table based on some condition on data from another table, we can use the DELETE statement with a USING clause. Example: DELETE FROM nodes_ways_speeds USING nodes_ways WHERE nodes_ways_speeds.to_node_ways_id = nodes_ways.id AND nodes_ways.area IN (5,6) Handeling duplicates in the INSERT statement \u00b6 To handle duplicates on INSERT , PostgreSQL provides the ON CONFLICT clause (see the INSERT documentation). The options are: DO NOTHING : do nothing DO UPDATE SET <column name> = <value> : update the column to the given value Random oredering \u00b6 To order the result set randomly, we can use the RANDOM() function in the ORDER BY clause: SELECT ... FROM ... ORDER BY RANDOM() Random ordering with a seed (Pseudo-random ordering) \u00b6 To receive a determinisic (repeatable) random ordering, we can use the setseed function: SELECT setseed(0.5); SELECT ... FROM ... ORDER BY RANDOM(); Note that we need two queries, one for setting the seed and one for the actual query. If we does not have an option to call arbitrary queries, we have to use UNION : SELECT col_1, ..., col_n FROM ( SELECT _, null AS col_1, ..., null AS col_n FROM setseed(0.5) UNION ALL SELECT null AS _, col_1, ..., col_n FROM ... OFFSET 1 ) ORDER BY RANDOM() The OFFSET 1 is used to skip the first row, which is the result of the setseed function. The union is the only way to guarantee that the seed will be set before the actual query. Other options, such as WITH or SELECT ... FROM (SELECT setseed(0.5)) do not guarantee the order of execution, and produce a different result for each call. PostGis \u00b6 PolsGIS is an extension for PostgreSQL that adds support for spatial data. To enable the extension, we need to: install the extension e.g., using the bundeled stack builder application enable the extension in the database SQL CREATE EXTENSION postgis; Geometry columns \u00b6 Postgis features can be utilized with geometry and geography column types. To add a new geometry column: ADD COLUMN <COLUUMN NAME> geometry(<GEOMETRY TYPE>, <SRID>) Spatial Indexing \u00b6 Documentation Analogously to standard SQL column indicis, there are spatial indices in PostGIS. The only difference is that we need to add the USING GIST at the end of the CREATE INDEX statement: CREATE INDEX nodes_geom_idx ON nodes USING GIST (geom); Converting between geometry types \u00b6 There are dedicated functions whcich we can use to convert between geometry types: ST_Multi : converts geometries to their multi-variant, e.g., LineString to MultiLineString . Compute area surrounding geometries \u00b6 If we are ok with a convex envelope of the geometries, we can simply use the St_ConvexHull functon. Howeever, if we need the exact shape, We have to use the St_ConcaveHull function which computes the concave hull of a geometry. The St_ConcaveHull function takes an additional parameter param_pctconvex which determines how concave is the result: 0 means strictly concave, while 1 means convex hull. Note that while lowere values leads to more acurate results, the computation is much slower. There is also another parameter param_allow_holes which determines whether holes in the object are permited (default false). Split area to polygons based on points \u00b6 The split of spece into polygons based on a set of points is called Voronoi diagram . In PostGIS, we have a `ST_Voronoi_Polygons for that. To obtain a set of polygons from a set of points, it is necessary to Aggregate the rows ( ST_Collect ) Compute the polygon geometry ( ST_Voronoi_Polygons ) Disaggregate the geometry into individual polygons ( ST_Dump ) Also, there is an important aspect of how far the polygons will reach outside of the points. By default, it enlarge the area determined by the points by about 50%. If we need a larger area, we can use the extend_to parameter. If we need a smaller area, however, we need to compute the intersection with this smaller area afterwards manually. Full example: SELECT st_intersection( (st_dump( st_voronoipolygons(st_collect(<GEOMETRY COLUMN>)) )).geom, (<select clause for area of desired voronoi polygons>) ) AS geom FROM ... If we need to join the polygons to the original points, we need to do it manually (e.g. by JOIN ... ON ST_Within(<POINT COLUMN>, <VORONOI POLYGONS GEO DUMP>) ). Other Useful Functions \u00b6 ST_GeometryType : returns the type of the geometry as a string. [ ST_Within ] ST_Within (A, B) if A is completly inside B ST_Intersects ST_Intersects(g1, g2) if g1 and g2 have at least one point in common. ST_Transform : ST_Transform(g, srid) transforms geometry g to a projection defined by the srid and returns the result as a geometry. ST_Buffer : ST_Buffer(g, radius) computes a geometry that is an extension of g by radius to all directions St_Collect aggregates data into single geometry. It is usually apllied to a geometry column in an SQL selection. ST_Union ST_Equals ST_MakeLine : make line between two points ST_SetSRID : sets the SRID of the geometry and returns the result as a new geometry. ST_MakePoint : creates a point geometry from the given coordinates. ST_Area : computes the area of a geometry. The units of the result are the same as the units of the SRID of the geometry (use UTM coordinate system for getting the area in square meters). PL/pgSQL \u00b6 PL/pgSQL is a procedural language available in PostgreSQL databases. It can be used inside: functions procedures DO command Blocks \u00b6 Documentation The base components of PL/pgSQL is a block . It's syntax is as follows: DECLARE <variable declarations> BEGIN <statements> END Here, the DECLARE part is optional. In a block, each statement or declaration ends with a semicolon. We can nest the blocks, i.e., we can put a block inside another block, between the BEGIN and END keywords. In this case, we put a semicolon after the inner block. Only the outer block that contains the whole content of the function/procedure/ DO command does not require a semicolon. Variables \u00b6 In PL/PgSQL, all variables must be declared before assignmant in the DECLARE block which is before the sql body of the function/procedure/ DO . The syntax is: <variable name> <variable type>[ = <varianle value>]; Example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments() LANGUAGE plpgsql AS $$ DECLARE dataset_quality smallint = 1; BEGIN ... The, the value of a variable can be change using the classical assignment syntax: <variable name> = <varianle value>; Be carful to not use a variable name equal to the name of some of the columns used in the same context, which results in a name clash. Nothe that := is also a valid assignment operator . There is no difference between = and := in plpgsql. The := operator is sometimes preferred to avoid confusion with the = operator used for comparison. Assigning a value to a variable using SQL \u00b6 There are two options how to assign a value to a variable using SQL: using the INTO clause in the SELECT statement using a SELECT statement as rvlaue of the assignment Example with INTO : SELECT name INTO customer_name FROM customers WHERE id = 1 EXAMPLE with SELECT as rvalue: customer_name = (SELECT name FROM customers WHERE id = 1) Note that the behavior of the assignment may not be intuitive: if the SELECT statement returns no rows, the variable is set to NULL if the SELECT statement returns more than one row, an error is raised Functions and procedures \u00b6 The root of a PL/pgSQL function content is a block . Returning data \u00b6 Documentation When using SQL language, we can return the data just by executing a SELECT statement: CREATE OR REPLACE FUNCTION <function name> (<parameters>) LANGUAGE SQL AS $$ SELECT ... $$ When using PL/pgSQL, we have to use the RETURN statement. To make it even more complicated, there are three versions of the RETURN statement: RETURN <expression> : This one is for: returning a single scalar value return from function with return type void . In this case, the <expression> is empty. RETURN NEXT <expression> and RETURN QUERY <query> : These are for returning a set of rows. As the RETURN NEXT and RETURN QUERY do not terminate the function, we can mix them with the RETURN statement. Additionally, the RETURN QUERY can be used multiple times in a single function and therefore, it is valid to use all three versions in a single function. RETURN NEXT and RETURN QUERY \u00b6 Documentation For returning a set of rows ( table or setof type) in PL/PgSQL , we have to use the RETURN NEXT or RETURN QUERY statement. Besides the fact that these statements are used for returning a set of rows, there is another important difference between them and the RETURN statement: they do not terminate the execution of the function . This is useful as sometimes, we need to do some cleanup after selecting the rows to be returned from function, or we need to build the result in a loop. In classical programming languages, we use variables for this purpose. In PG/plSQL , we can also use the RETURN NEXT and RETURN QUERY constructs. Example: RETURN QUERY SELECT ...; DROP TABLE target_ways; RETURN; The RETURN QUERY differs from RETURN NEXT in following ways: its argument is a query, not an expression. Therefore we can use it directly with the SELECT statement, instead of using a variable. it appends the result of the query to the result set. Therefore, it can be used multiple times in a single function. it cannot be used for returning a single value even if the query returns a single value. If we have a single value return type and need to do some postprocessing between selecting the value and returning from the function, we have to use a variable instead. Branching \u00b6 PL/pgSQL has the following branching: IF <condition> THEN ... [ELSEIF ... ] [ELSE ... ] END IF Note that NULL values are treated the same as in SQL: if the condition evaluates to NULL , the branch is not executed . This is different from other programming languages, where NULL is treated as false . Logging \u00b6 documentation . Basic logging can be done using the RAISE command: RAISE NOTICE 'Some message'; We can add parameters by using the % placeholder: RAISE NOTICE 'Some message %', <expression>; Only scalar expressions are allowed as parameters. If we need to log multiple rows, we have to use a loop: FOR row IN SELECT ... LOOP RAISE NOTICE 'Some message %, %', row.column_1, row.column_2; END LOOP; Executing functions \u00b6 In SQL, any function is executed using the SELECT statement: SELECT * FROM <function name>(<function arguments>); -- for functions with return value SELECT <function name>(<function arguments>); -- for functions returning void or if we do not care about the return value In PL/pgSQL, the second syntax where the return value is not used is not allowed. Instead, we have to use the PERFORM statement: PERFORM <function name>(<function arguments>); Exceptions \u00b6 In PL/pgSQL, we can have both exceptions raised by the database and exceptions that are manually raised in user code. Handling exceptions \u00b6 Documentation In PL/pgSQL, we can handle exceptions by extending the block with the EXCEPTION part. The syntax is as follows: BEGIN ... EXCEPTION WHEN <exception condition> THEN <exception handling> END The list of built-in exception codes can be found in the documentation . The most common exceptions are: P0001 , raise_exception : A user-raised exception that does not specify the SQLSTATE. 00000 , successful_completion : The normal completion of a statement. No exception should ever be raised with this SQLSTATE. Apart from the built-in codes, we can introduce our own. They have to be five characters long and start with a letter. The <exception handling> part can be any valid PL/pgSQL code. It can contain multiple statements and can be nested. The <exception condition> can have three forms: WHEN <exception name> : catches the exception with the specified name WHEN SQLSTATE '<SQLSTATE>' : catches the exception with the specified SQLSTATE WHEN OTHERS : catches all exceptions that are not caught by the previous WHEN clauses There can be many WHEN clauses in a single EXCEPTION block. If there is no matching WHEN clause for the exception, the exception is propagated as if there was no EXCEPTION block at all. Note that the code 00000 cannot be caught by the WHEN '00000' clause. Such condition is interpreted as WHEN OTHERS . When handling an exception, we can use two special built-in variables: SQLSTATE : the SQLSTATE of the exception - a five-character code that specifies the exception type SQLERRM : the error message of the exception Raising exceptions \u00b6 Documentation To raise an exception, we use the RAISE statement, the same as for logging. The difference is in the level parameter, which we must set to EXCEPTION (default): RAISE EXCEPTION 'Some message'; -- exception RAISE 'Some message'; -- exception RAISE <another level> 'Some message'; -- just a message Query diagnostics \u00b6 Various information about the last query can be obtained using the GET DIAGNOSTIC command. For example, the number of rows affected by the last query can be obtained using the ROW_COUNT parameter: GET DIAGNOSTIC <variable> = ROW_COUNT; The result is stored in the variable <variable> . Note that this constract is not available in the SQL queries but only in a PL/pgSQL block. For other diagnostic fields, see the documentation . Transaction management \u00b6 By default, PostgreSQL starts a transaction at the beginning of the outermost block , and end it at the end of the block. We can manually commit or rollback the transaction, but only inside a procedure that is on a procedure-only call stack: procedure 1 -> procedure 2 -> procedure 3 : all three procedures can commit or rollback the transaction procedure 1 -> function 2 -> procedure 3 : only procedure 1 can commit or rollback the transaction function 1 -> procedure 2 -> function 3 : neither procedure nor function can commit or rollback the transaction. Subtransactions \u00b6 So far, we covered the top-level transactions. However, there are also subtransactions. Subtransactions cannot be handled manually, however, we can manipulate them, if we know the logic of the automatic subtransaction management. The subtransactions are started: inside each block with the exception management these subtransactions are rolled back if an exception is raised psql \u00b6 Documentation psql is a basic command line utitilty for manipulating postgres database. To connect, just type psql . By default, it will connect as a user corresponding to the OS user name to the default database for that user. To change it: -U <user name> : connect as a different user, -d <db name> : connect to the specified database. This way, we connect to the database in interactive mode . In this mode, we can write multiple SQL commands using one connection. The commands are only executed when correct ending punctuation (usually a semicolon) is encountered. Otherwise, enter just adds a new line and the command is not executed. To quit the interactive mode, use the \\q meta-command. To execute command immediatelly without starting interactive session, use the -c parameter: psql -d <db name> -c \"<command>\" Do not forget to quote the command. Also, note that certain SQL commands, such as CREATE DATABASE requires its own session. To combine them with other SQL commands, you can use multiple -c parameters: psql -d <db name> -c \"CREATE DATABASE <db name>\" -c \"<other command>\" Other useful parameters: -X : do not read the ~/.psqlrc file. This is useful when debuging the psql commands or running scripts, as it disables any customizations. -p : specify the port number. (default is 5432) -l : list all databases Default database \u00b6 Note that no matter what SQL commands you plan to execute, you have to connect to a specific database . Ommitting the -d parameter will not connect you to the server in some admin mode, but instead, it will connect you to the default database, which is usually the database with the same name as the user name. If there is no such database, the connection will fail. meta-commands \u00b6 The psql has its own set of commands, called meta-commands . These commands start with a backslash ( \\ ) and can be used inside SQL queries (either interactively, or in the -c argument). Example: psql -d <db name> -c \"\\l+\" The above command lists all databases, including additional information. Note that the meta-commands cannot be combined with SQL queries passed to the -c parameter . As -c argument, we can use either: a plain SQL query without any meta-commands a single meta-command without any SQL query (like the example above) (In \\copy public.nodes FROM nodes.csv CSV HEADER , the string after \\copy is a list of arguments, not an SQL query) When we want to combine a meta-command with an SQL query, we need to use some of the workarounds: use the psql interactive mode use the psql --file parameter to execute a script from a file pipe the commands to psql using echo : bash echo \"COPY (SELECT * FROM opendata.public.nodes WHERE area = 13) TO STDOUT WITH CSV HEADER \\g 'nodes.csv'\" | psql -d opendata Useful meta-commands: \\d : list all tables and views in the current schema. We can pass a name as an argument to list only the table or view with the given name. \\d+ : same as \\d , but with additional information. \\q : quit the interactive mode. \\i : execute a script from a file. \\sf <function name> : show the source code of a function. Executing SQL files \u00b6 In normal mode, we can execute SQL files using the -f parameter: psql -d <db name> -f <file name> In interactive mode, we can execute SQL files using the \\i meta-command. Note that backslash ( \\ ) in the file path needs to be escaped or replaced with a forward slash ( / ) . Importing data \u00b6 A simple SQL data (database dump) can be imported using the psql command: psql -d <db name> -f <file name> we can also import from stdin by omitting the -f parameter: psql -d <db name> < <file name> # or <command> | psql -d <db name> Importing compressed SQL dump \u00b6 To import a compressed SQL dump we need to know how the dump was compressed. If it was compressed using the pg_dump with the -Z parameter, we use a pg_restore command: bash pg_restore -d <db name> <file name> If it was compressed using a compression tool, we need to pipe the output to the decompression tool and then to psql : bash <decompression tool> < <file name> | psql -d <db name> Importing data from csv \u00b6 The easiest way to import data from csv is to use the psql \\copy meta-command: psql -d <db name> -c \"\\copy <table name> FROM <file name> CSV HEADER\" The syntax of this command and its parameters is almost identical to the COPY command. Important parameters: CSV : the file is in CSV format HEADER : the first line of the file contains column names Except simplicity, the \\copy command has another advantage over the COPY command: it has the same access rights as the user that is currently logged in. The COPY command, on the other hand, has the access rights of the user that started the database server. The COPY command, however, has an advantage over the \\copy command when we access a remote database. In that case, the \\copy command would first download the whole file to the client and then upload it to the server. The COPY command, on the other hand, can be executed directly on the server, so the data are not downloaded to the client. Importing data from csv with columns than are not in the db table \u00b6 The COPY (and \\copy ) command can be used to copy the input into a database table even if it contains only a subset of database table columns. However, it does not work the other way, i.e, all input columns have to be used. If only a subset of input columns needs to be used, or some of the input columns requires processing, we need to use some workaround. The prefered mathod depends on the characte of the data: data do not match the table, but they are small: load the data with pandas process the data as needed use pandas.to_sql to upload the data data do not match the table and they are large: preprocess the data with batch commands use COPY (or \\copy ) to upload the data data do not match the table and they are large and dirty : use the file_fdw module: create a table for SQL mapping with tolerant column types (e.g., text for problematic columns) select from the mapping to the real table Importing data from a shapefile \u00b6 There are multiple options: shp2psql : simple tool that creates sql from a shapefile easy start, almost no configuration always imports all data from shapefile, cannot be configured to skip columns included in the Postgres instalation ogr2ogr needs to be installed, part of GDAL QGIS The db manger can be used to export data from QGIS data can be viewed before import only suitable for creating new table, not for appending to an existing one Importing data from GeoJSON \u00b6 For a single geometry stored in a GeoJSON file, the function ST_GeomFromGeoJSON can be used. just copy the geometry part of the file change the geometry type to match the type in db don't forget to surround the object with curly brackets For the whole document, the ogr2ogr tool can be used. Exporting data \u00b6 The data can be exported to SQL using the pg_dump command. The simplest usage is: pg_dump -f <output file name> <db name> Important parameters: -s : export only the schema, not the data If we need to further process the data, we can use the stdout as an output simply by omitting the -f parameter: pg_dump <db name> | <command> Exporting the database server configuration \u00b6 To export the database server configuration, we can use the pg_dumpall command: pg_dumpall -f <output file name> Compressing the SQL dump \u00b6 To compress the SQL dump, we have two options: use the pg_dump command with the -Z parameter, e.g., pg_dump <db name> -Z1 -f <output file name> or pipe the output to a compression tool, e.g., pg_dump <db name> | gzip > <output file name> Exporting data to csv \u00b6 When exporting large data sets, it is not wise to export them as an SQL dump. To export data to csv, we can use either: psql \\copy command COPY command in SQL The simplest way is to use the \\copy command. However, it may be slow if we call psql from a client and we want to export the data to a file on the server, because the data are first sent to the client and then back to the server. The COPY command is the fastest way to export data to the server. However, by default, it can be tricky to use, as the sql server needs to have write access to the file. To overcome this problem, we can use the following workaround: choose STDOUT as an output for the COPY command at the end of the command, add \\g <file name> to redirect the output to a file. Example: echo \"COPY (SELECT * FROM opendata.public.nodes WHERE area = 13) TO STDOUT WITH CSV HEADER \\g 'nodes.csv'\" | psql -d opendata Information and statistics \u00b6 To show the table size , run: SELECT pg_size_pretty(pg_total_relation_size('<table name>')); To show the version of the PostgreSQL server, run: SELECT version(); To list all extensions for a database, run: psql -d <db name> -c \"\\dx\" To check if a specific table exists, run: SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = '<schema_name>' AND table_name = '<table_name>'); Databases \u00b6 To list all databases, we can use the -l parameter: psql -l To get more information about the databases, we can use the \\l+ meta-command. To check if a specific database exists, we can use the following query: SELECT EXISTS (SELECT 1 FROM pg_database WHERE datname = '<db name>'); PgRouting \u00b6 documentation PgRouting is a PostgreSQL extension focused on graph/network manpulation. It contains functions for: finding the strongly connected components: pgr_strongComponents graph contraction/simplification : pgr_contraction creating vertices from edges: pgr_createVerticesTable Finding strongly connected components \u00b6 The function pgr_strongComponents finds the strongly connected components of a graph. The only parameter of the script is a query that should return edge data in the folowing format: id , source , target , cost , reverse_cost . The first three parameters are obvious. The cost parameter does not have any effect. You should provide a negative reverse_cost , othervise, the edge will be considered as bidirectional! Creating vertices from edges \u00b6 The function pgr_createVerticesTable creates a table of vertices from a table of edges. The function has the following parameters: edges_table : the name of the table with edges. It must be a standard table, temporary tables are not supported. the_geom : the name of the geometry column in the edges_table source : the name of the source column in the edges_table target : the name of the target column in the edges_table The return value is OK if the function was successful and FAIL if it was not. The created vertices table is named <edges_table>_vertices_pgr and contains the following columns: id : the id of the vertex the_geom : the geometry of the vertex cnt : the number of edges that are connected to the vertex chk : an integer that indicates if the vertex might have a problem. ein : the number of incoming edges eout : the number of outgoing edges Note that only the id , and the_geom columns are filled with data. The other columns are filled with NULL . To fill the cnt and chk columns, we can use the pgr_analyzeGraph function. To fill all columns, we need to use the pgr_analyzeOneway function. However, this function is cumbersome to use, as it requires a lot of parameters. Testing with PgTAP \u00b6 Official documentation PgTAP is the only testing framework for PostgreSQL. It is a system framwork: it requires administrative privileges to install. Installation \u00b6 Linux \u00b6 If you are using Linux, you may (depending on your distribution) be able to use you distribution's package management system to install pgTAP. For instance, on Debian, Ubuntu, or Linux Mint pgTAP can be installed with the command: sudo apt-get install pgtap On other systems pgTAP has to be downloaded and built. First, download pgTAP from PGXN (click the green download button in the upper-right). Extract the downloaded zip file, and (at the command line) navigate to the extracted folder. To build pgTAP and install it into a PostgreSQL database, run the following commands: make make install make installcheck Windows \u00b6 To install pgtap for PostgreSQL on Windows, follow these steps: Clone the pgtap repository Open PowerShell ( pwsh ) as an Administrator it is necessary to copy files into the ProgramFiles directory. run the pgtap_install.ps1 script as an administrator with the following command: PowerShell pgtap_install.ps1 <path to pgtap clone> This script will copy the necessary files to the PostgreSQL installation directory. These instructions were adapted from issue#192 of the pgtap repository. Basic Usage - Test scripts \u00b6 The easiest way to write tests in PgTAP is to write procedural SQL scripts that contain the tests. The basic test can look like this: BEGIN; -- Start a transaction -- Specify that we plan to run 2 tests SELECT plan(2); -- Test 1: Check if basic arithmetic works as expected SELECT is(1 + 1, 2, '1 + 1 equals 2'); -- Test 2: Verify string concatenation SELECT is('Hello ' || 'World', 'Hello World', 'String concatenation works'); -- -- Run the tests and return results SELECT * FROM finish(); ROLLBACK; -- Rollback the transaction here, we create two assertions using the is function from the pgtap library. Then, we run the tests using the finish function. The whole test is wrapped in a transaction, so the database is not modified by the test. To execute the test, we can: run the test SQL directly in the SQL console or with the psql command use the pg_prove command to run the test from the command line pg_prove \u00b6 The pg_prove is a Perl test runner that can be used to run the pgtap tests. We need to install it first using cpanm TAP::Parser::SourceHandler::pgTAP . Then, we can use it as: pg_prove -d <db name> -U <user name> <test file> Test as functions \u00b6 Instead of writing the tests in a procedural SQL script, we can write them as functions ( but not procedures! ). This can help as organizing tests and also to prepare the data for the tests. Test functions must return the SETOF TEXT type. This correspond to the type returned by the provided test assertions . Test runner \u00b6 There is also a runner function runtests that can be used to run multiple tests at once: SELECT * FROM runtests(); This function has four variants: runtests() , runtests(<schema>) , runtests(<pattern>) , and runtests(<schema>, <pattern>) . The schema parameter is used to specify the schema where the tests are searched. The pattern parameter is used to specify the pattern that the test names must match. The pattern uses the same syntax as the LIKE operator in SQL. Fixtures \u00b6 Sometimes, we need to call some functions before and after the tests. We can indeed call these functions in from the test functions, but in case we need to run the same functions for multiple tests, it is desirable to automate this process using fixtures. The runtests function supports fixtures, it recognize them by the prefix. Again, only functions, not procedures are supported. The following fixtures are supported: startup : runs before the tests setup : runs before each test teardown : runs after each test shutdown : runs after the tests Unfortunatelly, the fixture search does not reflect the filtration <pattern> . Therefore, all fixtures in the schema are always run. To overcome this, we have to supply a custom function for executing the tests. Also note that all the fixtures are executed for all the tests, not just the ones with the matching name. Therefore they cannot be used for preparing the data for individual tests. Rolling back \u00b6 All tests are executed in a transaction and each test is then in a subtransaction. All database changes are rolled back except the ones that are in the startup and shutdown fixtures [source] . Additionally, some changes, e.g. the sequence numbers cannot be rolled back. Test assertions \u00b6 PgTAP provides a set of functions that works as test assertions. All functions have an optional <message> parameter. Notable functions are: ok(<condition>, <message>) : checks if the <condition> is true is(<computed>, <expected>, <message>) or isnt(<computed>, <not expected>, <message>) : checks if <computed> and <expected> are the same, or not (respectively). matches(<text>, <pattern>, <message>) or doesnt_match(<text>, <pattern>, <message>) : checks if <text> matches the <pattern> where <pattern> is a regular expression, or not (respectively). alike(<text>, <pattern>, <message>) or unalike(<text>, <pattern>, <message>) : checks if <text> is similar to <pattern> , or not (respectively). The LIKE operator is used for the comparison. cmp_ok(<computed>, <operator>, <expected>, <message>) : checks if <computed> is related to <expected> by the <operator> . The <operator> can be one of the following: = , != , > , >= , < , <= . isa_ok(<value>, <type>, <message>) : checks if <value> is of type <type> . throws_ok(<sql>, <error_code>, <error_message>, <message>) : checks if the <sql> throws an exception. all parameters except the <sql> are optional lives_ok(<sql>, <message>) : checks if the <sql> does not throw an exception. Additionaly, if the test condition is complicated, we can just manually return using the pass(<message>) or fail(<message>) functions. Result set assertions \u00b6 results_eq(<sql>, <sql>, <description>) : Compares two SQL query results row-by-row for equality, ensuring integrity and order. results_eq(<sql>, <array>, <description>) : Compares SQL query results to an array. results_eq(<cursor>, <cursor>, <description>) : Compares results from two cursors. results_eq(<sql>, <cursor>, <description>) : Compares SQL query results to cursor results. results_eq(<cursor>, <array>, <description>) : Compares cursor results to an array. results_ne(<...>, <description>) : The inverse of results_eq() . Checks if result sets are not equal (accepts the same argument variations as results_eq ). set_eq(<...>, <description>) : Compares result sets for equality, ignoring order (accepts the same argument variations as results_eq ). set_ne(<...>, <description>) : The inverse of set_eq() . Checks if result sets are not equal, ignoring order (accepts the same argument variations as results_eq ). set_has(<sql>, <sql>, <description>) : Checks if the first query's results contain all rows returned by the second query, ignoring order. set_hasnt(<sql>, <sql>, <description>) : The inverse of set_has . Checks if the first query's results do not contain all rows from the second query. bag_eq(<sql>, <sql>, <description>) : Compares result sets like set_eq but considers duplicate rows. If a row appears N times in the first query, it must appear N times in the second. bag_ne(<sql>, <sql>, <description>) : The inverse of bag_eq() . is_empty(<sql>, <description>) : Checks that the SQL query returns no rows. isnt_empty(<sql>, <description>) : Checks that the SQL query returns at least one row. row_eq(<sql>, <record>, <description>) : Checks that the SQL query returns a single row identical to the provided <record> . Additional functions( Diagnostics ) \u00b6 The function diag(<string>, <string>, ...) , can be used for outputting diagnostic messages. For example: -- Output a diagnostic message if the collation is not en_US.UTF-8. SELECT diag( E'These tests expect LC_COLLATE to be en_US.UTF-8,\\n', 'but yours is set to ', setting, E'.\\n', 'As a result, some tests may fail. YMMV.' ) FROM pg_settings WHERE name = 'lc_collate' AND setting <> 'en_US.UTF-8'; which outputs # These tests expect LC_COLLATE to be en_US.UTF-8, # but yours is set to en_US.ISO8859-1. # As a result, some tests may fail. YMMV. This function basically concatenates all the arguments and outputs them as a single line of text. All the arguments have to be strings. XML \u00b6 documentation PostgreSQL has a built-in support for XML. Important functions: xpath(<path>, <text>, <namespace bindings>) : returns an array of XML nodes that match the given <path> evaluated on the given <text> . Besides functions, there is a powerful XMLTABLE expression that can be used to extract tabular data from XML documents. Handling XML namespaces \u00b6 All XML methods in PostgreSQL require the namespace bindings to be specified, as per the XPath standard. The only exception is when work with XML documents that do not use namespaces. We specify the namespace bindings as an array of arrays, where each inner array has two elements: the prefix and the namespace URI. Example: SELECT xpath( '/ns:root/ns:child', '<root xmlns:ns=\"http://example.com/ns\"><child>...</child></root>', ARRAY[ARRAY['ns', 'http://example.com/ns']]); Note that the namespace prefixes specified in the bindings are completely unrelated to the prefixes used in the XML document. They may be the same, but they do not have to be. Also don't forget that XPath requires the namespace prefixes even for the default namespace. XMLTABLE \u00b6 Documentation XMLTABLE expression can extract tabular data from XML documents. The syntax is as follows: XMLTABLE( [XMLNAMESPACES(<namespace bindings>),] <xpath expression>, PASSING <xml document expression> COLUMNS <column name> <data type> PATH <xpath relative expression>, ... <column name> <data type> PATH <xpath relative expression> ) If the XML document contains namespaces, we have to specify the namespace bindings in the XMLNAMESPACES clause. The <namespace bindings> is a comma-separated list of namespace bindings in the format <namespace URI> AS <prefix> . Example: XMLNAMESPACES( 'http://graphml.graphdrawing.org/xmlns' AS dns, 'http://www.yworks.com/xml/yfiles-common/3.0' AS y ) The <xpath expression> is an XPath expression that specifies the path to the XML nodes from which we want to extract data. The <xpath relative expression> is then another XPath expression that is relative to the <xpath expression> and specifies the path to the XML nodes or attributes used to fill a specific column. The <xml document expression> is an expression that evaluates to an XML document. Typically, it is a column in the select statement, or a subquery. Example: SELECT some_element FROM xml_documents, XMLTABLE( XMLNAMESPACES(...), '/dns:graph/dns:node' PASSING xml_document.xml_data COLUMNS some_element TEXT PATH ... ) FROM xml_documents WHERE id = 1; Note that the XMLTABLE expression may only be used in the FROM clause. Therefore, if we, for example, need to join the result of the XMLTABLE , instead of using: JOIN XMLTABLE(...) we have to use: JOIN (SELECT <columns> FROM XMLTABLE(...)) Troubleshooting \u00b6 If the db tools are unresponsive on certain tasks/queries, check if the table needed for those queries is not locke by some problematic query. Select PostgreSQL version \u00b6 SELECT version() Select PostGIS version \u00b6 SELECT PostGIS_version() Tried to send an out-of-range integer as a 2-byte value \u00b6 This error is caused by a too large number of values in the insert statement. The maximum index is a 2-byte number (max value: 32767). The solution is to split the insert statement into smaller bulks.","title":"PostgreSQL Manual"},{"location":"Programming/Database/PostgreSQL%20Manual/#data-types","text":"official documentation","title":"Data types"},{"location":"Programming/Database/PostgreSQL%20Manual/#date","text":"official documentation date : for dates time for time timestmp for both date and time interval","title":"Date"},{"location":"Programming/Database/PostgreSQL%20Manual/#select-a-part-of-datetimetimestamp","text":"If we want just a part of a date, time, or timestamp, we can use the extract function. Example: SELECT extract(hour FROM <date column name>) FROM... Other parts can be extracted too. To extract day of week , we can use isodow (assigns 1 to Monday and 7 to Sunday).","title":"Select a part of date/time/timestamp"},{"location":"Programming/Database/PostgreSQL%20Manual/#auto-incrementing-columns","text":"In PostgreSQL, sequences are used for auto-incrementing columns. When you are creating a new db table or adding a new column, the process of creating a new sequence can be automated by choosing an identity or a serial column type. When updating an aexisting column, a manual intervention is required: change the column to some numerical datatype create the sequence: SQL CREATE SEQUENCE <SEQUENCE NAME> OWNED BY <TABLE NAME>.<COLUMN NAME>; adjust the value of the sequence: SQL SELECT setval(pg_get_serial_sequence('<TABLE NAME>', '<COLUMN NAME>'), max(<COLUMN NAME>)) FROM <TABLE NAME>; set the column to be incremented by the sequence: SQL ALTER TABLE <TABLE NAME> ALTER COLUMN <COLUMN NAME> SET DEFAULT nextval('<SEQUENCE NAME>');","title":"Auto incrementing columns"},{"location":"Programming/Database/PostgreSQL%20Manual/#strings","text":"official documentation The most important string type is text , it is a variable unlimited length string, like we know it from programming languages. Other types are: name : An internal type for database object names. It is limited to 63 characters. There are many string function available, For all functions, check the documentation .","title":"Strings"},{"location":"Programming/Database/PostgreSQL%20Manual/#formatting-strings","text":"For formatting strings, we can use the format . Unlike in other languages, there are different types: %I : SQL identifier. This one is used for, e.g., table names. The string is quoted if necessary. %L : SQL Literal. Automatically quotes the string if it is not NULL . %s : String. For everything else.","title":"Formatting strings"},{"location":"Programming/Database/PostgreSQL%20Manual/#arrays","text":"arrays array functions and operators Arrays are declared as <type>[] , e.g., integer[] . The type can be any type, including composite types. Array literals are declared using we use single quatation and curly brackets, e.g., '{1, 2, 3}' . For the same purpose, we can use the ARRAY constructtor sytax: ARRAY[1, 2, 3] . If we use the ARRAY constructor with an empty array, we have to specify the type of the array, e.g., ARRAY[]::integer[] . To compute array length , use array_length(<array>, 1) where 1 stands for the first dimension. Note that most of the array functions return NULL if the array is empty. To check that some value match at least some member of the array, we use ANY : SELECT ... FROM tab WHERE tab.a = ANY(<array>)","title":"Arrays"},{"location":"Programming/Database/PostgreSQL%20Manual/#working-with-the-array-members-individualy","text":"For using the members of an array in the SELECT or JOIN , we have to first split the array using the unnest function. This function transforms the result set to a form where there is a separate row for each member of the array (a kind of inverse operation to group by). If we want to also keep the array index, we can use the WITH ORDINALITY expression, as shown in the manual or on SO .","title":"Working with the array members individualy"},{"location":"Programming/Database/PostgreSQL%20Manual/#converting-a-result-set-query-result-to-an-array","text":"There are two ways to convert a result set to array in PostgreSQL: If we want to convert the whole result set to a single array, we use the array constructor : SQL SELECT array(<query>); If the query aggregates the rows and we want to create an array for each group, we use the array_agg function (see aggregate functions ): SQL SELECT array_agg(<column name>) FROM <table name>; note that counterintuitively, the array_agg function returns NULL if the result set is empty, not an empty array.","title":"Converting a result set (query result) to an array"},{"location":"Programming/Database/PostgreSQL%20Manual/#creating-an-array-to-a-result-set","text":"The opposite operation to the section above is to convert an array to a result set. This can be done using the unnest function: SELECT unnest(<array>) AS <column name>;","title":"Creating an array to a result set"},{"location":"Programming/Database/PostgreSQL%20Manual/#hstore","text":"A specific feature of PostgreSQL is the hstore column type. It enables to store structured data in a single column. It can be used to dump variables that we do not plan to utilize in the database (i.e., in the SELECT, JOIN statements) frequently. This data type requires the hstore extension to be enabled in the database. To enable it, use the following command: CREATE EXTENSION hstore; When we, exceptionally, want to access a variable from a hstore column, we can use the following syntax: SELECT <COLUMN NAME>-><VARIABLE NAME> AS ...","title":"hstore"},{"location":"Programming/Database/PostgreSQL%20Manual/#schemas","text":"official documentation Schemas in PostgreSQL are implemented as namespaces according to the SQL standard. They are intended to organize the database objects into logical groups. Unfortunately, they cannot be nested, so they can hardly be used as a replacement for namespaces/packages/modules in programming languages.","title":"Schemas"},{"location":"Programming/Database/PostgreSQL%20Manual/#search-path","text":"official documentation Usually, we does not have to qualify the table name with the schema name, as the schema name is in the search path . By default the search path is set to \"$user\", public , which means that the tables are first searched in the schema with the same name as the user, and then in the public schema. To show the current search path, run: SHOW search_path; To change the search path, run: SET search_path TO <schema name list>;","title":"Search path"},{"location":"Programming/Database/PostgreSQL%20Manual/#creating-objects-tables-indexes-etc","text":"The syntax for creating objects is mostly the same as in the SQL standard: CREATE <object type> <object name> <object parameters>","title":"Creating objects (tables, indexes, etc.)"},{"location":"Programming/Database/PostgreSQL%20Manual/#if-not-exists-modifier","text":"An important modifier is the IF NOT EXISTS clause that prevents errors when the object already exists. This is very useful if we want to update the database schema that is in the development phase with some automated script without the need to drop the database every time. However, this modifier is not available for all objects. It is available for: TABLE INDEX SEQUENCE VIEW MATERIALIZED VIEW PROCEDURE FUNCTION TRIGGER SCHEMA However, it is not available for: DATABASE USER CONSTRAINT For these objects, we have to use some workaround. There are three options in general: check if the object exists before creating it ( SO example with Constraint ) delete the object before creating it catch the error and ignore it ( SO example with Constraint )","title":"IF NOT EXISTS modifier"},{"location":"Programming/Database/PostgreSQL%20Manual/#create-table-as","text":"documentation The CREATE TABLE AS syntax is mostly equivalent to the SQL standard. An addition is the WITH NO DATA which we can put after the SELECT statement: CREATE TABLE <table name> AS SELECT <select statement> WITH NO DATA This way, we use the result of the SELECT statement to create the table structure, but the table is empty.","title":"CREATE TABLE AS"},{"location":"Programming/Database/PostgreSQL%20Manual/#procedures-and-functions","text":"To store a set of SQL commands for later use, PostgreSQL provides two options: procedures and functions. Both are similar and can use SQL, PL/pgSQL, or other languages supported by PostgreSQL. The key differences are: functions return a value, while procedures do not. However, procedures can return a value using an OUT parameter. functions can be called in SQL queries, while procedures require a separate CALL statement. functions: ... SELECT <function name>(<function arguments>) ... procedures: CALL <procedure name>(<procedure arguments>); procedures can manage transactions, while functions cannot.","title":"Procedures and functions"},{"location":"Programming/Database/PostgreSQL%20Manual/#syntax","text":"Both functions and procedures have the following syntax: CREATE OR REPLACE <FUNCTION/PROCEDURE> <name> (<parameters>) <return definition - only for functions> LANGUAGE <language name> AS <body> where <body> can be both a delimited string: AS $$ <function content> $$ OR an active SQL body, if we use SQL language (since PostgreSQL 14): BEGIN ATOMIC <sql statements> END There are some differences between those syntaxes (e.g., the second one works only for SQL and is evaluated/checked for validity at the time of creation), but in most cases, they are interchangable. The <function content> depends on the language used: for SQL, it is a set of SQL statements for PL/pgSQL, it is a block . See the functions and procedures section of the PL/pgSQL chapter.","title":"Syntax"},{"location":"Programming/Database/PostgreSQL%20Manual/#functions","text":"To call a function, we use the SELECT statement: SELECT * FROM <function signature> To create a function, we use the CREATE FUNCTION statement . Unlike for procedures, we need to specify a return type for function, either as an OUT / INOUT parameter, or using the RETURNS clause. To return tabular data, we use TABLE or SETOF <row type> return type: RETURNS TABLE(<param 1 name> <param 1 type>, ..., <param n name> <param n type>) RETURNS SETOF <row type> Note that TABLE and SETOF types can only be used as a return type of a function.","title":"Functions"},{"location":"Programming/Database/PostgreSQL%20Manual/#procedures","text":"To exectue a stored procedure, use: CALL <procedure name>(<procedure arguments>); To create a procedure, use the CREATE PROCEDURE command . The syntax is as follows: CREATE PROCEDURE <name> (<params>) LANGUAGE <language name> <procedure body>","title":"Procedures"},{"location":"Programming/Database/PostgreSQL%20Manual/#function-and-procedure-parameters","text":"The definition of parameters of functions and procedures are composed of three parts: mode: IN , OUT , or INOUT (optional, default is IN ) name: the name of the parameter (optional, default for OUT parameters generated) type: the type of the parameter Unlike in programing languages, there is no implicit type cast of the program arguments, including literals. Therefore, we need to cast all parameters explicitely, as in the following example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments(target_area_id smallint) ... CALL compute_speeds_for_segments(1::smallint);","title":"Function and procedure parameters"},{"location":"Programming/Database/PostgreSQL%20Manual/#deciding-the-language","text":"For simple statements, we can use the SQL language. We need the PL/pgSQL if: we need to use variables or control statements specific to PL/pgSQL we need to use temporary tables, as the SQL language fails to recognize them if they are created inside the function/procedure","title":"Deciding the language"},{"location":"Programming/Database/PostgreSQL%20Manual/#conditional-filters-based-on-the-value-of-a-variable-or-input-parameter","text":"To add some filter to the WHERE or ON clause of a query based on the value of a variable or input parameter, we can use the following technique: set the variable or input parameter to NULL if we do not want to apply the filter in the filter test for disjunction of NULL or the filter condition Example: SELECT ... FROM ... WHERE param IS NULL OR some_column = param","title":"Conditional filters based on the value of a variable or input parameter"},{"location":"Programming/Database/PostgreSQL%20Manual/#organizing-functions-and-procedures","text":"A natural task that emerge if there is a lot of functions and procedures is to organize them into packages. Unfortunately, PostgreSQL does not support any such feature. The closest thing is a schema, however, schemas are not suitable for this purpose, as they cannot be nested.","title":"Organizing functions and procedures"},{"location":"Programming/Database/PostgreSQL%20Manual/#temporary-tables","text":"To use a result set efficiently in a function or procedure, we often use temporary tables. Unlike in other relational database systems, in PostgreSQl, the lifetime of a temporary table is bound to a session. Therefoe, if we call a function that creates a tempoary table multiple times in a single session, we encounter an error, because the table already exists. To tackle this problem, we need to delete all temporary tables manually. Luckily, there is a special DISCARD command that can be used to dtop all temporary tables at once: DISCARD TEMPORARY;","title":"Temporary tables"},{"location":"Programming/Database/PostgreSQL%20Manual/#do-command","text":"The DO command can be used to execude an anonymus code block in any of the languages suported by PostgreSQL. It behaves like a function with no parameters and no return value. Syntax: DO [LANGUAGE <lang name>] <code> The default language is plpgsql . Example: DO $$ BEGIN RAISE NOTICE 'Hello world'; END $$","title":"DO command"},{"location":"Programming/Database/PostgreSQL%20Manual/#window-functions","text":"PostgreSQL supports an extended syntax for window functions . We can use it for example to retrieve the value of a column that has maximum value in another column, as demonstrated in an SO answer .","title":"Window functions"},{"location":"Programming/Database/PostgreSQL%20Manual/#return-data-from-insert-update-and-delete-statements","text":"In PostgreSQL, the INSERT , UPDATE , and DELETE statements can return data with the RETURNING clause. The syntax is as follows: INSERT INTO <table name> ... RETURNING <column name> This is especially useful when inserting data into a table with an auto-incrementing column, as we can retrieve the value of the column after the insert.","title":"Return data from INSERT, UPDATE, and DELETE statements"},{"location":"Programming/Database/PostgreSQL%20Manual/#various-specific-tasks","text":"","title":"Various specific tasks"},{"location":"Programming/Database/PostgreSQL%20Manual/#selecting-rows-for-deletion-based-on-data-from-another-table","text":"If we want to delete rows from a table based on some condition on data from another table, we can use the DELETE statement with a USING clause. Example: DELETE FROM nodes_ways_speeds USING nodes_ways WHERE nodes_ways_speeds.to_node_ways_id = nodes_ways.id AND nodes_ways.area IN (5,6)","title":"Selecting rows for deletion based on data from another table"},{"location":"Programming/Database/PostgreSQL%20Manual/#handeling-duplicates-in-the-insert-statement","text":"To handle duplicates on INSERT , PostgreSQL provides the ON CONFLICT clause (see the INSERT documentation). The options are: DO NOTHING : do nothing DO UPDATE SET <column name> = <value> : update the column to the given value","title":"Handeling duplicates in the INSERT statement"},{"location":"Programming/Database/PostgreSQL%20Manual/#random-oredering","text":"To order the result set randomly, we can use the RANDOM() function in the ORDER BY clause: SELECT ... FROM ... ORDER BY RANDOM()","title":"Random oredering"},{"location":"Programming/Database/PostgreSQL%20Manual/#random-ordering-with-a-seed-pseudo-random-ordering","text":"To receive a determinisic (repeatable) random ordering, we can use the setseed function: SELECT setseed(0.5); SELECT ... FROM ... ORDER BY RANDOM(); Note that we need two queries, one for setting the seed and one for the actual query. If we does not have an option to call arbitrary queries, we have to use UNION : SELECT col_1, ..., col_n FROM ( SELECT _, null AS col_1, ..., null AS col_n FROM setseed(0.5) UNION ALL SELECT null AS _, col_1, ..., col_n FROM ... OFFSET 1 ) ORDER BY RANDOM() The OFFSET 1 is used to skip the first row, which is the result of the setseed function. The union is the only way to guarantee that the seed will be set before the actual query. Other options, such as WITH or SELECT ... FROM (SELECT setseed(0.5)) do not guarantee the order of execution, and produce a different result for each call.","title":"Random ordering with a seed (Pseudo-random ordering)"},{"location":"Programming/Database/PostgreSQL%20Manual/#postgis","text":"PolsGIS is an extension for PostgreSQL that adds support for spatial data. To enable the extension, we need to: install the extension e.g., using the bundeled stack builder application enable the extension in the database SQL CREATE EXTENSION postgis;","title":"PostGis"},{"location":"Programming/Database/PostgreSQL%20Manual/#geometry-columns","text":"Postgis features can be utilized with geometry and geography column types. To add a new geometry column: ADD COLUMN <COLUUMN NAME> geometry(<GEOMETRY TYPE>, <SRID>)","title":"Geometry columns"},{"location":"Programming/Database/PostgreSQL%20Manual/#spatial-indexing","text":"Documentation Analogously to standard SQL column indicis, there are spatial indices in PostGIS. The only difference is that we need to add the USING GIST at the end of the CREATE INDEX statement: CREATE INDEX nodes_geom_idx ON nodes USING GIST (geom);","title":"Spatial Indexing"},{"location":"Programming/Database/PostgreSQL%20Manual/#converting-between-geometry-types","text":"There are dedicated functions whcich we can use to convert between geometry types: ST_Multi : converts geometries to their multi-variant, e.g., LineString to MultiLineString .","title":"Converting between geometry types"},{"location":"Programming/Database/PostgreSQL%20Manual/#compute-area-surrounding-geometries","text":"If we are ok with a convex envelope of the geometries, we can simply use the St_ConvexHull functon. Howeever, if we need the exact shape, We have to use the St_ConcaveHull function which computes the concave hull of a geometry. The St_ConcaveHull function takes an additional parameter param_pctconvex which determines how concave is the result: 0 means strictly concave, while 1 means convex hull. Note that while lowere values leads to more acurate results, the computation is much slower. There is also another parameter param_allow_holes which determines whether holes in the object are permited (default false).","title":"Compute area surrounding geometries"},{"location":"Programming/Database/PostgreSQL%20Manual/#split-area-to-polygons-based-on-points","text":"The split of spece into polygons based on a set of points is called Voronoi diagram . In PostGIS, we have a `ST_Voronoi_Polygons for that. To obtain a set of polygons from a set of points, it is necessary to Aggregate the rows ( ST_Collect ) Compute the polygon geometry ( ST_Voronoi_Polygons ) Disaggregate the geometry into individual polygons ( ST_Dump ) Also, there is an important aspect of how far the polygons will reach outside of the points. By default, it enlarge the area determined by the points by about 50%. If we need a larger area, we can use the extend_to parameter. If we need a smaller area, however, we need to compute the intersection with this smaller area afterwards manually. Full example: SELECT st_intersection( (st_dump( st_voronoipolygons(st_collect(<GEOMETRY COLUMN>)) )).geom, (<select clause for area of desired voronoi polygons>) ) AS geom FROM ... If we need to join the polygons to the original points, we need to do it manually (e.g. by JOIN ... ON ST_Within(<POINT COLUMN>, <VORONOI POLYGONS GEO DUMP>) ).","title":"Split area to polygons based on points"},{"location":"Programming/Database/PostgreSQL%20Manual/#other-useful-functions","text":"ST_GeometryType : returns the type of the geometry as a string. [ ST_Within ] ST_Within (A, B) if A is completly inside B ST_Intersects ST_Intersects(g1, g2) if g1 and g2 have at least one point in common. ST_Transform : ST_Transform(g, srid) transforms geometry g to a projection defined by the srid and returns the result as a geometry. ST_Buffer : ST_Buffer(g, radius) computes a geometry that is an extension of g by radius to all directions St_Collect aggregates data into single geometry. It is usually apllied to a geometry column in an SQL selection. ST_Union ST_Equals ST_MakeLine : make line between two points ST_SetSRID : sets the SRID of the geometry and returns the result as a new geometry. ST_MakePoint : creates a point geometry from the given coordinates. ST_Area : computes the area of a geometry. The units of the result are the same as the units of the SRID of the geometry (use UTM coordinate system for getting the area in square meters).","title":"Other Useful Functions"},{"location":"Programming/Database/PostgreSQL%20Manual/#plpgsql","text":"PL/pgSQL is a procedural language available in PostgreSQL databases. It can be used inside: functions procedures DO command","title":"PL/pgSQL"},{"location":"Programming/Database/PostgreSQL%20Manual/#blocks","text":"Documentation The base components of PL/pgSQL is a block . It's syntax is as follows: DECLARE <variable declarations> BEGIN <statements> END Here, the DECLARE part is optional. In a block, each statement or declaration ends with a semicolon. We can nest the blocks, i.e., we can put a block inside another block, between the BEGIN and END keywords. In this case, we put a semicolon after the inner block. Only the outer block that contains the whole content of the function/procedure/ DO command does not require a semicolon.","title":"Blocks"},{"location":"Programming/Database/PostgreSQL%20Manual/#variables","text":"In PL/PgSQL, all variables must be declared before assignmant in the DECLARE block which is before the sql body of the function/procedure/ DO . The syntax is: <variable name> <variable type>[ = <varianle value>]; Example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments() LANGUAGE plpgsql AS $$ DECLARE dataset_quality smallint = 1; BEGIN ... The, the value of a variable can be change using the classical assignment syntax: <variable name> = <varianle value>; Be carful to not use a variable name equal to the name of some of the columns used in the same context, which results in a name clash. Nothe that := is also a valid assignment operator . There is no difference between = and := in plpgsql. The := operator is sometimes preferred to avoid confusion with the = operator used for comparison.","title":"Variables"},{"location":"Programming/Database/PostgreSQL%20Manual/#assigning-a-value-to-a-variable-using-sql","text":"There are two options how to assign a value to a variable using SQL: using the INTO clause in the SELECT statement using a SELECT statement as rvlaue of the assignment Example with INTO : SELECT name INTO customer_name FROM customers WHERE id = 1 EXAMPLE with SELECT as rvalue: customer_name = (SELECT name FROM customers WHERE id = 1) Note that the behavior of the assignment may not be intuitive: if the SELECT statement returns no rows, the variable is set to NULL if the SELECT statement returns more than one row, an error is raised","title":"Assigning a value to a variable using SQL"},{"location":"Programming/Database/PostgreSQL%20Manual/#functions-and-procedures","text":"The root of a PL/pgSQL function content is a block .","title":"Functions and procedures"},{"location":"Programming/Database/PostgreSQL%20Manual/#returning-data","text":"Documentation When using SQL language, we can return the data just by executing a SELECT statement: CREATE OR REPLACE FUNCTION <function name> (<parameters>) LANGUAGE SQL AS $$ SELECT ... $$ When using PL/pgSQL, we have to use the RETURN statement. To make it even more complicated, there are three versions of the RETURN statement: RETURN <expression> : This one is for: returning a single scalar value return from function with return type void . In this case, the <expression> is empty. RETURN NEXT <expression> and RETURN QUERY <query> : These are for returning a set of rows. As the RETURN NEXT and RETURN QUERY do not terminate the function, we can mix them with the RETURN statement. Additionally, the RETURN QUERY can be used multiple times in a single function and therefore, it is valid to use all three versions in a single function.","title":"Returning data"},{"location":"Programming/Database/PostgreSQL%20Manual/#return-next-and-return-query","text":"Documentation For returning a set of rows ( table or setof type) in PL/PgSQL , we have to use the RETURN NEXT or RETURN QUERY statement. Besides the fact that these statements are used for returning a set of rows, there is another important difference between them and the RETURN statement: they do not terminate the execution of the function . This is useful as sometimes, we need to do some cleanup after selecting the rows to be returned from function, or we need to build the result in a loop. In classical programming languages, we use variables for this purpose. In PG/plSQL , we can also use the RETURN NEXT and RETURN QUERY constructs. Example: RETURN QUERY SELECT ...; DROP TABLE target_ways; RETURN; The RETURN QUERY differs from RETURN NEXT in following ways: its argument is a query, not an expression. Therefore we can use it directly with the SELECT statement, instead of using a variable. it appends the result of the query to the result set. Therefore, it can be used multiple times in a single function. it cannot be used for returning a single value even if the query returns a single value. If we have a single value return type and need to do some postprocessing between selecting the value and returning from the function, we have to use a variable instead.","title":"RETURN NEXT and RETURN QUERY"},{"location":"Programming/Database/PostgreSQL%20Manual/#branching","text":"PL/pgSQL has the following branching: IF <condition> THEN ... [ELSEIF ... ] [ELSE ... ] END IF Note that NULL values are treated the same as in SQL: if the condition evaluates to NULL , the branch is not executed . This is different from other programming languages, where NULL is treated as false .","title":"Branching"},{"location":"Programming/Database/PostgreSQL%20Manual/#logging","text":"documentation . Basic logging can be done using the RAISE command: RAISE NOTICE 'Some message'; We can add parameters by using the % placeholder: RAISE NOTICE 'Some message %', <expression>; Only scalar expressions are allowed as parameters. If we need to log multiple rows, we have to use a loop: FOR row IN SELECT ... LOOP RAISE NOTICE 'Some message %, %', row.column_1, row.column_2; END LOOP;","title":"Logging"},{"location":"Programming/Database/PostgreSQL%20Manual/#executing-functions","text":"In SQL, any function is executed using the SELECT statement: SELECT * FROM <function name>(<function arguments>); -- for functions with return value SELECT <function name>(<function arguments>); -- for functions returning void or if we do not care about the return value In PL/pgSQL, the second syntax where the return value is not used is not allowed. Instead, we have to use the PERFORM statement: PERFORM <function name>(<function arguments>);","title":"Executing functions"},{"location":"Programming/Database/PostgreSQL%20Manual/#exceptions","text":"In PL/pgSQL, we can have both exceptions raised by the database and exceptions that are manually raised in user code.","title":"Exceptions"},{"location":"Programming/Database/PostgreSQL%20Manual/#handling-exceptions","text":"Documentation In PL/pgSQL, we can handle exceptions by extending the block with the EXCEPTION part. The syntax is as follows: BEGIN ... EXCEPTION WHEN <exception condition> THEN <exception handling> END The list of built-in exception codes can be found in the documentation . The most common exceptions are: P0001 , raise_exception : A user-raised exception that does not specify the SQLSTATE. 00000 , successful_completion : The normal completion of a statement. No exception should ever be raised with this SQLSTATE. Apart from the built-in codes, we can introduce our own. They have to be five characters long and start with a letter. The <exception handling> part can be any valid PL/pgSQL code. It can contain multiple statements and can be nested. The <exception condition> can have three forms: WHEN <exception name> : catches the exception with the specified name WHEN SQLSTATE '<SQLSTATE>' : catches the exception with the specified SQLSTATE WHEN OTHERS : catches all exceptions that are not caught by the previous WHEN clauses There can be many WHEN clauses in a single EXCEPTION block. If there is no matching WHEN clause for the exception, the exception is propagated as if there was no EXCEPTION block at all. Note that the code 00000 cannot be caught by the WHEN '00000' clause. Such condition is interpreted as WHEN OTHERS . When handling an exception, we can use two special built-in variables: SQLSTATE : the SQLSTATE of the exception - a five-character code that specifies the exception type SQLERRM : the error message of the exception","title":"Handling exceptions"},{"location":"Programming/Database/PostgreSQL%20Manual/#raising-exceptions","text":"Documentation To raise an exception, we use the RAISE statement, the same as for logging. The difference is in the level parameter, which we must set to EXCEPTION (default): RAISE EXCEPTION 'Some message'; -- exception RAISE 'Some message'; -- exception RAISE <another level> 'Some message'; -- just a message","title":"Raising exceptions"},{"location":"Programming/Database/PostgreSQL%20Manual/#query-diagnostics","text":"Various information about the last query can be obtained using the GET DIAGNOSTIC command. For example, the number of rows affected by the last query can be obtained using the ROW_COUNT parameter: GET DIAGNOSTIC <variable> = ROW_COUNT; The result is stored in the variable <variable> . Note that this constract is not available in the SQL queries but only in a PL/pgSQL block. For other diagnostic fields, see the documentation .","title":"Query diagnostics"},{"location":"Programming/Database/PostgreSQL%20Manual/#transaction-management","text":"By default, PostgreSQL starts a transaction at the beginning of the outermost block , and end it at the end of the block. We can manually commit or rollback the transaction, but only inside a procedure that is on a procedure-only call stack: procedure 1 -> procedure 2 -> procedure 3 : all three procedures can commit or rollback the transaction procedure 1 -> function 2 -> procedure 3 : only procedure 1 can commit or rollback the transaction function 1 -> procedure 2 -> function 3 : neither procedure nor function can commit or rollback the transaction.","title":"Transaction management"},{"location":"Programming/Database/PostgreSQL%20Manual/#subtransactions","text":"So far, we covered the top-level transactions. However, there are also subtransactions. Subtransactions cannot be handled manually, however, we can manipulate them, if we know the logic of the automatic subtransaction management. The subtransactions are started: inside each block with the exception management these subtransactions are rolled back if an exception is raised","title":"Subtransactions"},{"location":"Programming/Database/PostgreSQL%20Manual/#psql","text":"Documentation psql is a basic command line utitilty for manipulating postgres database. To connect, just type psql . By default, it will connect as a user corresponding to the OS user name to the default database for that user. To change it: -U <user name> : connect as a different user, -d <db name> : connect to the specified database. This way, we connect to the database in interactive mode . In this mode, we can write multiple SQL commands using one connection. The commands are only executed when correct ending punctuation (usually a semicolon) is encountered. Otherwise, enter just adds a new line and the command is not executed. To quit the interactive mode, use the \\q meta-command. To execute command immediatelly without starting interactive session, use the -c parameter: psql -d <db name> -c \"<command>\" Do not forget to quote the command. Also, note that certain SQL commands, such as CREATE DATABASE requires its own session. To combine them with other SQL commands, you can use multiple -c parameters: psql -d <db name> -c \"CREATE DATABASE <db name>\" -c \"<other command>\" Other useful parameters: -X : do not read the ~/.psqlrc file. This is useful when debuging the psql commands or running scripts, as it disables any customizations. -p : specify the port number. (default is 5432) -l : list all databases","title":"psql"},{"location":"Programming/Database/PostgreSQL%20Manual/#default-database","text":"Note that no matter what SQL commands you plan to execute, you have to connect to a specific database . Ommitting the -d parameter will not connect you to the server in some admin mode, but instead, it will connect you to the default database, which is usually the database with the same name as the user name. If there is no such database, the connection will fail.","title":"Default database"},{"location":"Programming/Database/PostgreSQL%20Manual/#meta-commands","text":"The psql has its own set of commands, called meta-commands . These commands start with a backslash ( \\ ) and can be used inside SQL queries (either interactively, or in the -c argument). Example: psql -d <db name> -c \"\\l+\" The above command lists all databases, including additional information. Note that the meta-commands cannot be combined with SQL queries passed to the -c parameter . As -c argument, we can use either: a plain SQL query without any meta-commands a single meta-command without any SQL query (like the example above) (In \\copy public.nodes FROM nodes.csv CSV HEADER , the string after \\copy is a list of arguments, not an SQL query) When we want to combine a meta-command with an SQL query, we need to use some of the workarounds: use the psql interactive mode use the psql --file parameter to execute a script from a file pipe the commands to psql using echo : bash echo \"COPY (SELECT * FROM opendata.public.nodes WHERE area = 13) TO STDOUT WITH CSV HEADER \\g 'nodes.csv'\" | psql -d opendata Useful meta-commands: \\d : list all tables and views in the current schema. We can pass a name as an argument to list only the table or view with the given name. \\d+ : same as \\d , but with additional information. \\q : quit the interactive mode. \\i : execute a script from a file. \\sf <function name> : show the source code of a function.","title":"meta-commands"},{"location":"Programming/Database/PostgreSQL%20Manual/#executing-sql-files","text":"In normal mode, we can execute SQL files using the -f parameter: psql -d <db name> -f <file name> In interactive mode, we can execute SQL files using the \\i meta-command. Note that backslash ( \\ ) in the file path needs to be escaped or replaced with a forward slash ( / ) .","title":"Executing SQL files"},{"location":"Programming/Database/PostgreSQL%20Manual/#importing-data","text":"A simple SQL data (database dump) can be imported using the psql command: psql -d <db name> -f <file name> we can also import from stdin by omitting the -f parameter: psql -d <db name> < <file name> # or <command> | psql -d <db name>","title":"Importing data"},{"location":"Programming/Database/PostgreSQL%20Manual/#importing-compressed-sql-dump","text":"To import a compressed SQL dump we need to know how the dump was compressed. If it was compressed using the pg_dump with the -Z parameter, we use a pg_restore command: bash pg_restore -d <db name> <file name> If it was compressed using a compression tool, we need to pipe the output to the decompression tool and then to psql : bash <decompression tool> < <file name> | psql -d <db name>","title":"Importing compressed SQL dump"},{"location":"Programming/Database/PostgreSQL%20Manual/#importing-data-from-csv","text":"The easiest way to import data from csv is to use the psql \\copy meta-command: psql -d <db name> -c \"\\copy <table name> FROM <file name> CSV HEADER\" The syntax of this command and its parameters is almost identical to the COPY command. Important parameters: CSV : the file is in CSV format HEADER : the first line of the file contains column names Except simplicity, the \\copy command has another advantage over the COPY command: it has the same access rights as the user that is currently logged in. The COPY command, on the other hand, has the access rights of the user that started the database server. The COPY command, however, has an advantage over the \\copy command when we access a remote database. In that case, the \\copy command would first download the whole file to the client and then upload it to the server. The COPY command, on the other hand, can be executed directly on the server, so the data are not downloaded to the client.","title":"Importing data from csv"},{"location":"Programming/Database/PostgreSQL%20Manual/#importing-data-from-csv-with-columns-than-are-not-in-the-db-table","text":"The COPY (and \\copy ) command can be used to copy the input into a database table even if it contains only a subset of database table columns. However, it does not work the other way, i.e, all input columns have to be used. If only a subset of input columns needs to be used, or some of the input columns requires processing, we need to use some workaround. The prefered mathod depends on the characte of the data: data do not match the table, but they are small: load the data with pandas process the data as needed use pandas.to_sql to upload the data data do not match the table and they are large: preprocess the data with batch commands use COPY (or \\copy ) to upload the data data do not match the table and they are large and dirty : use the file_fdw module: create a table for SQL mapping with tolerant column types (e.g., text for problematic columns) select from the mapping to the real table","title":"Importing data from csv with columns than are not in the db table"},{"location":"Programming/Database/PostgreSQL%20Manual/#importing-data-from-a-shapefile","text":"There are multiple options: shp2psql : simple tool that creates sql from a shapefile easy start, almost no configuration always imports all data from shapefile, cannot be configured to skip columns included in the Postgres instalation ogr2ogr needs to be installed, part of GDAL QGIS The db manger can be used to export data from QGIS data can be viewed before import only suitable for creating new table, not for appending to an existing one","title":"Importing data from a shapefile"},{"location":"Programming/Database/PostgreSQL%20Manual/#importing-data-from-geojson","text":"For a single geometry stored in a GeoJSON file, the function ST_GeomFromGeoJSON can be used. just copy the geometry part of the file change the geometry type to match the type in db don't forget to surround the object with curly brackets For the whole document, the ogr2ogr tool can be used.","title":"Importing data from GeoJSON"},{"location":"Programming/Database/PostgreSQL%20Manual/#exporting-data","text":"The data can be exported to SQL using the pg_dump command. The simplest usage is: pg_dump -f <output file name> <db name> Important parameters: -s : export only the schema, not the data If we need to further process the data, we can use the stdout as an output simply by omitting the -f parameter: pg_dump <db name> | <command>","title":"Exporting data"},{"location":"Programming/Database/PostgreSQL%20Manual/#exporting-the-database-server-configuration","text":"To export the database server configuration, we can use the pg_dumpall command: pg_dumpall -f <output file name>","title":"Exporting the database server configuration"},{"location":"Programming/Database/PostgreSQL%20Manual/#compressing-the-sql-dump","text":"To compress the SQL dump, we have two options: use the pg_dump command with the -Z parameter, e.g., pg_dump <db name> -Z1 -f <output file name> or pipe the output to a compression tool, e.g., pg_dump <db name> | gzip > <output file name>","title":"Compressing the SQL dump"},{"location":"Programming/Database/PostgreSQL%20Manual/#exporting-data-to-csv","text":"When exporting large data sets, it is not wise to export them as an SQL dump. To export data to csv, we can use either: psql \\copy command COPY command in SQL The simplest way is to use the \\copy command. However, it may be slow if we call psql from a client and we want to export the data to a file on the server, because the data are first sent to the client and then back to the server. The COPY command is the fastest way to export data to the server. However, by default, it can be tricky to use, as the sql server needs to have write access to the file. To overcome this problem, we can use the following workaround: choose STDOUT as an output for the COPY command at the end of the command, add \\g <file name> to redirect the output to a file. Example: echo \"COPY (SELECT * FROM opendata.public.nodes WHERE area = 13) TO STDOUT WITH CSV HEADER \\g 'nodes.csv'\" | psql -d opendata","title":"Exporting data to csv"},{"location":"Programming/Database/PostgreSQL%20Manual/#information-and-statistics","text":"To show the table size , run: SELECT pg_size_pretty(pg_total_relation_size('<table name>')); To show the version of the PostgreSQL server, run: SELECT version(); To list all extensions for a database, run: psql -d <db name> -c \"\\dx\" To check if a specific table exists, run: SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = '<schema_name>' AND table_name = '<table_name>');","title":"Information and statistics"},{"location":"Programming/Database/PostgreSQL%20Manual/#databases","text":"To list all databases, we can use the -l parameter: psql -l To get more information about the databases, we can use the \\l+ meta-command. To check if a specific database exists, we can use the following query: SELECT EXISTS (SELECT 1 FROM pg_database WHERE datname = '<db name>');","title":"Databases"},{"location":"Programming/Database/PostgreSQL%20Manual/#pgrouting","text":"documentation PgRouting is a PostgreSQL extension focused on graph/network manpulation. It contains functions for: finding the strongly connected components: pgr_strongComponents graph contraction/simplification : pgr_contraction creating vertices from edges: pgr_createVerticesTable","title":"PgRouting"},{"location":"Programming/Database/PostgreSQL%20Manual/#finding-strongly-connected-components","text":"The function pgr_strongComponents finds the strongly connected components of a graph. The only parameter of the script is a query that should return edge data in the folowing format: id , source , target , cost , reverse_cost . The first three parameters are obvious. The cost parameter does not have any effect. You should provide a negative reverse_cost , othervise, the edge will be considered as bidirectional!","title":"Finding strongly connected components"},{"location":"Programming/Database/PostgreSQL%20Manual/#creating-vertices-from-edges","text":"The function pgr_createVerticesTable creates a table of vertices from a table of edges. The function has the following parameters: edges_table : the name of the table with edges. It must be a standard table, temporary tables are not supported. the_geom : the name of the geometry column in the edges_table source : the name of the source column in the edges_table target : the name of the target column in the edges_table The return value is OK if the function was successful and FAIL if it was not. The created vertices table is named <edges_table>_vertices_pgr and contains the following columns: id : the id of the vertex the_geom : the geometry of the vertex cnt : the number of edges that are connected to the vertex chk : an integer that indicates if the vertex might have a problem. ein : the number of incoming edges eout : the number of outgoing edges Note that only the id , and the_geom columns are filled with data. The other columns are filled with NULL . To fill the cnt and chk columns, we can use the pgr_analyzeGraph function. To fill all columns, we need to use the pgr_analyzeOneway function. However, this function is cumbersome to use, as it requires a lot of parameters.","title":"Creating vertices from edges"},{"location":"Programming/Database/PostgreSQL%20Manual/#testing-with-pgtap","text":"Official documentation PgTAP is the only testing framework for PostgreSQL. It is a system framwork: it requires administrative privileges to install.","title":"Testing with PgTAP"},{"location":"Programming/Database/PostgreSQL%20Manual/#installation","text":"","title":"Installation"},{"location":"Programming/Database/PostgreSQL%20Manual/#linux","text":"If you are using Linux, you may (depending on your distribution) be able to use you distribution's package management system to install pgTAP. For instance, on Debian, Ubuntu, or Linux Mint pgTAP can be installed with the command: sudo apt-get install pgtap On other systems pgTAP has to be downloaded and built. First, download pgTAP from PGXN (click the green download button in the upper-right). Extract the downloaded zip file, and (at the command line) navigate to the extracted folder. To build pgTAP and install it into a PostgreSQL database, run the following commands: make make install make installcheck","title":"Linux"},{"location":"Programming/Database/PostgreSQL%20Manual/#windows","text":"To install pgtap for PostgreSQL on Windows, follow these steps: Clone the pgtap repository Open PowerShell ( pwsh ) as an Administrator it is necessary to copy files into the ProgramFiles directory. run the pgtap_install.ps1 script as an administrator with the following command: PowerShell pgtap_install.ps1 <path to pgtap clone> This script will copy the necessary files to the PostgreSQL installation directory. These instructions were adapted from issue#192 of the pgtap repository.","title":"Windows"},{"location":"Programming/Database/PostgreSQL%20Manual/#basic-usage-test-scripts","text":"The easiest way to write tests in PgTAP is to write procedural SQL scripts that contain the tests. The basic test can look like this: BEGIN; -- Start a transaction -- Specify that we plan to run 2 tests SELECT plan(2); -- Test 1: Check if basic arithmetic works as expected SELECT is(1 + 1, 2, '1 + 1 equals 2'); -- Test 2: Verify string concatenation SELECT is('Hello ' || 'World', 'Hello World', 'String concatenation works'); -- -- Run the tests and return results SELECT * FROM finish(); ROLLBACK; -- Rollback the transaction here, we create two assertions using the is function from the pgtap library. Then, we run the tests using the finish function. The whole test is wrapped in a transaction, so the database is not modified by the test. To execute the test, we can: run the test SQL directly in the SQL console or with the psql command use the pg_prove command to run the test from the command line","title":"Basic Usage - Test scripts"},{"location":"Programming/Database/PostgreSQL%20Manual/#pg_prove","text":"The pg_prove is a Perl test runner that can be used to run the pgtap tests. We need to install it first using cpanm TAP::Parser::SourceHandler::pgTAP . Then, we can use it as: pg_prove -d <db name> -U <user name> <test file>","title":"pg_prove"},{"location":"Programming/Database/PostgreSQL%20Manual/#test-as-functions","text":"Instead of writing the tests in a procedural SQL script, we can write them as functions ( but not procedures! ). This can help as organizing tests and also to prepare the data for the tests. Test functions must return the SETOF TEXT type. This correspond to the type returned by the provided test assertions .","title":"Test as functions"},{"location":"Programming/Database/PostgreSQL%20Manual/#test-runner","text":"There is also a runner function runtests that can be used to run multiple tests at once: SELECT * FROM runtests(); This function has four variants: runtests() , runtests(<schema>) , runtests(<pattern>) , and runtests(<schema>, <pattern>) . The schema parameter is used to specify the schema where the tests are searched. The pattern parameter is used to specify the pattern that the test names must match. The pattern uses the same syntax as the LIKE operator in SQL.","title":"Test runner"},{"location":"Programming/Database/PostgreSQL%20Manual/#fixtures","text":"Sometimes, we need to call some functions before and after the tests. We can indeed call these functions in from the test functions, but in case we need to run the same functions for multiple tests, it is desirable to automate this process using fixtures. The runtests function supports fixtures, it recognize them by the prefix. Again, only functions, not procedures are supported. The following fixtures are supported: startup : runs before the tests setup : runs before each test teardown : runs after each test shutdown : runs after the tests Unfortunatelly, the fixture search does not reflect the filtration <pattern> . Therefore, all fixtures in the schema are always run. To overcome this, we have to supply a custom function for executing the tests. Also note that all the fixtures are executed for all the tests, not just the ones with the matching name. Therefore they cannot be used for preparing the data for individual tests.","title":"Fixtures"},{"location":"Programming/Database/PostgreSQL%20Manual/#rolling-back","text":"All tests are executed in a transaction and each test is then in a subtransaction. All database changes are rolled back except the ones that are in the startup and shutdown fixtures [source] . Additionally, some changes, e.g. the sequence numbers cannot be rolled back.","title":"Rolling back"},{"location":"Programming/Database/PostgreSQL%20Manual/#test-assertions","text":"PgTAP provides a set of functions that works as test assertions. All functions have an optional <message> parameter. Notable functions are: ok(<condition>, <message>) : checks if the <condition> is true is(<computed>, <expected>, <message>) or isnt(<computed>, <not expected>, <message>) : checks if <computed> and <expected> are the same, or not (respectively). matches(<text>, <pattern>, <message>) or doesnt_match(<text>, <pattern>, <message>) : checks if <text> matches the <pattern> where <pattern> is a regular expression, or not (respectively). alike(<text>, <pattern>, <message>) or unalike(<text>, <pattern>, <message>) : checks if <text> is similar to <pattern> , or not (respectively). The LIKE operator is used for the comparison. cmp_ok(<computed>, <operator>, <expected>, <message>) : checks if <computed> is related to <expected> by the <operator> . The <operator> can be one of the following: = , != , > , >= , < , <= . isa_ok(<value>, <type>, <message>) : checks if <value> is of type <type> . throws_ok(<sql>, <error_code>, <error_message>, <message>) : checks if the <sql> throws an exception. all parameters except the <sql> are optional lives_ok(<sql>, <message>) : checks if the <sql> does not throw an exception. Additionaly, if the test condition is complicated, we can just manually return using the pass(<message>) or fail(<message>) functions.","title":"Test assertions"},{"location":"Programming/Database/PostgreSQL%20Manual/#result-set-assertions","text":"results_eq(<sql>, <sql>, <description>) : Compares two SQL query results row-by-row for equality, ensuring integrity and order. results_eq(<sql>, <array>, <description>) : Compares SQL query results to an array. results_eq(<cursor>, <cursor>, <description>) : Compares results from two cursors. results_eq(<sql>, <cursor>, <description>) : Compares SQL query results to cursor results. results_eq(<cursor>, <array>, <description>) : Compares cursor results to an array. results_ne(<...>, <description>) : The inverse of results_eq() . Checks if result sets are not equal (accepts the same argument variations as results_eq ). set_eq(<...>, <description>) : Compares result sets for equality, ignoring order (accepts the same argument variations as results_eq ). set_ne(<...>, <description>) : The inverse of set_eq() . Checks if result sets are not equal, ignoring order (accepts the same argument variations as results_eq ). set_has(<sql>, <sql>, <description>) : Checks if the first query's results contain all rows returned by the second query, ignoring order. set_hasnt(<sql>, <sql>, <description>) : The inverse of set_has . Checks if the first query's results do not contain all rows from the second query. bag_eq(<sql>, <sql>, <description>) : Compares result sets like set_eq but considers duplicate rows. If a row appears N times in the first query, it must appear N times in the second. bag_ne(<sql>, <sql>, <description>) : The inverse of bag_eq() . is_empty(<sql>, <description>) : Checks that the SQL query returns no rows. isnt_empty(<sql>, <description>) : Checks that the SQL query returns at least one row. row_eq(<sql>, <record>, <description>) : Checks that the SQL query returns a single row identical to the provided <record> .","title":"Result set assertions"},{"location":"Programming/Database/PostgreSQL%20Manual/#additional-functions-diagnostics","text":"The function diag(<string>, <string>, ...) , can be used for outputting diagnostic messages. For example: -- Output a diagnostic message if the collation is not en_US.UTF-8. SELECT diag( E'These tests expect LC_COLLATE to be en_US.UTF-8,\\n', 'but yours is set to ', setting, E'.\\n', 'As a result, some tests may fail. YMMV.' ) FROM pg_settings WHERE name = 'lc_collate' AND setting <> 'en_US.UTF-8'; which outputs # These tests expect LC_COLLATE to be en_US.UTF-8, # but yours is set to en_US.ISO8859-1. # As a result, some tests may fail. YMMV. This function basically concatenates all the arguments and outputs them as a single line of text. All the arguments have to be strings.","title":"Additional functions( Diagnostics )"},{"location":"Programming/Database/PostgreSQL%20Manual/#xml","text":"documentation PostgreSQL has a built-in support for XML. Important functions: xpath(<path>, <text>, <namespace bindings>) : returns an array of XML nodes that match the given <path> evaluated on the given <text> . Besides functions, there is a powerful XMLTABLE expression that can be used to extract tabular data from XML documents.","title":"XML"},{"location":"Programming/Database/PostgreSQL%20Manual/#handling-xml-namespaces","text":"All XML methods in PostgreSQL require the namespace bindings to be specified, as per the XPath standard. The only exception is when work with XML documents that do not use namespaces. We specify the namespace bindings as an array of arrays, where each inner array has two elements: the prefix and the namespace URI. Example: SELECT xpath( '/ns:root/ns:child', '<root xmlns:ns=\"http://example.com/ns\"><child>...</child></root>', ARRAY[ARRAY['ns', 'http://example.com/ns']]); Note that the namespace prefixes specified in the bindings are completely unrelated to the prefixes used in the XML document. They may be the same, but they do not have to be. Also don't forget that XPath requires the namespace prefixes even for the default namespace.","title":"Handling XML namespaces"},{"location":"Programming/Database/PostgreSQL%20Manual/#xmltable","text":"Documentation XMLTABLE expression can extract tabular data from XML documents. The syntax is as follows: XMLTABLE( [XMLNAMESPACES(<namespace bindings>),] <xpath expression>, PASSING <xml document expression> COLUMNS <column name> <data type> PATH <xpath relative expression>, ... <column name> <data type> PATH <xpath relative expression> ) If the XML document contains namespaces, we have to specify the namespace bindings in the XMLNAMESPACES clause. The <namespace bindings> is a comma-separated list of namespace bindings in the format <namespace URI> AS <prefix> . Example: XMLNAMESPACES( 'http://graphml.graphdrawing.org/xmlns' AS dns, 'http://www.yworks.com/xml/yfiles-common/3.0' AS y ) The <xpath expression> is an XPath expression that specifies the path to the XML nodes from which we want to extract data. The <xpath relative expression> is then another XPath expression that is relative to the <xpath expression> and specifies the path to the XML nodes or attributes used to fill a specific column. The <xml document expression> is an expression that evaluates to an XML document. Typically, it is a column in the select statement, or a subquery. Example: SELECT some_element FROM xml_documents, XMLTABLE( XMLNAMESPACES(...), '/dns:graph/dns:node' PASSING xml_document.xml_data COLUMNS some_element TEXT PATH ... ) FROM xml_documents WHERE id = 1; Note that the XMLTABLE expression may only be used in the FROM clause. Therefore, if we, for example, need to join the result of the XMLTABLE , instead of using: JOIN XMLTABLE(...) we have to use: JOIN (SELECT <columns> FROM XMLTABLE(...))","title":"XMLTABLE"},{"location":"Programming/Database/PostgreSQL%20Manual/#troubleshooting","text":"If the db tools are unresponsive on certain tasks/queries, check if the table needed for those queries is not locke by some problematic query.","title":"Troubleshooting"},{"location":"Programming/Database/PostgreSQL%20Manual/#select-postgresql-version","text":"SELECT version()","title":"Select PostgreSQL version"},{"location":"Programming/Database/PostgreSQL%20Manual/#select-postgis-version","text":"SELECT PostGIS_version()","title":"Select PostGIS version"},{"location":"Programming/Database/PostgreSQL%20Manual/#tried-to-send-an-out-of-range-integer-as-a-2-byte-value","text":"This error is caused by a too large number of values in the insert statement. The maximum index is a 2-byte number (max value: 32767). The solution is to split the insert statement into smaller bulks.","title":"Tried to send an out-of-range integer as a 2-byte value"},{"location":"Programming/Database/SQL%20Manual/","text":"SQL is a standard for relational databases. There is no complete implementation of the standard. The standard is not free, you have to pay for it. Therefore, it is sometimes hard to say what exactly is SQL and what is not. Operators \u00b6 Comparison operators: = : equal <> or != : not equal > : greater than < : less than >= : greater or equal <= : less or equal String operators: || : concatenation LIKE : pattern matching Literals \u00b6 In SQL, there are three basic types of literals: numeric : 1 , 1.2 , 1.2e3 string : 'string' , \"string\" boolean : true , false When we need a constant value for other types, we usually use either a constructor function or a specifically formatted string literal. String literals can also span multiple lines , we do not need any operator to split the line (unlike in Python and despite the JetBrains IDEs adds a string concatenation operator automatically on newline). NULL in SQL \u00b6 wiki In SQL NULL is a marker for missing values. When using NULL as an operand, it propagates, meaning that the operation returns NULL: SELECT 1 + NULL; -- returns NULL SELECT 'The value is ' || NULL; -- returns NULL This may be counterintuitive, especially in the case of comparison operators: SELECT 1 = NULL; -- returns NULL SELECT 1 != NULL; -- returns NULL SELECT NULL = NULL; -- returns NULL The only exception are logical operators: SELECT TRUE OR NULL; -- returns TRUE SELECT FALSE AND NULL; -- returns FALSE To solve the problem with comparison operators, SQL has special operators IS NULL and IS NOT NULL : SELECT 1 IS NULL; -- returns FALSE SELECT 1 IS NOT NULL; -- returns TRUE SELECT NULL IS NULL; -- returns TRUE SELECT NULL IS NOT NULL; -- returns FALSE Basically, for any a that evaluates to NULL , a IS NULL returns TRUE , while a IS NOT NULL returns FALSE . For any other value of a , the opposite is true. WITH \u00b6 Statement for defining variables pointing to temporary data, that can be used in the related SELECT statement. Usage: WITH <var> AS (<sql that assigns data to var>) <sql that use the variable> Note that the variable has to appear in the FROM clause ! Multiple variables in the WITH statement ahould be delimited by a comma: WITH <var> AS (<sql that assigns data to var>), <var 2> AS (<sql that assigns data to var 2>) <sql that use the variables> SELECT \u00b6 Most common SQL statement, syntax: SELECT <EXPRESSION> [AS <ALIAS>][, <EXPRESSION 2> [AS <ALIAS 2> ...]] The most common expression is just a column name. Selecting row number \u00b6 We can select row numbers using the function ROW_SELECT() in the SELECT statement: SELECT ... ROW_NUMBER() OVER([<PARTITIONING AND NUMBERING ORDER>]) AS <RESULT COLUMN NAME>, ... Inside the OVER statement, we can specify the order of the row numbering. Note however, that this does not order the result, for that, we use the ORDER BY statement. If we want the rown numbering to correspond with the row order in the result, we can left the OVER statement empty. Select unique rows \u00b6 To select unique rows, we can use the DISTINCT keyword: SELECT DISTINCT <column name> FROM ... Count selecting rows \u00b6 The count() function can be used to count the selection. The standard syntax is: SELECT count(1) FROM ... Count distinct \u00b6 To count distinct values in a selection we can use: SELECT count(DISTINCT <column name>) FROM... Select from another column if the specified column is NULL \u00b6 We can use a replacement column using the coalesce function: SELECT coalesce (<primary column>, <secondary column>) The secondary column value will be used if the primary column value in the row is NULL . UNION \u00b6 UNION and UNION ALL are important keywords that enables to merge query results verticaly, i.e., appending rows of one query to the results set of another. The difference between them is that UNION discards duplicate rows, while UNION ALL keeps them The UNUION statement appends one SELECT statement to another, but some statements that appears to be part of the SELECT needs to stay outside (i.e., be specified just once for the whole union), namely ORDER BY , and LIMIT . In contrast, the GROUP BY and HAVING statement stays inside each individual select. Select hardcoded values \u00b6 For selecting hardcoded values, we can use the VALUES statement. The syntax is: VALUES (<column 1>, <column 2>, ...), (<column 1>, <column 2>, ...), ... Full example of selecting hardcoded values: SELECT * FROM (VALUES (1, 'a'), (2, 'b')) AS t (id, label) JOIN \u00b6 Classical syntax: JOIN <table name> [[AS] <ALIAS>] ON <CONDITION> The alias is obligatory if there are duplicate names (e.g., we are joining one table twice) Types \u00b6 INNER (default): when there is nothing to join, the row is discarded [LEFT/RIGHT/FULL] OUTER : when there is nothing to join, the missing values are set to null CROSS : creates cartesian product between tables OUTER JOIN \u00b6 The OUTER JOIN has three subtypes LEFT : joins right table to left, fills the missing rows on right with null RIGHT : joins left to right, fills the missing rows on left with null FULL : both LEFT and RIGHT JOIN is performend Properties \u00b6 When there are multiple matching rows, all of them are matched (i.e., it creates duplicit rows) If you want to filter the tables before join, you need to specify the condition inside ON caluse Join Only One Row \u00b6 Joining a specific row \u00b6 Sometimes, we want to join only one row from many fulfilling some condition. One way to do that is to use a subquery: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY column_in_b DESC LIMIT 1 ) This query joins table b to table a , using the row with the highest column_in_b . Note however, that all rows from a will be joined to the same row from b . To use a different row from b depending on a , we need to look outside the subquery to filter out b according to a . The folowing query, which should do exactly that, is invalid : SELECT * FROM a JOIN ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 ) The problem here is that the subquery cannot refer the tables outside in, in the preceeeding FROM clause. Luckily, in the most use db systems, there is a magical keyword LATERAL that enables exacly that: SELECT * FROM a JOIN LATERAL ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 ) Join random row \u00b6 To join a random row, we can use the RANDOM function in ordering, e.g.: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY RANDOM() LIMIT 1 ) However, this subquery is evaluated just once, hence we have the same problem as with the first example in this section: to every row in a , we are joining the same random row from b . To force the db system to execute the subquery for each row in a , we need to refer a in the subquery, even with some useless filter (and of course we need a LATERAL join for that): SELECT * FROM a JOIN LATERAL( SELECT * FROM b WHERE a.column_in_a IS NOT NULL ORDER BY RANDOM() LIMIT 1 ) Joining any row find active nodes \u00b6 Sometimes, we need to join any matching row from B to find a set of active (referenced) rows in A. For example, we need to find a set of custommers with pending orders. IF the average number of matches in B is low, we proceed with normal join and then aggregate the results or use DISTINCT . However, sometimes, the average number of matching rows in B can be very high (e.g., finding all countries with at least one MC Donald). For those cases, the LATERAL join is again the solution: SELECT * FROM zones JOIN LATERAL ( SELECT id AS request_id FROM demand WHERE zones.id = demand.destination LIMIT 1 ) demand ON TRUE For more information about the trade offs of this solution, check the SO answer Getting All Combinations of Rows \u00b6 This can be done using the CROSS JOIN , e.g.: SELECT * FROM table_a CROSS JOIN table_b The proble arises when you want to filter one of the tables before joining, because CROSS JOIN does not support the ON clause (see more in the Oracle docs ). Then you can use the equivalent INNER JOIN : SELECT * FROM table_a INNER JOIN table_b ON true Here you replace the true value with your intended condition. Inverse JOIN \u00b6 Sometimes, it is useful to find all rows in table A that has no match in table B. The usual approach is to use LEFT JOIN and filter out non null rows after the join: SELECT ... FROM tableA LEFT JOIN tableB WHERE tableB.id is NULL Joining a table on multiple options \u00b6 There are situations, where we want to join a single table on multiple possible matches. For example, we want to match all people having birthday or name day the same day as some promotion is happanning. The foollowing query is a straighrorward solution: SELECT * FROM promo JOIN people ON promo.date = people.birthaday OR promo.date = people.name_day However, as we explain in the performance chapter, using OR in SQL is almost never a good solution. The usual way of gatting rid of OR is to use IN : SELECT * FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day) Nevertheless, this query can still lead to problems, despite being more compact. When we use any complex condition while joining tables, we risk poor performance. In general, it is better to use just simple column-to-column matches, even when there are more joins as a result. If you have performance problems, consult the \"Replacing OR \" section in the \"Performance Optimization\" chapter. GROUP BY \u00b6 wiki GROUP BY is used to aggregate data. Usual syntax: GROUP BY <LIST OF COLUMNS> Note that we need to use some aggreagte function for all columns in the SELECT clause that are not present in the GROUP BY list. On the other hand, we can use columns not present in the SELECT in the GROUP BY statement. Using aggregate functions for the whole result set while using GROUP BY \u00b6 If a query contains a GROUP BY statement, all aggregate functions (e.g., count , avg ) are applied to groups, i.e., for each row of the result set. If we need to apply an aggregate function to the whole result set while using GROUP BY , we need to specify it using the OVER statement: SELECT count(1) OVER () as result_count ... This will add a coulumn with a total count to each row of the result. If we do not need the actual groups, but only the distinct count, we can use a LIMIT statement. Get any row from each group \u00b6 Sometimes, we need a value from a non-grouped column, but we do not care which one. The reson can be, for example, that we no that the values are the same for all rows in the group. There is no dedicated aggregation for this case, but we can use some simple ones as MIN or MAX . Window functions \u00b6 Sometimes, we would need an aggregate function that somehow use two different columns (e.g., value of col A for the row where col B is largest). For that, we cannot use the classical aggregation, but we rather have to use a window function . A window function is a function that for each row returns a value computed from one or multiple rows. Syntactically, we recognize a window function by the OVER clause that determines the rows used as an input for the function. Functions with the same name can exist as aggregate and window functions. Window functions are evaluated after the GROUP BY clause and aggregate functions. Specifiing the range \u00b6 () : the whole result set (PARTITION BY <column set>) : the rows with the same values for that set of columns We can also order the result for the selected range using ORDER BY inside the parantheses. In some SQL dialects (e.g., PostgreSQL), there are even more sophisticated ways how to specify the range for the window functions. ORDER BY \u00b6 By default, there is no guarantee that the result will be in any particular order. To sort the result, we need to add an ORDER BY statement at the end of the query: ORDER BY <LIST OF COLUMNS> INSERT \u00b6 Standard syntax for the INSERT statement is INSERT INTO <table> (<col_1>, <col_2>,...) VALUES (<val_1>, <val_2>,...) If we fill all columns and we are confident with the column ordering, we can omit columns: INSERT INTO <table> VALUES (<val_1>, <val_2>,...) Sometimes, we need to handle the eroro cases, e.g., the case when the record already exists. The solutions for these cases are, however, database specific. INSERT SELECT \u00b6 If we want to duplicate the records, we can use: INSERT INTO <table> SELECT * FROM <table> [WHERE <condition>] If we need to fill some columns ourselfs: INSERT INTO <table> (<col_1>, <col_2>,...) SELECT <col_1>, <any expression> FROM <anything suported by select> [WHERE <condition>] UPDATE \u00b6 UPDATE query has the following structure: UPDATE table_name SET column1 = value1, column2 = value2...., columnN = valueN WHERE <condition> Unfortunately, the statement is ready to update N records with one set of values, but not to update N records with N set of values. To do that, we have only an option to select from another table: UPDATE table_name SET column1 = other_table.column1 FROM other_table WHERE other_table.id = table_name.id Don't forget the WHERE clause here, otherwise, you are matching the whole result set returned by the FROM clause to each row of the table. Use the table for updating itself \u00b6 If we are abou to update the table using date stored in int, we need to use aliases: UPDATE nodes_ways new SET way_id = ways.osm_id FROM nodes_ways old JOIN ways ON old.way_id = ways.id AND old.area = ways.area WHERE new.way_id = old.way_id AND new.area = old.area AND new.position = old.position DELETE \u00b6 To delete records from a table, we use the DELETE statement: DELETE FROM <table_name> WHERE <condition> If some data from another table are required for the selection of the records to be deleted, the syntax varies depending on the database engine. EXPLAIN \u00b6 Sources official documentation official cosumentation: usage https://docs.gitlab.com/ee/development/understanding_explain_plans.html Remarks: \u00b6 to show the actual run times, we need to run EXPLAIN ANALYZE Nodes \u00b6 Sources Plan nodes source code PG documentation with nodes described Node example: -> Parallel Seq Scan on records_mon (cost=0.00..4053077.22 rows=2074111 width=6) (actual time=398.243..74465.389 rows=7221902 loops=2) Filter: ((trip_length_0_1_mi = '0'::double precision) AND (trip_length_1_2_mi = '0'::double precision) AND (trip_length_2_3_mi = '0'::double precision) AND (trip_length_3_4_mi = '0'::double precision) AND (trip_length_4_5_mi = '0'::double precision)) Rows Removed by Filter: 8639817 Buffers: shared hit=157989 read=3805864 Description: In first parantheses, there are expected values: cost the estimated cost in arbitrary units. The format is startup cost..total cost , where startup cost is a flat cost of the node, an init cost, while total cost is the estimated cost of the node. Averege per loop. rows : expected number of rows produced by this node. Averege per loop. width the width of each row in bytes In the second parantheses, there are measured results: actual time : The measured execution time in miliseconds. The format is startup time..total time . rows The real number of rows returned. The Rows Removed by the Filter indicates the number of rows that were filtered out. The Buffers statistic shows the number of buffers used. Each buffer consists of 8 KB of data. Keys \u00b6 Keys serves as a way of identifying table rows, they are unique . There are many type of keys, see databastar article for the terminology overview. Primary key \u00b6 Most important keys in ORM are primary keys. Each table should have a single primary key. A primary key has to be non null. When choosing primary key, we can either use a uniqu combination of database colums: a natural key use and extra column: surogate key If we use a natural key and it is composed from multiple columns, we call it a composite key The following table summarize the adventages and disadvantages of each of the solutions: Area | Property | Natural key | Composite key | Surrogate key | |-|-|-|-|-| | usage | SQL joins | easy | hard | easy | || changing natural key columns | hard | hard | easy | | Performance | extra space | none | A lot if there are reference tables, otherwise none | one extra column | || space for indexes | normal | extra | normal || extra insertion time | no | A lot if there are reference tables, otherwise none | yes | || join performance | suboptimal due to sparse values | even more sub-optimal due to sparse values | optimal From the table above, we can see that using natural keys should be considered only if rows can be identified by a single column and we have a strong confidence in that natural id, specifically in its uniquness and timelessnes. Non-primary (UNIQUE) keys \u00b6 Sometimes, we need to enforse a uniqueness of a set of columns that does not compose a primary key (e.g., we use a surogate key). We can use a non primary key for that. One of the differences between primary and non-primary keys is that non-primary keys can be null, and each of the null values is considered unique. Indices \u00b6 Indices are essential for speeding queries containing conditions (including conditional joins). The basic syntax for creating an index is: CREATE INDEX <index name> ON <table name>(<column name>); Show Table Indices \u00b6 MySQL: SHOW INDEX FROM <tablename>; PostgreSQL SELECT * FROM pg_indexes WHERE tablename = '<tablename >'; Show Indices From All Tables \u00b6 MySQL: SELECT DISTINCT TABLE_NAME, INDEX_NAME FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = '<schemaname>'; CREATE TABLE \u00b6 The syntax is: CREATE TABLE <TABLE NAME> ( <COLUNM 1 NAME> <COLUNM 1 TYPE>, ... <COLUNM N NAME> <COLUNM N TYPE> [, PRIMARY KEY (<LIST OF KEY COLUMNS>)] ) The primary key is optional, but usually, any table should have one. Each row has to have a unique primary key. An index is created automatically for the list of primary key columns. ALTER TABLE \u00b6 Add column generated from other columns \u00b6 SQL: ALTER TABLE <table name> ADD <column name> AS (<generator expression>); In MySQL we have to add the data type: ALTER TABLE <tablename> ADD <columnname> <datatype> AS (<generator expression>); In PostgreSQL, the syntax is quite different: ALTER TABLE <tablename> ADD <columnname> <datatype> GENERATED ALWAYS AS (<generator expression>) STORED The advantage of the Postgres approach is that the column is set as generated, therefore it is generated for the new rows automatically. Procedures and functions \u00b6 In SQL there are two constructs that are able to encapsulate SQL statements to increase resusability and readability: functions and procedues. The main difference between these two is that functions are intended to be called inline from SLQ statements, while procedures cannot be used in SQL statements and instead, they are used as a wrapper of a set of SQL statement to be called repeatedly with different arguments. The extensive summary of the different capabilities of functions and procedures is on SO . Calling a procedure \u00b6 The keyword for calling a procedure differs between database system, refer to the documentation for your system for the right keyword. Creating a procedure \u00b6 The syntax for calling a procedure differs between database system, refer to the documentation for your system for the right syntax. Parameters \u00b6 Procedures and functions can have parameters similar to parameters in programming languages. We can use those parameters in the body of a function/procedure equally as a column or constant. Parameters can have default values , suplied after the = sign. We can test whether a default argument was supplied by testing the parameter for the default value. Views \u00b6 Views are basically named SQL queries stored in database. The queries are run on each view invocation, unles the view is materialized. The syntax is: CREATE VIEW <VIEW NAME> AS <QUERY> Modifying the view \u00b6 The view can be modified with CREATE OR REPLACE VIEW , however, existing columns cannot be changed . If you need to change existing columns, drop the view first. Schemas \u00b6 Schema in SQL is a container or namespace for tables, views, and other database objects. This means that we can have multiple objects with the same name in different schemas. The SQL schema should not be confused with the database schema, which is a logical structure of the database. Sadly, the concept of SQL schema is not standardized across different database systems. The following table shows how SQL schema is implemented in different systems: Database System Schema concept default schema MySQL Databases are used as schemas - PostgreSQL Multiple schemas in a single database as per the SQL standard public Oracle Each user has an associated schema (user = schema) user name SQL Server Multiple schemas in a single database as per the SQL standard dbo Performace Optimization \u00b6 When the query is slow, first inspect the following checklist: Do not use OR or IN for a set of columns (see replacing OR below). Check that all column and combination of columns used in conditions ( WHERE ) are indexed. Check that all foreign keys are indexed. Check that all joins are simple joins (column to column, or set of columns to a matching set of columns). If nothing from the above works, try to start with a simple query and add more complex pars to find where the problem is. If decomposing the query also does not bing light into the problem, refer to either one of the subsections below, or to the external sources. Also, note that some IDEs limits the number of returned rows automatically, which can hide serious problems and confuse you. Try to remove the limit when testing the performance this way. Replacing OR \u00b6 We can slow down the query significantly using OR or IN statements if the set of available options is not constant (e.g., IN(1, 2) is okish, while IN(origin, destination) can have drastic performance impact). To get rid of these disjunctioncs, we can use the UNION statement, basically duplicating the query. The resulting query will be double in size, but much faster: SELECT people.id FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day); -- can be revritten as SELECT * FROM promo JOIN people ON promo.date = people.birthaday UNION SELECT * FROM promo JOIN people ON promo.date = people.name_day GROUP BY people.id A specific join makes the query slow \u00b6 If a single join makes the query slow, there is a great chance that the index is not used for the join. Even if the table has an index on the referenced column(s), the join can still not use it if we are joining not to the table itself but to: a subquery, a variable created with a WITH statement, a view, or temborary table created from the indexed table. You can solve the situation by creating a materialized view or temporary table instead, and adding inedices to the table manualy. Specifically, you need to split the query into multiple queries: delete the materialized view/table if exists create the materialized view/table create the required indices perform the actual query that utilizes th view/table Of course we can skip the first three steps if the materialized view is constant for all queries, which is common during the testing phase. Slow DELETE \u00b6 When the delete is slow, the cause can be the missing index on the child table that refers to the table we are deleting from. Other possible causes are listed in the SO answer . Sources \u00b6 Database development mistakes made by application developers","title":"SQL Manual"},{"location":"Programming/Database/SQL%20Manual/#operators","text":"Comparison operators: = : equal <> or != : not equal > : greater than < : less than >= : greater or equal <= : less or equal String operators: || : concatenation LIKE : pattern matching","title":"Operators"},{"location":"Programming/Database/SQL%20Manual/#literals","text":"In SQL, there are three basic types of literals: numeric : 1 , 1.2 , 1.2e3 string : 'string' , \"string\" boolean : true , false When we need a constant value for other types, we usually use either a constructor function or a specifically formatted string literal. String literals can also span multiple lines , we do not need any operator to split the line (unlike in Python and despite the JetBrains IDEs adds a string concatenation operator automatically on newline).","title":"Literals"},{"location":"Programming/Database/SQL%20Manual/#null-in-sql","text":"wiki In SQL NULL is a marker for missing values. When using NULL as an operand, it propagates, meaning that the operation returns NULL: SELECT 1 + NULL; -- returns NULL SELECT 'The value is ' || NULL; -- returns NULL This may be counterintuitive, especially in the case of comparison operators: SELECT 1 = NULL; -- returns NULL SELECT 1 != NULL; -- returns NULL SELECT NULL = NULL; -- returns NULL The only exception are logical operators: SELECT TRUE OR NULL; -- returns TRUE SELECT FALSE AND NULL; -- returns FALSE To solve the problem with comparison operators, SQL has special operators IS NULL and IS NOT NULL : SELECT 1 IS NULL; -- returns FALSE SELECT 1 IS NOT NULL; -- returns TRUE SELECT NULL IS NULL; -- returns TRUE SELECT NULL IS NOT NULL; -- returns FALSE Basically, for any a that evaluates to NULL , a IS NULL returns TRUE , while a IS NOT NULL returns FALSE . For any other value of a , the opposite is true.","title":"NULL in SQL"},{"location":"Programming/Database/SQL%20Manual/#with","text":"Statement for defining variables pointing to temporary data, that can be used in the related SELECT statement. Usage: WITH <var> AS (<sql that assigns data to var>) <sql that use the variable> Note that the variable has to appear in the FROM clause ! Multiple variables in the WITH statement ahould be delimited by a comma: WITH <var> AS (<sql that assigns data to var>), <var 2> AS (<sql that assigns data to var 2>) <sql that use the variables>","title":"WITH"},{"location":"Programming/Database/SQL%20Manual/#select","text":"Most common SQL statement, syntax: SELECT <EXPRESSION> [AS <ALIAS>][, <EXPRESSION 2> [AS <ALIAS 2> ...]] The most common expression is just a column name.","title":"SELECT"},{"location":"Programming/Database/SQL%20Manual/#selecting-row-number","text":"We can select row numbers using the function ROW_SELECT() in the SELECT statement: SELECT ... ROW_NUMBER() OVER([<PARTITIONING AND NUMBERING ORDER>]) AS <RESULT COLUMN NAME>, ... Inside the OVER statement, we can specify the order of the row numbering. Note however, that this does not order the result, for that, we use the ORDER BY statement. If we want the rown numbering to correspond with the row order in the result, we can left the OVER statement empty.","title":"Selecting row number"},{"location":"Programming/Database/SQL%20Manual/#select-unique-rows","text":"To select unique rows, we can use the DISTINCT keyword: SELECT DISTINCT <column name> FROM ...","title":"Select unique rows"},{"location":"Programming/Database/SQL%20Manual/#count-selecting-rows","text":"The count() function can be used to count the selection. The standard syntax is: SELECT count(1) FROM ...","title":"Count selecting rows"},{"location":"Programming/Database/SQL%20Manual/#count-distinct","text":"To count distinct values in a selection we can use: SELECT count(DISTINCT <column name>) FROM...","title":"Count distinct"},{"location":"Programming/Database/SQL%20Manual/#select-from-another-column-if-the-specified-column-is-null","text":"We can use a replacement column using the coalesce function: SELECT coalesce (<primary column>, <secondary column>) The secondary column value will be used if the primary column value in the row is NULL .","title":"Select from another column if the specified column is NULL"},{"location":"Programming/Database/SQL%20Manual/#union","text":"UNION and UNION ALL are important keywords that enables to merge query results verticaly, i.e., appending rows of one query to the results set of another. The difference between them is that UNION discards duplicate rows, while UNION ALL keeps them The UNUION statement appends one SELECT statement to another, but some statements that appears to be part of the SELECT needs to stay outside (i.e., be specified just once for the whole union), namely ORDER BY , and LIMIT . In contrast, the GROUP BY and HAVING statement stays inside each individual select.","title":"UNION"},{"location":"Programming/Database/SQL%20Manual/#select-hardcoded-values","text":"For selecting hardcoded values, we can use the VALUES statement. The syntax is: VALUES (<column 1>, <column 2>, ...), (<column 1>, <column 2>, ...), ... Full example of selecting hardcoded values: SELECT * FROM (VALUES (1, 'a'), (2, 'b')) AS t (id, label)","title":"Select hardcoded values"},{"location":"Programming/Database/SQL%20Manual/#join","text":"Classical syntax: JOIN <table name> [[AS] <ALIAS>] ON <CONDITION> The alias is obligatory if there are duplicate names (e.g., we are joining one table twice)","title":"JOIN"},{"location":"Programming/Database/SQL%20Manual/#types","text":"INNER (default): when there is nothing to join, the row is discarded [LEFT/RIGHT/FULL] OUTER : when there is nothing to join, the missing values are set to null CROSS : creates cartesian product between tables","title":"Types"},{"location":"Programming/Database/SQL%20Manual/#outer-join","text":"The OUTER JOIN has three subtypes LEFT : joins right table to left, fills the missing rows on right with null RIGHT : joins left to right, fills the missing rows on left with null FULL : both LEFT and RIGHT JOIN is performend","title":"OUTER JOIN"},{"location":"Programming/Database/SQL%20Manual/#properties","text":"When there are multiple matching rows, all of them are matched (i.e., it creates duplicit rows) If you want to filter the tables before join, you need to specify the condition inside ON caluse","title":"Properties"},{"location":"Programming/Database/SQL%20Manual/#join-only-one-row","text":"","title":"Join Only One Row"},{"location":"Programming/Database/SQL%20Manual/#joining-a-specific-row","text":"Sometimes, we want to join only one row from many fulfilling some condition. One way to do that is to use a subquery: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY column_in_b DESC LIMIT 1 ) This query joins table b to table a , using the row with the highest column_in_b . Note however, that all rows from a will be joined to the same row from b . To use a different row from b depending on a , we need to look outside the subquery to filter out b according to a . The folowing query, which should do exactly that, is invalid : SELECT * FROM a JOIN ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 ) The problem here is that the subquery cannot refer the tables outside in, in the preceeeding FROM clause. Luckily, in the most use db systems, there is a magical keyword LATERAL that enables exacly that: SELECT * FROM a JOIN LATERAL ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 )","title":"Joining a specific row"},{"location":"Programming/Database/SQL%20Manual/#join-random-row","text":"To join a random row, we can use the RANDOM function in ordering, e.g.: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY RANDOM() LIMIT 1 ) However, this subquery is evaluated just once, hence we have the same problem as with the first example in this section: to every row in a , we are joining the same random row from b . To force the db system to execute the subquery for each row in a , we need to refer a in the subquery, even with some useless filter (and of course we need a LATERAL join for that): SELECT * FROM a JOIN LATERAL( SELECT * FROM b WHERE a.column_in_a IS NOT NULL ORDER BY RANDOM() LIMIT 1 )","title":"Join random row"},{"location":"Programming/Database/SQL%20Manual/#joining-any-row-find-active-nodes","text":"Sometimes, we need to join any matching row from B to find a set of active (referenced) rows in A. For example, we need to find a set of custommers with pending orders. IF the average number of matches in B is low, we proceed with normal join and then aggregate the results or use DISTINCT . However, sometimes, the average number of matching rows in B can be very high (e.g., finding all countries with at least one MC Donald). For those cases, the LATERAL join is again the solution: SELECT * FROM zones JOIN LATERAL ( SELECT id AS request_id FROM demand WHERE zones.id = demand.destination LIMIT 1 ) demand ON TRUE For more information about the trade offs of this solution, check the SO answer","title":"Joining any row find active nodes"},{"location":"Programming/Database/SQL%20Manual/#getting-all-combinations-of-rows","text":"This can be done using the CROSS JOIN , e.g.: SELECT * FROM table_a CROSS JOIN table_b The proble arises when you want to filter one of the tables before joining, because CROSS JOIN does not support the ON clause (see more in the Oracle docs ). Then you can use the equivalent INNER JOIN : SELECT * FROM table_a INNER JOIN table_b ON true Here you replace the true value with your intended condition.","title":"Getting All Combinations of Rows"},{"location":"Programming/Database/SQL%20Manual/#inverse-join","text":"Sometimes, it is useful to find all rows in table A that has no match in table B. The usual approach is to use LEFT JOIN and filter out non null rows after the join: SELECT ... FROM tableA LEFT JOIN tableB WHERE tableB.id is NULL","title":"Inverse JOIN"},{"location":"Programming/Database/SQL%20Manual/#joining-a-table-on-multiple-options","text":"There are situations, where we want to join a single table on multiple possible matches. For example, we want to match all people having birthday or name day the same day as some promotion is happanning. The foollowing query is a straighrorward solution: SELECT * FROM promo JOIN people ON promo.date = people.birthaday OR promo.date = people.name_day However, as we explain in the performance chapter, using OR in SQL is almost never a good solution. The usual way of gatting rid of OR is to use IN : SELECT * FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day) Nevertheless, this query can still lead to problems, despite being more compact. When we use any complex condition while joining tables, we risk poor performance. In general, it is better to use just simple column-to-column matches, even when there are more joins as a result. If you have performance problems, consult the \"Replacing OR \" section in the \"Performance Optimization\" chapter.","title":"Joining a table on multiple options"},{"location":"Programming/Database/SQL%20Manual/#group-by","text":"wiki GROUP BY is used to aggregate data. Usual syntax: GROUP BY <LIST OF COLUMNS> Note that we need to use some aggreagte function for all columns in the SELECT clause that are not present in the GROUP BY list. On the other hand, we can use columns not present in the SELECT in the GROUP BY statement.","title":"GROUP BY"},{"location":"Programming/Database/SQL%20Manual/#using-aggregate-functions-for-the-whole-result-set-while-using-group-by","text":"If a query contains a GROUP BY statement, all aggregate functions (e.g., count , avg ) are applied to groups, i.e., for each row of the result set. If we need to apply an aggregate function to the whole result set while using GROUP BY , we need to specify it using the OVER statement: SELECT count(1) OVER () as result_count ... This will add a coulumn with a total count to each row of the result. If we do not need the actual groups, but only the distinct count, we can use a LIMIT statement.","title":"Using aggregate functions for the whole result set while using GROUP BY"},{"location":"Programming/Database/SQL%20Manual/#get-any-row-from-each-group","text":"Sometimes, we need a value from a non-grouped column, but we do not care which one. The reson can be, for example, that we no that the values are the same for all rows in the group. There is no dedicated aggregation for this case, but we can use some simple ones as MIN or MAX .","title":"Get any row from each group"},{"location":"Programming/Database/SQL%20Manual/#window-functions","text":"Sometimes, we would need an aggregate function that somehow use two different columns (e.g., value of col A for the row where col B is largest). For that, we cannot use the classical aggregation, but we rather have to use a window function . A window function is a function that for each row returns a value computed from one or multiple rows. Syntactically, we recognize a window function by the OVER clause that determines the rows used as an input for the function. Functions with the same name can exist as aggregate and window functions. Window functions are evaluated after the GROUP BY clause and aggregate functions.","title":"Window functions"},{"location":"Programming/Database/SQL%20Manual/#specifiing-the-range","text":"() : the whole result set (PARTITION BY <column set>) : the rows with the same values for that set of columns We can also order the result for the selected range using ORDER BY inside the parantheses. In some SQL dialects (e.g., PostgreSQL), there are even more sophisticated ways how to specify the range for the window functions.","title":"Specifiing the range"},{"location":"Programming/Database/SQL%20Manual/#order-by","text":"By default, there is no guarantee that the result will be in any particular order. To sort the result, we need to add an ORDER BY statement at the end of the query: ORDER BY <LIST OF COLUMNS>","title":"ORDER BY"},{"location":"Programming/Database/SQL%20Manual/#insert","text":"Standard syntax for the INSERT statement is INSERT INTO <table> (<col_1>, <col_2>,...) VALUES (<val_1>, <val_2>,...) If we fill all columns and we are confident with the column ordering, we can omit columns: INSERT INTO <table> VALUES (<val_1>, <val_2>,...) Sometimes, we need to handle the eroro cases, e.g., the case when the record already exists. The solutions for these cases are, however, database specific.","title":"INSERT"},{"location":"Programming/Database/SQL%20Manual/#insert-select","text":"If we want to duplicate the records, we can use: INSERT INTO <table> SELECT * FROM <table> [WHERE <condition>] If we need to fill some columns ourselfs: INSERT INTO <table> (<col_1>, <col_2>,...) SELECT <col_1>, <any expression> FROM <anything suported by select> [WHERE <condition>]","title":"INSERT SELECT"},{"location":"Programming/Database/SQL%20Manual/#update","text":"UPDATE query has the following structure: UPDATE table_name SET column1 = value1, column2 = value2...., columnN = valueN WHERE <condition> Unfortunately, the statement is ready to update N records with one set of values, but not to update N records with N set of values. To do that, we have only an option to select from another table: UPDATE table_name SET column1 = other_table.column1 FROM other_table WHERE other_table.id = table_name.id Don't forget the WHERE clause here, otherwise, you are matching the whole result set returned by the FROM clause to each row of the table.","title":"UPDATE"},{"location":"Programming/Database/SQL%20Manual/#use-the-table-for-updating-itself","text":"If we are abou to update the table using date stored in int, we need to use aliases: UPDATE nodes_ways new SET way_id = ways.osm_id FROM nodes_ways old JOIN ways ON old.way_id = ways.id AND old.area = ways.area WHERE new.way_id = old.way_id AND new.area = old.area AND new.position = old.position","title":"Use the table for updating itself"},{"location":"Programming/Database/SQL%20Manual/#delete","text":"To delete records from a table, we use the DELETE statement: DELETE FROM <table_name> WHERE <condition> If some data from another table are required for the selection of the records to be deleted, the syntax varies depending on the database engine.","title":"DELETE"},{"location":"Programming/Database/SQL%20Manual/#explain","text":"Sources official documentation official cosumentation: usage https://docs.gitlab.com/ee/development/understanding_explain_plans.html","title":"EXPLAIN"},{"location":"Programming/Database/SQL%20Manual/#remarks","text":"to show the actual run times, we need to run EXPLAIN ANALYZE","title":"Remarks:"},{"location":"Programming/Database/SQL%20Manual/#nodes","text":"Sources Plan nodes source code PG documentation with nodes described Node example: -> Parallel Seq Scan on records_mon (cost=0.00..4053077.22 rows=2074111 width=6) (actual time=398.243..74465.389 rows=7221902 loops=2) Filter: ((trip_length_0_1_mi = '0'::double precision) AND (trip_length_1_2_mi = '0'::double precision) AND (trip_length_2_3_mi = '0'::double precision) AND (trip_length_3_4_mi = '0'::double precision) AND (trip_length_4_5_mi = '0'::double precision)) Rows Removed by Filter: 8639817 Buffers: shared hit=157989 read=3805864 Description: In first parantheses, there are expected values: cost the estimated cost in arbitrary units. The format is startup cost..total cost , where startup cost is a flat cost of the node, an init cost, while total cost is the estimated cost of the node. Averege per loop. rows : expected number of rows produced by this node. Averege per loop. width the width of each row in bytes In the second parantheses, there are measured results: actual time : The measured execution time in miliseconds. The format is startup time..total time . rows The real number of rows returned. The Rows Removed by the Filter indicates the number of rows that were filtered out. The Buffers statistic shows the number of buffers used. Each buffer consists of 8 KB of data.","title":"Nodes"},{"location":"Programming/Database/SQL%20Manual/#keys","text":"Keys serves as a way of identifying table rows, they are unique . There are many type of keys, see databastar article for the terminology overview.","title":"Keys"},{"location":"Programming/Database/SQL%20Manual/#primary-key","text":"Most important keys in ORM are primary keys. Each table should have a single primary key. A primary key has to be non null. When choosing primary key, we can either use a uniqu combination of database colums: a natural key use and extra column: surogate key If we use a natural key and it is composed from multiple columns, we call it a composite key The following table summarize the adventages and disadvantages of each of the solutions: Area | Property | Natural key | Composite key | Surrogate key | |-|-|-|-|-| | usage | SQL joins | easy | hard | easy | || changing natural key columns | hard | hard | easy | | Performance | extra space | none | A lot if there are reference tables, otherwise none | one extra column | || space for indexes | normal | extra | normal || extra insertion time | no | A lot if there are reference tables, otherwise none | yes | || join performance | suboptimal due to sparse values | even more sub-optimal due to sparse values | optimal From the table above, we can see that using natural keys should be considered only if rows can be identified by a single column and we have a strong confidence in that natural id, specifically in its uniquness and timelessnes.","title":"Primary key"},{"location":"Programming/Database/SQL%20Manual/#non-primary-unique-keys","text":"Sometimes, we need to enforse a uniqueness of a set of columns that does not compose a primary key (e.g., we use a surogate key). We can use a non primary key for that. One of the differences between primary and non-primary keys is that non-primary keys can be null, and each of the null values is considered unique.","title":"Non-primary (UNIQUE) keys"},{"location":"Programming/Database/SQL%20Manual/#indices","text":"Indices are essential for speeding queries containing conditions (including conditional joins). The basic syntax for creating an index is: CREATE INDEX <index name> ON <table name>(<column name>);","title":"Indices"},{"location":"Programming/Database/SQL%20Manual/#show-table-indices","text":"MySQL: SHOW INDEX FROM <tablename>; PostgreSQL SELECT * FROM pg_indexes WHERE tablename = '<tablename >';","title":"Show Table Indices"},{"location":"Programming/Database/SQL%20Manual/#show-indices-from-all-tables","text":"MySQL: SELECT DISTINCT TABLE_NAME, INDEX_NAME FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = '<schemaname>';","title":"Show Indices From All Tables"},{"location":"Programming/Database/SQL%20Manual/#create-table","text":"The syntax is: CREATE TABLE <TABLE NAME> ( <COLUNM 1 NAME> <COLUNM 1 TYPE>, ... <COLUNM N NAME> <COLUNM N TYPE> [, PRIMARY KEY (<LIST OF KEY COLUMNS>)] ) The primary key is optional, but usually, any table should have one. Each row has to have a unique primary key. An index is created automatically for the list of primary key columns.","title":"CREATE TABLE"},{"location":"Programming/Database/SQL%20Manual/#alter-table","text":"","title":"ALTER TABLE"},{"location":"Programming/Database/SQL%20Manual/#add-column-generated-from-other-columns","text":"SQL: ALTER TABLE <table name> ADD <column name> AS (<generator expression>); In MySQL we have to add the data type: ALTER TABLE <tablename> ADD <columnname> <datatype> AS (<generator expression>); In PostgreSQL, the syntax is quite different: ALTER TABLE <tablename> ADD <columnname> <datatype> GENERATED ALWAYS AS (<generator expression>) STORED The advantage of the Postgres approach is that the column is set as generated, therefore it is generated for the new rows automatically.","title":"Add column generated from other columns"},{"location":"Programming/Database/SQL%20Manual/#procedures-and-functions","text":"In SQL there are two constructs that are able to encapsulate SQL statements to increase resusability and readability: functions and procedues. The main difference between these two is that functions are intended to be called inline from SLQ statements, while procedures cannot be used in SQL statements and instead, they are used as a wrapper of a set of SQL statement to be called repeatedly with different arguments. The extensive summary of the different capabilities of functions and procedures is on SO .","title":"Procedures and functions"},{"location":"Programming/Database/SQL%20Manual/#calling-a-procedure","text":"The keyword for calling a procedure differs between database system, refer to the documentation for your system for the right keyword.","title":"Calling a procedure"},{"location":"Programming/Database/SQL%20Manual/#creating-a-procedure","text":"The syntax for calling a procedure differs between database system, refer to the documentation for your system for the right syntax.","title":"Creating a procedure"},{"location":"Programming/Database/SQL%20Manual/#parameters","text":"Procedures and functions can have parameters similar to parameters in programming languages. We can use those parameters in the body of a function/procedure equally as a column or constant. Parameters can have default values , suplied after the = sign. We can test whether a default argument was supplied by testing the parameter for the default value.","title":"Parameters"},{"location":"Programming/Database/SQL%20Manual/#views","text":"Views are basically named SQL queries stored in database. The queries are run on each view invocation, unles the view is materialized. The syntax is: CREATE VIEW <VIEW NAME> AS <QUERY>","title":"Views"},{"location":"Programming/Database/SQL%20Manual/#modifying-the-view","text":"The view can be modified with CREATE OR REPLACE VIEW , however, existing columns cannot be changed . If you need to change existing columns, drop the view first.","title":"Modifying the view"},{"location":"Programming/Database/SQL%20Manual/#schemas","text":"Schema in SQL is a container or namespace for tables, views, and other database objects. This means that we can have multiple objects with the same name in different schemas. The SQL schema should not be confused with the database schema, which is a logical structure of the database. Sadly, the concept of SQL schema is not standardized across different database systems. The following table shows how SQL schema is implemented in different systems: Database System Schema concept default schema MySQL Databases are used as schemas - PostgreSQL Multiple schemas in a single database as per the SQL standard public Oracle Each user has an associated schema (user = schema) user name SQL Server Multiple schemas in a single database as per the SQL standard dbo","title":"Schemas"},{"location":"Programming/Database/SQL%20Manual/#performace-optimization","text":"When the query is slow, first inspect the following checklist: Do not use OR or IN for a set of columns (see replacing OR below). Check that all column and combination of columns used in conditions ( WHERE ) are indexed. Check that all foreign keys are indexed. Check that all joins are simple joins (column to column, or set of columns to a matching set of columns). If nothing from the above works, try to start with a simple query and add more complex pars to find where the problem is. If decomposing the query also does not bing light into the problem, refer to either one of the subsections below, or to the external sources. Also, note that some IDEs limits the number of returned rows automatically, which can hide serious problems and confuse you. Try to remove the limit when testing the performance this way.","title":"Performace Optimization"},{"location":"Programming/Database/SQL%20Manual/#replacing-or","text":"We can slow down the query significantly using OR or IN statements if the set of available options is not constant (e.g., IN(1, 2) is okish, while IN(origin, destination) can have drastic performance impact). To get rid of these disjunctioncs, we can use the UNION statement, basically duplicating the query. The resulting query will be double in size, but much faster: SELECT people.id FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day); -- can be revritten as SELECT * FROM promo JOIN people ON promo.date = people.birthaday UNION SELECT * FROM promo JOIN people ON promo.date = people.name_day GROUP BY people.id","title":"Replacing OR"},{"location":"Programming/Database/SQL%20Manual/#a-specific-join-makes-the-query-slow","text":"If a single join makes the query slow, there is a great chance that the index is not used for the join. Even if the table has an index on the referenced column(s), the join can still not use it if we are joining not to the table itself but to: a subquery, a variable created with a WITH statement, a view, or temborary table created from the indexed table. You can solve the situation by creating a materialized view or temporary table instead, and adding inedices to the table manualy. Specifically, you need to split the query into multiple queries: delete the materialized view/table if exists create the materialized view/table create the required indices perform the actual query that utilizes th view/table Of course we can skip the first three steps if the materialized view is constant for all queries, which is common during the testing phase.","title":"A specific join makes the query slow"},{"location":"Programming/Database/SQL%20Manual/#slow-delete","text":"When the delete is slow, the cause can be the missing index on the child table that refers to the table we are deleting from. Other possible causes are listed in the SO answer .","title":"Slow DELETE"},{"location":"Programming/Database/SQL%20Manual/#sources","text":"Database development mistakes made by application developers","title":"Sources"},{"location":"Programming/Java/Debugging%20Java/","text":"General rules \u00b6 Warnings can help you to spot the problems. Check that they are enabled ( Xlint ). Sometimes, warnings may help to understand the problem. However, they are not emmited due to compilation error. Try to comment out the errorneous code and compile the code to see all warnings. Missing Resources \u00b6 First, check if the resources are where you expected in the jar or in the target folder. The structure is described here on SO . If the resources are not there, try to rebuild the project.","title":"Debugging Java"},{"location":"Programming/Java/Debugging%20Java/#general-rules","text":"Warnings can help you to spot the problems. Check that they are enabled ( Xlint ). Sometimes, warnings may help to understand the problem. However, they are not emmited due to compilation error. Try to comment out the errorneous code and compile the code to see all warnings.","title":"General rules"},{"location":"Programming/Java/Debugging%20Java/#missing-resources","text":"First, check if the resources are where you expected in the jar or in the target folder. The structure is described here on SO . If the resources are not there, try to rebuild the project.","title":"Missing Resources"},{"location":"Programming/Java/Jackson/","text":"Documentation Jackson is a (de)serialization library primarily focused to the JSON format. It supports annotations for automatic serialization and deserialization of Java objects. Jackson have two basic modes: Object mapper mode , JSON is (de)serialized from/to Java objects Parser/Generator mode , JSON is parsed/generated one token at a time To make it even more complicated, the Object mapper mode can be used in two ways: Data binding , where the JSON is (de)serialized from/to Java objects that we provide for this purpose Tree model , where the JSON is (de)serialized from/to a tree structure of Jackson objects Tutorials: Official databind tutorial Jenkov tutorial (old but gold) Object mapper with data binding \u00b6 The basic usage is: public class MyClass{ public String par1; public int par2; ... } ObjectMapper mapper = new ObjectMapper(); // Deserialization MyClass myClass = mapper.readValue(new File(\"path/to/file.json\"), MyClass.class); // Serialization MyClass myObject = new MyClass(); ... mapper.writeValue(new File(\"path/to/file.json\"), myObject); By defualt, Jackson: (de)serialize from/to public fields or public setters/getters with the same name as the JSON property all JSON properties are deserialized and all public fields or public getters are serialized if there is no matching field or setter to a JSON property, the deserialization fails However, we can customize the (de)serialization process using many annotations. This way, we can ignore some fields or JSON properties, use constructors or factory methods for deserialization, (de)serialize from/to arrays and objects, and many other things. Ignore specific fields \u00b6 If you want to ignore specific fields during serialization or deserialization, you can use the @JsonIgnoreProperties annotation . Use it as a class or type annotation. Example: @JsonIgnoreProperties({ \"par1\", \"par2\" }) public class ConfigModel { // \"par1\" and \"par2\" will be ignored } This annotation can also prevent the \"Unrecognized field\" error during deserialization, as the ignored fields does not have to be present as Java class members. we can achieve the same by field annotations, specifically the @JsonIgnore annotation. Example: public class ConfigModel { @JsonIgnore public String par1; @JsonIgnore public int par2; } Include only specific fields \u00b6 If the fields to be (de)serialized are only a minority of all fields, we can use a reverse approach: the @JsonIncludeProperties annotation. For example: @JsonIncludeProperties({ \"par1\", \"par2\" }) public class ConfigModel { // only \"par1\" and \"par2\" will be (de)serialized } Note that unlike for ignoring, there is no field annotation for including only specific fields. The @JsonInclude annotation serves a different purpose. Represent a class by a single member \u00b6 If we want the java class to be represented by a single value in the serialized file, we can achieve that by adding the @JsonValue annotation above the member or method that should represent the class. Note, however, that this only works for simple values, because the member serializers are not called, the members is serialized as a simple value instead. If you want to represent a class by a single but complex member, use a custom serializer instead . An equivalent annotation for deserialization is the @JsonCreator annotation which should be placed above a constructor or factory method. Deserialization \u00b6 The standard usage is: ObjectMapper mapper = new ObjectMapper(); // ... configure mapper File file = new File(<PATH TO FILE>); Instance instance = mapper.readValue(file, Instance.class); By default, new objects are created for all members in the object hierarchy that are either present in the serialized file. New objects are created using the setter, if exists, otherwise, the costructor is called. Multiple Setters \u00b6 If there are multiple setters, we need to specify the one that should be used for deserialization by marking it with the @JsonSetter annotation. Update existing instance \u00b6 You can update an existing instance using the readerForUpdating method: ObjectReader or = mapper.readerForUpdating(instance); // special reader or.readValue(file) // we can use the method we already know on the object reader Note that by default, the update is shalow . Only the instance object itself is updated, but its members are brand new objects. If you want to keep all objects from an existing object hierarchy, you need to use the @JsonMerge annotation. You should put this annotation above any member of the root object you want to update instead of replacing it. The @JsonUpgrade annotation is recursive : the members of the member annotated with @JsonUpgrade are updated as well and so on. Updating polymorphic types \u00b6 For updating polymorpic type, the rule is that the exact type has to match. Also, you need jackson-databind version 2.14.0 or greater. Read just part of the file \u00b6 For reading just part of the file, use the at selector taht is available in the ObjectReader class. We need to first obtain the reader from a mapper, and then use the selector: ObjectReader reader = mapper.readerFor(Instance.class); Instance instance = reader.at(\"data\").readValue(file) Note that if the path parameter of the at method is incorrect, the method throws an exception with the message: \"no content to map due to end-of-input\". Check that some node is present \u00b6 To check for presence of a node, we should use the JsonPointer class: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); We can also use the JsonPointer in the at method: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); if(found){ Instance instance = reader.at(jsonPointer).readValue(file, Instance.class) } Interface or abstract class \u00b6 When serializing interface or abstract class, it is important to include the implementation type into serialization. Otherwise, the deserialization fails, because it cannot determine the concreate type. To serialize the concrete type, we can use the @JsonRypeInfo and JsonSubTypes annotations: @JsonTypeInfo( use = JsonTypeInfo.Id.CLASS, // what value should we store, here the class name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) public interface Interface { ... } Te above code will work, the full class name will be serialized in the file, however. If we want to use a shorter syntax, i.e., some codename for the class, we need to specify a mapping between this codename and the conreate class: @JsonTypeInfo( use = JsonTypeInfo.Id.NAME, // what value should we store, here a custom name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) @JsonSubTypes({ @JsonSubTypes.Type(value = Implementation.class, name = \"Implementation name\") }) public interface Interface { ... } Custom deserializer \u00b6 An alternative to @JsonTypeInfo is to use a custom deserializer: https://stackoverflow.com/questions/44122782/jackson-deserialize-based-on-type Custom deserializer \u00b6 If our architecture is so complex or specific that none of the Jackson annotations can help us to achieve the desired behavior, we can use a custom deserializer. For that we need to: Implement a custom deserializer by extending the JsonDeserializer class Registering the deserializer in the ObjectMapper Creating a custom deserializer for class \u00b6 The only method we need to implement is the: T deserialize(JsonParser jsonParser, DeserializationContext deserializationContext) where T is the type of the object we are deserializing. To get the Jaksons representation of the JSON tree, we can call: JsonNode node = jsonParser.getCodec().readTree(jsonParser); We can get all the fields by calling using the node.fields() method. For arrays, there is a method node.elements() ; Registering the deserializer \u00b6 ObjectMapper mapper = new ObjectMapper(); SimpleModule module = new SimpleModule(); module.addDeserializer(OurClass.class, new OurClassDeserializer()); mapper.registerModule(module); Custom deserializer with generic types \u00b6 When we need a custom deserializer for a generic class, we need to use a wildcard to cover multiple values of generic argument: public class OurDeserializer extends JsonDeserializer<OurGenericClass<?>>{ ... } If we also need to get the generic argument type from JSON, we need to implement the ContextualDeserializer interface. This is discribed in a SO answer . Custom deserializer with inheritance \u00b6 We can have a common deserializer for both parent and child class, or multiple child classes. However, it is necessary to make the deserializer generic and register the deserializer for all classes, not just for the parent. Example: public class PolymorphicDeserializer<T extends Parent> extends JsonDeserializer<T> @Override public T deserialize(JsonParser p, DeserializationContext ctxt) throws IOException, JsonProcessingException { ... } } module.addDeserializer(ChildA.class, new PolymorphicDeserializer<>()); module.addDeserializer(ChildB.class, new ProcedureParameterDeserializer<>()); Serialization \u00b6 Standard serialization: // compressed mapper.writer().writeValue(file, object); // or with indents, etc. mapper.writerWithDefaultPrettyPrinter().writeValue(file, object); By default, new objects are created for all members in the object hierarchy that are either: public value mebers (fields) public getters: public methods with name get<NAME> , where <NAME> is a name of some value member Other getters with different name are called only if there is an annotation above them. A special annotation dedicated for this is @JsonProperty . Appending to an existing file \u00b6 To append to an existing file, we need to create an output stream in the append mode and then use it in jackson: ObjectMapper mapper = ... JsonGenerator generator = mapper.getFactory().createGenerator(new FileOutputStream(new File(<FILENAME>), true)); mapper.writeValue(generator, output); Complex member filters \u00b6 Insted of adding annotations to each member we want to ignore, we can also apply some more compex filters, to do that, we need to: add a @JsonFilter(\"<FILTER NAME>\") annotation to all classes for which we want to use the filter create the filter pass a set of all filters we want to use to the writer we are using for serialization The example below keeps only members inherited from MyClass : // object on which we apply the filter @JsonFilter(\"myFilter\") class targetClass{ ... } // filter PropertyFilter filter = new SimpleBeanPropertyFilter() { @Override public void serializeAsField( Object pojo, JsonGenerator jgen, SerializerProvider provider, PropertyWriter writer ) throws Exception { if(writer.getType().isTypeOrSubTypeOf(MyClass.class)){ writer.serializeAsField(pojo, jgen, provider); } } }; FilterProvider filters = new SimpleFilterProvider().addFilter(\"myFilter\", filter); // use a writer created with filters mapper.writer(filters).writeValue(generator, output); ... Flatting the hierarchy \u00b6 When we desire to simplify the object hierarchy, we can use the @JsonUnwrapped annotation above a member of a class. With this annotation, the annotated member object will be skipped while all its members will be serialized into its parent. Custom serializer \u00b6 If the serialization requirements are too complex to be expressed using Jackson annotations, we can use a custom serialzier: public class MyCustomSerializer extends JsonSerializer<MyClass> { @Override public void serialize(MyClass myClass, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException { ... } } Analogously to custom deserializer, we can register custom serializer either in the object mapper: SimpleModule module = new SimpleModule(); module.addSerializer(MyClass.class, new MyCustomSerializer()); mapper.registerModule(module); or by annotating the class: @JsonSerialize(using = MyCustomSerializer.class) public class MyClass{ ... } You can call standard serializers from custom serializers using the SerializerProvider and JsonGenerator instances supplied as a parameters of the serialize method. For example the standard serialized value of som inner/member object class can be obtained using: serializerProvider.defaultSerializeValue(myInnerInstance, jsonGenerator); Annotations \u00b6 Multiple objects \u00b6 If there are multiple objects involved in the (de)serializetion, we can use the @JsonCreator and @JsonProperty annotations to split the work: @JsonCreator public ConfigModel( @JsonProperty(\"first\") ClassA instanceA, @JsonProperty(\"second\") ClassB instanceB ) { ... } in the above examples, the first and second are keys mapping the objects in the serialized file from which instanceA and instanceB should be created. Default Serializers and Deserializers \u00b6 By default, Jackson can serialize and deserialize many types of objects: all primitive types all primitive wrappers String BigInteger BigDecimal and many others Java 8 Date and Time (de)serialization \u00b6 Among the types that are not covered in basic Jakson package are the Java 8 types for date and time. To (de)serialize them, we have two options: use the jackson-datatype-jsr310 module that adds support for the ISO-8601 format for date and time use custom serializers and deserializers if we need a different format To use the jackson-datatype-jsr310 module, we need to add it to the dependencies: <dependency> <groupId>com.fasterxml.jackson.datatype</groupId> <artifactId>jackson-datatype-jsr310</artifactId> <version>2.12.0</version> </dependency> and then register the module in the object mapper: ObjectMapper mapper = new ObjectMapper(); mapper.registerModule(new JavaTimeModule()); Object mapper with tree model \u00b6 Jenkov tutorial on JsonNode To get the tree model, we can use the readTree method of the ObjectMapper class: ObjectMapper mapper = new ObjectMapper(); JsonNode node = mapper.readTree(new File(\"path/to/file.json\")); Iterating over JSON array \u00b6 First we need to convert the node representing the array to the ArrayNode class. Then we can iterate over the array using for each or the get method: ArrayNode array = (ArrayNode) node.get(\"array\"); for(JsonNode element: array){ ... } // or for(int i = 0; i < array.size(); i++){ JsonNode element = array.get(i); ... } Iterating over JSON object \u00b6 First we need to convert the node representing the object to the ObjectNode class. Then we can iterate over the object using the fieldNames method: ObjectNode object = (ObjectNode) node.get(\"object\"); for(String fieldName: object.fieldNames()){ JsonNode value = object.get(fieldName); ... } CSV with Jackson \u00b6 documentation To read a CSV file with Jackson, we need to use the CsvMapper class. The basic usage is: CsvMapper mapper = new CsvMapper(); // iterator construction MappingIterator<<row type>> it = mapper....readValues(new File(\"path/to/file.csv\"); // iterator usage while(it.hasNext()){ <row type> row = it.next(); ... } Here the <row type is the target type that should represent a single row. Depending on it, there will be different code in place of mapper... . For example, to get a list of strings for each row, we can use: MappingIterator<List<String>> it = mapper.readerForListOf(String.class)... Use the header \u00b6 If the CSV file has a header, we can use it to load each row into a map of key-value pairs: CsvSchema schema = CsvSchema.emptySchema().withHeader(); MappingIterator<Map<String, String>> it = mapper.readerForMapOf(String.class).with(schema).readValues(new File(\"path/to/file.csv\"));","title":"Jackson"},{"location":"Programming/Java/Jackson/#object-mapper-with-data-binding","text":"The basic usage is: public class MyClass{ public String par1; public int par2; ... } ObjectMapper mapper = new ObjectMapper(); // Deserialization MyClass myClass = mapper.readValue(new File(\"path/to/file.json\"), MyClass.class); // Serialization MyClass myObject = new MyClass(); ... mapper.writeValue(new File(\"path/to/file.json\"), myObject); By defualt, Jackson: (de)serialize from/to public fields or public setters/getters with the same name as the JSON property all JSON properties are deserialized and all public fields or public getters are serialized if there is no matching field or setter to a JSON property, the deserialization fails However, we can customize the (de)serialization process using many annotations. This way, we can ignore some fields or JSON properties, use constructors or factory methods for deserialization, (de)serialize from/to arrays and objects, and many other things.","title":"Object mapper with data binding"},{"location":"Programming/Java/Jackson/#ignore-specific-fields","text":"If you want to ignore specific fields during serialization or deserialization, you can use the @JsonIgnoreProperties annotation . Use it as a class or type annotation. Example: @JsonIgnoreProperties({ \"par1\", \"par2\" }) public class ConfigModel { // \"par1\" and \"par2\" will be ignored } This annotation can also prevent the \"Unrecognized field\" error during deserialization, as the ignored fields does not have to be present as Java class members. we can achieve the same by field annotations, specifically the @JsonIgnore annotation. Example: public class ConfigModel { @JsonIgnore public String par1; @JsonIgnore public int par2; }","title":"Ignore specific fields"},{"location":"Programming/Java/Jackson/#include-only-specific-fields","text":"If the fields to be (de)serialized are only a minority of all fields, we can use a reverse approach: the @JsonIncludeProperties annotation. For example: @JsonIncludeProperties({ \"par1\", \"par2\" }) public class ConfigModel { // only \"par1\" and \"par2\" will be (de)serialized } Note that unlike for ignoring, there is no field annotation for including only specific fields. The @JsonInclude annotation serves a different purpose.","title":"Include only specific fields"},{"location":"Programming/Java/Jackson/#represent-a-class-by-a-single-member","text":"If we want the java class to be represented by a single value in the serialized file, we can achieve that by adding the @JsonValue annotation above the member or method that should represent the class. Note, however, that this only works for simple values, because the member serializers are not called, the members is serialized as a simple value instead. If you want to represent a class by a single but complex member, use a custom serializer instead . An equivalent annotation for deserialization is the @JsonCreator annotation which should be placed above a constructor or factory method.","title":"Represent a class by a single member"},{"location":"Programming/Java/Jackson/#deserialization","text":"The standard usage is: ObjectMapper mapper = new ObjectMapper(); // ... configure mapper File file = new File(<PATH TO FILE>); Instance instance = mapper.readValue(file, Instance.class); By default, new objects are created for all members in the object hierarchy that are either present in the serialized file. New objects are created using the setter, if exists, otherwise, the costructor is called.","title":"Deserialization"},{"location":"Programming/Java/Jackson/#multiple-setters","text":"If there are multiple setters, we need to specify the one that should be used for deserialization by marking it with the @JsonSetter annotation.","title":"Multiple Setters"},{"location":"Programming/Java/Jackson/#update-existing-instance","text":"You can update an existing instance using the readerForUpdating method: ObjectReader or = mapper.readerForUpdating(instance); // special reader or.readValue(file) // we can use the method we already know on the object reader Note that by default, the update is shalow . Only the instance object itself is updated, but its members are brand new objects. If you want to keep all objects from an existing object hierarchy, you need to use the @JsonMerge annotation. You should put this annotation above any member of the root object you want to update instead of replacing it. The @JsonUpgrade annotation is recursive : the members of the member annotated with @JsonUpgrade are updated as well and so on.","title":"Update existing instance"},{"location":"Programming/Java/Jackson/#updating-polymorphic-types","text":"For updating polymorpic type, the rule is that the exact type has to match. Also, you need jackson-databind version 2.14.0 or greater.","title":"Updating polymorphic types"},{"location":"Programming/Java/Jackson/#read-just-part-of-the-file","text":"For reading just part of the file, use the at selector taht is available in the ObjectReader class. We need to first obtain the reader from a mapper, and then use the selector: ObjectReader reader = mapper.readerFor(Instance.class); Instance instance = reader.at(\"data\").readValue(file) Note that if the path parameter of the at method is incorrect, the method throws an exception with the message: \"no content to map due to end-of-input\".","title":"Read just part of the file"},{"location":"Programming/Java/Jackson/#check-that-some-node-is-present","text":"To check for presence of a node, we should use the JsonPointer class: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); We can also use the JsonPointer in the at method: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); if(found){ Instance instance = reader.at(jsonPointer).readValue(file, Instance.class) }","title":"Check that some node is present"},{"location":"Programming/Java/Jackson/#interface-or-abstract-class","text":"When serializing interface or abstract class, it is important to include the implementation type into serialization. Otherwise, the deserialization fails, because it cannot determine the concreate type. To serialize the concrete type, we can use the @JsonRypeInfo and JsonSubTypes annotations: @JsonTypeInfo( use = JsonTypeInfo.Id.CLASS, // what value should we store, here the class name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) public interface Interface { ... } Te above code will work, the full class name will be serialized in the file, however. If we want to use a shorter syntax, i.e., some codename for the class, we need to specify a mapping between this codename and the conreate class: @JsonTypeInfo( use = JsonTypeInfo.Id.NAME, // what value should we store, here a custom name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) @JsonSubTypes({ @JsonSubTypes.Type(value = Implementation.class, name = \"Implementation name\") }) public interface Interface { ... }","title":"Interface or abstract class"},{"location":"Programming/Java/Jackson/#custom-deserializer","text":"An alternative to @JsonTypeInfo is to use a custom deserializer: https://stackoverflow.com/questions/44122782/jackson-deserialize-based-on-type","title":"Custom deserializer"},{"location":"Programming/Java/Jackson/#custom-deserializer_1","text":"If our architecture is so complex or specific that none of the Jackson annotations can help us to achieve the desired behavior, we can use a custom deserializer. For that we need to: Implement a custom deserializer by extending the JsonDeserializer class Registering the deserializer in the ObjectMapper","title":"Custom deserializer"},{"location":"Programming/Java/Jackson/#creating-a-custom-deserializer-for-class","text":"The only method we need to implement is the: T deserialize(JsonParser jsonParser, DeserializationContext deserializationContext) where T is the type of the object we are deserializing. To get the Jaksons representation of the JSON tree, we can call: JsonNode node = jsonParser.getCodec().readTree(jsonParser); We can get all the fields by calling using the node.fields() method. For arrays, there is a method node.elements() ;","title":"Creating a custom deserializer for class"},{"location":"Programming/Java/Jackson/#registering-the-deserializer","text":"ObjectMapper mapper = new ObjectMapper(); SimpleModule module = new SimpleModule(); module.addDeserializer(OurClass.class, new OurClassDeserializer()); mapper.registerModule(module);","title":"Registering the deserializer"},{"location":"Programming/Java/Jackson/#custom-deserializer-with-generic-types","text":"When we need a custom deserializer for a generic class, we need to use a wildcard to cover multiple values of generic argument: public class OurDeserializer extends JsonDeserializer<OurGenericClass<?>>{ ... } If we also need to get the generic argument type from JSON, we need to implement the ContextualDeserializer interface. This is discribed in a SO answer .","title":"Custom deserializer with generic types"},{"location":"Programming/Java/Jackson/#custom-deserializer-with-inheritance","text":"We can have a common deserializer for both parent and child class, or multiple child classes. However, it is necessary to make the deserializer generic and register the deserializer for all classes, not just for the parent. Example: public class PolymorphicDeserializer<T extends Parent> extends JsonDeserializer<T> @Override public T deserialize(JsonParser p, DeserializationContext ctxt) throws IOException, JsonProcessingException { ... } } module.addDeserializer(ChildA.class, new PolymorphicDeserializer<>()); module.addDeserializer(ChildB.class, new ProcedureParameterDeserializer<>());","title":"Custom deserializer with inheritance"},{"location":"Programming/Java/Jackson/#serialization","text":"Standard serialization: // compressed mapper.writer().writeValue(file, object); // or with indents, etc. mapper.writerWithDefaultPrettyPrinter().writeValue(file, object); By default, new objects are created for all members in the object hierarchy that are either: public value mebers (fields) public getters: public methods with name get<NAME> , where <NAME> is a name of some value member Other getters with different name are called only if there is an annotation above them. A special annotation dedicated for this is @JsonProperty .","title":"Serialization"},{"location":"Programming/Java/Jackson/#appending-to-an-existing-file","text":"To append to an existing file, we need to create an output stream in the append mode and then use it in jackson: ObjectMapper mapper = ... JsonGenerator generator = mapper.getFactory().createGenerator(new FileOutputStream(new File(<FILENAME>), true)); mapper.writeValue(generator, output);","title":"Appending to an existing file"},{"location":"Programming/Java/Jackson/#complex-member-filters","text":"Insted of adding annotations to each member we want to ignore, we can also apply some more compex filters, to do that, we need to: add a @JsonFilter(\"<FILTER NAME>\") annotation to all classes for which we want to use the filter create the filter pass a set of all filters we want to use to the writer we are using for serialization The example below keeps only members inherited from MyClass : // object on which we apply the filter @JsonFilter(\"myFilter\") class targetClass{ ... } // filter PropertyFilter filter = new SimpleBeanPropertyFilter() { @Override public void serializeAsField( Object pojo, JsonGenerator jgen, SerializerProvider provider, PropertyWriter writer ) throws Exception { if(writer.getType().isTypeOrSubTypeOf(MyClass.class)){ writer.serializeAsField(pojo, jgen, provider); } } }; FilterProvider filters = new SimpleFilterProvider().addFilter(\"myFilter\", filter); // use a writer created with filters mapper.writer(filters).writeValue(generator, output); ...","title":"Complex member filters"},{"location":"Programming/Java/Jackson/#flatting-the-hierarchy","text":"When we desire to simplify the object hierarchy, we can use the @JsonUnwrapped annotation above a member of a class. With this annotation, the annotated member object will be skipped while all its members will be serialized into its parent.","title":"Flatting the hierarchy"},{"location":"Programming/Java/Jackson/#custom-serializer","text":"If the serialization requirements are too complex to be expressed using Jackson annotations, we can use a custom serialzier: public class MyCustomSerializer extends JsonSerializer<MyClass> { @Override public void serialize(MyClass myClass, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException { ... } } Analogously to custom deserializer, we can register custom serializer either in the object mapper: SimpleModule module = new SimpleModule(); module.addSerializer(MyClass.class, new MyCustomSerializer()); mapper.registerModule(module); or by annotating the class: @JsonSerialize(using = MyCustomSerializer.class) public class MyClass{ ... } You can call standard serializers from custom serializers using the SerializerProvider and JsonGenerator instances supplied as a parameters of the serialize method. For example the standard serialized value of som inner/member object class can be obtained using: serializerProvider.defaultSerializeValue(myInnerInstance, jsonGenerator);","title":"Custom serializer"},{"location":"Programming/Java/Jackson/#annotations","text":"","title":"Annotations"},{"location":"Programming/Java/Jackson/#multiple-objects","text":"If there are multiple objects involved in the (de)serializetion, we can use the @JsonCreator and @JsonProperty annotations to split the work: @JsonCreator public ConfigModel( @JsonProperty(\"first\") ClassA instanceA, @JsonProperty(\"second\") ClassB instanceB ) { ... } in the above examples, the first and second are keys mapping the objects in the serialized file from which instanceA and instanceB should be created.","title":"Multiple objects"},{"location":"Programming/Java/Jackson/#default-serializers-and-deserializers","text":"By default, Jackson can serialize and deserialize many types of objects: all primitive types all primitive wrappers String BigInteger BigDecimal and many others","title":"Default Serializers and Deserializers"},{"location":"Programming/Java/Jackson/#java-8-date-and-time-deserialization","text":"Among the types that are not covered in basic Jakson package are the Java 8 types for date and time. To (de)serialize them, we have two options: use the jackson-datatype-jsr310 module that adds support for the ISO-8601 format for date and time use custom serializers and deserializers if we need a different format To use the jackson-datatype-jsr310 module, we need to add it to the dependencies: <dependency> <groupId>com.fasterxml.jackson.datatype</groupId> <artifactId>jackson-datatype-jsr310</artifactId> <version>2.12.0</version> </dependency> and then register the module in the object mapper: ObjectMapper mapper = new ObjectMapper(); mapper.registerModule(new JavaTimeModule());","title":"Java 8 Date and Time (de)serialization"},{"location":"Programming/Java/Jackson/#object-mapper-with-tree-model","text":"Jenkov tutorial on JsonNode To get the tree model, we can use the readTree method of the ObjectMapper class: ObjectMapper mapper = new ObjectMapper(); JsonNode node = mapper.readTree(new File(\"path/to/file.json\"));","title":"Object mapper with tree model"},{"location":"Programming/Java/Jackson/#iterating-over-json-array","text":"First we need to convert the node representing the array to the ArrayNode class. Then we can iterate over the array using for each or the get method: ArrayNode array = (ArrayNode) node.get(\"array\"); for(JsonNode element: array){ ... } // or for(int i = 0; i < array.size(); i++){ JsonNode element = array.get(i); ... }","title":"Iterating over JSON array"},{"location":"Programming/Java/Jackson/#iterating-over-json-object","text":"First we need to convert the node representing the object to the ObjectNode class. Then we can iterate over the object using the fieldNames method: ObjectNode object = (ObjectNode) node.get(\"object\"); for(String fieldName: object.fieldNames()){ JsonNode value = object.get(fieldName); ... }","title":"Iterating over JSON object"},{"location":"Programming/Java/Jackson/#csv-with-jackson","text":"documentation To read a CSV file with Jackson, we need to use the CsvMapper class. The basic usage is: CsvMapper mapper = new CsvMapper(); // iterator construction MappingIterator<<row type>> it = mapper....readValues(new File(\"path/to/file.csv\"); // iterator usage while(it.hasNext()){ <row type> row = it.next(); ... } Here the <row type is the target type that should represent a single row. Depending on it, there will be different code in place of mapper... . For example, to get a list of strings for each row, we can use: MappingIterator<List<String>> it = mapper.readerForListOf(String.class)...","title":"CSV with Jackson"},{"location":"Programming/Java/Jackson/#use-the-header","text":"If the CSV file has a header, we can use it to load each row into a map of key-value pairs: CsvSchema schema = CsvSchema.emptySchema().withHeader(); MappingIterator<Map<String, String>> it = mapper.readerForMapOf(String.class).with(schema).readValues(new File(\"path/to/file.csv\"));","title":"Use the header"},{"location":"Programming/Java/Java%20Manual/","text":"Primitive types \u00b6 Java has the following primitive types: Integer types : byte , short , int , long Floating point types : float , double Character type : char Boolean type : boolean Arrays \u00b6 Arrays in Java are objects, and they are created using the new keyword. The syntax is: int[] myArray = new int[10]; The array is then filled with zeros. The length of the array is fixed and cannot be changed. The array can be initialized using the following syntax: int[] myArray = {1, 2, 3, 4, 5}; For working with arrays, the static methods from [] java.util.Arrays ](https://docs.oracle.com/javase/8/docs/api/java/util/Arrays.html) class can be used. Some of the most important methods are: copyOf and copyOfRange for copying arrays fill for filling arrays with a specific value Classes \u00b6 In java, there are the following types of classes: Standard class : similar to classes in other languages. Record class : a simple aggregate of values. Record class \u00b6 official specification Record class is a simple aggregate class. It is designed to replace the standard class in cases where the class is used only to hold the data (POJO). It is defined using the record keyword like this: record MyRecord(int par1, String par2){ ... } The advantage over the standard class is that the record class automatically declare data members for all parameters of the record header, and it generates the following methods: equals hashCode toString getters for all parameters constructor that accepts all parameters Standard Types \u00b6 Strings \u00b6 Classical string literals has to be enclosed in quotes ( \"my string\" ). They cannot contain some characters (newline, \" ). The backslash ( \\ ) character is reserved for Java escape sequences and need to be ecaped as \\\\ . In addition, since Java 15, there are text blocks, enclosed in three quotes: \"\"\" My long \"text block\" \"\"\" The properties of text blocks: can be used as standard string literals can contain newlines, quotes, and some other symbols that have to be escaped in standard string literals. The new line after opening triple quoute is obligatory. Note that backslash ( \\ ) is still interpreted as Java escape character, so we need to escape it using \\\\ . There are no fstrings in Java, the best way for using variables in strings is to use the String.format method: String s = String.format(\"My string with %s and %d\", \"text\", 5); Collections \u00b6 Java has a rich set of collections. Still, many are missing, notably some tuple class. Also, the typical tuple unpacking known from Python or C++ is not possible in Java and we have to use a verbose way of 1) creating a temporary object for all values, 2) accessing the values using the getters. List \u00b6 List initialization in Java is complicated compared to other languages. we can initialize the list as: List a = List.of(1, 2, 3); However, this method returns an immutable list. If we need a mutable list, we have to pass the list to the ArrayList constructor: List a = new ArrayList<>(List.of(1, 2, 3)); but with such a complicated syntax, we can use an input array directly: List a = new ArrayList<>(Arrays.asList(1, 2, 3)); Set \u00b6 From java 9, sets can be simply initialized as: Set<int> nums = Set.of(1,2,3); Note that this method returns an immutable set. In the earlier versions of Java, the Collections.singleton method can be used. Sorting \u00b6 Some collections can be sorted using the sort member method. It is a stable sort, which use the natural order of the elements by default, but it can be customized using a comparator. The comparator interface expects two elements and should return a negative number if the first element is smaller, a positive number if the first element is greater, and zero if the elements are equal. Overloading \u00b6 When calling an oveladed method with a null argument, we receive a compilation error: reference to <METHOD> is ambiguous . A solution to this problem is to cast the null pointer to the desired type: public set(string s){ ... } public set(MyClass c){ .... } set(null) // doesn't work set((MyClass) null) // works set((String) null) // works Regular expressions \u00b6 First thing we need to know about regexes in java is that we need to escape all backslashes (refer to the Strings chapter), there are no raw strings in Java. We can work with regexes either by using specialized high level methods that accepts regex as string, or by using the tools from the java.util.regex package. Testing if string matches a regex \u00b6 We can do it simply by: String MyString = \"tests\"; String MyRegex = \"t\\\\p+\" boolean matches = myString.matches(myRegex); Exceptions \u00b6 The exception mechanism in Java is similar to other languages. The big difference is, however, that in Java, all exception not derived from the RuntimeException class are checked , which means that their handeling is enforced by the compiler. The following code does not compile, because the java.langException class is a checked exception class. private testMethod{ ... throw new Exception(); } Therefore, we need to handle the exception using the try cache block or add throws <EXCEPTION TYPE> to the method declaration. When deciding between try/catch and throws , the rule of thumb is to use the try/cache if we can handle the exception and throws if we want to leave it for the caller. The problem arises when the method we are in implements an interface that does not have the throws declaration we need. Then the only solution is to use try/cache with a cache that does not handle the exception, and indicate the problem to the caller differently, e.g, by returning a null pointer. Logging \u00b6 Java has a built-in logging mechanism in the java.util.logging package. Apart from that, there are many other logging libraries. In this section, we focus on the SLF4J library which is an abstraction. With SLF4J, we can switch between different logging libraries without changing the code. To use SLF4J, we need to add the following dependency to the pom.xml file: <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version><version></version> </dependency> Additionally, we need to add a backend of our choice. For example, we can use the logback library: <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-classic</artifactId> <version><version></version> </dependency> The SLF4J library detects the available backend automatically. Note that for some backends, the newest version may not be supported by the SLF4J library. In such cases, SLF4J does not detect the backend and reports fallback to the nop backend. For the list of supported backend versions, see the SLF4J manual . Genericity \u00b6 Oracle official tutorial for Java 8 Like many other languages, Java offers generic types with the classical syntax: class GenericClass<T>{ ... }; An object is then created as: GenericClass<OtherClass> gc = new GenericClass<OtherClass>(); Diamond operator \u00b6 At all places where the type can be inferred, we can use the dianmond operator ( <> ) without specifiyng a type: GenericClass<OtherClass> gc = new GenericClass<>(); Raw types \u00b6 specification For the backward compatibility of Java library classes that were converted to generic classes a concept of raw types was introduces. However, in addition, this concept brings a lot of very subtle compile errors, which are caused by a nonintentional creation of raw type instead of a specific type. GenericClass gc = new GenericClass(); // raw type The parameterized type can be assigned to the raw type, but when we do it the other way arround, the code emmits warning and is not runtime safe: GenericClass<OtherClass> generic = new GenericClass<>(); GenericClass raw = generic // OK GenericClass raw = new GenericClass(); GenericClass<OtherClass> generic = raw // unsafe Calling the generic methods on raw types has a simmilar consequences. But the worst property of raw types is that all generic non-static members of raw types that are not inherited and are also eraw types. Therefore, the genericity is lost even for the completely unrelated type parameters, even specific types are erased to Object . The consequence can be seen on the following example: class GenericClass<T>{ public List<String> getListofStrings() }; GenericClass raw = new GenericClass(); raw.getListofStrings(); // returns List<Object> !!! Generic method \u00b6 A generic method in Java is written as: public <<GENERIC PARAM NAME>> <RESULT TYPE> <METHOD NAME>(<PARAMS>){ ... } example: public static <T> ProcedureParameterValidationResult<T> error(String msg){ return new ProcedureParameterValidationResult<>(false, msg, null); } if the generic parameter cannot be infered from the method arguments, we can call the method with explicit generic argument like this: <<GENERIC PARAM VALUE>><METHOD NAME>(<ARGUMENTS>); example: ProcedureParameterValidationResult.<String>error(\"Value cannot be empty.\"); Retrieving the type of a generic argument \u00b6 Because the genericity in Java is resolved at runtime, it is not possible to use the generic parameter as a type. What does it mean? Whille it is possible to use Myclass.class it is not possible to use T.class . The same applies for the class methods. The usual solution is to pass the class to the generic method as a method parameter: void processGeneric(Class<T> classType){ ... } Default generic argument \u00b6 It is not possible to set the default value of a generic parameter. If we want to achive such behavior, we need to create a new class: class GeneriClass<T>{ ... } class StringClass extends GenericClass<String>{}; Wildcards \u00b6 The wildcard symbol ( ? ) represents a concrete but unspecified type. It can be bounded by superclass/interface ( ? exctends MyInterface ), or by sublcass ( ? super MyClass ). The typical use cases: single non-generic method accepting generic types of multiple generic parameter values: myMethod(Wrapper<? extends MyInterface> wrapper){ MyInterface mi = wrapper.get(); ... } method that can process generic class but does not need to work with the generic parameters at all: getLength(List<?> list){ return list.size(); } Difference between MyClass , MyClass<Object> , and MyClass<?> \u00b6 class MyClass<T>{ private T content; private set(T content){ this.content = content; } private List<String> getList; } // raw type processMyClass(MyClass myClass){...}; // My class of object types processMyClass(MyClass<Object> myClass){...}; // My class of any specific type processMyClass(MyClass<?> myClass){...}; MyClass (raw type) MyClass<Object> MyClass<?> type safe (Check types at compile time) no yes yes set can be called with any Object any Object null only getList() returns List<Object> List<String> List<String> processMyClass can be called with any List List<Object> only any List Enums \u00b6 Java enumerations are declared using the enum keyword: public enum Semaphore{ RED, ORAGNGE, GREEN } If we need to add some additional members to the enum, we need to end the list of enum items with a semicolon first: public enum Semaphore{ RED, ORAGNGE, GREEN; public void doSomething(){ ... } } Iterating over enum items \u00b6 we can iterate over enum items using the values method: for(Semaphore s: Semaphore.values()){ ... } The above code iterates over values of a specific enum. If we need an iteration over any enum (generic), we need to use a class method: Class<E> enumClass; for(E item: enumClass.getEnumConstants()){ } Converting a string to enum \u00b6 We can convert a String to enum using the valueOf method that is generated for each enum class> Semaphore sem = Semaphore.valueOf(\"RED\"); Note that the string has to match the enum value exactly, including the case. Extra whitespaces are not permited. There is also a generic static valueOf method in the Enum class, which we can use for generic enums: Class<E> enumClass; E genEnumInst = Enum.valueOf(enumClass, \"RED\"); Note that here E has to be a properly typed enum ( Enum<E> ), not the raw enum ( Enum ). File System \u00b6 The modern class for working with paths is the Path class from the java.nio.file package. Other important class is the Files class, which contains static methods for working with files and directories. Important methods: exists isDirectory isRegularFile methods for iterating over files (see next section) Iterating over files in a directory \u00b6 We can iterate the files in a directory using the Files class: Files.list : flat list of files Files.walk : recursive list of files Both methods return a Stream<Path> object. Creating a directory \u00b6 The directory can be created from any Path object using the Files.createDirectory method. We can also create a directory from a File object using the mkdir and mkdirs methods. Date and Time \u00b6 Baeldung tutorial The following classes are intended for working with date and time in Java: LocalDate : date without time LocalTime : time without date LocalDateTime : date and time The LocalDateTime also has its timezone/localization aware counterpart: ZonedDateTime : date and time with timezone Rounding \u00b6 There is no built-in method for rounding the date and time, we need to set the fields manually. For example, to round the time to the nearest hour: LocalDateTime ldt = LocalDateTime.now(); if(ldt.getMinute() >= 30){ ldt = ldt.plusHours(1); } ldt = ldt.withMinute(0).withSecond(0).withNano(0); Lambda expressions \u00b6 wiki An important thing about lambda expressions in Java is that we can only use them to create types satisfying some functional interface. This means that: They can be used only in a context where a functional interface is expected They need to be convertible to that interface. The example that demonstrate this behavior is bellow. Usually, we use some standard interface, but here we create a new one for clarity: interface OurFunctionalInterface(){ int operation(int a) } ... public void process(int num, OurFunctionalInterface op){ ... } With the above interface and method that uses it, we can call process(0, a -> a + 5) Which is an equivalent of writing OurFunctionalInterface addFive = a -> a + 5; process(0, addFive); Syntax \u00b6 A full syntax for lambda expressions in Java is: (<PARAMS>) -> { <EXPRESSIONS> } If there is only one expression, we can ommit the code block: (<PARAMS>) -> <EXPRESSION> And also, we can ommit the return statement from that expression. The two lambda epressions below are equivalent: (a, b) -> {return a + b;} (a, b) -> a + b If there is only one parameter, we can ommit the parantheses: <PARAM> -> <EXPRESSION> By default, the parameter types are infered from the functional interface. If we need more specific parameters for our function, we can specify the parameter type, we have to specify all of them however: (a, b) -> a + b // valid (int a, int b) -> a + b // valid (int a, b) -> a + b // invalid Also, it is necesary to use the parantheses, if we use the param type. Method references \u00b6 We can also create lambda functions from existing mehods (if they satisfy the desired interface) using a mechanism called method reference . For example, we can use the Integer.sum(int a, int b) method in conext where the IntBinaryOperator interface is required. Instead of IntBinaryOperator op = (a, b) -> a + b; // new lambda body we can write: IntBinaryOperator op = Integer::sum; // lambda from existing function Exceptions in lambda expressions \u00b6 Beware that the checked exceptions thrown inside lambda expressions has to be caught inside the lambda expression. The following code does not compile: try{ process(0, a -> a.methodThatThrows()) catch(exception e){ ... } Instead, we have to write: process(0, a -> { try{ return a.methodThatThrows()); catch(exception e){ ... } } Iteration \u00b6 Java for each loop has the following syntax: for(<TYPE> <VARNAME>: <ITERABLE>){ ... } Here, the <ITERABLE> can be either a Java Iterable , or an array. Enumerated iteration \u00b6 There is no enumerate equivalent in Java. One can use a stream API range method , however, it is less readable than standard for loop because the code execuded in loop has to be in a separate function. Iterating using an iterator \u00b6 The easiest way how to iterate if we have a given iterator is to use its forEachRemaining method. It takes a Consumer object as an argument, iterates using the iterator, and calls Consumer.accept method on each iteration. The example below uses a lambda function that satisfies the consumer interface: class Our{ void process(){ ... } ... } Iterator<Our> objectsIterator = ...; objectsIterator.forEachRemaining(o -> o.process()); Functional Programming \u00b6 Turning array to stream \u00b6 Arrays.stream(<ARRAY>); Creating stream from iterator \u00b6 The easiest way is to first create an Iterable from the iterator and then use the StreamSupport.stream method: Iterable<JsonNode> iterable = () -> iterator; var stream = StreamSupport.stream(iterable.spliterator(), false); Filtering \u00b6 A lambda function can be supplied as a filter: stream.filter(number -> number > 5) returns a stream with numbers greater then five. Transformation \u00b6 We can transform the stream with the map function: transformed = stream.map(object -> doSomething(object)) Materialize stream \u00b6 We can materrialize stream with the collect metod: List<String> result = stringStream.collect(Collectors.toList()); Which can be, in case of List shorten to: List<String> result = stringStream.toList(); var \u00b6 openjdk document Using var is similar to auto in C++. Unlike in C++, it can be used only for local variables. The var can be tricky when using together with diamond operator or generic methods. The compilation works fine, however, the raw type will be created. Type cast \u00b6 Java use a tradidional casting syntax: (<TARGET TYPE>) <VAR> There are two different types of cast: value cast , which is used for value types (only primitive types in Java) and change the data reference cast , which is used for Java objects and does not change it, it just change the objects interface Reference cast \u00b6 The upcasting (casting to a supertype) is done implicitely by the compiler, so instead of Object o = (Object) new MyClass(); we can do Object o = new MyClass(); Typically, we need to use the explicit cast for downcasting : Object o = ...; MyClass c = (MyClass) o; // when we are sure that o is an instance of MyClass... Downcasting to an incorrect type leads to a runtime ClassCastException . Casting to an unrelated type is not allowed by the compiler. Casting generic types \u00b6 Casting with generic types is notoriously dangerous due to type erasure. The real types of generic arguments are not known at runtime, therefore, an incorrect cast does not raise an error: Object o = new Object(); String s = (String) o; // ClassCatsException List<Object> lo = new ArrayList<>(); List<String> ls = (List<String>) lo; // OK at runtime, therefore, it emmits warning at compile time. Casting case expression \u00b6 Often, when we work with an interface, we have to treat each implementation differently. The best way to handle that is to use the polymorphism, i.e., add a dedicated method for treating the object to the interface and handle the differences in each implementation. However, sometimes, we cannot modify the interface as it comes from the outside of our codebase, or we do not want to put the code to the interface because it belongs to a completely different part of the application (e.g., GUI vs database connection). A typicall solution for this is to use branching on instanceof and a consecutive cast: if(obj instance of Apple){ Apple apple = (Apple) obj; ... } else if(obj instance of Peach){ Peach peach = (Peach) obj; ... } ... Casting switch \u00b6 This approach works, it is safe, but it is also error prone. A new preview swich statement is ready to replace this technique: switch(obj) case Apple apple: ... break; case Peach peach: ... break; ... } Note that unsafe type opperations are not allowed in these new switch statements, and an error is emitted instead of warning: List list = ... List<String> ls = (List<String>) list; // warning: unchecked switch(list){ case List<String> ls: // error: cannot be safely cast ... } Random numbers \u00b6 Generate random integer \u00b6 To get an infinite iterator of random integers, we can use the Random.ints method: var it = new Random().ints(0, map.nodesFromAllGraphs.size()).iterator();\\ int a = it.nextInt(); Reflection \u00b6 Reflection is a toolset for working with Java types dynamically. Get class object \u00b6 To obtain the class object, we can use: the forName method of the Class class: Java Class<?> c = Class.forName(\"java.lang.String\"); This way, we can load classes dynamically at runtime. myString.getClass() if we have an instance or String.class if we know the class at compile time. from an object of the class (e.g.,. The method can be then iterated using the getDeclaredMethods method of the Class class. Test if a class is a superclass or interface of another class \u00b6 It can be tested simply as: ClassA.isAssignableFrom(ClassB) Get field or method by name \u00b6 Field field = myClass.getDeclaredField(\"fieldName\"); Method method = myClass.getMethod(\"methodName\", <PARAM TYPES>); Here, the <PARAM TYPES> is a list of classes that represent the types of the method parameters, e.g., String.class, MyClass.class for a method that accepts a string and an instance of MyClass . Call method using Method object \u00b6 method.invoke(myObject, <PARAMS>); SQL \u00b6 Java has a build in support for SQL in package java.sql . The typical operation: create a PreparedStatement from the cursor using the SQL string as an input Fill the parameters of the PreparedStatement Execute the query Filling the query parameters \u00b6 This process consist of safe replacement of ? marks in the SQL string with real values. The PreparedStatement class has dedicated methods for that: setString for strings setObject for complex objects Each method has the index of the ? to be replaced as the first parameter. The index start from 1 . Note that these methods can be used to supply arguments for data part of the SQL statement. If the ? is used for table names or SQL keywords, the query execution will fail. Therefore, if you need to dynamically set the non-data part of an SQL query (e.g., table name), you need to sanitize the argument manually and add it to the SQl querry. Processing the result \u00b6 The java sql framework returns the result of the SQL query in form of a ResultSet . To process the result set, we need to iterate the rows using the next method and for each row, we can access the column values using one of the methods (depending on the dtat type in the column), each of which accepts either column index or label as a parameter. Example: var result = statement.executeQuery(); while(result.next()){ var str = result.getString(\"column_name\")); ... } Jackson \u00b6 Described in the Jackson manual. Dependency injection with Guice \u00b6 Dependency injection is a design pattern that allows interaction between objects without wiring them manually. It works best in large applications with many independent components. When we want to use Singletons in our application, it is time to consider using a dependency injection. Guice is very convenient as it has many features that make using DI even easier. For example, any component can be used in DI by simply annotating the constructor with @Inject . Note that there can be only one constructor annotated with @Inject in a class. To mark a component that should be used as a singleton, we annotate the class with @Singleton annotation. Assisted injection \u00b6 If the class has a constructor with some parameters that are not provided by the Guice, we can use the assisted injection Assisted Injection is a mechanism to automatically generate factories for classes that have a constructor with mixed Guice and non-Guice parameters. Instead of a factory class, we provide only a factory interface: public interface MyFactory { MyClass create(String param1, int param2); } This factory can than be used to create the object: class MyClass{ @Inject MyClass( <parameters provided by Guice>, @Assisted String param1, @Assisted int param2 ){ ... } } The non-Guice parameters are assigned using the parameters of the factory's create method. The matching is determined by the parameter type. If the parameter type is not unique, among all @Assisted parameters, we have to provide the @Assisted annotation with the value parameter: public interface MyFactory { MyClass create(@Assisted(\"param1\") String param1, @Assisted(\"param2\") String param2); } class MyClass{ @Inject MyClass( <parameters provided by Guice>, @Assisted(\"param1\") String param1, @Assisted(\"param2\") String param2 ){ ... } } Progress bar \u00b6 For the progress bar, there is a good library called progressbar The usage is simple, just wrap any iterable, iterator, stream or similar object with the ProgressBar.wrap method: ArrayList<Integer> list = ... for(int i: ProgressBar.wrap(list, \"Processing\")){ ... } // or with a stream Stream<Integer> stream = ... progressStream = ProgressBar.wrap(stream, \"Processing\"); progressStream.forEach(i -> ...); // or with an iterator Iterator<Integer> iterator = ... progressIterator = ProgressBar.wrap(iterator, \"Processing\"); while(iterator.hasNext()){ ... }","title":"Java Manual"},{"location":"Programming/Java/Java%20Manual/#primitive-types","text":"Java has the following primitive types: Integer types : byte , short , int , long Floating point types : float , double Character type : char Boolean type : boolean","title":"Primitive types"},{"location":"Programming/Java/Java%20Manual/#arrays","text":"Arrays in Java are objects, and they are created using the new keyword. The syntax is: int[] myArray = new int[10]; The array is then filled with zeros. The length of the array is fixed and cannot be changed. The array can be initialized using the following syntax: int[] myArray = {1, 2, 3, 4, 5}; For working with arrays, the static methods from [] java.util.Arrays ](https://docs.oracle.com/javase/8/docs/api/java/util/Arrays.html) class can be used. Some of the most important methods are: copyOf and copyOfRange for copying arrays fill for filling arrays with a specific value","title":"Arrays"},{"location":"Programming/Java/Java%20Manual/#classes","text":"In java, there are the following types of classes: Standard class : similar to classes in other languages. Record class : a simple aggregate of values.","title":"Classes"},{"location":"Programming/Java/Java%20Manual/#record-class","text":"official specification Record class is a simple aggregate class. It is designed to replace the standard class in cases where the class is used only to hold the data (POJO). It is defined using the record keyword like this: record MyRecord(int par1, String par2){ ... } The advantage over the standard class is that the record class automatically declare data members for all parameters of the record header, and it generates the following methods: equals hashCode toString getters for all parameters constructor that accepts all parameters","title":"Record class"},{"location":"Programming/Java/Java%20Manual/#standard-types","text":"","title":"Standard Types"},{"location":"Programming/Java/Java%20Manual/#strings","text":"Classical string literals has to be enclosed in quotes ( \"my string\" ). They cannot contain some characters (newline, \" ). The backslash ( \\ ) character is reserved for Java escape sequences and need to be ecaped as \\\\ . In addition, since Java 15, there are text blocks, enclosed in three quotes: \"\"\" My long \"text block\" \"\"\" The properties of text blocks: can be used as standard string literals can contain newlines, quotes, and some other symbols that have to be escaped in standard string literals. The new line after opening triple quoute is obligatory. Note that backslash ( \\ ) is still interpreted as Java escape character, so we need to escape it using \\\\ . There are no fstrings in Java, the best way for using variables in strings is to use the String.format method: String s = String.format(\"My string with %s and %d\", \"text\", 5);","title":"Strings"},{"location":"Programming/Java/Java%20Manual/#collections","text":"Java has a rich set of collections. Still, many are missing, notably some tuple class. Also, the typical tuple unpacking known from Python or C++ is not possible in Java and we have to use a verbose way of 1) creating a temporary object for all values, 2) accessing the values using the getters.","title":"Collections"},{"location":"Programming/Java/Java%20Manual/#list","text":"List initialization in Java is complicated compared to other languages. we can initialize the list as: List a = List.of(1, 2, 3); However, this method returns an immutable list. If we need a mutable list, we have to pass the list to the ArrayList constructor: List a = new ArrayList<>(List.of(1, 2, 3)); but with such a complicated syntax, we can use an input array directly: List a = new ArrayList<>(Arrays.asList(1, 2, 3));","title":"List"},{"location":"Programming/Java/Java%20Manual/#set","text":"From java 9, sets can be simply initialized as: Set<int> nums = Set.of(1,2,3); Note that this method returns an immutable set. In the earlier versions of Java, the Collections.singleton method can be used.","title":"Set"},{"location":"Programming/Java/Java%20Manual/#sorting","text":"Some collections can be sorted using the sort member method. It is a stable sort, which use the natural order of the elements by default, but it can be customized using a comparator. The comparator interface expects two elements and should return a negative number if the first element is smaller, a positive number if the first element is greater, and zero if the elements are equal.","title":"Sorting"},{"location":"Programming/Java/Java%20Manual/#overloading","text":"When calling an oveladed method with a null argument, we receive a compilation error: reference to <METHOD> is ambiguous . A solution to this problem is to cast the null pointer to the desired type: public set(string s){ ... } public set(MyClass c){ .... } set(null) // doesn't work set((MyClass) null) // works set((String) null) // works","title":"Overloading"},{"location":"Programming/Java/Java%20Manual/#regular-expressions","text":"First thing we need to know about regexes in java is that we need to escape all backslashes (refer to the Strings chapter), there are no raw strings in Java. We can work with regexes either by using specialized high level methods that accepts regex as string, or by using the tools from the java.util.regex package.","title":"Regular expressions"},{"location":"Programming/Java/Java%20Manual/#testing-if-string-matches-a-regex","text":"We can do it simply by: String MyString = \"tests\"; String MyRegex = \"t\\\\p+\" boolean matches = myString.matches(myRegex);","title":"Testing if string matches a regex"},{"location":"Programming/Java/Java%20Manual/#exceptions","text":"The exception mechanism in Java is similar to other languages. The big difference is, however, that in Java, all exception not derived from the RuntimeException class are checked , which means that their handeling is enforced by the compiler. The following code does not compile, because the java.langException class is a checked exception class. private testMethod{ ... throw new Exception(); } Therefore, we need to handle the exception using the try cache block or add throws <EXCEPTION TYPE> to the method declaration. When deciding between try/catch and throws , the rule of thumb is to use the try/cache if we can handle the exception and throws if we want to leave it for the caller. The problem arises when the method we are in implements an interface that does not have the throws declaration we need. Then the only solution is to use try/cache with a cache that does not handle the exception, and indicate the problem to the caller differently, e.g, by returning a null pointer.","title":"Exceptions"},{"location":"Programming/Java/Java%20Manual/#logging","text":"Java has a built-in logging mechanism in the java.util.logging package. Apart from that, there are many other logging libraries. In this section, we focus on the SLF4J library which is an abstraction. With SLF4J, we can switch between different logging libraries without changing the code. To use SLF4J, we need to add the following dependency to the pom.xml file: <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version><version></version> </dependency> Additionally, we need to add a backend of our choice. For example, we can use the logback library: <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-classic</artifactId> <version><version></version> </dependency> The SLF4J library detects the available backend automatically. Note that for some backends, the newest version may not be supported by the SLF4J library. In such cases, SLF4J does not detect the backend and reports fallback to the nop backend. For the list of supported backend versions, see the SLF4J manual .","title":"Logging"},{"location":"Programming/Java/Java%20Manual/#genericity","text":"Oracle official tutorial for Java 8 Like many other languages, Java offers generic types with the classical syntax: class GenericClass<T>{ ... }; An object is then created as: GenericClass<OtherClass> gc = new GenericClass<OtherClass>();","title":"Genericity"},{"location":"Programming/Java/Java%20Manual/#diamond-operator","text":"At all places where the type can be inferred, we can use the dianmond operator ( <> ) without specifiyng a type: GenericClass<OtherClass> gc = new GenericClass<>();","title":"Diamond operator"},{"location":"Programming/Java/Java%20Manual/#raw-types","text":"specification For the backward compatibility of Java library classes that were converted to generic classes a concept of raw types was introduces. However, in addition, this concept brings a lot of very subtle compile errors, which are caused by a nonintentional creation of raw type instead of a specific type. GenericClass gc = new GenericClass(); // raw type The parameterized type can be assigned to the raw type, but when we do it the other way arround, the code emmits warning and is not runtime safe: GenericClass<OtherClass> generic = new GenericClass<>(); GenericClass raw = generic // OK GenericClass raw = new GenericClass(); GenericClass<OtherClass> generic = raw // unsafe Calling the generic methods on raw types has a simmilar consequences. But the worst property of raw types is that all generic non-static members of raw types that are not inherited and are also eraw types. Therefore, the genericity is lost even for the completely unrelated type parameters, even specific types are erased to Object . The consequence can be seen on the following example: class GenericClass<T>{ public List<String> getListofStrings() }; GenericClass raw = new GenericClass(); raw.getListofStrings(); // returns List<Object> !!!","title":"Raw types"},{"location":"Programming/Java/Java%20Manual/#generic-method","text":"A generic method in Java is written as: public <<GENERIC PARAM NAME>> <RESULT TYPE> <METHOD NAME>(<PARAMS>){ ... } example: public static <T> ProcedureParameterValidationResult<T> error(String msg){ return new ProcedureParameterValidationResult<>(false, msg, null); } if the generic parameter cannot be infered from the method arguments, we can call the method with explicit generic argument like this: <<GENERIC PARAM VALUE>><METHOD NAME>(<ARGUMENTS>); example: ProcedureParameterValidationResult.<String>error(\"Value cannot be empty.\");","title":"Generic method"},{"location":"Programming/Java/Java%20Manual/#retrieving-the-type-of-a-generic-argument","text":"Because the genericity in Java is resolved at runtime, it is not possible to use the generic parameter as a type. What does it mean? Whille it is possible to use Myclass.class it is not possible to use T.class . The same applies for the class methods. The usual solution is to pass the class to the generic method as a method parameter: void processGeneric(Class<T> classType){ ... }","title":"Retrieving the type of a generic argument"},{"location":"Programming/Java/Java%20Manual/#default-generic-argument","text":"It is not possible to set the default value of a generic parameter. If we want to achive such behavior, we need to create a new class: class GeneriClass<T>{ ... } class StringClass extends GenericClass<String>{};","title":"Default generic argument"},{"location":"Programming/Java/Java%20Manual/#wildcards","text":"The wildcard symbol ( ? ) represents a concrete but unspecified type. It can be bounded by superclass/interface ( ? exctends MyInterface ), or by sublcass ( ? super MyClass ). The typical use cases: single non-generic method accepting generic types of multiple generic parameter values: myMethod(Wrapper<? extends MyInterface> wrapper){ MyInterface mi = wrapper.get(); ... } method that can process generic class but does not need to work with the generic parameters at all: getLength(List<?> list){ return list.size(); }","title":"Wildcards"},{"location":"Programming/Java/Java%20Manual/#difference-between-myclass-myclassobject-and-myclass","text":"class MyClass<T>{ private T content; private set(T content){ this.content = content; } private List<String> getList; } // raw type processMyClass(MyClass myClass){...}; // My class of object types processMyClass(MyClass<Object> myClass){...}; // My class of any specific type processMyClass(MyClass<?> myClass){...}; MyClass (raw type) MyClass<Object> MyClass<?> type safe (Check types at compile time) no yes yes set can be called with any Object any Object null only getList() returns List<Object> List<String> List<String> processMyClass can be called with any List List<Object> only any List","title":"Difference between MyClass, MyClass&lt;Object&gt;, and MyClass&lt;?&gt;"},{"location":"Programming/Java/Java%20Manual/#enums","text":"Java enumerations are declared using the enum keyword: public enum Semaphore{ RED, ORAGNGE, GREEN } If we need to add some additional members to the enum, we need to end the list of enum items with a semicolon first: public enum Semaphore{ RED, ORAGNGE, GREEN; public void doSomething(){ ... } }","title":"Enums"},{"location":"Programming/Java/Java%20Manual/#iterating-over-enum-items","text":"we can iterate over enum items using the values method: for(Semaphore s: Semaphore.values()){ ... } The above code iterates over values of a specific enum. If we need an iteration over any enum (generic), we need to use a class method: Class<E> enumClass; for(E item: enumClass.getEnumConstants()){ }","title":"Iterating over enum items"},{"location":"Programming/Java/Java%20Manual/#converting-a-string-to-enum","text":"We can convert a String to enum using the valueOf method that is generated for each enum class> Semaphore sem = Semaphore.valueOf(\"RED\"); Note that the string has to match the enum value exactly, including the case. Extra whitespaces are not permited. There is also a generic static valueOf method in the Enum class, which we can use for generic enums: Class<E> enumClass; E genEnumInst = Enum.valueOf(enumClass, \"RED\"); Note that here E has to be a properly typed enum ( Enum<E> ), not the raw enum ( Enum ).","title":"Converting a string to enum"},{"location":"Programming/Java/Java%20Manual/#file-system","text":"The modern class for working with paths is the Path class from the java.nio.file package. Other important class is the Files class, which contains static methods for working with files and directories. Important methods: exists isDirectory isRegularFile methods for iterating over files (see next section)","title":"File System"},{"location":"Programming/Java/Java%20Manual/#iterating-over-files-in-a-directory","text":"We can iterate the files in a directory using the Files class: Files.list : flat list of files Files.walk : recursive list of files Both methods return a Stream<Path> object.","title":"Iterating over files in a directory"},{"location":"Programming/Java/Java%20Manual/#creating-a-directory","text":"The directory can be created from any Path object using the Files.createDirectory method. We can also create a directory from a File object using the mkdir and mkdirs methods.","title":"Creating a directory"},{"location":"Programming/Java/Java%20Manual/#date-and-time","text":"Baeldung tutorial The following classes are intended for working with date and time in Java: LocalDate : date without time LocalTime : time without date LocalDateTime : date and time The LocalDateTime also has its timezone/localization aware counterpart: ZonedDateTime : date and time with timezone","title":"Date and Time"},{"location":"Programming/Java/Java%20Manual/#rounding","text":"There is no built-in method for rounding the date and time, we need to set the fields manually. For example, to round the time to the nearest hour: LocalDateTime ldt = LocalDateTime.now(); if(ldt.getMinute() >= 30){ ldt = ldt.plusHours(1); } ldt = ldt.withMinute(0).withSecond(0).withNano(0);","title":"Rounding"},{"location":"Programming/Java/Java%20Manual/#lambda-expressions","text":"wiki An important thing about lambda expressions in Java is that we can only use them to create types satisfying some functional interface. This means that: They can be used only in a context where a functional interface is expected They need to be convertible to that interface. The example that demonstrate this behavior is bellow. Usually, we use some standard interface, but here we create a new one for clarity: interface OurFunctionalInterface(){ int operation(int a) } ... public void process(int num, OurFunctionalInterface op){ ... } With the above interface and method that uses it, we can call process(0, a -> a + 5) Which is an equivalent of writing OurFunctionalInterface addFive = a -> a + 5; process(0, addFive);","title":"Lambda expressions"},{"location":"Programming/Java/Java%20Manual/#syntax","text":"A full syntax for lambda expressions in Java is: (<PARAMS>) -> { <EXPRESSIONS> } If there is only one expression, we can ommit the code block: (<PARAMS>) -> <EXPRESSION> And also, we can ommit the return statement from that expression. The two lambda epressions below are equivalent: (a, b) -> {return a + b;} (a, b) -> a + b If there is only one parameter, we can ommit the parantheses: <PARAM> -> <EXPRESSION> By default, the parameter types are infered from the functional interface. If we need more specific parameters for our function, we can specify the parameter type, we have to specify all of them however: (a, b) -> a + b // valid (int a, int b) -> a + b // valid (int a, b) -> a + b // invalid Also, it is necesary to use the parantheses, if we use the param type.","title":"Syntax"},{"location":"Programming/Java/Java%20Manual/#method-references","text":"We can also create lambda functions from existing mehods (if they satisfy the desired interface) using a mechanism called method reference . For example, we can use the Integer.sum(int a, int b) method in conext where the IntBinaryOperator interface is required. Instead of IntBinaryOperator op = (a, b) -> a + b; // new lambda body we can write: IntBinaryOperator op = Integer::sum; // lambda from existing function","title":"Method references"},{"location":"Programming/Java/Java%20Manual/#exceptions-in-lambda-expressions","text":"Beware that the checked exceptions thrown inside lambda expressions has to be caught inside the lambda expression. The following code does not compile: try{ process(0, a -> a.methodThatThrows()) catch(exception e){ ... } Instead, we have to write: process(0, a -> { try{ return a.methodThatThrows()); catch(exception e){ ... } }","title":"Exceptions in lambda expressions"},{"location":"Programming/Java/Java%20Manual/#iteration","text":"Java for each loop has the following syntax: for(<TYPE> <VARNAME>: <ITERABLE>){ ... } Here, the <ITERABLE> can be either a Java Iterable , or an array.","title":"Iteration"},{"location":"Programming/Java/Java%20Manual/#enumerated-iteration","text":"There is no enumerate equivalent in Java. One can use a stream API range method , however, it is less readable than standard for loop because the code execuded in loop has to be in a separate function.","title":"Enumerated iteration"},{"location":"Programming/Java/Java%20Manual/#iterating-using-an-iterator","text":"The easiest way how to iterate if we have a given iterator is to use its forEachRemaining method. It takes a Consumer object as an argument, iterates using the iterator, and calls Consumer.accept method on each iteration. The example below uses a lambda function that satisfies the consumer interface: class Our{ void process(){ ... } ... } Iterator<Our> objectsIterator = ...; objectsIterator.forEachRemaining(o -> o.process());","title":"Iterating using an iterator"},{"location":"Programming/Java/Java%20Manual/#functional-programming","text":"","title":"Functional Programming"},{"location":"Programming/Java/Java%20Manual/#turning-array-to-stream","text":"Arrays.stream(<ARRAY>);","title":"Turning array to stream"},{"location":"Programming/Java/Java%20Manual/#creating-stream-from-iterator","text":"The easiest way is to first create an Iterable from the iterator and then use the StreamSupport.stream method: Iterable<JsonNode> iterable = () -> iterator; var stream = StreamSupport.stream(iterable.spliterator(), false);","title":"Creating stream from iterator"},{"location":"Programming/Java/Java%20Manual/#filtering","text":"A lambda function can be supplied as a filter: stream.filter(number -> number > 5) returns a stream with numbers greater then five.","title":"Filtering"},{"location":"Programming/Java/Java%20Manual/#transformation","text":"We can transform the stream with the map function: transformed = stream.map(object -> doSomething(object))","title":"Transformation"},{"location":"Programming/Java/Java%20Manual/#materialize-stream","text":"We can materrialize stream with the collect metod: List<String> result = stringStream.collect(Collectors.toList()); Which can be, in case of List shorten to: List<String> result = stringStream.toList();","title":"Materialize stream"},{"location":"Programming/Java/Java%20Manual/#var","text":"openjdk document Using var is similar to auto in C++. Unlike in C++, it can be used only for local variables. The var can be tricky when using together with diamond operator or generic methods. The compilation works fine, however, the raw type will be created.","title":"var"},{"location":"Programming/Java/Java%20Manual/#type-cast","text":"Java use a tradidional casting syntax: (<TARGET TYPE>) <VAR> There are two different types of cast: value cast , which is used for value types (only primitive types in Java) and change the data reference cast , which is used for Java objects and does not change it, it just change the objects interface","title":"Type cast"},{"location":"Programming/Java/Java%20Manual/#reference-cast","text":"The upcasting (casting to a supertype) is done implicitely by the compiler, so instead of Object o = (Object) new MyClass(); we can do Object o = new MyClass(); Typically, we need to use the explicit cast for downcasting : Object o = ...; MyClass c = (MyClass) o; // when we are sure that o is an instance of MyClass... Downcasting to an incorrect type leads to a runtime ClassCastException . Casting to an unrelated type is not allowed by the compiler.","title":"Reference cast"},{"location":"Programming/Java/Java%20Manual/#casting-generic-types","text":"Casting with generic types is notoriously dangerous due to type erasure. The real types of generic arguments are not known at runtime, therefore, an incorrect cast does not raise an error: Object o = new Object(); String s = (String) o; // ClassCatsException List<Object> lo = new ArrayList<>(); List<String> ls = (List<String>) lo; // OK at runtime, therefore, it emmits warning at compile time.","title":"Casting generic types"},{"location":"Programming/Java/Java%20Manual/#casting-case-expression","text":"Often, when we work with an interface, we have to treat each implementation differently. The best way to handle that is to use the polymorphism, i.e., add a dedicated method for treating the object to the interface and handle the differences in each implementation. However, sometimes, we cannot modify the interface as it comes from the outside of our codebase, or we do not want to put the code to the interface because it belongs to a completely different part of the application (e.g., GUI vs database connection). A typicall solution for this is to use branching on instanceof and a consecutive cast: if(obj instance of Apple){ Apple apple = (Apple) obj; ... } else if(obj instance of Peach){ Peach peach = (Peach) obj; ... } ...","title":"Casting case expression"},{"location":"Programming/Java/Java%20Manual/#casting-switch","text":"This approach works, it is safe, but it is also error prone. A new preview swich statement is ready to replace this technique: switch(obj) case Apple apple: ... break; case Peach peach: ... break; ... } Note that unsafe type opperations are not allowed in these new switch statements, and an error is emitted instead of warning: List list = ... List<String> ls = (List<String>) list; // warning: unchecked switch(list){ case List<String> ls: // error: cannot be safely cast ... }","title":"Casting switch"},{"location":"Programming/Java/Java%20Manual/#random-numbers","text":"","title":"Random numbers"},{"location":"Programming/Java/Java%20Manual/#generate-random-integer","text":"To get an infinite iterator of random integers, we can use the Random.ints method: var it = new Random().ints(0, map.nodesFromAllGraphs.size()).iterator();\\ int a = it.nextInt();","title":"Generate random integer"},{"location":"Programming/Java/Java%20Manual/#reflection","text":"Reflection is a toolset for working with Java types dynamically.","title":"Reflection"},{"location":"Programming/Java/Java%20Manual/#get-class-object","text":"To obtain the class object, we can use: the forName method of the Class class: Java Class<?> c = Class.forName(\"java.lang.String\"); This way, we can load classes dynamically at runtime. myString.getClass() if we have an instance or String.class if we know the class at compile time. from an object of the class (e.g.,. The method can be then iterated using the getDeclaredMethods method of the Class class.","title":"Get class object"},{"location":"Programming/Java/Java%20Manual/#test-if-a-class-is-a-superclass-or-interface-of-another-class","text":"It can be tested simply as: ClassA.isAssignableFrom(ClassB)","title":"Test if a class is a superclass or interface of another class"},{"location":"Programming/Java/Java%20Manual/#get-field-or-method-by-name","text":"Field field = myClass.getDeclaredField(\"fieldName\"); Method method = myClass.getMethod(\"methodName\", <PARAM TYPES>); Here, the <PARAM TYPES> is a list of classes that represent the types of the method parameters, e.g., String.class, MyClass.class for a method that accepts a string and an instance of MyClass .","title":"Get field or method by name"},{"location":"Programming/Java/Java%20Manual/#call-method-using-method-object","text":"method.invoke(myObject, <PARAMS>);","title":"Call method using Method object"},{"location":"Programming/Java/Java%20Manual/#sql","text":"Java has a build in support for SQL in package java.sql . The typical operation: create a PreparedStatement from the cursor using the SQL string as an input Fill the parameters of the PreparedStatement Execute the query","title":"SQL"},{"location":"Programming/Java/Java%20Manual/#filling-the-query-parameters","text":"This process consist of safe replacement of ? marks in the SQL string with real values. The PreparedStatement class has dedicated methods for that: setString for strings setObject for complex objects Each method has the index of the ? to be replaced as the first parameter. The index start from 1 . Note that these methods can be used to supply arguments for data part of the SQL statement. If the ? is used for table names or SQL keywords, the query execution will fail. Therefore, if you need to dynamically set the non-data part of an SQL query (e.g., table name), you need to sanitize the argument manually and add it to the SQl querry.","title":"Filling the query parameters"},{"location":"Programming/Java/Java%20Manual/#processing-the-result","text":"The java sql framework returns the result of the SQL query in form of a ResultSet . To process the result set, we need to iterate the rows using the next method and for each row, we can access the column values using one of the methods (depending on the dtat type in the column), each of which accepts either column index or label as a parameter. Example: var result = statement.executeQuery(); while(result.next()){ var str = result.getString(\"column_name\")); ... }","title":"Processing the result"},{"location":"Programming/Java/Java%20Manual/#jackson","text":"Described in the Jackson manual.","title":"Jackson"},{"location":"Programming/Java/Java%20Manual/#dependency-injection-with-guice","text":"Dependency injection is a design pattern that allows interaction between objects without wiring them manually. It works best in large applications with many independent components. When we want to use Singletons in our application, it is time to consider using a dependency injection. Guice is very convenient as it has many features that make using DI even easier. For example, any component can be used in DI by simply annotating the constructor with @Inject . Note that there can be only one constructor annotated with @Inject in a class. To mark a component that should be used as a singleton, we annotate the class with @Singleton annotation.","title":"Dependency injection with Guice"},{"location":"Programming/Java/Java%20Manual/#assisted-injection","text":"If the class has a constructor with some parameters that are not provided by the Guice, we can use the assisted injection Assisted Injection is a mechanism to automatically generate factories for classes that have a constructor with mixed Guice and non-Guice parameters. Instead of a factory class, we provide only a factory interface: public interface MyFactory { MyClass create(String param1, int param2); } This factory can than be used to create the object: class MyClass{ @Inject MyClass( <parameters provided by Guice>, @Assisted String param1, @Assisted int param2 ){ ... } } The non-Guice parameters are assigned using the parameters of the factory's create method. The matching is determined by the parameter type. If the parameter type is not unique, among all @Assisted parameters, we have to provide the @Assisted annotation with the value parameter: public interface MyFactory { MyClass create(@Assisted(\"param1\") String param1, @Assisted(\"param2\") String param2); } class MyClass{ @Inject MyClass( <parameters provided by Guice>, @Assisted(\"param1\") String param1, @Assisted(\"param2\") String param2 ){ ... } }","title":"Assisted injection"},{"location":"Programming/Java/Java%20Manual/#progress-bar","text":"For the progress bar, there is a good library called progressbar The usage is simple, just wrap any iterable, iterator, stream or similar object with the ProgressBar.wrap method: ArrayList<Integer> list = ... for(int i: ProgressBar.wrap(list, \"Processing\")){ ... } // or with a stream Stream<Integer> stream = ... progressStream = ProgressBar.wrap(stream, \"Processing\"); progressStream.forEach(i -> ...); // or with an iterator Iterator<Integer> iterator = ... progressIterator = ProgressBar.wrap(iterator, \"Processing\"); while(iterator.hasNext()){ ... }","title":"Progress bar"},{"location":"Programming/Java/Java%20Workflow/","text":"Java developement stack \u00b6 We use this stack: Toolchain: JDK Package manager: standalone Maven IDE: Netbeans, Idea JDK \u00b6 Java developem kit is the standard Java toolchain. Most comon tools are: javac for compilation java for execution javac \u00b6 The syntax is: javac [options] [sourcefiles] Warning control \u00b6 By default, all warnings are disabled and only a summary of the warnings is displayed in the output (i.e., all warning types encountered, without specific lines where they occur). To enable all warnings use the -Xlint argument. To enable/disable specific warnings, use -Xlint:<warning name> and -Xlint:-<warning name> , respectively. Maven \u00b6 Described in the Maven manual. Netbeans \u00b6 Install the latest version of Apache Netbeans Configuration: Autosaving: Editor -> Autosave Tab size, tabs instead of spaces, 120 char marker line: Editor -> Formatting -> All Languages Multi-row tabs: Appearance -> DocumentTabs Git labels on projects: Team -> Versioning -> Show Versioning Labels Internet Browser: General -> web browser Javadoc config: if the javadoc for java SE does not work out of the box, maybe there is a wrong URL. Go to Tools -> Java Platforms -> Javadoc and enter there the path where the Javadoc is accessible online Install the basic plugins Markdown Project configuration \u00b6 Configure Maven Goals \u00b6 These can be configured in Project properties -> Actions Troubleshooting \u00b6 If there is a serious problem, one way to solve it can be to delete the Netbeans cache located in ~\\AppData\\Local\\NetBeans\\Cache . Idea \u00b6 Configuration \u00b6 Configuration is done in File -> Settings . A lot of configuration we do in Idea Settings only applies to the current project. Therefore, it is a good idea to check whether the settings is not present in the template for new projects, so that we do not have to set it up for every new project. The settings template is located in File -> New Project Setup . Settings synchronization \u00b6 Log in into JetBrains Toolbox or to the App Click on the gear icon on the top-right and choose Sync Check all categories and click on pull settings from the cloud Resatart Ida to update all the settings More on Jetbrains Maven configuration \u00b6 By default, Idea uses the bundled Maven, which is almost never desired. It is best to swich to the system Maven, which we should done in: File -> Settings -> Build, Execution, Deployment -> Build Tools -> Maven -> Maven home directory for existing projects File -> New Project Setup -> Maven -> Maven home directory for new projects JDK configuration \u00b6 The correct JDK has to be set up in various places: compielr has to be at least target jdk, set it in: File -> Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Project bytecode version and Per-module bytecode version language level should be the same as target jdk: File -> Project Structure... -> Modules -> Language Level Compilation \u00b6 Everything is compiled in the background automatically. However, if we need to compile manually using maven, e.g., to activate certain plugins, we can compile using the maven tab on the right. Running Projects \u00b6 To add or edit run configurations, click on the run configuration dropdown left of the run button and choose Edit Configurations... . If the required configuration field is not present, it may be necessary to activate it by clicking on Modify options and choosing the desired option. Developing the whole project stack at once \u00b6 If we are developing a whole stack of projects at once, it is best if we can navigate between them easily. However, it is not possible to open multiple projects in the same window in Idea (like in Netbeans). Instead, we need to open the project on the top of the stack and then add the other projects as modules. To add a module, click File -> Project Structure -> Modules and add the module. Running Maven Goals \u00b6 Maven goals can be run from a dedicated tab on the right. The goals in the tab are divided into two categories: Lifecycle goals are the most common goals, which are used to build the project Plugins goals are the goals of the plugins used in the project To run the goal, just double-click on it. If the run need some special configuration, right-click on the goal and choose Modify Run Configuration... If the plugin is missing from the list, it may be necessary to reload the plugins. Click the Reload All Maven Projects button in the top left corner of the maven tab. If the goal we want to run is neither a Lifecycle goal nor a Plugin goal (e.g., exec ), we can run it by creating a run configuration and selecting maven type. Than, we select the maven goal by inserting it as a first argument in the Command line field. Maven goal configuration \u00b6 To run a maven goal with a specific profile, add the profile name to the Profiles field. Troubleshooting \u00b6 Idea does not detect manually installed Maven artifact \u00b6 Sometimes, idea cannot recognize installed maven artifact and marks it as missing (red). Fix: right click on the project or pom file -> Maven -> Reload project Idea does not see environment variables \u00b6 These may be new environment variables, which were not present when Idea was started. Restart Idea to see the new environment variables. Idea does not set the correct classpath \u00b6 Sometimes, Idea does not set the correct classpath for the project, and in consequence, there are errors reported in the editor, and the project fails to run (but compiles). Solution: Invalidate the cache and restart Idea. This can be done by clicking File -> Invalidate Caches / Restart... -> Invalidate and Restart . Java version \u00b6 list of Java versions There are multiple java versions to set: source version determines the least version of java that supports all language tools used in the source code. It is the least version that can be used to compile the source code. target version determines the least version of java that can be used to run your java application. compile version is the actual version used for compiling the code runtime version is the actual version used to run the Java program The target version can be higher than the source version, but not he other way around. Most of the time, however, we use the same version for source and for target. The actual version used for compilation must be equal or higher than the source version. The actual version used for running the program must be equal or higher than the target version. Usually, these versions needs to be set: in the project compilation tool (maven) to configure the project maven compilation in the IDE project properties to: configure the compilation of individual files, which is executed in the background to report the compilation errors at real time. configure the runtime environment for the program execution Sometimes, the Java version used for running Maven has to be also set because it cannot be lower then the Java version used for project compilation using Maven. SO explanation Setting Java version in Maven \u00b6 Since Java 9, both Java versions can be set at once with the release option: <properties> <maven.compiler.release>10</maven.compiler.release> </properties> or equivalently <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> <release>10</release> </configuration> </plugin> The old way is to use separate properties: <properties> <maven.compiler.target>10</maven.compiler.target> <maven.compiler.source>10</maven.compiler.source> </properties> Note that the Java version configured in the pom file cannot be greater then the Java version used to run the Maven. Setting Java version in Netbeans \u00b6 Note that for maven projects, this is automatically set to match the properties in the pom file. For the Netbeans real-time compiler, the cross compilation does not make sense, so both source and target Java version is set in one place: Right click on project -> Properties -> Sources In the bottom, change the Source/Binary Format Setting Java version in IDEA \u00b6 In idea, an extra step is sometimes necessary: to set the Java Language Level in the project settings: File -> Project Structure... -> Modules -> Language Level . The language level has to be lower or equal to the target version set in the maven configuration. Setting Java version used for executing Maven \u00b6 If the Maven is executed from command line, edit the JAVA_HOME system property. If the Maven is executed from Netbeans, edit Project Properties -> Build -> Compile -> Java Platform Enabling preview features \u00b6 The preview features can be enabled using the --enable-preview argument. In Maven, this has to be passed to the compiler plugin: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <enablePreview>true</enablePreview> </configuration> </plugin> </plugins> </build> For Maven compiler plugin older then version 3.10.1, use: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <compilerArgs> <arg>--enable-preview</arg> </compilerArgs> </configuration> </plugin> </plugins> </build> Unfortunatelly, it is also necessary to enable preview features in the IDE configuration: In Netbeans, add --enable-preview to Project Properties -> Run -> VM options . In IDEA: add --enable-preview to Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Override compiler parameters per-module add --enable-preview to run configuration -> Build and run vm options Problems \u00b6 Runtime version error \u00b6 A typical runtime version error is: java.lang.UnsupportedClassVersionError: <executed class> has been compiled by a more recent version of the Java Runtime (class file version <higher version>), this version of the Java Runtime only recognizes class file versions up to <lower version> First, we schould check the reported versions. There are two possible scenarios: The <lower version> is unexpectedly low. This means that we run the program with a lower version of JDK then the one we compiled it with. The solution is to set a different JDK to run the program. The <higher version> is unexpectedly high. This means that the class was compiled on some other machine, we have uninstalled the newest JDK, or we have set the wrong JDK in the compilation settings. The solution is to clean the project and recompile it. If it does not help, the problem can be in oneof the dependencies. Check the version of java required at runtime \u00b6 There are various tactics, here listed from the easiest to the most universal: check the library documentation check the library pom.xml file check the MANIFEST.MF file in the library jar check the class file version using javap : javap -v <class file> Gurobi \u00b6 Gurobi is a commercial project not contained in any public maven repositories. It is necessary to install the gurobi maven artifact manualy. Potential problems \u00b6 unsatisfied linker error : Check if the gurobi version in the error log matches the gurobi version installed. Testing with JUnit \u00b6 homepage First, we need to add the JUnit dependency to pom.xml : <dependency> <groupId>org.junit.jupiter</groupId> <artifactId>junit-jupiter</artifactId> <version><version></version> <scope>test</scope> </dependency> In test files, we just mark any test method with the @Test imorted as org.junit.jupiter.api.Test . Assertions are then imported statically from org.junit.jupiter.api.Assertions . Make part of the project optional \u00b6 Sometimes we want to make a part of the project optional, so that it is not required at runtime or even at compile time. This is useful if that part of the project is: not essential for the project to work dependent on some external library, which can be cumbersome to install The required steps are usually: Create a maven profile for the optional part of the project Move optional dependencies to the profile Move optional source files outside the src/main/java directory Use the build-helper-maven-plugin in the profile to add the optional source files to the project At runtime, load the optional part of the project using the reflection We now describe these steps in detail. Maven profiles and optional dependencies \u00b6 A maven profile belongs to the profiles section of the pom.xml file. The structure is simple: <profiles> <profile> <id>optional</id> <dependencies> <!-- optional dependencies --> </dependencies> <build> <plugins> <!-- optional plugins --> </plugins> </build> </profile> Moving optional source files outside the src/main/java directory \u00b6 The optional source files can be moved anywhere, but a logical place is to create a new directory under src/main , e.g., src/main/<lib>-optional . Then we need to tell maven to include these files in the project if the optional profile is activated. This is done using the build-helper-maven-plugin in the optional profile: <profiles> <profile> <id>optional</id> <build> <plugins> build-helper-maven-plugin here </plugins> </build> </profile> </profiles> For the build-helper-maven-plugin configuration, see the Maven manual . Loading the optional part of the project at runtime \u00b6 First, we should check if the optional part of the project is present at runtime: public static boolean libAvailable(){ try { Class.forName(\"com.example.lib\"); return true; } catch (ClassNotFoundException e) { return false; } } Then, we can load the optional part of the project using reflection: if(libAvailable()){ Class<?> libClass = Class.forName(\"com.example.lib\"); Object lib = libClass.newInstance(); Method method = libClass.getMethod(\"someMethod\"); method.invoke(lib); } else{ // handle missing library } Archive \u00b6 AIC maven repo access \u00b6 To Access the AIC maven repo, copy maven settings from another computer (located in ~/.m2)","title":"Java Workflow"},{"location":"Programming/Java/Java%20Workflow/#java-developement-stack","text":"We use this stack: Toolchain: JDK Package manager: standalone Maven IDE: Netbeans, Idea","title":"Java developement stack"},{"location":"Programming/Java/Java%20Workflow/#jdk","text":"Java developem kit is the standard Java toolchain. Most comon tools are: javac for compilation java for execution","title":"JDK"},{"location":"Programming/Java/Java%20Workflow/#javac","text":"The syntax is: javac [options] [sourcefiles]","title":"javac"},{"location":"Programming/Java/Java%20Workflow/#warning-control","text":"By default, all warnings are disabled and only a summary of the warnings is displayed in the output (i.e., all warning types encountered, without specific lines where they occur). To enable all warnings use the -Xlint argument. To enable/disable specific warnings, use -Xlint:<warning name> and -Xlint:-<warning name> , respectively.","title":"Warning control"},{"location":"Programming/Java/Java%20Workflow/#maven","text":"Described in the Maven manual.","title":"Maven"},{"location":"Programming/Java/Java%20Workflow/#netbeans","text":"Install the latest version of Apache Netbeans Configuration: Autosaving: Editor -> Autosave Tab size, tabs instead of spaces, 120 char marker line: Editor -> Formatting -> All Languages Multi-row tabs: Appearance -> DocumentTabs Git labels on projects: Team -> Versioning -> Show Versioning Labels Internet Browser: General -> web browser Javadoc config: if the javadoc for java SE does not work out of the box, maybe there is a wrong URL. Go to Tools -> Java Platforms -> Javadoc and enter there the path where the Javadoc is accessible online Install the basic plugins Markdown","title":"Netbeans"},{"location":"Programming/Java/Java%20Workflow/#project-configuration","text":"","title":"Project configuration"},{"location":"Programming/Java/Java%20Workflow/#configure-maven-goals","text":"These can be configured in Project properties -> Actions","title":"Configure Maven Goals"},{"location":"Programming/Java/Java%20Workflow/#troubleshooting","text":"If there is a serious problem, one way to solve it can be to delete the Netbeans cache located in ~\\AppData\\Local\\NetBeans\\Cache .","title":"Troubleshooting"},{"location":"Programming/Java/Java%20Workflow/#idea","text":"","title":"Idea"},{"location":"Programming/Java/Java%20Workflow/#configuration","text":"Configuration is done in File -> Settings . A lot of configuration we do in Idea Settings only applies to the current project. Therefore, it is a good idea to check whether the settings is not present in the template for new projects, so that we do not have to set it up for every new project. The settings template is located in File -> New Project Setup .","title":"Configuration"},{"location":"Programming/Java/Java%20Workflow/#settings-synchronization","text":"Log in into JetBrains Toolbox or to the App Click on the gear icon on the top-right and choose Sync Check all categories and click on pull settings from the cloud Resatart Ida to update all the settings More on Jetbrains","title":"Settings synchronization"},{"location":"Programming/Java/Java%20Workflow/#maven-configuration","text":"By default, Idea uses the bundled Maven, which is almost never desired. It is best to swich to the system Maven, which we should done in: File -> Settings -> Build, Execution, Deployment -> Build Tools -> Maven -> Maven home directory for existing projects File -> New Project Setup -> Maven -> Maven home directory for new projects","title":"Maven configuration"},{"location":"Programming/Java/Java%20Workflow/#jdk-configuration","text":"The correct JDK has to be set up in various places: compielr has to be at least target jdk, set it in: File -> Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Project bytecode version and Per-module bytecode version language level should be the same as target jdk: File -> Project Structure... -> Modules -> Language Level","title":"JDK configuration"},{"location":"Programming/Java/Java%20Workflow/#compilation","text":"Everything is compiled in the background automatically. However, if we need to compile manually using maven, e.g., to activate certain plugins, we can compile using the maven tab on the right.","title":"Compilation"},{"location":"Programming/Java/Java%20Workflow/#running-projects","text":"To add or edit run configurations, click on the run configuration dropdown left of the run button and choose Edit Configurations... . If the required configuration field is not present, it may be necessary to activate it by clicking on Modify options and choosing the desired option.","title":"Running Projects"},{"location":"Programming/Java/Java%20Workflow/#developing-the-whole-project-stack-at-once","text":"If we are developing a whole stack of projects at once, it is best if we can navigate between them easily. However, it is not possible to open multiple projects in the same window in Idea (like in Netbeans). Instead, we need to open the project on the top of the stack and then add the other projects as modules. To add a module, click File -> Project Structure -> Modules and add the module.","title":"Developing the whole project stack at once"},{"location":"Programming/Java/Java%20Workflow/#running-maven-goals","text":"Maven goals can be run from a dedicated tab on the right. The goals in the tab are divided into two categories: Lifecycle goals are the most common goals, which are used to build the project Plugins goals are the goals of the plugins used in the project To run the goal, just double-click on it. If the run need some special configuration, right-click on the goal and choose Modify Run Configuration... If the plugin is missing from the list, it may be necessary to reload the plugins. Click the Reload All Maven Projects button in the top left corner of the maven tab. If the goal we want to run is neither a Lifecycle goal nor a Plugin goal (e.g., exec ), we can run it by creating a run configuration and selecting maven type. Than, we select the maven goal by inserting it as a first argument in the Command line field.","title":"Running Maven Goals"},{"location":"Programming/Java/Java%20Workflow/#maven-goal-configuration","text":"To run a maven goal with a specific profile, add the profile name to the Profiles field.","title":"Maven goal configuration"},{"location":"Programming/Java/Java%20Workflow/#troubleshooting_1","text":"","title":"Troubleshooting"},{"location":"Programming/Java/Java%20Workflow/#idea-does-not-detect-manually-installed-maven-artifact","text":"Sometimes, idea cannot recognize installed maven artifact and marks it as missing (red). Fix: right click on the project or pom file -> Maven -> Reload project","title":"Idea does not detect manually installed Maven artifact"},{"location":"Programming/Java/Java%20Workflow/#idea-does-not-see-environment-variables","text":"These may be new environment variables, which were not present when Idea was started. Restart Idea to see the new environment variables.","title":"Idea does not see environment variables"},{"location":"Programming/Java/Java%20Workflow/#idea-does-not-set-the-correct-classpath","text":"Sometimes, Idea does not set the correct classpath for the project, and in consequence, there are errors reported in the editor, and the project fails to run (but compiles). Solution: Invalidate the cache and restart Idea. This can be done by clicking File -> Invalidate Caches / Restart... -> Invalidate and Restart .","title":"Idea does not set the correct classpath"},{"location":"Programming/Java/Java%20Workflow/#java-version","text":"list of Java versions There are multiple java versions to set: source version determines the least version of java that supports all language tools used in the source code. It is the least version that can be used to compile the source code. target version determines the least version of java that can be used to run your java application. compile version is the actual version used for compiling the code runtime version is the actual version used to run the Java program The target version can be higher than the source version, but not he other way around. Most of the time, however, we use the same version for source and for target. The actual version used for compilation must be equal or higher than the source version. The actual version used for running the program must be equal or higher than the target version. Usually, these versions needs to be set: in the project compilation tool (maven) to configure the project maven compilation in the IDE project properties to: configure the compilation of individual files, which is executed in the background to report the compilation errors at real time. configure the runtime environment for the program execution Sometimes, the Java version used for running Maven has to be also set because it cannot be lower then the Java version used for project compilation using Maven. SO explanation","title":"Java version"},{"location":"Programming/Java/Java%20Workflow/#setting-java-version-in-maven","text":"Since Java 9, both Java versions can be set at once with the release option: <properties> <maven.compiler.release>10</maven.compiler.release> </properties> or equivalently <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> <release>10</release> </configuration> </plugin> The old way is to use separate properties: <properties> <maven.compiler.target>10</maven.compiler.target> <maven.compiler.source>10</maven.compiler.source> </properties> Note that the Java version configured in the pom file cannot be greater then the Java version used to run the Maven.","title":"Setting Java version in Maven"},{"location":"Programming/Java/Java%20Workflow/#setting-java-version-in-netbeans","text":"Note that for maven projects, this is automatically set to match the properties in the pom file. For the Netbeans real-time compiler, the cross compilation does not make sense, so both source and target Java version is set in one place: Right click on project -> Properties -> Sources In the bottom, change the Source/Binary Format","title":"Setting Java version in Netbeans"},{"location":"Programming/Java/Java%20Workflow/#setting-java-version-in-idea","text":"In idea, an extra step is sometimes necessary: to set the Java Language Level in the project settings: File -> Project Structure... -> Modules -> Language Level . The language level has to be lower or equal to the target version set in the maven configuration.","title":"Setting Java version in IDEA"},{"location":"Programming/Java/Java%20Workflow/#setting-java-version-used-for-executing-maven","text":"If the Maven is executed from command line, edit the JAVA_HOME system property. If the Maven is executed from Netbeans, edit Project Properties -> Build -> Compile -> Java Platform","title":"Setting Java version used for executing Maven"},{"location":"Programming/Java/Java%20Workflow/#enabling-preview-features","text":"The preview features can be enabled using the --enable-preview argument. In Maven, this has to be passed to the compiler plugin: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <enablePreview>true</enablePreview> </configuration> </plugin> </plugins> </build> For Maven compiler plugin older then version 3.10.1, use: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <compilerArgs> <arg>--enable-preview</arg> </compilerArgs> </configuration> </plugin> </plugins> </build> Unfortunatelly, it is also necessary to enable preview features in the IDE configuration: In Netbeans, add --enable-preview to Project Properties -> Run -> VM options . In IDEA: add --enable-preview to Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Override compiler parameters per-module add --enable-preview to run configuration -> Build and run vm options","title":"Enabling preview features"},{"location":"Programming/Java/Java%20Workflow/#problems","text":"","title":"Problems"},{"location":"Programming/Java/Java%20Workflow/#runtime-version-error","text":"A typical runtime version error is: java.lang.UnsupportedClassVersionError: <executed class> has been compiled by a more recent version of the Java Runtime (class file version <higher version>), this version of the Java Runtime only recognizes class file versions up to <lower version> First, we schould check the reported versions. There are two possible scenarios: The <lower version> is unexpectedly low. This means that we run the program with a lower version of JDK then the one we compiled it with. The solution is to set a different JDK to run the program. The <higher version> is unexpectedly high. This means that the class was compiled on some other machine, we have uninstalled the newest JDK, or we have set the wrong JDK in the compilation settings. The solution is to clean the project and recompile it. If it does not help, the problem can be in oneof the dependencies.","title":"Runtime version error"},{"location":"Programming/Java/Java%20Workflow/#check-the-version-of-java-required-at-runtime","text":"There are various tactics, here listed from the easiest to the most universal: check the library documentation check the library pom.xml file check the MANIFEST.MF file in the library jar check the class file version using javap : javap -v <class file>","title":"Check the version of java required at runtime"},{"location":"Programming/Java/Java%20Workflow/#gurobi","text":"Gurobi is a commercial project not contained in any public maven repositories. It is necessary to install the gurobi maven artifact manualy.","title":"Gurobi"},{"location":"Programming/Java/Java%20Workflow/#potential-problems","text":"unsatisfied linker error : Check if the gurobi version in the error log matches the gurobi version installed.","title":"Potential problems"},{"location":"Programming/Java/Java%20Workflow/#testing-with-junit","text":"homepage First, we need to add the JUnit dependency to pom.xml : <dependency> <groupId>org.junit.jupiter</groupId> <artifactId>junit-jupiter</artifactId> <version><version></version> <scope>test</scope> </dependency> In test files, we just mark any test method with the @Test imorted as org.junit.jupiter.api.Test . Assertions are then imported statically from org.junit.jupiter.api.Assertions .","title":"Testing with JUnit"},{"location":"Programming/Java/Java%20Workflow/#make-part-of-the-project-optional","text":"Sometimes we want to make a part of the project optional, so that it is not required at runtime or even at compile time. This is useful if that part of the project is: not essential for the project to work dependent on some external library, which can be cumbersome to install The required steps are usually: Create a maven profile for the optional part of the project Move optional dependencies to the profile Move optional source files outside the src/main/java directory Use the build-helper-maven-plugin in the profile to add the optional source files to the project At runtime, load the optional part of the project using the reflection We now describe these steps in detail.","title":"Make part of the project optional"},{"location":"Programming/Java/Java%20Workflow/#maven-profiles-and-optional-dependencies","text":"A maven profile belongs to the profiles section of the pom.xml file. The structure is simple: <profiles> <profile> <id>optional</id> <dependencies> <!-- optional dependencies --> </dependencies> <build> <plugins> <!-- optional plugins --> </plugins> </build> </profile>","title":"Maven profiles and optional dependencies"},{"location":"Programming/Java/Java%20Workflow/#moving-optional-source-files-outside-the-srcmainjava-directory","text":"The optional source files can be moved anywhere, but a logical place is to create a new directory under src/main , e.g., src/main/<lib>-optional . Then we need to tell maven to include these files in the project if the optional profile is activated. This is done using the build-helper-maven-plugin in the optional profile: <profiles> <profile> <id>optional</id> <build> <plugins> build-helper-maven-plugin here </plugins> </build> </profile> </profiles> For the build-helper-maven-plugin configuration, see the Maven manual .","title":"Moving optional source files outside the src/main/java directory"},{"location":"Programming/Java/Java%20Workflow/#loading-the-optional-part-of-the-project-at-runtime","text":"First, we should check if the optional part of the project is present at runtime: public static boolean libAvailable(){ try { Class.forName(\"com.example.lib\"); return true; } catch (ClassNotFoundException e) { return false; } } Then, we can load the optional part of the project using reflection: if(libAvailable()){ Class<?> libClass = Class.forName(\"com.example.lib\"); Object lib = libClass.newInstance(); Method method = libClass.getMethod(\"someMethod\"); method.invoke(lib); } else{ // handle missing library }","title":"Loading the optional part of the project at runtime"},{"location":"Programming/Java/Java%20Workflow/#archive","text":"","title":"Archive"},{"location":"Programming/Java/Java%20Workflow/#aic-maven-repo-access","text":"To Access the AIC maven repo, copy maven settings from another computer (located in ~/.m2)","title":"AIC maven repo access"},{"location":"Programming/Java/Maven/","text":"Maven is a dependency management and build tool for Java. The project configuration for Maven is stored in a single file called pom.xml . Both the project dependencies and the build configuration are stored in this file. The maven is executed using the as mvn <goal> . Typical goals are compile : compile the project test : run tests package : package the project into a jar or war file install : install the project into the local maven repository deploy : deploy the project to a remote repository Maven Packages: Artifacts \u00b6 All Maven dependencies and plugins installed in local system repositories can be either: downloaded from a remote repository installed from a local file or project Note that if we want the artifact to be downloaded from a remote repository, we do not have to take any action. It will be downloaded automatically when needed. By default, Maven searches for artifacts in the Maven Central repository . If we want to use a different repository, we have to add it manually. List locally installed artifacts \u00b6 There is no built-in command to list all locally installed artifacts. However, we can use the info plugin : mvn ninja.fido:info-maven-plugin:list Install artifacts from a remote repository manually \u00b6 If we want to install an artifact from a remote repository manually, we can use the dependency:get goal. The syntax is as follows: mvn dependency:get -Dartifact=<GROUP ID>:<ARTIFACT ID>:<VERSION> Debugging missing Artifacts \u00b6 All Maven dependencies should work out of the box. If some dependencies cannot be resolved: check that the dependencies are on the maven central. if not, check that they are in some special repo and check that the repo is present in the pom of the project that requires the dependency Variables \u00b6 Variables in the pom.xml file can be used using the ${variable} syntax. To use some environment variables, we can use the ${env.variable} . Note that contrary to all expectations, maven goals do not fail if a variable is not defined . Instead, the variable is replaced with an empty string. This can lead to unexpected behavior, so it is usually preferable to fail. For that, we can use the enforcer plugin with the requireProperty rule: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-enforcer-plugin</artifactId> <version>3.4.1</version> <executions> <execution> <id>enforce-property</id> <goals> <goal>enforce</goal> </goals> <configuration> <rules> <requireProperty> <property>basedir</property> <message>You must set a basedir property!</message> </requireProperty> </rules> <fail>true</fail> </configuration> </execution> </executions> </plugin> Compilation \u00b6 reference Compilation is handeled by the Maven compiler plugin. Usually, typing mvn compile is enough to compile the project. If you need any customization, add it to the compiler plugin configuration <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> .... </configuration> </plugin> Passing arguments to javac \u00b6 A single argument can be passed using the <compilerArgument> property. For more compiler arguments, use <compilerArgs> : <compilerArgs> <arg>-Xmaxerrs=1000</arg> <arg>-Xlint</arg> </compilerArgs> Dependencies \u00b6 Maven dependencies are defined in the pom.xml file, in the <dependencies> section. Each dependency has the following structure: <dependency> <groupId>org.apache.maven</groupId> <artifactId>maven-core</artifactId> <version>3.0</version> <scope>compile</scope> </dependency> The scope property is optional and the default value is compile . The dependencies are automatically downloaded if we run a maven goal that requires them (e.g., compile , test , install ). If we want to download them manually, we can use the dependency:resolve goal. To list all dependencies of a project, we can use the dependency:list goal. Using dependencies distributed as a jar \u00b6 Sometimes, we possess a jar file that is not present in any maven repository. We can still install it to the local repository as a new artifact using the install:install-file goal: mvn install:install-file -Dfile=<PATH TO JAR> -DgroupId=<GROUP ID> -DartifactId=<ARTEIFACT ID> -Dversion=<VERSION> -Dpackaging=jar Here the <PATH TO JAR> is a path to the jar library. The <GROUP ID> , <ARTEIFACT ID> , and <VERSION> can have arbitrary values, they just have to correspond with the values specified in the pom dependency. Be careful with PowerShell: in PowerShell, program parameters starting with minus and containing dot need to be quoted, for example: '-DgroupId=com.example.app' instead of -DgroupId=com.example.app Using dependencies distributed as a zip \u00b6 Sometimes, the dependency is distributed as a zip file containing sources. Typically, the manual for such a library tells us to modify the classpath to include the sources. However, Maven handles the classpath automatically, so it is not wise to modify it manually. The best way to use it is to unpack the zip and add the sources to the project during compilation. For that, we can use the build-helper-maven-plugin plugin: <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>build-helper-maven-plugin</artifactId> <version>3.2.0</version> <executions> <execution> <id>add-lib-sources</id> <phase>generate-sources</phase> <goals> <goal>add-source</goal> </goals> <configuration> <sources> <source>path/to/lib/folder</source> </sources> </configuration> </execution> </executions> </plugin> Tests \u00b6 Tests are usually executed with the Maven Surefire plugin using the test goal. Test configuration \u00b6 Tests are run using the Maven Surefire plugin . Usually, the default configuration is enough. However, if we want to customize the test execution, we can add the plugin to our pom and configure it: <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>3.0.0-M5</version> <configuration> ... configuration ... </configuration> </plugin> </plugins> ... other plugins ... </build> For example, to exclude some tests, the following configuration can be used: <configuration> <excludes> <exclude>**/IncompleteTest.java</exclude> </excludes> </configuration> Run a single tests file \u00b6 To run a single test file, use: mvn test -Dtest=\"<TEST CLASS NAME>\" The name should be just a class name without the package and without file extension: mvn test -Dtest=\"OSMFileTest\" We can also use a fully qualified name, if there are more test classes with the same name: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.OSMFileTest\" Run tests using a pattern \u00b6 More test can be run with a pattern, for example: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.**\" runs all tests within the input package. Execute Programs from Maven \u00b6 For executing programs, Maven has the exec plugin. This plugin has two goals: exec:java : for Java programs exec:exec : for any program However, the exec:java goal is not very flexible. It uses the same JVM as the calling Maven process, so the JVM cannot be configured in any way. Notably, with the exec:java goal, it is not possible to: pass JVM arguments like -Xmx set the library path using -Djava.library.path Note that when working with PowerShell, we typically encounter the problem with arguments that start with - and contain a dot. For guide how to solve this, refer to the PowerShell manual . exec:java \u00b6 Basic example: mvn exec:java -Dexec.mainClass=test.Main Other usefull arguments: - -Dexec.args=\"arg1 arg2 arg3\" exec:exec \u00b6 Basic example: mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"Xmx30g -classpath %classpath test.Main\" The -classpath %classpath argument is obligatory and it is used to pass the project classpath to the program. Note that here, the -Dexec.args parameter is used both for vm and program arguments. The order is: JVM arguments, classpath main class program arguments Example: mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"-Xmx30g -classpath %classpath test.Main arg1 arg2\" We can also use -Dexec.mainClass with exec:exec , but we need to refer it in the -classpath argument. The following three maven commands run the same Java program: mvn exec:java -Dexec.mainClass=test.Main mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"-classpath %classpath test.Main\" mvn exec:exec -Dexec.executable=\"java\" -Dexec.mainClass=test.Main -Dexec.args=\"-classpath %classpath ${exec.mainClass}\" Configure the exec plugin in the pom \u00b6 We can add a configuration of the exec plugin to the pom.xml , so we do not have to type the arguments or the main class every time we run the program. However, this way, we have to supply all program arguments in the pom.xml file. It is not possible to pass some parameters from the command line and some from the pom.xml file configuration. Note that when using the exec:ecec goal, this includes the JVM arguments as well. HTTPS certificates \u00b6 Sometimes, it can happen that maven cannot connect to a repository with this error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target This error signals that the server SSL certificate of the maven repo (when using HTTPS) is not present in the local SSL certificate keystore. This can have two reasons, to disintiguish between them, try to access the repo from your browser over https: If you can access the repo from your browser, it means that the server has a valid SSL certificate, but it is not in zour local keystore (just in the browser keystore). You can solve this problem by adding the certificate to your java SSL keystore (see below). if you cannot access the server from your browser, it is likely that the server does not have a valid SSL certificate, and you have to solve it on the serer side. Adding a new SSL certificate to your keystore \u00b6 Open the repo URL in your browser Export the certificate: Chrome: click on the padlock icon left to address in address bar, select Certificate -> Details -> Copy to File and save in format \"Der-encoded binary, single certificate\". Firefox: click on HTTPS certificate chain (the lock icon right next to URL address). Click more info -> security -> show certificate -> details -> export.. . Pickup the name and choose file type *.cer Determine the keystore location: 3.1 Using maven --version , find out the location of the java used by Maven 3.2 The keystore is saved in file: <JAVA LOCATION>/lib/security/cacerts Open console as administrator and add the certificate to the keystore using: keytool -import -keystore \"<PATH TO cacerts>\" -file \"PATH TO TH EXPORTED *.cer FILE\" You can check that the operation was sucessful by listing all certificates: keytool -keystore \"<PATH TO cacerts>\" -list Debugging maven \u00b6 First, try to look at the versions of related dependencies and plugins. Old versions of these can cause many problems. No tests found using the Dtest argument of the test goal \u00b6 Check the class name/path/pattern If the name works, but pattern does not, it can be caused by an old version of the surefire plugin that use a different patten syntax. uncompilable source code \u00b6 Try to clean and compile again Project Publishing \u00b6 This section s focused on Maven projects but most of the steps should apply to all Java projects. Steps: Cleanup Add License to all files Test Update Changelog Update pom of the project and related projects Install Locally Deploy Cleanup \u00b6 The following cleanup steps should be done: clean garbage/IDE files check that they are in gitignore remove already tracked files by git rm -r --cached <path> remove unused imports In Netbeans: Refactor -> Inspect and Transform -> browse -> imports -> organize imports Add license information: \u00b6 There is a nice program called licenseheaders that can be used for that. Unfortunatelly, the program is currently broken and a fork must be used . Syntax: licenseheaders -t <license type> -o \"<Owner>\" -cy -n <Software name> -u \"<link to github>\" A full path to a license template has to be given. Updating poms \u00b6 In general, it is desirable to update the version of each related project from SNAPSHOT to the release version and then test the whole setup. In complex setups, we need to: Change the version of the parent project Change the version of the project's dependencies Change the version of the project itself Update the versions of projects in parent pom Update the versions of projects in the dependency section of each related project (if this is not done by the parent pom) If we fullfill all these steps, we can deploy the project. Both SNAPSHOTs and releases can be deployed using the deploy goal. The repository is chosen based on the version of the artifact (SNAPSHOT or release). Deploying the project to Maven Central \u00b6 Deploying to Maven Central has many advantages: it is free, reliable, and every maven user can get our artifact without any additional configuration. However, it is also a bit complicated due to security requirements and high quality standards. The process is usually as follows: namespace (groupId) registration deploying of artifacts to the namespace Note that for one group id registration, we can deploy multiple artifacts, as long as the registered group id is equal to, or a prefix of, the group id of the artifact . The whole process is described in detail in the Central Repository Documentation . As of 2024-02-15, there is an ongoing migration from OSSRH to the new Central Portal. Because of that, we will describe the current (OSSRH) process only briefly, as it is likely to change in the future. Also, we will describe only the process for deploying artifacts, as the namespace registration in OSSRH is not possible anymore. To see your currently registered namespaces, go to the your profile at sonatype Jira portal and look insides the issues listed on right in the Activity Stream. Deploying artifacts to OSSRH \u00b6 Requirements: required metadata in the pom javadoc and sources must be supplied all files must be signed with GPG various plugins and configuration need to be set up in the pom user authentication for the OSSRH must be set up in the settings.xml file Required metadata \u00b6 Official documentation (at the end) The following metadata must be present in the pom: project tags: name description url license information: ```XML MIT License https://opensource.org/licenses/MIT developer information: XML <developers> <developer> <name>Full Name</name> <email>user email</email> <url>user url</url> </developer> </developers> scm information: XML <scm> <connection>scm:git:git://github.com/simpligility/ossrh-demo.git</connection> <developerConnection>scm:git:ssh://github.com:simpligility/ossrh-demo.git</developerConnection> <url>http://github.com/simpligility/ossrh-demo/tree/master</url> </scm> Javadoc and sources setup \u00b6 The following configuration is suitable for deploying with sources and javadoc: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-source-plugin</artifactId> <version>2.2.1</version> <executions> <execution> <id>attach-sources</id> <goals> <goal>jar-no-fork</goal> </goals> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-javadoc-plugin</artifactId> <version>2.9.1</version> <executions> <execution> <id>attach-javadocs</id> <goals> <goal>jar</goal> </goals> </execution> </executions> </plugin> Signing the artifacts with GPG \u00b6 To sign the artifacts with GPG, we need to add the gp plugin to the pom: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-gpg-plugin</artifactId> <version>1.5</version> <executions> <execution> <id>sign-artifacts</id> <phase>verify</phase> <goals> <goal>sign</goal> </goals> </execution> </executions> </plugin> For this plugin to work, the GPG must be installed and available in the system path. Storing the GPG passphrase \u00b6 To make our life easier, we can store the GPG passphrase in the settings.xml file. The following configuration must be present in the profiles section: <profile> <id>ossrh</id> <activation> <activeByDefault>true</activeByDefault> </activation> <properties> <gpg.executable>gpg</gpg.executable> <!-- this should not be needed as gpg is default... --> <gpg.passphrase>F1D06949</gpg.passphrase> </properties> </profile> Required plugins and configuration \u00b6 For the deployment to OSSRH, we need to use several plugins and add some configuration to the pom. First, we need to set up the distribution management: <distributionManagement> <snapshotRepository> <id>ossrh</id> <url><SNAPSHOT URL></url> </snapshotRepository> <repository> <id>ossrh</id> <url><RELEASE URL></url> </repository> </distributionManagement> The and are links to the repo we have received after the namespace registration approval. Note that we need to use the original URLs, even if they do not match the links in the documentation. The current (2024-02-15) URLs are: https://s01.oss.sonatype.org/content/repositories/snapshots https://s01.oss.sonatype.org/service/local/staging/deploy/maven2/ Next, we need a staging plugin to deploy the artifacts to the OSSRH staging repository. The plugin is configured as follows: <plugin> <groupId>org.sonatype.plugins</groupId> <artifactId>nexus-staging-maven-plugin</artifactId> <version><latest version here></version> <extensions>true</extensions> <configuration> <serverId>ossrh</serverId> <nexusUrl><REPO ROOT URL></nexusUrl> <autoReleaseAfterClose>true</autoReleaseAfterClose> </configuration> </plugin> Here, the <REPO ROOT URL> is the root URL of the repositories above, e.g., for recent versions, it is https://s01.oss.sonatype.org/ . Setting up user authentication for the OSSRH \u00b6 The following configuration must be present in the settings.xml file: <servers> <server> <id>ossrh</id> <username>your username</username> <password>your password</password> </server> </servers> Profiles \u00b6 Maven profiles can be used to supply different configurations in a single pom.xml file. This can be used for example to: support different build environments (e.g., development, testing, production) support different build targets (e.g., different Java versions) support different build configurations (e.g., optional dependencies) Profiles are defined in the profiles section of the pom.xml file. Each profile has a unique id and can contain any configuration that can be present in the pom.xml file. To manually activate a profile, we can use the -P argument of the mvn command: mvn <goal> -P <profile id> Note that the profile needs to be selected for all relevant goals, not just for the compilation . For example, if we have and optional dependency in the profile, we need to select the profile for compile , but also for exec , install , etc, otherwise the dependency will not be found at runtime. Displaying active profiles \u00b6 To display the active profiles, use the following command: mvn help:active-profiles Note that again, this will only show the profiles that are activated in the settings.xml file or the profiles that are activated by default. To test an optional profile, we need to activate it even for the help:active-profiles goal: mvn help:active-profiles -P <profile id> Creating maven plugins \u00b6 Official guide Maven plugins can be created as maven projects. Specifics pom configuration: The packaging is maven-plugin The name of the plugin is <artifactId>-maven-plugin among the dependencies, there should be maven-plugin-api and maven-plugin-annotations To use a cclass method as an entry point for a plugin goal, we annotate it with @Mojo(name = \"<goal name>\") . Testing a maven plugin \u00b6 The best way to test the plugin is to run it separately, even if it should be later bound to a lifecycle phase. To run the plugin, we use the following syntax: mvn <group id>:<artifact id>:<goal name> # Example: mvn com.test:example-maven-plugin:example This is indeed very verbose. To shorten it, we can add a special configuration to the system settings.xml file: <pluginGroups> <pluginGroup>com.test</pluginGroup> </pluginGroups> Now, we can run the plugin using: mvn example:example Debugging a maven plugin in IntelliJ IDEA \u00b6 Maven plugins can be easily debugged in IntelliJ IDEA. However, it is important to understand that unlike maven plugins are run from the local repository, not from the project source code. Therefore, we need to install the plugin after each change . Make the plugin runnable from any directory \u00b6 Normally, the goals can be run only from project directories (where the pom.xml is present). To make the plugin runnable from any directory, we need to annotate the goal with @requiresProject = false : @Mojo(name = \"example\", requiresProject = false) Various useful tasks \u00b6 Displaying the classpath \u00b6 To display the classpath, use the following command: mvn dependency:build-classpath","title":"Maven"},{"location":"Programming/Java/Maven/#maven-packages-artifacts","text":"All Maven dependencies and plugins installed in local system repositories can be either: downloaded from a remote repository installed from a local file or project Note that if we want the artifact to be downloaded from a remote repository, we do not have to take any action. It will be downloaded automatically when needed. By default, Maven searches for artifacts in the Maven Central repository . If we want to use a different repository, we have to add it manually.","title":"Maven Packages: Artifacts"},{"location":"Programming/Java/Maven/#list-locally-installed-artifacts","text":"There is no built-in command to list all locally installed artifacts. However, we can use the info plugin : mvn ninja.fido:info-maven-plugin:list","title":"List locally installed artifacts"},{"location":"Programming/Java/Maven/#install-artifacts-from-a-remote-repository-manually","text":"If we want to install an artifact from a remote repository manually, we can use the dependency:get goal. The syntax is as follows: mvn dependency:get -Dartifact=<GROUP ID>:<ARTIFACT ID>:<VERSION>","title":"Install artifacts from a remote repository manually"},{"location":"Programming/Java/Maven/#debugging-missing-artifacts","text":"All Maven dependencies should work out of the box. If some dependencies cannot be resolved: check that the dependencies are on the maven central. if not, check that they are in some special repo and check that the repo is present in the pom of the project that requires the dependency","title":"Debugging missing Artifacts"},{"location":"Programming/Java/Maven/#variables","text":"Variables in the pom.xml file can be used using the ${variable} syntax. To use some environment variables, we can use the ${env.variable} . Note that contrary to all expectations, maven goals do not fail if a variable is not defined . Instead, the variable is replaced with an empty string. This can lead to unexpected behavior, so it is usually preferable to fail. For that, we can use the enforcer plugin with the requireProperty rule: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-enforcer-plugin</artifactId> <version>3.4.1</version> <executions> <execution> <id>enforce-property</id> <goals> <goal>enforce</goal> </goals> <configuration> <rules> <requireProperty> <property>basedir</property> <message>You must set a basedir property!</message> </requireProperty> </rules> <fail>true</fail> </configuration> </execution> </executions> </plugin>","title":"Variables"},{"location":"Programming/Java/Maven/#compilation","text":"reference Compilation is handeled by the Maven compiler plugin. Usually, typing mvn compile is enough to compile the project. If you need any customization, add it to the compiler plugin configuration <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> .... </configuration> </plugin>","title":"Compilation"},{"location":"Programming/Java/Maven/#passing-arguments-to-javac","text":"A single argument can be passed using the <compilerArgument> property. For more compiler arguments, use <compilerArgs> : <compilerArgs> <arg>-Xmaxerrs=1000</arg> <arg>-Xlint</arg> </compilerArgs>","title":"Passing arguments to javac"},{"location":"Programming/Java/Maven/#dependencies","text":"Maven dependencies are defined in the pom.xml file, in the <dependencies> section. Each dependency has the following structure: <dependency> <groupId>org.apache.maven</groupId> <artifactId>maven-core</artifactId> <version>3.0</version> <scope>compile</scope> </dependency> The scope property is optional and the default value is compile . The dependencies are automatically downloaded if we run a maven goal that requires them (e.g., compile , test , install ). If we want to download them manually, we can use the dependency:resolve goal. To list all dependencies of a project, we can use the dependency:list goal.","title":"Dependencies"},{"location":"Programming/Java/Maven/#using-dependencies-distributed-as-a-jar","text":"Sometimes, we possess a jar file that is not present in any maven repository. We can still install it to the local repository as a new artifact using the install:install-file goal: mvn install:install-file -Dfile=<PATH TO JAR> -DgroupId=<GROUP ID> -DartifactId=<ARTEIFACT ID> -Dversion=<VERSION> -Dpackaging=jar Here the <PATH TO JAR> is a path to the jar library. The <GROUP ID> , <ARTEIFACT ID> , and <VERSION> can have arbitrary values, they just have to correspond with the values specified in the pom dependency. Be careful with PowerShell: in PowerShell, program parameters starting with minus and containing dot need to be quoted, for example: '-DgroupId=com.example.app' instead of -DgroupId=com.example.app","title":"Using dependencies distributed as a jar"},{"location":"Programming/Java/Maven/#using-dependencies-distributed-as-a-zip","text":"Sometimes, the dependency is distributed as a zip file containing sources. Typically, the manual for such a library tells us to modify the classpath to include the sources. However, Maven handles the classpath automatically, so it is not wise to modify it manually. The best way to use it is to unpack the zip and add the sources to the project during compilation. For that, we can use the build-helper-maven-plugin plugin: <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>build-helper-maven-plugin</artifactId> <version>3.2.0</version> <executions> <execution> <id>add-lib-sources</id> <phase>generate-sources</phase> <goals> <goal>add-source</goal> </goals> <configuration> <sources> <source>path/to/lib/folder</source> </sources> </configuration> </execution> </executions> </plugin>","title":"Using dependencies distributed as a zip"},{"location":"Programming/Java/Maven/#tests","text":"Tests are usually executed with the Maven Surefire plugin using the test goal.","title":"Tests"},{"location":"Programming/Java/Maven/#test-configuration","text":"Tests are run using the Maven Surefire plugin . Usually, the default configuration is enough. However, if we want to customize the test execution, we can add the plugin to our pom and configure it: <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>3.0.0-M5</version> <configuration> ... configuration ... </configuration> </plugin> </plugins> ... other plugins ... </build> For example, to exclude some tests, the following configuration can be used: <configuration> <excludes> <exclude>**/IncompleteTest.java</exclude> </excludes> </configuration>","title":"Test configuration"},{"location":"Programming/Java/Maven/#run-a-single-tests-file","text":"To run a single test file, use: mvn test -Dtest=\"<TEST CLASS NAME>\" The name should be just a class name without the package and without file extension: mvn test -Dtest=\"OSMFileTest\" We can also use a fully qualified name, if there are more test classes with the same name: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.OSMFileTest\"","title":"Run a single tests file"},{"location":"Programming/Java/Maven/#run-tests-using-a-pattern","text":"More test can be run with a pattern, for example: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.**\" runs all tests within the input package.","title":"Run tests using a pattern"},{"location":"Programming/Java/Maven/#execute-programs-from-maven","text":"For executing programs, Maven has the exec plugin. This plugin has two goals: exec:java : for Java programs exec:exec : for any program However, the exec:java goal is not very flexible. It uses the same JVM as the calling Maven process, so the JVM cannot be configured in any way. Notably, with the exec:java goal, it is not possible to: pass JVM arguments like -Xmx set the library path using -Djava.library.path Note that when working with PowerShell, we typically encounter the problem with arguments that start with - and contain a dot. For guide how to solve this, refer to the PowerShell manual .","title":"Execute Programs from Maven"},{"location":"Programming/Java/Maven/#execjava","text":"Basic example: mvn exec:java -Dexec.mainClass=test.Main Other usefull arguments: - -Dexec.args=\"arg1 arg2 arg3\"","title":"exec:java"},{"location":"Programming/Java/Maven/#execexec","text":"Basic example: mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"Xmx30g -classpath %classpath test.Main\" The -classpath %classpath argument is obligatory and it is used to pass the project classpath to the program. Note that here, the -Dexec.args parameter is used both for vm and program arguments. The order is: JVM arguments, classpath main class program arguments Example: mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"-Xmx30g -classpath %classpath test.Main arg1 arg2\" We can also use -Dexec.mainClass with exec:exec , but we need to refer it in the -classpath argument. The following three maven commands run the same Java program: mvn exec:java -Dexec.mainClass=test.Main mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"-classpath %classpath test.Main\" mvn exec:exec -Dexec.executable=\"java\" -Dexec.mainClass=test.Main -Dexec.args=\"-classpath %classpath ${exec.mainClass}\"","title":"exec:exec"},{"location":"Programming/Java/Maven/#configure-the-exec-plugin-in-the-pom","text":"We can add a configuration of the exec plugin to the pom.xml , so we do not have to type the arguments or the main class every time we run the program. However, this way, we have to supply all program arguments in the pom.xml file. It is not possible to pass some parameters from the command line and some from the pom.xml file configuration. Note that when using the exec:ecec goal, this includes the JVM arguments as well.","title":"Configure the exec plugin in the pom"},{"location":"Programming/Java/Maven/#https-certificates","text":"Sometimes, it can happen that maven cannot connect to a repository with this error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target This error signals that the server SSL certificate of the maven repo (when using HTTPS) is not present in the local SSL certificate keystore. This can have two reasons, to disintiguish between them, try to access the repo from your browser over https: If you can access the repo from your browser, it means that the server has a valid SSL certificate, but it is not in zour local keystore (just in the browser keystore). You can solve this problem by adding the certificate to your java SSL keystore (see below). if you cannot access the server from your browser, it is likely that the server does not have a valid SSL certificate, and you have to solve it on the serer side.","title":"HTTPS certificates"},{"location":"Programming/Java/Maven/#adding-a-new-ssl-certificate-to-your-keystore","text":"Open the repo URL in your browser Export the certificate: Chrome: click on the padlock icon left to address in address bar, select Certificate -> Details -> Copy to File and save in format \"Der-encoded binary, single certificate\". Firefox: click on HTTPS certificate chain (the lock icon right next to URL address). Click more info -> security -> show certificate -> details -> export.. . Pickup the name and choose file type *.cer Determine the keystore location: 3.1 Using maven --version , find out the location of the java used by Maven 3.2 The keystore is saved in file: <JAVA LOCATION>/lib/security/cacerts Open console as administrator and add the certificate to the keystore using: keytool -import -keystore \"<PATH TO cacerts>\" -file \"PATH TO TH EXPORTED *.cer FILE\" You can check that the operation was sucessful by listing all certificates: keytool -keystore \"<PATH TO cacerts>\" -list","title":"Adding a new SSL certificate to your keystore"},{"location":"Programming/Java/Maven/#debugging-maven","text":"First, try to look at the versions of related dependencies and plugins. Old versions of these can cause many problems.","title":"Debugging maven"},{"location":"Programming/Java/Maven/#no-tests-found-using-the-dtest-argument-of-the-test-goal","text":"Check the class name/path/pattern If the name works, but pattern does not, it can be caused by an old version of the surefire plugin that use a different patten syntax.","title":"No tests found using the Dtest argument of the test goal"},{"location":"Programming/Java/Maven/#uncompilable-source-code","text":"Try to clean and compile again","title":"uncompilable source code"},{"location":"Programming/Java/Maven/#project-publishing","text":"This section s focused on Maven projects but most of the steps should apply to all Java projects. Steps: Cleanup Add License to all files Test Update Changelog Update pom of the project and related projects Install Locally Deploy","title":"Project Publishing"},{"location":"Programming/Java/Maven/#cleanup","text":"The following cleanup steps should be done: clean garbage/IDE files check that they are in gitignore remove already tracked files by git rm -r --cached <path> remove unused imports In Netbeans: Refactor -> Inspect and Transform -> browse -> imports -> organize imports","title":"Cleanup"},{"location":"Programming/Java/Maven/#add-license-information","text":"There is a nice program called licenseheaders that can be used for that. Unfortunatelly, the program is currently broken and a fork must be used . Syntax: licenseheaders -t <license type> -o \"<Owner>\" -cy -n <Software name> -u \"<link to github>\" A full path to a license template has to be given.","title":"Add license information:"},{"location":"Programming/Java/Maven/#updating-poms","text":"In general, it is desirable to update the version of each related project from SNAPSHOT to the release version and then test the whole setup. In complex setups, we need to: Change the version of the parent project Change the version of the project's dependencies Change the version of the project itself Update the versions of projects in parent pom Update the versions of projects in the dependency section of each related project (if this is not done by the parent pom) If we fullfill all these steps, we can deploy the project. Both SNAPSHOTs and releases can be deployed using the deploy goal. The repository is chosen based on the version of the artifact (SNAPSHOT or release).","title":"Updating poms"},{"location":"Programming/Java/Maven/#deploying-the-project-to-maven-central","text":"Deploying to Maven Central has many advantages: it is free, reliable, and every maven user can get our artifact without any additional configuration. However, it is also a bit complicated due to security requirements and high quality standards. The process is usually as follows: namespace (groupId) registration deploying of artifacts to the namespace Note that for one group id registration, we can deploy multiple artifacts, as long as the registered group id is equal to, or a prefix of, the group id of the artifact . The whole process is described in detail in the Central Repository Documentation . As of 2024-02-15, there is an ongoing migration from OSSRH to the new Central Portal. Because of that, we will describe the current (OSSRH) process only briefly, as it is likely to change in the future. Also, we will describe only the process for deploying artifacts, as the namespace registration in OSSRH is not possible anymore. To see your currently registered namespaces, go to the your profile at sonatype Jira portal and look insides the issues listed on right in the Activity Stream.","title":"Deploying the project to Maven Central"},{"location":"Programming/Java/Maven/#deploying-artifacts-to-ossrh","text":"Requirements: required metadata in the pom javadoc and sources must be supplied all files must be signed with GPG various plugins and configuration need to be set up in the pom user authentication for the OSSRH must be set up in the settings.xml file","title":"Deploying artifacts to OSSRH"},{"location":"Programming/Java/Maven/#required-metadata","text":"Official documentation (at the end) The following metadata must be present in the pom: project tags: name description url license information: ```XML MIT License https://opensource.org/licenses/MIT developer information: XML <developers> <developer> <name>Full Name</name> <email>user email</email> <url>user url</url> </developer> </developers> scm information: XML <scm> <connection>scm:git:git://github.com/simpligility/ossrh-demo.git</connection> <developerConnection>scm:git:ssh://github.com:simpligility/ossrh-demo.git</developerConnection> <url>http://github.com/simpligility/ossrh-demo/tree/master</url> </scm>","title":"Required metadata"},{"location":"Programming/Java/Maven/#javadoc-and-sources-setup","text":"The following configuration is suitable for deploying with sources and javadoc: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-source-plugin</artifactId> <version>2.2.1</version> <executions> <execution> <id>attach-sources</id> <goals> <goal>jar-no-fork</goal> </goals> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-javadoc-plugin</artifactId> <version>2.9.1</version> <executions> <execution> <id>attach-javadocs</id> <goals> <goal>jar</goal> </goals> </execution> </executions> </plugin>","title":"Javadoc and sources setup"},{"location":"Programming/Java/Maven/#signing-the-artifacts-with-gpg","text":"To sign the artifacts with GPG, we need to add the gp plugin to the pom: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-gpg-plugin</artifactId> <version>1.5</version> <executions> <execution> <id>sign-artifacts</id> <phase>verify</phase> <goals> <goal>sign</goal> </goals> </execution> </executions> </plugin> For this plugin to work, the GPG must be installed and available in the system path.","title":"Signing the artifacts with GPG"},{"location":"Programming/Java/Maven/#storing-the-gpg-passphrase","text":"To make our life easier, we can store the GPG passphrase in the settings.xml file. The following configuration must be present in the profiles section: <profile> <id>ossrh</id> <activation> <activeByDefault>true</activeByDefault> </activation> <properties> <gpg.executable>gpg</gpg.executable> <!-- this should not be needed as gpg is default... --> <gpg.passphrase>F1D06949</gpg.passphrase> </properties> </profile>","title":"Storing the GPG passphrase"},{"location":"Programming/Java/Maven/#required-plugins-and-configuration","text":"For the deployment to OSSRH, we need to use several plugins and add some configuration to the pom. First, we need to set up the distribution management: <distributionManagement> <snapshotRepository> <id>ossrh</id> <url><SNAPSHOT URL></url> </snapshotRepository> <repository> <id>ossrh</id> <url><RELEASE URL></url> </repository> </distributionManagement> The and are links to the repo we have received after the namespace registration approval. Note that we need to use the original URLs, even if they do not match the links in the documentation. The current (2024-02-15) URLs are: https://s01.oss.sonatype.org/content/repositories/snapshots https://s01.oss.sonatype.org/service/local/staging/deploy/maven2/ Next, we need a staging plugin to deploy the artifacts to the OSSRH staging repository. The plugin is configured as follows: <plugin> <groupId>org.sonatype.plugins</groupId> <artifactId>nexus-staging-maven-plugin</artifactId> <version><latest version here></version> <extensions>true</extensions> <configuration> <serverId>ossrh</serverId> <nexusUrl><REPO ROOT URL></nexusUrl> <autoReleaseAfterClose>true</autoReleaseAfterClose> </configuration> </plugin> Here, the <REPO ROOT URL> is the root URL of the repositories above, e.g., for recent versions, it is https://s01.oss.sonatype.org/ .","title":"Required plugins and configuration"},{"location":"Programming/Java/Maven/#setting-up-user-authentication-for-the-ossrh","text":"The following configuration must be present in the settings.xml file: <servers> <server> <id>ossrh</id> <username>your username</username> <password>your password</password> </server> </servers>","title":"Setting up user authentication for the OSSRH"},{"location":"Programming/Java/Maven/#profiles","text":"Maven profiles can be used to supply different configurations in a single pom.xml file. This can be used for example to: support different build environments (e.g., development, testing, production) support different build targets (e.g., different Java versions) support different build configurations (e.g., optional dependencies) Profiles are defined in the profiles section of the pom.xml file. Each profile has a unique id and can contain any configuration that can be present in the pom.xml file. To manually activate a profile, we can use the -P argument of the mvn command: mvn <goal> -P <profile id> Note that the profile needs to be selected for all relevant goals, not just for the compilation . For example, if we have and optional dependency in the profile, we need to select the profile for compile , but also for exec , install , etc, otherwise the dependency will not be found at runtime.","title":"Profiles"},{"location":"Programming/Java/Maven/#displaying-active-profiles","text":"To display the active profiles, use the following command: mvn help:active-profiles Note that again, this will only show the profiles that are activated in the settings.xml file or the profiles that are activated by default. To test an optional profile, we need to activate it even for the help:active-profiles goal: mvn help:active-profiles -P <profile id>","title":"Displaying active profiles"},{"location":"Programming/Java/Maven/#creating-maven-plugins","text":"Official guide Maven plugins can be created as maven projects. Specifics pom configuration: The packaging is maven-plugin The name of the plugin is <artifactId>-maven-plugin among the dependencies, there should be maven-plugin-api and maven-plugin-annotations To use a cclass method as an entry point for a plugin goal, we annotate it with @Mojo(name = \"<goal name>\") .","title":"Creating maven plugins"},{"location":"Programming/Java/Maven/#testing-a-maven-plugin","text":"The best way to test the plugin is to run it separately, even if it should be later bound to a lifecycle phase. To run the plugin, we use the following syntax: mvn <group id>:<artifact id>:<goal name> # Example: mvn com.test:example-maven-plugin:example This is indeed very verbose. To shorten it, we can add a special configuration to the system settings.xml file: <pluginGroups> <pluginGroup>com.test</pluginGroup> </pluginGroups> Now, we can run the plugin using: mvn example:example","title":"Testing a maven plugin"},{"location":"Programming/Java/Maven/#debugging-a-maven-plugin-in-intellij-idea","text":"Maven plugins can be easily debugged in IntelliJ IDEA. However, it is important to understand that unlike maven plugins are run from the local repository, not from the project source code. Therefore, we need to install the plugin after each change .","title":"Debugging a maven plugin in IntelliJ IDEA"},{"location":"Programming/Java/Maven/#make-the-plugin-runnable-from-any-directory","text":"Normally, the goals can be run only from project directories (where the pom.xml is present). To make the plugin runnable from any directory, we need to annotate the goal with @requiresProject = false : @Mojo(name = \"example\", requiresProject = false)","title":"Make the plugin runnable from any directory"},{"location":"Programming/Java/Maven/#various-useful-tasks","text":"","title":"Various useful tasks"},{"location":"Programming/Java/Maven/#displaying-the-classpath","text":"To display the classpath, use the following command: mvn dependency:build-classpath","title":"Displaying the classpath"},{"location":"Programming/Python/Matplotlib%20Manual/","text":"","title":"Matplotlib Manual"},{"location":"Programming/Python/Pandas%20Manual/","text":"Main principles \u00b6 Pandas extensively uses the term axis. In Pandas, axis 0 is vertical (rows) and axis 1 is horizontal (columns). Pandas Data Types \u00b6 Documentation of basic data types Mainly, pandas uses the numpy data types . Object \u00b6 If pandas does not recognize the type of the column, or there are multiple types in the column, it uses the object type. However this may sound like a wonderful solution, it causes many problems, so be sure to avoid object type columns at all costs. Typically, the problem arises when we try to apply a vector operation to the column: we round a column with mix of floats and ints: fail ( loop of ufunc does not support argument 0 of type float which has no callable rint method ) we need to apply string functions, but the column contains numbers as well The solution is usually: fill the missing values with the fillna function convert the column to str type using the astype function apply string functions to clear the data convert the column to the desired type Categorical data \u00b6 Sometimes, it can be usefull to treat a column as a categorical variable instead of a string or a number. For that, we can use the Categorical class. Typically, we: define the categorical type: python cat_type = pd.Categorical(categories=['a', 'b', 'c']) use the categorical type to, e.g., convert a column to the categorical type: python df['col'] = df['col'].astype(cat_type) Important parameters of the Categorical constructor: categories : By default, the categories are inferred from the data. However, we can specify the categories explicitly using the categories parameter. When using this parameter, each value is converted to the matching category specified in the list. Note that the category must to be an exact match, including the data type, case (when using strings), etc . Unmatched values are converted to NaN . ordered : if True , the categories are ordered in the order of the categories parameter. Reading categorical data from a file \u00b6 We can read data into categories using the dtype parameter. However, this way, we cannot specify the categories explicitly. To do that, we need to read the data into the correct data type, and then convert the column/index to the categorical type after the dataframe is created. Datetime \u00b6 Pandas has a special type for datetime values. One of its dangerous properties is that zero parts of the datetime are truncated both when displaying and on export: df = pd.DataFrame({'date': pd.to_datetime(['2021-01-01 00:00:00', '2021-01-01 00:00:00'])}) print(df) # output: # '2021-01-01' # '2021-01-01' Creating a DataFrame \u00b6 The DataFrame class has a constructor that supports multiple formats of input data as well as many configuration parameters. Therefore , for most formats of input data, we can create a dataframe using the constructor. However, we can also crete a dataframe using the from_* functions, and for some formats, these functions are the only way to create a dataframe. From a dictionary \u00b6 When having a dictionary, we can choose between two options the constructor and the from_dict function. The required syntax depend on the shape of the dictionary with respect to the required dataframe. Keys are column names, values are list of column values \u00b6 df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) # or equivalently df.DataFrame.from_dict({'col1': [1, 2], 'col2': [3, 4]}) Note that the values of the dictionary have to be lists. If we have a dictionary with values that are not lists (i.e., only one row), we have to use the orient parameter to specify the orientation of the data and then transpose the dataframe: d = {'col1': 1, 'col2': 2} df = pd.DataFrame.from_dict(d, orient='index').T # or equivalently df = pd.DataFrame([d], columns=d.keys()) Keys are indices, values are values of a single column \u00b6 df = pd.DataFrame.from_dict({'row1': 1, 'row2': 2}, orient='index', columns=['Values']) Keys are indices, values are values of single row \u00b6 df = pd.DataFrame.from_dict({'row1': [1, 2], 'row2': [3, 4]}, orient='index') Keys are one column, values are another column \u00b6 d = {'row1 col1': 'row1 col2', 'row2 col1': 'row2 col2' df = pd.DataFrame.from_dict(d.items()) # or equivalently df = pd.DataFrame({'col1': d.keys(), 'col2': d.values()}) From a list of dictionaries \u00b6 df = pd.DataFrame([{'col1': 1, 'col2': 3}, {'col1': 2, 'col2': 4}]) From a list of lists \u00b6 df = pd.DataFrame([[1, 3], [2, 4]], columns=['col1', 'col2']) Creating a zero or constant-filled dataframe \u00b6 To create a dataframe filled with a constant, we can use the dataframe constructor and pass the constant as the first (data) argument: df = pd.DataFrame(0, index=range(10), columns=['col1', 'col2']) Generating the index \u00b6 As displayed in the above example, we can generate a numerical index using the range function. However, there are more options: date index with date_range pd.date_range(<start date>, <end date>, freq=<frequency>) Determining the data type \u00b6 By defualt, Pandas infers the data type of the columns by the content. However, this has some limitations: The data are processed line by line to avoid excessive memory usage. Therefore, if the data contains a value not compatible with the type inferred from the first batch of data, the previously processed data have to be processed again. If the data are not complete or valid, the data is typically inferred as object type, instead of reporting an error. To specify the data type of the columns we proceed according to the desired data type: for datetime data, we use the parse_dates parameter of the read_csv function for other data types , we use the dtype parameter of the read_csv function. Obtaining info about dataset \u00b6 For a DataFrame df : column names: df.columns column types: df.dtypes number of rows: len(df) Iteration \u00b6 Standard Iteration \u00b6 https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas Iteration without modifying the dataframe. From the fastest to the slowest. Vector operations \u00b6 List Comprehensions \u00b6 Apply \u00b6 The apply function can be used to apply a function to each row or column of the dataframe. For iterating over rows, we need to set the axis parameter to 1. Example: df['new_col'] = df.apply(lambda row: row['col1'] + row['col2'], axis=1) itertuples() \u00b6 https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.html Returns dataframe rows as pandas named tuples with index as the first member of the tuple. iterrows() \u00b6 Documentation returns a tuple (index, data) it does not preserve the dtype items() \u00b6 Documentation Iterates over columns Iteration with modification \u00b6 For modification, the best strategy is to: select what we want to modify (see selection ) modify the selection with the assignment operator. The right side of the assignment operator can be the result of an iteration. Iteration pairs of consecutive rows \u00b6 We can use the zip function to iterate over pairs of consecutive rows. For example, if we want to access each row as a Series , we can use the iterrows function with zip : for row1, row2 in zip(df[:-1].iterrows(), df[1:].iterrows()): print(row1[1], row2[1]) # row1[0] and row2[0] are the indices of the rows... Filtration \u00b6 filtered = df[df['max_delay'] == x] or equivalently: filtered = df[df.max_delay == x] Filtration by Multiple Columns \u00b6 Example: filtered = df[(df['max_delay'] == x) & (df['exp_length'] == y)] Accept multiple values \u00b6 For that, we can use the isin function: filtered = df[df['max_delay'].isin([x, y])] Using the query function \u00b6 The query function can be used for more complicated filters. It is more flexible and the syntax is less verbose. The above filter can be rewriten as: filtered = df.query('max_delay == x and exp_length == y'] Special filter functions can be used in the query function as well: filtered = df.query('max_delay == x and exp_length.isnull()') Filtering Series \u00b6 A seris can be filtered even simpler then the dataframe: s = df['col'] sf = s[s <= 10] # now we have a Series with values from df['col'] less than 10 Useful filter functions \u00b6 null values: <column selection>.isnull() non null/nan values: <column selection>.notnull() filtring using the string value: <column selection>.str.<string function> filtering dates: <column selection>.dt.<date function> Selection \u00b6 If we want to select a part of the dataframe (a set of rows and columns) independently of the values of the dataframe (for that, see filtration ), we can use these methods: loc : select by index, works for both rows and columns iloc : select by position, works for both rows and columns [] : select by index, works only for columns There are also other methods that works for selection but does not work for setting values, such as: xs : select by label, works for both rows and columns The return type of the selection is determined by the number of selected rows and columns. For a single row or column, the result is a series, for multiple rows and columns, the result is a dataframe. If we want to get a dataframe for a single row or column, we can use the [] operator with a list of values: df[['col1']] # or df.loc[['row1']] # or df.iloc[[0]] loc \u00b6 The operator loc has many possible input parameters, the most common syntax is df.loc[<row selection>, <column selection>] each selection has the form of <start label>:<end label> . For the whole column, we therefore use: df.loc[:, <column name>] Difference between array operator on dataframe and on loc \u00b6 Both methods can be used both for getting and setting the column: a = df['col'] # or equivalently a = df.loc[:, 'col'] df2['col'] = a # or equivalently df2.loc[:, 'col'] = a The difference between these two methods is apparent when we want to use a chained selection, i.e., selecting from a selection. While the loc selects the appropriate columns in one step, so we know that we still refer to the original dataframe, the array operator operations are separate, and therefore, the result value can refer to a temporary: dfmi.loc[:, ('one', 'second')] = value # we set a value of a part of dfmi dfmi['one']['second'] = value # can be dangerous, we can set value to a temporary This problem is indicated by a SettingWithCopy warning. Sometimes it is not obvious that we use a chain of array operator selections, e.g.: sel = df[['a', 'b']] ..... sel['a'] = ... # we possibly edit a temporary! For more, see the dovumentation . iloc \u00b6 The iloc method works similarly to the loc method, but it uses the position instead of the label. To select more values, we can use the slice syntax df.iloc[<start position>:<end position>:<step>,<collumn slicing...>] Be aware that if the iloc operator selects by single value (e.g.: df.iloc[3] ), it returns the single row as series . To get a dataframe slice, we need to use a list of values (e.g.: df.iloc[[3]] ). Selecting all columns but one \u00b6 If we do not mind copying the dataframe, we can use the drop function. Otherwise, we can use the loc method and supply the filtered column lables obtained using the columns property: df.loc[:, df.columns != '<column to skip>'] Multi-index selection \u00b6 documentation When selecting from a dataframe with a multi-index, things get a bit more complicated. There are three ways how to select from a multi-index dataframe: if we need to match a single value on one level , there is a neet xs function does not support ranges, only specific values python df.xs(<value>, level=<level name or number>) for range selection , we can use loc with slices: simple, but it can be verbose for complex selections python df.loc[(slice(15, 30), slice(None)), ...] for complex selections , we can use loc with IndexSlice object. more readable, but requires the IndexSlice object to be created first. python idx = pd.IndexSlice df.loc[idx[:, 15:30], ...] Using xs \u00b6 The xs function can be used to select from a multi-index dataframe. However, slices (ranges) are not supported. Example: df.xs(15, level=1) # selects all rows with level 1 equal to 15 Using loc \u00b6 The general loc usage is the same as for a single index dataframe: df.loc[<row selection>, <column selection>] However, each selection is now a tuple, where each element of the tuple corresponds to one level of the multi-index: df.loc[(<row selection level 1>, <row selection level 2>, ...), (<column selection level 1>, <column selection level 2>, ...)] Because each selection is a tuple, it is crucial to always specify both dimensions (rows and columns), even if we want to select all columns . Otherwise, part of the row selection may be interpreted as a column selection, leading to a missing key error (as described in the documentation ). Each <selection> can be: a specific value, a list of values, or a slice. Note that we have to use the slice function, as pandas uses the standard slice syntax for something else. We can skip lower levels to select all values from those levels. However, we cannot skip upper levels. If we want to select all values from the upper level, we need to use the slice(None) for that level: df.loc[(slice(None), slice(15, 30)), ...] Note that for multi-index slicing, the index needs to be sorted . If it is not, we can use the sort_index function. pandas slicing documentation . Using loc with IndexSlice for more readable syntax \u00b6 We can obtain the same result with a more readable syntax using the IndexSlice object: idx = pd.IndexSlice dft.loc[idx[:, 15:30], ...] Again, it is crucial to always specify both dimensions even if we want to select all columns : df.loc[idx[:, 15:30],:] # select all columns for rows with index[1] between 15 and 30 df.loc[idx[:, 15:30]] # try to select all rows and columns between 15 and 30 (produces key error most likely) Select row with a maximum value in a column \u00b6 To get the index of the row with the maximum value in a column, we can use the idxmax function: df['col'].idxmax() Then we can use the loc method to get the row. Selecting a single value (cell, scalar) \u00b6 When we select a single value from a dataframe, the result is sometimes a series, especially when we use a filtration. To get a scalar, we can use the item() method: df.loc[<row>, <column>].item() Sorting \u00b6 for sorting the dataframe, we can use the sort_values function. The first argument is the list of columns to sort by, starting with the most important column. Example: df.sort_values(['col1', 'col2']) If we want to use a custom sorting function, we can use the key argument. The key function should satisfy the classical python sorting interface (see Python manual ) and it should be a vector function, i.e., instead of returning a single position for a given value, it should return a vector of positions for a given vector of values. Example: def key_fn(column: list): return [len(x) for x in l] df.sort_values('col', key=key_fn) Working with columns \u00b6 Adding a column \u00b6 The preferable way is to use the assign function: # adds a column named 'instance_length' with constant value result_df_5_nyc_mv.assign(instance_length = 5) Multiple columns can be added at once: trips = trips.assign(dropoff_datetime = 0, dropoff_GPS_lon = 0, dropoff_GPS_lat = 0, pickup_GPS_lon = 0, pickup_GPS_lat = 0) Rename a column \u00b6 To rename a column, we can use the pandas rename function: df.rename(columns={'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}) # or equivalently df.rename({'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}, axis='columns') Rename a Series (column) \u00b6 The column name in the series object is actually the name of the series. To rename the series, we can use the rename function, or we can set the name property of the series: s.rename('<new name>') # or equivalently s.name = '<new name>' Index \u00b6 In pandas, the index of a dataframe is a specialized structure that is used to select rows and columns fast. Compared to databases where, for the user, the index is boud to a column or a set of columns, in pandas, the index contains the data itself . Instead of <column A> and <index for column A> , we have just <index A> . containing both values and data structures for fast selection. Another difference compared to databases is that pandas also has a column index , with the same properties as the row index. Index of a dataframe df can be accessed by df.index . Standard range operation can be applied to index. Any data type can be used as an index. However, with certain data types, we have to be careful with the index: float : Float index can cause problems with precision, that can change with data transformation operations like grouping: python df.index = Index([0.7, 0.8, 0.9]) new_df = df.groupby('col') new_df.index # can be Index([0.6999999999999999.... Therefore, it is best to avoid float index, in favor of more stable data types (strings, Categorical) Selecting just a single index level from a multi-index \u00b6 If we want to select just a single index level, we can use the get_level_values function: df.index.get_level_values(<level>) Note however, that this function returns duplicated values when there are multiple values in other levels. To get unique values, we can use the unique function. There is also another method, that returns unique values: the level property: df.index.levels[<level>] However, this way, we can get outdated values , as the values are not always updated when the index is changed. To get the updated values, we need to call the method remove_unused_levels after each change of the index. Renaming the index \u00b6 The Index.rename function can be used for that. Replacing the index \u00b6 We have several options how to replace the index with a new index: Using columns as a new index: for that, we can use the set_index function. Using an existing index to create a new index For that, we can use the reindex function. Creating index from scratch reindex \u00b6 This function can be used to replace the index, or a single index level in case of a multi-index. Note that the reindex function can only be used for unique values . If we have a multi-index with duplicate values on the level we want to reindex, we need to create a new index from scratch (see the Creating multi-index section). The first parameter is the new index: df.reindex(df.index + 1) # creates a new index by adding 1 to the old index Important parameters: fill_value : the value to use for missing values. By default, the missing values are filled with NaN . level : the level to reindex in case of a multi-index. Creating index from scratch \u00b6 To create an index from scratch, we just assign the index to the dataframe index property: df.index = pd.Index([1, 2, 3, 4, 5]) We can also assign a range directly to the index: df.index = range(5) Creating multi-index \u00b6 documentation There is a MultiIndex constructor that can be used to create a multi-index. However, most of the time, we use dedicated factory functions: MultiIndex.from_arrays : creates a multi-index from an array of arrays (e.g. a list of lists). Each array is a level of the multi-index. array here is basically anything iterable. We can use even the old index object: python df.index = pd.MultiIndex.from_arrays([ df.index.get_level_values(0), df.index.get_level_values(1) ]) MultiIndex.from_tuples : creates a multi-index from a list of tuples. Each tuple is a full index for a single row. MultiIndex.from_product : creates a multi-index from the cartesian product of the given iterables. Example: ```python df.index = pd.MultiIndex.from_product([['one', 'two'], ['a', 'b']]) results in \u00b6 MultiIndex([ \u00b6 ('one', 'a'), \u00b6 ('one', 'b'), \u00b6 ('two', 'a'), \u00b6 ('two', 'b'), \u00b6 ]) \u00b6 ``` Important parameters for both the constructor and the factory functions: names : the names of the index levels Shifting a time/date index \u00b6 To shift a time or date or datetime index, we can use the shift function with the freq parameter. The freq use is important, as it change the mode of operation of the shift function: if freq is not specified, the index stays the same and the data are shifted by the specified number of periods. if freq is specified, the index is shifted by the specified number of periods while the data stay in the same position. Example: df.index = pd.date_range('2021-01-01', periods=10, freq='1h') df.shift(1, freq='1h') # shifts the index by 1 hour df.shift(1) # shifts the data by 1 hour against the index The shift function is not defined for MultiIndex! . For that, we need to create the index level manually: df.index = pd.MultiIndex.from_product([['one', 'two'], pd.date_range('2021-01-01', periods=10, freq='1h')]) new_index = df.index.levels[1] + pd.Timedelta(hours=1) df.index = df.index.set_levels(new_index, level=1) Aggregation and Transformation \u00b6 Analogously to SQL, pandas has a groupby function. However, unlike in SQL, which use this concept only for aggregation, in pandas, we can also use it for transformation. The usage is as follows: group = df.groupby(<columns>) # returns a groupby object grouped by the columns selection = group[<columns>] # we can select only some columns from the groupby object result = selection.<aggregation or transformation function> # we apply an aggregation or transformation function to the selected columns We can skip the selection step and apply the aggregation function directly to the groupby object. This way, the aggregation function is applied to all columns. Example (Sums the results for each group, column by column): df.groupby('col').sum() Note that unlike in SQL, the aggregation function does not have to return a single value. It can return a series or a dataframe. In that case, the result is a dataframe with the columns corresponding to the returned series/dataframe. In other words, the aggregation does not have to actually aggregate the data, it can also transform it . Selected groupby parameters : group_keys : By default, the group keys are added to the index of the result. For transformation functions, that do not perform any real grouping, we can turn off this behavior by setting this parameter to False default is True This parameter only affects transformation functions. Built-in Aggregate functions \u00b6 For the aggregate function, we can use one of the prepared aggregation functions. Classical functions(single value per group): sum size : count the number of rows in each group mean median min max count Custom aggegate function \u00b6 Also, there are more general aggregate functions: agg function that is usefull for applying different functions for different columns and apply : the most flexible function that can be used for custom aggregation and transformation operations. These two functions have different interfaces for the custom aggregation functions they call. These are summarized in the following table: property agg apply can just transform the data no yes can use data from one column in another column no yes applied to each specified column the whole dataframe representing single group output dataframe scalar, series, or dataframe can use multiple aggregate functions yes no agg \u00b6 Example: df.groupby('col').agg({'col1': 'sum', 'col2': 'mean'}) apply \u00b6 The apply function takes a custom function as an argument. That custom aggregation function: takes a DataFrame/Series (depending on the source object) as the first argument this dataframe/series contains the data for the group (all columns*) the key of each particular group can be accessed using the name attribute of the dataframe/series returns a Series, DataFrame, or a scalar when a scalar is returned, the result is a series with the scalar value for each group we do not have to reduce the data to a single value or a single row, we can just transform the data arbitrarily. The process works as follows: The dataframe is split into groups according to the groupby function. The custom function is applied to each group. The results are combined into a single dataframe. In other words, the custom function only sees the dataframe/series representing the group, not the whole dataframe/series. The grouping and combining aggreate results is done by the apply function. *There is a change in behavior of the apply function and the grouping columns between pandas 2 and 3. A new parameter include_groups was introduced for a smooth transition. The grouping columns are included in the result only if this parameter is set to True . in pandas 2, the default value of include_groups is True , but a warning is raised. To silence the warning, we can set the parameter to False (if we do not need the grouping columns), or select the aggregation columns explicitly in the selection step. in pandas 3, the default value of include_groups is False . Time aggregation \u00b6 documentation We can also aggregate by time. For that, we need an index or column with datetime values. We use the resample function. Example: df = pd.DataFrame({'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, index=pd.date_range('2021-01-01', periods=10, freq='1h')) df.resample('1D').sum() # results in # col1 # 2021-01-01 55 The groups are created by the following logic: the groups are aligned to the index of the dataframe (not to the records) each group has the duration specified by the rule parameter each group is labeled by the start of the group e.g. for rule='1D' , the group is labeled by the start of the day, if rule='1h' , then 12:00 means 12:00-13:00 records are assigned to the group based on their datetime value the records that exactly match the start/end of the group are assigned depending on the closed parameter left (default): the record is assigned to the group with the matching start right : the record is assigned to the group with the matching end There are two possible results of resampling (both may appear in the same dataframe): Downsampling : multiple records in the same time group. Upsampling : no values in the group. The missing values are filled with NaN . note that the NaN automatically changes the data type of the column to float , and it is not reverted by filling the missing values later. Therefore, when upsampling, we have to manually change the data type of each column to the original data type after filling the values. We can use the ffill function to fill in the missing values. Example: Python df.resample('1H').sum().ffill() Transformation functions \u00b6 The most common transformation functions are: - cumsum : cumulative sum, i.e., the sum of the current and all previous values. - diff : difference between the current and the previous row. - the periods parameter specifies which row to use for the difference. By default, it is the previous row (periods=1). For next row, use periods=-1, but note that the result is then negative. We can use the abs function to get the absolute value. - ffill : Forward fill, i.e., fill the missing values with the next available value. Custom transformation function \u00b6 For custom transformation functions, we use the transform function. Joins \u00b6 Similarly to SQL, Pandas has a way to join two dataframes. There are two functions for that: merge : the most general function that has the behavior known from SQL join : a more specialized function, The following table lists the most important differences between the two functions: property merge join default join type inner left join right table via column (default) or index ( right_index=True ) index join left table via column (default) or index ( left_index=True ) index, or column ( on=key_or_keys ) There is also a static pd.merge function. All merge and join methods are just wrappers around this function. The indexes are lost after the join (if not used for the join). To keep an index, we can store it as a column before the join. Appending and Concatenating data \u00b6 In pandas, there is a concat function that can be used to concatenate data: pd.concat([df1, df2]) It can concatenate dataframes or series and it can concatenate vertically (by rows, default) or horizontally (by columns) By default, the indices from both input parameters are preserved. To reset the index, we can use the ignore_index parameter. Alternatively, to preserve one of the indices, we can set the index of the other dataframe to the index of the first dataframe before the concatenation using the set_index function. I/O \u00b6 csv \u00b6 For reading csv files, we can use the read_csv function. Important params: sep : separator header : row number to use as column names. If None , no header is used skiprows : number of rows to skip from the beginning delim_whitespace : if True , the whitespace is used as a separator. The sep parameter is ignored in that case. This is a way how to read a file with variable number of whitespaces between columns. For export, we can use the to_csv method: df.to_csv(<file name> [, <other params>]) Useful parameters: index : if False , the index is not exported index_label : the name of the index column Json \u00b6 For exporting to json, we can use the to_json function. By default, the data are exported as a list of columns. To export the data as a list of rows, we can use the orient parameter: df.to_json(<file name>, orient='records') Other important parameters: indent : the number of spaces to use for indentation Excel \u00b6 For reading excel files, we can use the read_excel function. Important params: sheet_name : the name of the sheet to read. If None , the first sheet is read. Insert dataframe into db \u00b6 We can use the to_sql method for that: df.to_sql(<table name>, <sql alchemy engine> [, <other params>]) Important params: to append, not replace existing records: if_exists='append' do not import dataframe index: index=False For larger datasets, it is important to not insert everything at once, while also tracking the progress. The following code does exactly that def chunker(seq, size): return (seq[pos:pos + size] for pos in range(0, len(seq), size)) chunksize = int(len(data) / 1000) # 0.1% with tqdm(total=len(data)) as pbar: for i, cdf in enumerate(chunker(data, chunksize)): cdf.to_sql(<table name>, <sqlalchemy_engine>) pbar.update(chunksize) If the speed is slow, it can be caused by a low upload speed of your internet connection. Note that due to the SQL syntax, the size of the SQL strings may be much larger than the size of the dataframe. Latex export \u00b6 Currently, the DataFrame.to_latex function is deprecated. The Styler class should be used for latex exports instead. You can get the Styler from the DataFrame using the style property. The usual workfolow is: Create a Styler object from the dataframe using the style property, Apply the desired formatting to the styler object, Export DataFrame to latex using the Styler.to_latex method. Keep in mind that the Styler object is immutable, so you need to assign the result of each formatting operation to a new variable or chain the calls . Example: # wrong, the format is not apllied df.style.format(...) df.style.to_latex(...) # correct: temp var s = df.style.format(...) s.to_latex(...) # correct: chain calls df.style.format(...).to_latex(...) Finally, to produce a nice output, tou should use print to display the latext string: print(styler.to_latex()) Formatting the index: columns and row labels \u00b6 The columns' and row labels' format is configures by the format_index function. Important parameters: axis : 0 for rows, 1 for columns (cannot be both) escape : by default, the index is not escaped, to do so, we need to set escape to 'latex' . Formatting or changing the values \u00b6 The values are formated by the format function. Important parameters: escape : by default, the values are not escaped, to do so, we need to set escape to 'latex' . na_rep : the string to use for missing values precision : the number of decimal places to use for floats Replacing values \u00b6 For replace some values for the presentation with something else, we can also use the format function.For example, to change the boolean presentation in column col we call: df.style.format({'col': lambda x: 'yes' if x else 'no'}) Hihglighting min/max values \u00b6 For highlighting the min/max values, we can use the highlight_min and highlight_max functions. Important parameters: subset : the columns in which the highlighting should be applied props : the css properties to apply to the highlighted cells Hiding some columns, rows, or indices \u00b6 For hiding some columns, rows, or indices, we can use the hide function. Important Parameters: axis : 0 for hiding row indices (default), 1 for hiding column names level : the level of the multi-index to hide (default is all levels) subset : the columns or rows to hide (default is all columns or rows) When used without the subset parameter, the hide function hides the whole index. To hide just a selected row or column from the data, the subset parameter has to be used. By default, the <index_name> refers to the row index. To hide a column : df.style.hide_columns(<column name>, axis=1) Changing the header (column labels) \u00b6 There is no equivalent to the header parameter of the old to_latex function in the new style system. Instead, it is necessary to change the column names of the dataframe. Exporting to latex \u00b6 For the export, we use the to_latex function. Important parameters: convert_css : if True , the css properties are converted to latex commands multirow_align : the alignment of the multirow cells. Options are t , c , b hrules : if set to True , the horizontal lines are added to the table, specifically to the top, bottom, and between the header and the body. Note that these hrules are realized as the \\toprule , \\midrule , and \\bottomrule commands from the booktabs package, so the package has to be imported . clines : configuration for hlines between rows. It is a string composed of two parts divided by ; (e.g.: skip-last;data ). The parts are: whether to skip last row or not ( skip-last or all ) whether to draw the lines between indices or the whole rows ( index or data ) Displaying the dataframe in console \u00b6 We can display the dataframe in the conslo print or int the log just by supplying the dataframe as an argument because it implements the __repr__ method. Sometimes, however, the default display parameters are not sufficient. In that case, we can use the set_option function to change the display parameters: pd.set_option('display.max_rows', 1000) Important parameters: display.max_rows : the maximum number of rows to display display.max_columns : the maximum number of columns to display display.max_colwidth : the maximum width of a column Other useful functions \u00b6 drop_duplicates to quickly drop duplicate rows based on a subset of columns. factorize to encode a series values as a categorical variable, i.e., assigns a different number to each unique value in series. pivot_table : function that can aggragate and transform a dataframe in one step. with this function, one can create a pivot table, but also a lot more. cut : function that can be used to discretize a continuous variable into bins. pivot_table \u00b6 The pivot_table function do a lot of things at once: it aggregates the data it transforms the data it sorts the data due to reindexing Although this function is very powerfall there are also many pitfalls. The most important ones are: column data type change for columns with missing values Column data type change for columns with missing values \u00b6 The tranformation often creates row-column combinations that do not exist in the original data. These are filled with NaN values. But some data types does not support NaN values, and in conclusion, the data type of the columns with missing values is changed to float . Possible solutions: we can use the fill_value parameter to fill the missing values with some value that is supported by the data type (e.g. -1 for integers) we can use the dropna parameter to drop the rows with missing values we can change the data type of the columns with missing values prior to calling the pivot_table function. For example, the pandas integer data types support NaN values. to_datetime \u00b6 The to_datetime function can convert various inputs to datetime. It can be used to both scalars and vectors. Important parameters: unit : the unit of the input, e.g., s for seconds. origin : the origin of the input, e.g., unix for unix timestamps. It can be also any specific datetime object. format : the format of the input, e.g., %Y-%m-%d for 2021-01-01 . squeeze \u00b6 The squeeze function removes the unnecessary dimension from a dataframe or series. It is usefull when we want to convert a dataframe with a single column to a series, or a series with a single value to a scalar. Geopandas \u00b6 homepage Geopandas is a GIS addon to pandas. Geometric operations are implemented in Shapely. Basics \u00b6 Geopandas works with GeoDataFrame , a subclass of pandas DataFrame . The GeoDataFrame can contain normal columns (i.e., Pandas, non-geometric columns) and a geometry column implemented by GeoSeries (instead of the usual Pandas Series ). Every geodataframe can have multiple geometry columns. Each column can be of a different geometry type and it can have a different coordinate system (projection). However, a geodataframe can have only active geometry column at a time. Active geometry column \u00b6 All geometric operations performed on a geodataframe are performed using the active geometry column. By default, if we create a geodataframe with a single geometry column, it becomes the active geometry column. However, sometimes we need to set it, for example: when we have multiple geometry columns, or when we rename the active geometry column. To set the active geometry column, we can use the set_geometry method. To check if the geodataframe has an active geometry column, we can check the active_geometry_name property: if gdf.active_geometry_name is None: print(\"No active geometry column\") else: print(f\"Active geometry column: {gdf.active_geometry_name}\") Setting the active geometry column \u00b6 To set the active geometry column, we can use the set_geometry method. gdf.set_geometry(<GEOMETRY COLUMN>, inplace=True) Limitations \u00b6 Do not ever copy paste the geometries from jupyter notebook as the coordinates are rounded! Use the to_wkt function instead. Create a geodataframe from CSV \u00b6 Geopandas has it's own read_csv function, however, it requires a very specific csv format, so it is usually easier to first import csv to pandas and then create geopandas dataframe from pandas dataframe. Converting pandas Dataframe to geopandas Dataframe \u00b6 The geopandas dataframe constructor accepts pandas dataframes, we just need to specify the geometry column and the coordinate system: gdf = gpd.GeoDataFrame( <PANDAS DATAFRAME> geometry=gpd.points_from_xy(<X COLUMN>, <Y COLUMN>), crs=<SRID> ) Create geodataframe from shapely \u00b6 To load data from shapely, execute gdf = gpd.read_file(<PATH TO FOLDER WITH SHAPEFILES>) Working with the geometry \u00b6 The geometry can be accessed using the geometry property of the geodataframe. GeoSeries operations \u00b6 There are many useful functions for working with the geometry columns or GeoSeries in general: line_merge : for each member of the series, this function merge MultiLineStrings into a single LineString if possible. union_all : compute the union of all geometries in the series. for linestring geometries, this typically creates a MultiLineString (i.e., it does no merge the lines into a single one) Merging linestrings in the whole series \u00b6 There is no dedicated function for merging linestrings in the whole series. We have to: create a multi-linestring geometry from the series using the union_all function merge the multi-linestring geometry into a single linestring with Shapely's line_merge function (see the Python manual for more details) Spliting multi-geometry columns \u00b6 If the geometry column contains multi-geometries, we can split them into separate rows using the explode function: gdf = gdf.explode() Insert geodataframe into db \u00b6 Simple insertion \u00b6 When the data are in the correct format and we don't need any customization for the db query, we can use the to_postgis method. For this to work: the geodataframe must have an active geometry column, the columns of the geodataframe have to match the corresponding database table. This process is same as when working with pandas Example: gdf.to_postgis(<TABLE NAME>, <SQL ALCHEMY CONNECTION>, if_exists='append') Customized Insertion: geoalchemy \u00b6 If we need some special insert statement, we cannot rely on the geodataframe.to_postgis function, as it is not flexible enough. The pandas dataframe.to_sql function is more flexible, however, it has trouble when working with geodata. The easiest options is therefore to use geoalchemy , the database wraper used in geopandas (extension of sqlalchemy , which is a database wrapper for pandas ). First, we need to create the insert statement. The example here uses a modification for handeling duplicite elements. meta = sqlalchemy.MetaData() # create a collection for geoalchemy database # objects table = geoalchemy2.Table( '<TABLE NAME>', meta, autoload_with=<SQL ALCHEMY CONNECTION>) insert_statement = sqlalchemy.dialects.postgresql.insert(table).on_conflict_do_nothing() In the above example, we create a geoalchemy representation of a table and then we use this representation to create a customized insert statement (the on_conflict_do_nothing is the speciality here.). Note that we use a speciatl PostgreSQL insert statement instead of the standard SQLAlchemy insert statement. Second, we need to prepare the data as a list of dictionary entries: list_to_insert = [ {'id': 0, 'geom': <GEOM>, ...}, {'id': 0, 'geom': <GEOM>, ...}, .... ] Note that the geometry in the geodataframe is in the shapely format. Therefore, we need to convert it to string using the geoalchemy from_shape function: geoalchemy2.shape.from_shape(<GEOMETRY>, srid=<SRID>) Finally, we can execute the query using an sqlalchemy connection: sqlalchemy_connection.execute(insert_statement, list_to_insert) Exporting geodataframe to a file \u00b6 The geodataframe can be exported to a file using the to_file method. The format is determined by the file extension. Pandas in Jupyter \u00b6 Displaying more rows or columns \u00b6 To display more rows or columns in Jupyter than the default, we have to set the pandas option_context to the desired values: # instead of df # write with pd.option_context('display.max_rows', 100, 'display.max_columns', 10): display(df)","title":"Pandas Manual"},{"location":"Programming/Python/Pandas%20Manual/#main-principles","text":"Pandas extensively uses the term axis. In Pandas, axis 0 is vertical (rows) and axis 1 is horizontal (columns).","title":"Main principles"},{"location":"Programming/Python/Pandas%20Manual/#pandas-data-types","text":"Documentation of basic data types Mainly, pandas uses the numpy data types .","title":"Pandas Data Types"},{"location":"Programming/Python/Pandas%20Manual/#object","text":"If pandas does not recognize the type of the column, or there are multiple types in the column, it uses the object type. However this may sound like a wonderful solution, it causes many problems, so be sure to avoid object type columns at all costs. Typically, the problem arises when we try to apply a vector operation to the column: we round a column with mix of floats and ints: fail ( loop of ufunc does not support argument 0 of type float which has no callable rint method ) we need to apply string functions, but the column contains numbers as well The solution is usually: fill the missing values with the fillna function convert the column to str type using the astype function apply string functions to clear the data convert the column to the desired type","title":"Object"},{"location":"Programming/Python/Pandas%20Manual/#categorical-data","text":"Sometimes, it can be usefull to treat a column as a categorical variable instead of a string or a number. For that, we can use the Categorical class. Typically, we: define the categorical type: python cat_type = pd.Categorical(categories=['a', 'b', 'c']) use the categorical type to, e.g., convert a column to the categorical type: python df['col'] = df['col'].astype(cat_type) Important parameters of the Categorical constructor: categories : By default, the categories are inferred from the data. However, we can specify the categories explicitly using the categories parameter. When using this parameter, each value is converted to the matching category specified in the list. Note that the category must to be an exact match, including the data type, case (when using strings), etc . Unmatched values are converted to NaN . ordered : if True , the categories are ordered in the order of the categories parameter.","title":"Categorical data"},{"location":"Programming/Python/Pandas%20Manual/#reading-categorical-data-from-a-file","text":"We can read data into categories using the dtype parameter. However, this way, we cannot specify the categories explicitly. To do that, we need to read the data into the correct data type, and then convert the column/index to the categorical type after the dataframe is created.","title":"Reading categorical data from a file"},{"location":"Programming/Python/Pandas%20Manual/#datetime","text":"Pandas has a special type for datetime values. One of its dangerous properties is that zero parts of the datetime are truncated both when displaying and on export: df = pd.DataFrame({'date': pd.to_datetime(['2021-01-01 00:00:00', '2021-01-01 00:00:00'])}) print(df) # output: # '2021-01-01' # '2021-01-01'","title":"Datetime"},{"location":"Programming/Python/Pandas%20Manual/#creating-a-dataframe","text":"The DataFrame class has a constructor that supports multiple formats of input data as well as many configuration parameters. Therefore , for most formats of input data, we can create a dataframe using the constructor. However, we can also crete a dataframe using the from_* functions, and for some formats, these functions are the only way to create a dataframe.","title":"Creating a DataFrame"},{"location":"Programming/Python/Pandas%20Manual/#from-a-dictionary","text":"When having a dictionary, we can choose between two options the constructor and the from_dict function. The required syntax depend on the shape of the dictionary with respect to the required dataframe.","title":"From a dictionary"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-column-names-values-are-list-of-column-values","text":"df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) # or equivalently df.DataFrame.from_dict({'col1': [1, 2], 'col2': [3, 4]}) Note that the values of the dictionary have to be lists. If we have a dictionary with values that are not lists (i.e., only one row), we have to use the orient parameter to specify the orientation of the data and then transpose the dataframe: d = {'col1': 1, 'col2': 2} df = pd.DataFrame.from_dict(d, orient='index').T # or equivalently df = pd.DataFrame([d], columns=d.keys())","title":"Keys are column names, values are list of column values"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-indices-values-are-values-of-a-single-column","text":"df = pd.DataFrame.from_dict({'row1': 1, 'row2': 2}, orient='index', columns=['Values'])","title":"Keys are indices, values are values of a single column"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-indices-values-are-values-of-single-row","text":"df = pd.DataFrame.from_dict({'row1': [1, 2], 'row2': [3, 4]}, orient='index')","title":"Keys are indices, values are values of single row"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-one-column-values-are-another-column","text":"d = {'row1 col1': 'row1 col2', 'row2 col1': 'row2 col2' df = pd.DataFrame.from_dict(d.items()) # or equivalently df = pd.DataFrame({'col1': d.keys(), 'col2': d.values()})","title":"Keys are one column, values are another column"},{"location":"Programming/Python/Pandas%20Manual/#from-a-list-of-dictionaries","text":"df = pd.DataFrame([{'col1': 1, 'col2': 3}, {'col1': 2, 'col2': 4}])","title":"From a list of dictionaries"},{"location":"Programming/Python/Pandas%20Manual/#from-a-list-of-lists","text":"df = pd.DataFrame([[1, 3], [2, 4]], columns=['col1', 'col2'])","title":"From a list of lists"},{"location":"Programming/Python/Pandas%20Manual/#creating-a-zero-or-constant-filled-dataframe","text":"To create a dataframe filled with a constant, we can use the dataframe constructor and pass the constant as the first (data) argument: df = pd.DataFrame(0, index=range(10), columns=['col1', 'col2'])","title":"Creating a zero or constant-filled dataframe"},{"location":"Programming/Python/Pandas%20Manual/#generating-the-index","text":"As displayed in the above example, we can generate a numerical index using the range function. However, there are more options: date index with date_range pd.date_range(<start date>, <end date>, freq=<frequency>)","title":"Generating the index"},{"location":"Programming/Python/Pandas%20Manual/#determining-the-data-type","text":"By defualt, Pandas infers the data type of the columns by the content. However, this has some limitations: The data are processed line by line to avoid excessive memory usage. Therefore, if the data contains a value not compatible with the type inferred from the first batch of data, the previously processed data have to be processed again. If the data are not complete or valid, the data is typically inferred as object type, instead of reporting an error. To specify the data type of the columns we proceed according to the desired data type: for datetime data, we use the parse_dates parameter of the read_csv function for other data types , we use the dtype parameter of the read_csv function.","title":"Determining the data type"},{"location":"Programming/Python/Pandas%20Manual/#obtaining-info-about-dataset","text":"For a DataFrame df : column names: df.columns column types: df.dtypes number of rows: len(df)","title":"Obtaining info about dataset"},{"location":"Programming/Python/Pandas%20Manual/#iteration","text":"","title":"Iteration"},{"location":"Programming/Python/Pandas%20Manual/#standard-iteration","text":"https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas Iteration without modifying the dataframe. From the fastest to the slowest.","title":"Standard Iteration"},{"location":"Programming/Python/Pandas%20Manual/#vector-operations","text":"","title":"Vector operations"},{"location":"Programming/Python/Pandas%20Manual/#list-comprehensions","text":"","title":"List Comprehensions"},{"location":"Programming/Python/Pandas%20Manual/#apply","text":"The apply function can be used to apply a function to each row or column of the dataframe. For iterating over rows, we need to set the axis parameter to 1. Example: df['new_col'] = df.apply(lambda row: row['col1'] + row['col2'], axis=1)","title":"Apply"},{"location":"Programming/Python/Pandas%20Manual/#itertuples","text":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.html Returns dataframe rows as pandas named tuples with index as the first member of the tuple.","title":"itertuples()"},{"location":"Programming/Python/Pandas%20Manual/#iterrows","text":"Documentation returns a tuple (index, data) it does not preserve the dtype","title":"iterrows()"},{"location":"Programming/Python/Pandas%20Manual/#items","text":"Documentation Iterates over columns","title":"items()"},{"location":"Programming/Python/Pandas%20Manual/#iteration-with-modification","text":"For modification, the best strategy is to: select what we want to modify (see selection ) modify the selection with the assignment operator. The right side of the assignment operator can be the result of an iteration.","title":"Iteration with modification"},{"location":"Programming/Python/Pandas%20Manual/#iteration-pairs-of-consecutive-rows","text":"We can use the zip function to iterate over pairs of consecutive rows. For example, if we want to access each row as a Series , we can use the iterrows function with zip : for row1, row2 in zip(df[:-1].iterrows(), df[1:].iterrows()): print(row1[1], row2[1]) # row1[0] and row2[0] are the indices of the rows...","title":"Iteration pairs of consecutive rows"},{"location":"Programming/Python/Pandas%20Manual/#filtration","text":"filtered = df[df['max_delay'] == x] or equivalently: filtered = df[df.max_delay == x]","title":"Filtration"},{"location":"Programming/Python/Pandas%20Manual/#filtration-by-multiple-columns","text":"Example: filtered = df[(df['max_delay'] == x) & (df['exp_length'] == y)]","title":"Filtration by Multiple Columns"},{"location":"Programming/Python/Pandas%20Manual/#accept-multiple-values","text":"For that, we can use the isin function: filtered = df[df['max_delay'].isin([x, y])]","title":"Accept multiple values"},{"location":"Programming/Python/Pandas%20Manual/#using-the-query-function","text":"The query function can be used for more complicated filters. It is more flexible and the syntax is less verbose. The above filter can be rewriten as: filtered = df.query('max_delay == x and exp_length == y'] Special filter functions can be used in the query function as well: filtered = df.query('max_delay == x and exp_length.isnull()')","title":"Using the query function"},{"location":"Programming/Python/Pandas%20Manual/#filtering-series","text":"A seris can be filtered even simpler then the dataframe: s = df['col'] sf = s[s <= 10] # now we have a Series with values from df['col'] less than 10","title":"Filtering Series"},{"location":"Programming/Python/Pandas%20Manual/#useful-filter-functions","text":"null values: <column selection>.isnull() non null/nan values: <column selection>.notnull() filtring using the string value: <column selection>.str.<string function> filtering dates: <column selection>.dt.<date function>","title":"Useful filter functions"},{"location":"Programming/Python/Pandas%20Manual/#selection","text":"If we want to select a part of the dataframe (a set of rows and columns) independently of the values of the dataframe (for that, see filtration ), we can use these methods: loc : select by index, works for both rows and columns iloc : select by position, works for both rows and columns [] : select by index, works only for columns There are also other methods that works for selection but does not work for setting values, such as: xs : select by label, works for both rows and columns The return type of the selection is determined by the number of selected rows and columns. For a single row or column, the result is a series, for multiple rows and columns, the result is a dataframe. If we want to get a dataframe for a single row or column, we can use the [] operator with a list of values: df[['col1']] # or df.loc[['row1']] # or df.iloc[[0]]","title":"Selection"},{"location":"Programming/Python/Pandas%20Manual/#loc","text":"The operator loc has many possible input parameters, the most common syntax is df.loc[<row selection>, <column selection>] each selection has the form of <start label>:<end label> . For the whole column, we therefore use: df.loc[:, <column name>]","title":"loc"},{"location":"Programming/Python/Pandas%20Manual/#difference-between-array-operator-on-dataframe-and-on-loc","text":"Both methods can be used both for getting and setting the column: a = df['col'] # or equivalently a = df.loc[:, 'col'] df2['col'] = a # or equivalently df2.loc[:, 'col'] = a The difference between these two methods is apparent when we want to use a chained selection, i.e., selecting from a selection. While the loc selects the appropriate columns in one step, so we know that we still refer to the original dataframe, the array operator operations are separate, and therefore, the result value can refer to a temporary: dfmi.loc[:, ('one', 'second')] = value # we set a value of a part of dfmi dfmi['one']['second'] = value # can be dangerous, we can set value to a temporary This problem is indicated by a SettingWithCopy warning. Sometimes it is not obvious that we use a chain of array operator selections, e.g.: sel = df[['a', 'b']] ..... sel['a'] = ... # we possibly edit a temporary! For more, see the dovumentation .","title":"Difference between array operator on dataframe and on loc"},{"location":"Programming/Python/Pandas%20Manual/#iloc","text":"The iloc method works similarly to the loc method, but it uses the position instead of the label. To select more values, we can use the slice syntax df.iloc[<start position>:<end position>:<step>,<collumn slicing...>] Be aware that if the iloc operator selects by single value (e.g.: df.iloc[3] ), it returns the single row as series . To get a dataframe slice, we need to use a list of values (e.g.: df.iloc[[3]] ).","title":"iloc"},{"location":"Programming/Python/Pandas%20Manual/#selecting-all-columns-but-one","text":"If we do not mind copying the dataframe, we can use the drop function. Otherwise, we can use the loc method and supply the filtered column lables obtained using the columns property: df.loc[:, df.columns != '<column to skip>']","title":"Selecting all columns but one"},{"location":"Programming/Python/Pandas%20Manual/#multi-index-selection","text":"documentation When selecting from a dataframe with a multi-index, things get a bit more complicated. There are three ways how to select from a multi-index dataframe: if we need to match a single value on one level , there is a neet xs function does not support ranges, only specific values python df.xs(<value>, level=<level name or number>) for range selection , we can use loc with slices: simple, but it can be verbose for complex selections python df.loc[(slice(15, 30), slice(None)), ...] for complex selections , we can use loc with IndexSlice object. more readable, but requires the IndexSlice object to be created first. python idx = pd.IndexSlice df.loc[idx[:, 15:30], ...]","title":"Multi-index selection"},{"location":"Programming/Python/Pandas%20Manual/#using-xs","text":"The xs function can be used to select from a multi-index dataframe. However, slices (ranges) are not supported. Example: df.xs(15, level=1) # selects all rows with level 1 equal to 15","title":"Using xs"},{"location":"Programming/Python/Pandas%20Manual/#using-loc","text":"The general loc usage is the same as for a single index dataframe: df.loc[<row selection>, <column selection>] However, each selection is now a tuple, where each element of the tuple corresponds to one level of the multi-index: df.loc[(<row selection level 1>, <row selection level 2>, ...), (<column selection level 1>, <column selection level 2>, ...)] Because each selection is a tuple, it is crucial to always specify both dimensions (rows and columns), even if we want to select all columns . Otherwise, part of the row selection may be interpreted as a column selection, leading to a missing key error (as described in the documentation ). Each <selection> can be: a specific value, a list of values, or a slice. Note that we have to use the slice function, as pandas uses the standard slice syntax for something else. We can skip lower levels to select all values from those levels. However, we cannot skip upper levels. If we want to select all values from the upper level, we need to use the slice(None) for that level: df.loc[(slice(None), slice(15, 30)), ...] Note that for multi-index slicing, the index needs to be sorted . If it is not, we can use the sort_index function. pandas slicing documentation .","title":"Using loc"},{"location":"Programming/Python/Pandas%20Manual/#using-loc-with-indexslice-for-more-readable-syntax","text":"We can obtain the same result with a more readable syntax using the IndexSlice object: idx = pd.IndexSlice dft.loc[idx[:, 15:30], ...] Again, it is crucial to always specify both dimensions even if we want to select all columns : df.loc[idx[:, 15:30],:] # select all columns for rows with index[1] between 15 and 30 df.loc[idx[:, 15:30]] # try to select all rows and columns between 15 and 30 (produces key error most likely)","title":"Using loc with IndexSlice for more readable syntax"},{"location":"Programming/Python/Pandas%20Manual/#select-row-with-a-maximum-value-in-a-column","text":"To get the index of the row with the maximum value in a column, we can use the idxmax function: df['col'].idxmax() Then we can use the loc method to get the row.","title":"Select row with a maximum value in a column"},{"location":"Programming/Python/Pandas%20Manual/#selecting-a-single-value-cell-scalar","text":"When we select a single value from a dataframe, the result is sometimes a series, especially when we use a filtration. To get a scalar, we can use the item() method: df.loc[<row>, <column>].item()","title":"Selecting a single value (cell, scalar)"},{"location":"Programming/Python/Pandas%20Manual/#sorting","text":"for sorting the dataframe, we can use the sort_values function. The first argument is the list of columns to sort by, starting with the most important column. Example: df.sort_values(['col1', 'col2']) If we want to use a custom sorting function, we can use the key argument. The key function should satisfy the classical python sorting interface (see Python manual ) and it should be a vector function, i.e., instead of returning a single position for a given value, it should return a vector of positions for a given vector of values. Example: def key_fn(column: list): return [len(x) for x in l] df.sort_values('col', key=key_fn)","title":"Sorting"},{"location":"Programming/Python/Pandas%20Manual/#working-with-columns","text":"","title":"Working with columns"},{"location":"Programming/Python/Pandas%20Manual/#adding-a-column","text":"The preferable way is to use the assign function: # adds a column named 'instance_length' with constant value result_df_5_nyc_mv.assign(instance_length = 5) Multiple columns can be added at once: trips = trips.assign(dropoff_datetime = 0, dropoff_GPS_lon = 0, dropoff_GPS_lat = 0, pickup_GPS_lon = 0, pickup_GPS_lat = 0)","title":"Adding a column"},{"location":"Programming/Python/Pandas%20Manual/#rename-a-column","text":"To rename a column, we can use the pandas rename function: df.rename(columns={'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}) # or equivalently df.rename({'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}, axis='columns')","title":"Rename a column"},{"location":"Programming/Python/Pandas%20Manual/#rename-a-series-column","text":"The column name in the series object is actually the name of the series. To rename the series, we can use the rename function, or we can set the name property of the series: s.rename('<new name>') # or equivalently s.name = '<new name>'","title":"Rename a Series (column)"},{"location":"Programming/Python/Pandas%20Manual/#index","text":"In pandas, the index of a dataframe is a specialized structure that is used to select rows and columns fast. Compared to databases where, for the user, the index is boud to a column or a set of columns, in pandas, the index contains the data itself . Instead of <column A> and <index for column A> , we have just <index A> . containing both values and data structures for fast selection. Another difference compared to databases is that pandas also has a column index , with the same properties as the row index. Index of a dataframe df can be accessed by df.index . Standard range operation can be applied to index. Any data type can be used as an index. However, with certain data types, we have to be careful with the index: float : Float index can cause problems with precision, that can change with data transformation operations like grouping: python df.index = Index([0.7, 0.8, 0.9]) new_df = df.groupby('col') new_df.index # can be Index([0.6999999999999999.... Therefore, it is best to avoid float index, in favor of more stable data types (strings, Categorical)","title":"Index"},{"location":"Programming/Python/Pandas%20Manual/#selecting-just-a-single-index-level-from-a-multi-index","text":"If we want to select just a single index level, we can use the get_level_values function: df.index.get_level_values(<level>) Note however, that this function returns duplicated values when there are multiple values in other levels. To get unique values, we can use the unique function. There is also another method, that returns unique values: the level property: df.index.levels[<level>] However, this way, we can get outdated values , as the values are not always updated when the index is changed. To get the updated values, we need to call the method remove_unused_levels after each change of the index.","title":"Selecting just a single index level from a multi-index"},{"location":"Programming/Python/Pandas%20Manual/#renaming-the-index","text":"The Index.rename function can be used for that.","title":"Renaming the index"},{"location":"Programming/Python/Pandas%20Manual/#replacing-the-index","text":"We have several options how to replace the index with a new index: Using columns as a new index: for that, we can use the set_index function. Using an existing index to create a new index For that, we can use the reindex function. Creating index from scratch","title":"Replacing the index"},{"location":"Programming/Python/Pandas%20Manual/#reindex","text":"This function can be used to replace the index, or a single index level in case of a multi-index. Note that the reindex function can only be used for unique values . If we have a multi-index with duplicate values on the level we want to reindex, we need to create a new index from scratch (see the Creating multi-index section). The first parameter is the new index: df.reindex(df.index + 1) # creates a new index by adding 1 to the old index Important parameters: fill_value : the value to use for missing values. By default, the missing values are filled with NaN . level : the level to reindex in case of a multi-index.","title":"reindex"},{"location":"Programming/Python/Pandas%20Manual/#creating-index-from-scratch","text":"To create an index from scratch, we just assign the index to the dataframe index property: df.index = pd.Index([1, 2, 3, 4, 5]) We can also assign a range directly to the index: df.index = range(5)","title":"Creating index from scratch"},{"location":"Programming/Python/Pandas%20Manual/#creating-multi-index","text":"documentation There is a MultiIndex constructor that can be used to create a multi-index. However, most of the time, we use dedicated factory functions: MultiIndex.from_arrays : creates a multi-index from an array of arrays (e.g. a list of lists). Each array is a level of the multi-index. array here is basically anything iterable. We can use even the old index object: python df.index = pd.MultiIndex.from_arrays([ df.index.get_level_values(0), df.index.get_level_values(1) ]) MultiIndex.from_tuples : creates a multi-index from a list of tuples. Each tuple is a full index for a single row. MultiIndex.from_product : creates a multi-index from the cartesian product of the given iterables. Example: ```python df.index = pd.MultiIndex.from_product([['one', 'two'], ['a', 'b']])","title":"Creating multi-index"},{"location":"Programming/Python/Pandas%20Manual/#results-in","text":"","title":"results in"},{"location":"Programming/Python/Pandas%20Manual/#multiindex","text":"","title":"MultiIndex(["},{"location":"Programming/Python/Pandas%20Manual/#one-a","text":"","title":"('one', 'a'),"},{"location":"Programming/Python/Pandas%20Manual/#one-b","text":"","title":"('one', 'b'),"},{"location":"Programming/Python/Pandas%20Manual/#two-a","text":"","title":"('two', 'a'),"},{"location":"Programming/Python/Pandas%20Manual/#two-b","text":"","title":"('two', 'b'),"},{"location":"Programming/Python/Pandas%20Manual/#_1","text":"``` Important parameters for both the constructor and the factory functions: names : the names of the index levels","title":"])"},{"location":"Programming/Python/Pandas%20Manual/#shifting-a-timedate-index","text":"To shift a time or date or datetime index, we can use the shift function with the freq parameter. The freq use is important, as it change the mode of operation of the shift function: if freq is not specified, the index stays the same and the data are shifted by the specified number of periods. if freq is specified, the index is shifted by the specified number of periods while the data stay in the same position. Example: df.index = pd.date_range('2021-01-01', periods=10, freq='1h') df.shift(1, freq='1h') # shifts the index by 1 hour df.shift(1) # shifts the data by 1 hour against the index The shift function is not defined for MultiIndex! . For that, we need to create the index level manually: df.index = pd.MultiIndex.from_product([['one', 'two'], pd.date_range('2021-01-01', periods=10, freq='1h')]) new_index = df.index.levels[1] + pd.Timedelta(hours=1) df.index = df.index.set_levels(new_index, level=1)","title":"Shifting a time/date index"},{"location":"Programming/Python/Pandas%20Manual/#aggregation-and-transformation","text":"Analogously to SQL, pandas has a groupby function. However, unlike in SQL, which use this concept only for aggregation, in pandas, we can also use it for transformation. The usage is as follows: group = df.groupby(<columns>) # returns a groupby object grouped by the columns selection = group[<columns>] # we can select only some columns from the groupby object result = selection.<aggregation or transformation function> # we apply an aggregation or transformation function to the selected columns We can skip the selection step and apply the aggregation function directly to the groupby object. This way, the aggregation function is applied to all columns. Example (Sums the results for each group, column by column): df.groupby('col').sum() Note that unlike in SQL, the aggregation function does not have to return a single value. It can return a series or a dataframe. In that case, the result is a dataframe with the columns corresponding to the returned series/dataframe. In other words, the aggregation does not have to actually aggregate the data, it can also transform it . Selected groupby parameters : group_keys : By default, the group keys are added to the index of the result. For transformation functions, that do not perform any real grouping, we can turn off this behavior by setting this parameter to False default is True This parameter only affects transformation functions.","title":"Aggregation and Transformation"},{"location":"Programming/Python/Pandas%20Manual/#built-in-aggregate-functions","text":"For the aggregate function, we can use one of the prepared aggregation functions. Classical functions(single value per group): sum size : count the number of rows in each group mean median min max count","title":"Built-in Aggregate functions"},{"location":"Programming/Python/Pandas%20Manual/#custom-aggegate-function","text":"Also, there are more general aggregate functions: agg function that is usefull for applying different functions for different columns and apply : the most flexible function that can be used for custom aggregation and transformation operations. These two functions have different interfaces for the custom aggregation functions they call. These are summarized in the following table: property agg apply can just transform the data no yes can use data from one column in another column no yes applied to each specified column the whole dataframe representing single group output dataframe scalar, series, or dataframe can use multiple aggregate functions yes no","title":"Custom aggegate function"},{"location":"Programming/Python/Pandas%20Manual/#agg","text":"Example: df.groupby('col').agg({'col1': 'sum', 'col2': 'mean'})","title":"agg"},{"location":"Programming/Python/Pandas%20Manual/#apply_1","text":"The apply function takes a custom function as an argument. That custom aggregation function: takes a DataFrame/Series (depending on the source object) as the first argument this dataframe/series contains the data for the group (all columns*) the key of each particular group can be accessed using the name attribute of the dataframe/series returns a Series, DataFrame, or a scalar when a scalar is returned, the result is a series with the scalar value for each group we do not have to reduce the data to a single value or a single row, we can just transform the data arbitrarily. The process works as follows: The dataframe is split into groups according to the groupby function. The custom function is applied to each group. The results are combined into a single dataframe. In other words, the custom function only sees the dataframe/series representing the group, not the whole dataframe/series. The grouping and combining aggreate results is done by the apply function. *There is a change in behavior of the apply function and the grouping columns between pandas 2 and 3. A new parameter include_groups was introduced for a smooth transition. The grouping columns are included in the result only if this parameter is set to True . in pandas 2, the default value of include_groups is True , but a warning is raised. To silence the warning, we can set the parameter to False (if we do not need the grouping columns), or select the aggregation columns explicitly in the selection step. in pandas 3, the default value of include_groups is False .","title":"apply"},{"location":"Programming/Python/Pandas%20Manual/#time-aggregation","text":"documentation We can also aggregate by time. For that, we need an index or column with datetime values. We use the resample function. Example: df = pd.DataFrame({'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, index=pd.date_range('2021-01-01', periods=10, freq='1h')) df.resample('1D').sum() # results in # col1 # 2021-01-01 55 The groups are created by the following logic: the groups are aligned to the index of the dataframe (not to the records) each group has the duration specified by the rule parameter each group is labeled by the start of the group e.g. for rule='1D' , the group is labeled by the start of the day, if rule='1h' , then 12:00 means 12:00-13:00 records are assigned to the group based on their datetime value the records that exactly match the start/end of the group are assigned depending on the closed parameter left (default): the record is assigned to the group with the matching start right : the record is assigned to the group with the matching end There are two possible results of resampling (both may appear in the same dataframe): Downsampling : multiple records in the same time group. Upsampling : no values in the group. The missing values are filled with NaN . note that the NaN automatically changes the data type of the column to float , and it is not reverted by filling the missing values later. Therefore, when upsampling, we have to manually change the data type of each column to the original data type after filling the values. We can use the ffill function to fill in the missing values. Example: Python df.resample('1H').sum().ffill()","title":"Time aggregation"},{"location":"Programming/Python/Pandas%20Manual/#transformation-functions","text":"The most common transformation functions are: - cumsum : cumulative sum, i.e., the sum of the current and all previous values. - diff : difference between the current and the previous row. - the periods parameter specifies which row to use for the difference. By default, it is the previous row (periods=1). For next row, use periods=-1, but note that the result is then negative. We can use the abs function to get the absolute value. - ffill : Forward fill, i.e., fill the missing values with the next available value.","title":"Transformation functions"},{"location":"Programming/Python/Pandas%20Manual/#custom-transformation-function","text":"For custom transformation functions, we use the transform function.","title":"Custom transformation function"},{"location":"Programming/Python/Pandas%20Manual/#joins","text":"Similarly to SQL, Pandas has a way to join two dataframes. There are two functions for that: merge : the most general function that has the behavior known from SQL join : a more specialized function, The following table lists the most important differences between the two functions: property merge join default join type inner left join right table via column (default) or index ( right_index=True ) index join left table via column (default) or index ( left_index=True ) index, or column ( on=key_or_keys ) There is also a static pd.merge function. All merge and join methods are just wrappers around this function. The indexes are lost after the join (if not used for the join). To keep an index, we can store it as a column before the join.","title":"Joins"},{"location":"Programming/Python/Pandas%20Manual/#appending-and-concatenating-data","text":"In pandas, there is a concat function that can be used to concatenate data: pd.concat([df1, df2]) It can concatenate dataframes or series and it can concatenate vertically (by rows, default) or horizontally (by columns) By default, the indices from both input parameters are preserved. To reset the index, we can use the ignore_index parameter. Alternatively, to preserve one of the indices, we can set the index of the other dataframe to the index of the first dataframe before the concatenation using the set_index function.","title":"Appending and Concatenating data"},{"location":"Programming/Python/Pandas%20Manual/#io","text":"","title":"I/O"},{"location":"Programming/Python/Pandas%20Manual/#csv","text":"For reading csv files, we can use the read_csv function. Important params: sep : separator header : row number to use as column names. If None , no header is used skiprows : number of rows to skip from the beginning delim_whitespace : if True , the whitespace is used as a separator. The sep parameter is ignored in that case. This is a way how to read a file with variable number of whitespaces between columns. For export, we can use the to_csv method: df.to_csv(<file name> [, <other params>]) Useful parameters: index : if False , the index is not exported index_label : the name of the index column","title":"csv"},{"location":"Programming/Python/Pandas%20Manual/#json","text":"For exporting to json, we can use the to_json function. By default, the data are exported as a list of columns. To export the data as a list of rows, we can use the orient parameter: df.to_json(<file name>, orient='records') Other important parameters: indent : the number of spaces to use for indentation","title":"Json"},{"location":"Programming/Python/Pandas%20Manual/#excel","text":"For reading excel files, we can use the read_excel function. Important params: sheet_name : the name of the sheet to read. If None , the first sheet is read.","title":"Excel"},{"location":"Programming/Python/Pandas%20Manual/#insert-dataframe-into-db","text":"We can use the to_sql method for that: df.to_sql(<table name>, <sql alchemy engine> [, <other params>]) Important params: to append, not replace existing records: if_exists='append' do not import dataframe index: index=False For larger datasets, it is important to not insert everything at once, while also tracking the progress. The following code does exactly that def chunker(seq, size): return (seq[pos:pos + size] for pos in range(0, len(seq), size)) chunksize = int(len(data) / 1000) # 0.1% with tqdm(total=len(data)) as pbar: for i, cdf in enumerate(chunker(data, chunksize)): cdf.to_sql(<table name>, <sqlalchemy_engine>) pbar.update(chunksize) If the speed is slow, it can be caused by a low upload speed of your internet connection. Note that due to the SQL syntax, the size of the SQL strings may be much larger than the size of the dataframe.","title":"Insert dataframe into db"},{"location":"Programming/Python/Pandas%20Manual/#latex-export","text":"Currently, the DataFrame.to_latex function is deprecated. The Styler class should be used for latex exports instead. You can get the Styler from the DataFrame using the style property. The usual workfolow is: Create a Styler object from the dataframe using the style property, Apply the desired formatting to the styler object, Export DataFrame to latex using the Styler.to_latex method. Keep in mind that the Styler object is immutable, so you need to assign the result of each formatting operation to a new variable or chain the calls . Example: # wrong, the format is not apllied df.style.format(...) df.style.to_latex(...) # correct: temp var s = df.style.format(...) s.to_latex(...) # correct: chain calls df.style.format(...).to_latex(...) Finally, to produce a nice output, tou should use print to display the latext string: print(styler.to_latex())","title":"Latex export"},{"location":"Programming/Python/Pandas%20Manual/#formatting-the-index-columns-and-row-labels","text":"The columns' and row labels' format is configures by the format_index function. Important parameters: axis : 0 for rows, 1 for columns (cannot be both) escape : by default, the index is not escaped, to do so, we need to set escape to 'latex' .","title":"Formatting the index: columns and row labels"},{"location":"Programming/Python/Pandas%20Manual/#formatting-or-changing-the-values","text":"The values are formated by the format function. Important parameters: escape : by default, the values are not escaped, to do so, we need to set escape to 'latex' . na_rep : the string to use for missing values precision : the number of decimal places to use for floats","title":"Formatting or changing the values"},{"location":"Programming/Python/Pandas%20Manual/#replacing-values","text":"For replace some values for the presentation with something else, we can also use the format function.For example, to change the boolean presentation in column col we call: df.style.format({'col': lambda x: 'yes' if x else 'no'})","title":"Replacing values"},{"location":"Programming/Python/Pandas%20Manual/#hihglighting-minmax-values","text":"For highlighting the min/max values, we can use the highlight_min and highlight_max functions. Important parameters: subset : the columns in which the highlighting should be applied props : the css properties to apply to the highlighted cells","title":"Hihglighting min/max values"},{"location":"Programming/Python/Pandas%20Manual/#hiding-some-columns-rows-or-indices","text":"For hiding some columns, rows, or indices, we can use the hide function. Important Parameters: axis : 0 for hiding row indices (default), 1 for hiding column names level : the level of the multi-index to hide (default is all levels) subset : the columns or rows to hide (default is all columns or rows) When used without the subset parameter, the hide function hides the whole index. To hide just a selected row or column from the data, the subset parameter has to be used. By default, the <index_name> refers to the row index. To hide a column : df.style.hide_columns(<column name>, axis=1)","title":"Hiding some columns, rows, or indices"},{"location":"Programming/Python/Pandas%20Manual/#changing-the-header-column-labels","text":"There is no equivalent to the header parameter of the old to_latex function in the new style system. Instead, it is necessary to change the column names of the dataframe.","title":"Changing the header (column labels)"},{"location":"Programming/Python/Pandas%20Manual/#exporting-to-latex","text":"For the export, we use the to_latex function. Important parameters: convert_css : if True , the css properties are converted to latex commands multirow_align : the alignment of the multirow cells. Options are t , c , b hrules : if set to True , the horizontal lines are added to the table, specifically to the top, bottom, and between the header and the body. Note that these hrules are realized as the \\toprule , \\midrule , and \\bottomrule commands from the booktabs package, so the package has to be imported . clines : configuration for hlines between rows. It is a string composed of two parts divided by ; (e.g.: skip-last;data ). The parts are: whether to skip last row or not ( skip-last or all ) whether to draw the lines between indices or the whole rows ( index or data )","title":"Exporting to latex"},{"location":"Programming/Python/Pandas%20Manual/#displaying-the-dataframe-in-console","text":"We can display the dataframe in the conslo print or int the log just by supplying the dataframe as an argument because it implements the __repr__ method. Sometimes, however, the default display parameters are not sufficient. In that case, we can use the set_option function to change the display parameters: pd.set_option('display.max_rows', 1000) Important parameters: display.max_rows : the maximum number of rows to display display.max_columns : the maximum number of columns to display display.max_colwidth : the maximum width of a column","title":"Displaying the dataframe in console"},{"location":"Programming/Python/Pandas%20Manual/#other-useful-functions","text":"drop_duplicates to quickly drop duplicate rows based on a subset of columns. factorize to encode a series values as a categorical variable, i.e., assigns a different number to each unique value in series. pivot_table : function that can aggragate and transform a dataframe in one step. with this function, one can create a pivot table, but also a lot more. cut : function that can be used to discretize a continuous variable into bins.","title":"Other useful functions"},{"location":"Programming/Python/Pandas%20Manual/#pivot_table","text":"The pivot_table function do a lot of things at once: it aggregates the data it transforms the data it sorts the data due to reindexing Although this function is very powerfall there are also many pitfalls. The most important ones are: column data type change for columns with missing values","title":"pivot_table"},{"location":"Programming/Python/Pandas%20Manual/#column-data-type-change-for-columns-with-missing-values","text":"The tranformation often creates row-column combinations that do not exist in the original data. These are filled with NaN values. But some data types does not support NaN values, and in conclusion, the data type of the columns with missing values is changed to float . Possible solutions: we can use the fill_value parameter to fill the missing values with some value that is supported by the data type (e.g. -1 for integers) we can use the dropna parameter to drop the rows with missing values we can change the data type of the columns with missing values prior to calling the pivot_table function. For example, the pandas integer data types support NaN values.","title":"Column data type change for columns with missing values"},{"location":"Programming/Python/Pandas%20Manual/#to_datetime","text":"The to_datetime function can convert various inputs to datetime. It can be used to both scalars and vectors. Important parameters: unit : the unit of the input, e.g., s for seconds. origin : the origin of the input, e.g., unix for unix timestamps. It can be also any specific datetime object. format : the format of the input, e.g., %Y-%m-%d for 2021-01-01 .","title":"to_datetime"},{"location":"Programming/Python/Pandas%20Manual/#squeeze","text":"The squeeze function removes the unnecessary dimension from a dataframe or series. It is usefull when we want to convert a dataframe with a single column to a series, or a series with a single value to a scalar.","title":"squeeze"},{"location":"Programming/Python/Pandas%20Manual/#geopandas","text":"homepage Geopandas is a GIS addon to pandas. Geometric operations are implemented in Shapely.","title":"Geopandas"},{"location":"Programming/Python/Pandas%20Manual/#basics","text":"Geopandas works with GeoDataFrame , a subclass of pandas DataFrame . The GeoDataFrame can contain normal columns (i.e., Pandas, non-geometric columns) and a geometry column implemented by GeoSeries (instead of the usual Pandas Series ). Every geodataframe can have multiple geometry columns. Each column can be of a different geometry type and it can have a different coordinate system (projection). However, a geodataframe can have only active geometry column at a time.","title":"Basics"},{"location":"Programming/Python/Pandas%20Manual/#active-geometry-column","text":"All geometric operations performed on a geodataframe are performed using the active geometry column. By default, if we create a geodataframe with a single geometry column, it becomes the active geometry column. However, sometimes we need to set it, for example: when we have multiple geometry columns, or when we rename the active geometry column. To set the active geometry column, we can use the set_geometry method. To check if the geodataframe has an active geometry column, we can check the active_geometry_name property: if gdf.active_geometry_name is None: print(\"No active geometry column\") else: print(f\"Active geometry column: {gdf.active_geometry_name}\")","title":"Active geometry column"},{"location":"Programming/Python/Pandas%20Manual/#setting-the-active-geometry-column","text":"To set the active geometry column, we can use the set_geometry method. gdf.set_geometry(<GEOMETRY COLUMN>, inplace=True)","title":"Setting the active geometry column"},{"location":"Programming/Python/Pandas%20Manual/#limitations","text":"Do not ever copy paste the geometries from jupyter notebook as the coordinates are rounded! Use the to_wkt function instead.","title":"Limitations"},{"location":"Programming/Python/Pandas%20Manual/#create-a-geodataframe-from-csv","text":"Geopandas has it's own read_csv function, however, it requires a very specific csv format, so it is usually easier to first import csv to pandas and then create geopandas dataframe from pandas dataframe.","title":"Create a geodataframe from CSV"},{"location":"Programming/Python/Pandas%20Manual/#converting-pandas-dataframe-to-geopandas-dataframe","text":"The geopandas dataframe constructor accepts pandas dataframes, we just need to specify the geometry column and the coordinate system: gdf = gpd.GeoDataFrame( <PANDAS DATAFRAME> geometry=gpd.points_from_xy(<X COLUMN>, <Y COLUMN>), crs=<SRID> )","title":"Converting pandas Dataframe to geopandas Dataframe"},{"location":"Programming/Python/Pandas%20Manual/#create-geodataframe-from-shapely","text":"To load data from shapely, execute gdf = gpd.read_file(<PATH TO FOLDER WITH SHAPEFILES>)","title":"Create geodataframe from shapely"},{"location":"Programming/Python/Pandas%20Manual/#working-with-the-geometry","text":"The geometry can be accessed using the geometry property of the geodataframe.","title":"Working with the geometry"},{"location":"Programming/Python/Pandas%20Manual/#geoseries-operations","text":"There are many useful functions for working with the geometry columns or GeoSeries in general: line_merge : for each member of the series, this function merge MultiLineStrings into a single LineString if possible. union_all : compute the union of all geometries in the series. for linestring geometries, this typically creates a MultiLineString (i.e., it does no merge the lines into a single one)","title":"GeoSeries operations"},{"location":"Programming/Python/Pandas%20Manual/#merging-linestrings-in-the-whole-series","text":"There is no dedicated function for merging linestrings in the whole series. We have to: create a multi-linestring geometry from the series using the union_all function merge the multi-linestring geometry into a single linestring with Shapely's line_merge function (see the Python manual for more details)","title":"Merging linestrings in the whole series"},{"location":"Programming/Python/Pandas%20Manual/#spliting-multi-geometry-columns","text":"If the geometry column contains multi-geometries, we can split them into separate rows using the explode function: gdf = gdf.explode()","title":"Spliting multi-geometry columns"},{"location":"Programming/Python/Pandas%20Manual/#insert-geodataframe-into-db","text":"","title":"Insert geodataframe into db"},{"location":"Programming/Python/Pandas%20Manual/#simple-insertion","text":"When the data are in the correct format and we don't need any customization for the db query, we can use the to_postgis method. For this to work: the geodataframe must have an active geometry column, the columns of the geodataframe have to match the corresponding database table. This process is same as when working with pandas Example: gdf.to_postgis(<TABLE NAME>, <SQL ALCHEMY CONNECTION>, if_exists='append')","title":"Simple insertion"},{"location":"Programming/Python/Pandas%20Manual/#customized-insertion-geoalchemy","text":"If we need some special insert statement, we cannot rely on the geodataframe.to_postgis function, as it is not flexible enough. The pandas dataframe.to_sql function is more flexible, however, it has trouble when working with geodata. The easiest options is therefore to use geoalchemy , the database wraper used in geopandas (extension of sqlalchemy , which is a database wrapper for pandas ). First, we need to create the insert statement. The example here uses a modification for handeling duplicite elements. meta = sqlalchemy.MetaData() # create a collection for geoalchemy database # objects table = geoalchemy2.Table( '<TABLE NAME>', meta, autoload_with=<SQL ALCHEMY CONNECTION>) insert_statement = sqlalchemy.dialects.postgresql.insert(table).on_conflict_do_nothing() In the above example, we create a geoalchemy representation of a table and then we use this representation to create a customized insert statement (the on_conflict_do_nothing is the speciality here.). Note that we use a speciatl PostgreSQL insert statement instead of the standard SQLAlchemy insert statement. Second, we need to prepare the data as a list of dictionary entries: list_to_insert = [ {'id': 0, 'geom': <GEOM>, ...}, {'id': 0, 'geom': <GEOM>, ...}, .... ] Note that the geometry in the geodataframe is in the shapely format. Therefore, we need to convert it to string using the geoalchemy from_shape function: geoalchemy2.shape.from_shape(<GEOMETRY>, srid=<SRID>) Finally, we can execute the query using an sqlalchemy connection: sqlalchemy_connection.execute(insert_statement, list_to_insert)","title":"Customized Insertion: geoalchemy"},{"location":"Programming/Python/Pandas%20Manual/#exporting-geodataframe-to-a-file","text":"The geodataframe can be exported to a file using the to_file method. The format is determined by the file extension.","title":"Exporting geodataframe to a file"},{"location":"Programming/Python/Pandas%20Manual/#pandas-in-jupyter","text":"","title":"Pandas in Jupyter"},{"location":"Programming/Python/Pandas%20Manual/#displaying-more-rows-or-columns","text":"To display more rows or columns in Jupyter than the default, we have to set the pandas option_context to the desired values: # instead of df # write with pd.option_context('display.max_rows', 100, 'display.max_columns', 10): display(df)","title":"Displaying more rows or columns"},{"location":"Programming/Python/Plotly%20Manual/","text":"In plotly, we have two options: plot quickly with plotly express full control using graph objects Note that these options can hardly be mixed. For example, we cannot use plotly express to create a figure and then add a subplot to it. Similarly, we cannot use the make_subplots function to create a figure and then add a plotly express plot to it. In general, it is easier to use plotly express, so we should use it if we are not affected by its limitations. The plotly express cannot create custom subplots . However, automatic \"facet\" subplots (same plot divided between multiple plots using some data attribute) are possible. z-order of the traces. For example, we need to first plot a trace using graph objects. Then it is much easier to plot the rest of the traces using graph objects as well. Plotly Express \u00b6 documentation Plotly express modul is loaded as: import plotly.express as px Common Parameters For All Types of Plots \u00b6 data_frame : the dataframe to use. Mandatory, first positional parameter. x : the name of the column to use as x axis. Mandatory, second positional parameter. color : the name of the column to use as color. facet_col : the name of the column to use as facet column. facet_row : the name of the column to use as facet row. color_discrete_sequence : the list of colors in hexadecimal format to use for the color column. If the number of colors is less than the number of categories, the colors are reused. If the number of colors is greater, the colors are truncated. title : the title of the plot. hover_data : the list of columns to show in the hover tooltip. Axes columns are shown automatically. text : text labels for the data points. Automatic color assignment \u00b6 If we use the color parameter of a graph, plotly express plots a trace for each color value and assigns a color to the trace. To customize this color, we can use two parameters: color_discrete_sequence : list of the colors to be used color_discrete_map : dictionary mapping the color values to the colors. With the color_discrete_sequence parameter, plotly express iterates through the list of colors and assigns the colors to the color values in the order they appear in the data. If the number of colors is less than the number of color values, the colors are reused. If the number of colors is greater, the colors are truncated. Therefore, this parameter is useful only if: the number of colors is equal to the number of color values the number of colors is greater than the number of color values and we use categorical colors, so the truncation does not matter. The color_discrete_map parameter is more flexible. We can manually assign the colors to the color values, to use the color scale optimally. Histogram \u00b6 documentation Note that the Plotly histogram is only good for simple cases of small size . See below for more details. Plotly express has histogram function for creating histograms. The basic syntax is: px.histogram(<dataframe>, <xcol name>) The y is then the number of occurences of each value in the x column. Important parameters: nbins : number of bins. Plotly Histogram Limitations \u00b6 Plotly histogram is only good for simple cases of small size. This is because it first stores all data points in JSON and only computes the bins on the javascript size. As a result, the function is slow and the size of the Jupyter notebook cell can be enormous (hundreds of MBs). For more complex figures, it is better to generate the histogram manually (using numpy or pandas) and then plot it using the px.bar function. Bar Chart \u00b6 documentation reference For bar charts, we use the px.bar function. The basic syntax is: px.bar(<dataframe>, <xcol name>, <y col name>) Important parameters: barmode : how to combine the bars in case of multiple traces. Can be group (default), stack (default in facet plots ), relative or overlay . bargap : the gap between bars. bar_groupgap : the gap between the bars from the same group (only for barmode = group ). Unfortunately, there is no way how to represent missing values in the bar chart (they appear as y = 0 ). To mark the missing values, we can use annotations. Why bar charts with a lot of records appear transparent? \u00b6 If the number of records is large, the bar chart may appear transparent. This is because each bar has a border, which has a brighter color. To prevent this effect, we have to remove the border: fig.update_traces(marker_line_width=0) Numerical vs categorical color \u00b6 The values in the color column are interpreted as numerical (continuous) if the column is numeric and as categorical if the column is of any other type. Even if the color column contains only integers, it is still interpreted as numerical with all consequences (color bar instead of categorical colors, color_discrete_sequence parameter is ignored, etc.). To force the categorical interpretation, we can convert the column to a string. Example: px.bar(df, x=\"x\", y=\"y\", color=df[\"color\"].astype(str)) Scatter Plot \u00b6 documentation For scatter plots, we use the scatter function. The basic syntax is: px.scatter(<dataframe>, <xcol name>, <y col name>) Important parameters: Line Chart \u00b6 documentation reference For line charts in plotly express, we use the px.line function. The basic syntax is: px.line(<dataframe>, <xcol name>, <y col name>) Automatic Subplots: Facet plots \u00b6 Facet plots can be created using the same plot function as for normal plotly express plots and supplying the facet_col and/or facet_row parameters. Example: fig = px.histogram(df, x=\"column\", facet_row=\"<col 1>\", facet_col=\"<col 2>\") Here, the figure will be devided into subplotts. Each row will share the <column 1> values, and each column will share the <column 2> values. The number of rows and columns will be determined automatically as the number of unique values in <column 1> and <column 2> , respectively. Independent axes between rows and columns \u00b6 It can happen that each row or column should have its own x or y axis due to a different scale. We can accomplish this by calling the update_xaxes and update_yaxes functions on the figure. Example: fig.update_xaxes(matches=None) fig.update_yaxes(matches=None) Sharing the axes titles between rows and columns \u00b6 Unfortunately, the axes titles cannot be easily shared between rows and columns. The only way is configure the axis text manually. Example: # remove y axis titles except for the first subplot for i in range(1, 4): fig.update_yaxes(title_text='', row=1, col=i) # set the text of the first y axis fig.update_yaxes(title_text=\"Comp. time relative to IH\", row=1, col=1) 3D Scatter Plot \u00b6 documentation For 3D scatter plots, we use the scatter_3d function. The basic syntax is: fig = px.scatter_3d(<dataframe>, <xcol name>, <y col name>, <z col name>) Plotly Graph Objects \u00b6 documentation If the plotly express is not enough, we can use the graph objects. We need to use the second option only for complex figures, for example: facet plots with more than one metric plots with custom traces behind the plotly express traces We can either add the graph objects to a plotly express figure or create a graph objects figure from scratch. Most of the time, we will use the first option, as using plotly express is easier. For adding traces to the figure, we can use the add_trace function. Important parameters: row , col : the row and column of the subplot where to add the trace. Both parameters have to be set. The numbering starts from 1. To make the figure from scratch , we can use the make_subplots function from the plotly.subplots module. Example: from plotly.subplots import make_subplots fig = make_subplots(rows=2, cols=2) Important parameters: rows , cols : the number of rows and columns in the figure shared_xaxes , shared_yaxes : configures the axes sharing. Possible values: False (default) : each subplot has its own axes True : only one axis per row (for shared_xaxes ) or column (for shared_yaxes ). row or col : equivalent to True , applicable only to shared_xaxes or shared_yaxes , respectively. all : all subplots share the same axes horizontal_spacing , vertical_spacing : the spacing between the subplots in relative units, values are in the range [0, 1] . The default is 0.2 for both, which means that the space between each subplot is 20% of the figure width/height. subplot_titles : the titles of the subplots. Common Parameters For All Types of Plots \u00b6 The main difference between the plotly express and the graph objects is the data parameters. For plotly express, we pass the dataframe and the column names for the x and y axes. In contrast, we pass the data to the graph functions as iterables, one for each axis . Therefore, we have to use a single column from the dataframe, not the whole dataframe, even if it contains just one column. Legend \u00b6 The name in the legend is determined by the name parameter of the trace. To share the legend between multiple traces, the following steps are needed: set the name parameter of all traces to the same value set the showlegend parameter of one trace to True and to False for all other traces Bar Chart \u00b6 The basic syntax is: go.Bar(x, y, ...) Some more complicated examples are in the documentation . Stacked or grouped bars \u00b6 Unlike plotly express, the graph objects do not have the color parameter to set the column to use for determining the group to which the bar belongs. There are two options how to create stacked or grouped bars: create a trace for each group manually and add them all to the figure crete the figure with plotly express and then extract the traces from the figure python for trace in occ_fig.data: fig.add_trace(trace, row=1, col=i + 1) Also, to set the bar mode, we need to use the update_layout as the go.Bar function does not have the barmode parameter. Example: fig.update_layout(barmode=\"stack\") Line and Scatter Plots \u00b6 line plot documentation Line plots, scatter plots and shapes, all of that can be created by the go.Scatter function. Example: go.Scatter(x, y, ...) Important parameters: mode : the mode of the line. Can be: lines , markers , lines+markers , text , lines+text , markers+text , lines+markers+text . The default is lines+markers if there are less than 20 data points and lines otherwise. line : dictionary containing the line parameters. The most important parameters are: color : the color of the line width : the width of the line Shapes \u00b6 documentation There are various ways how to create shapes in plotly: for simple higlight of regions on x or y axis, we can use the add_vrect and add_hrect functions ( introductory example , documentation ). only for recangles spaning the whole height or width of the plot The shapes can be created by supplying the coordinates of the shape to the go.Scatter function and setting the fill parameter to \"toself\" . ( documentation ) this method has a serious limitation: the filled scatter plots are always above the other traces finally, we can use the add_shape function to add a shape to the figure. There is no way how to fill shapes with a patern fill. Create subplots \u00b6 documentation and examples To create a figure with multiple subplots, we use the make_subplots function. Example: fig = make_subplots(rows=2, cols=2, start_cell=\"bottom-left\") fig.add_trace(go.Scatter(x=[1, 2, 3], y=[4, 5, 6]), row=1, col=1) ... fig.show() Importanta parameters: shared_xaxes and shared_yaxes : if True , the subplots will share the same x and y axes. x_title and y_title : the titles of the x and y axes. horizontal_spacing and vertical_spacing : the spacing between the subplots. 3D plots \u00b6 Common Parameters \u00b6 Setting color of the plots with continuous color scale \u00b6 If the plot type is continuous (e.g., surface plot, cone plot), we cannot set a color for the whole trace as a continuous color scale is used. However, we can set the colorscale using the colorscale parameter. Example: fig.add_trace(go.Surface(z=z, colorscale=\"Viridis\")) The color scale can be also set manually as a list of colors. This way, we can overcome the limitation of the continuous color scale and set the color of the whole trace. Example: fig.add_trace(go.Surface(z=z, colorscale=[[0, \"red\"], [1, \"red\"]])) 3D Scatter Plot \u00b6 documentation The 3D scatter plot is created using the go.Scatter3d function. Example: go.Scatter3d(x, y, z, ...) Surface Plots \u00b6 There are multiple types of plots that may be considered as surface plots in plotly: 3D surface ( go.Surface ): a surface plot defined by a grid of x, y, and a 2D array of z values of the shape (|x|, |y|). The surface is plotted for all combinations of x and y, which makes this function suitable only for cases where the surface is defined for all combinations of x and y. 3D mesh ( go.Mesh3d ): a surface plot defined by vertices (points) and faces (connections between the points). The connections are defined by triangles, which can be either defined manually or computed automatically using a triangulation algorithm. Tri-surface ( figure_factory.create_trisurf ): a surface plot created by a triangulation of the data points. It accepts triangles in a single argument. I have to further investigate, how it differs from the 3D mesh plot. 3D Surface Plot \u00b6 documentation The 3D surface plot is created using the go.Surface function. 3D Mesh Plot \u00b6 documentation The 3D mesh plot is created using the go.Mesh3d function. Example: x = [1, 1, 1] y = [1, 2, 1] z = [0, 0, 1] fig.add_trace(go.Mesh3d(x=x, y=y, z=z)) This way, the triangulation is used to compute the connections between the points. If we want to use a different triangulation, we can use the i , j and k parameters. These parameters define the indices (in the source data used for x , y , and z ) of the vertices that form the triangles. For example, i[0] , j[0] and k[0] define the vertices of the first triangle. Example code: x = [1, 1, 1] y = [1, 2, 1] z = [0, 0, 1] i = [0] j = [1] k = [2] fig.add_trace(go.Mesh3d(x=x, y=y, z=z, i=i, j=j, k=k)) Note that here, the i , j and k parameters are redundant, as there is only one possible triangulation of the three points. However, for more complex surfaces, there can be multiple valid triangulations, and the i , j and k parameters can be used to select the triangulation manually. Cone Plots \u00b6 documentation reference The main parameters of the cone plot are: x , y , z : the coordinates of the cone base u , v , w : the vector defining the direction and length of the cone Customizing the Figure \u00b6 The figure object can be custommized in many ways. Size and margins \u00b6 The size and margins can be set using the figure.update_layout function. The specific parameters are: width : the width of the figure in pixels height : the height of the figure in pixels autosize : needs to be False if we want to set the width and height manually margin : dictionary containing the margins ( l , r , t , b ) and one another property: pad . All properties are in pixels. l , r , t , b : distance between the plot and the figure border. Note that titles are not included in the plot, so we have make space for the titles if we set margins manually. pad : distance between the plotting area (i.e., the plotting coordinates for data points) and the axis lines. Most of the time, this should be set to 0 (default) Unfortunately, there is no way how to set the margins automatically to fit all content like titles, annotations, etc. documentation reference Customize axes \u00b6 For customizing the axes, we can use the figure.update_xaxes and figure.update_yaxes functions. By default, the functions will update all axes. To update only a specific axis, we can use the row and col parameters. axis reference The range of the axis is determined automatically as the range of the data plus some margin. If we want any other range, we need to set it manually using the range parameter, e.g.: range=[0, 1] . Unfortunately, there is no way how to automatically set the range to match the data range exactly Another thing we usually want to customize are the ticks. Important tick parameters are: dtick : the distance between the ticks tickvals : the exact values of the ticks. This overrides the dtick parameter. ticks : the position of the ticks. Can be outside , inside or \"\" (no ticks, default). ticklen : the length of the ticks in pixels tickformat : the format of the tick labels. Depending on the axis datatype, we can use number formats (e.g., \".2f\" for two decimal places), datetime formats (e.g., \"%Y-%m-%d\" for dates) or scientific notation (e.g., \"e\" for scientific notation). for percentage, we can use \".0%\" for integer percentage and \".1%\" for one decimal place. Note that this way, the % sign is added automatically to each tick label. If we do not want this, we can either set the text manually using the ticktext parameter, or multiply the data by 100. tickangle : the angle of the tick labels in degrees Other important parameters are: title_text : the title of the axis. linecolor : the color of the axis line gridcolor : the color of the grid lines mirror : if True , the axis line will be mirrored to the other side of the plot Drawing borders using axes \u00b6 The borders can be drawn using the axes. To draw a black border around the plot, we can use the following code: fig.update_xaxes(linecolor=\"black\", mirror=True) fig.update_yaxes(linecolor=\"black\", mirror=True) Customizing datetime axes \u00b6 Unfortunately, we cannot treat the datetime axes as expected, i.e., using the datetime objects. For example, to set the tick interval, we cannot use the datetime.timedelta object. Instead, we need to use the number of milliseconds. Example: fig.update_xaxes(dtick=1000 * 60 * 60 * 24 * 7) # one week interval Customizing shared axes \u00b6 By default, only tick labels are shared between subplots with shared axes. To share other properties (tick markers, axis title) we need to hide them manually and add them to the first subplot. Example: fig.update_yaxes(showticklabels=False) fig.update_yaxes(showticklabels=True, row=1, col=1) 3d axes \u00b6 documentation Unfortunately, the customization of the 3d axes works differently than for the 2d axes. The update_xaxes and update_yaxes functions do not work for the 3d axes. Instead, we need to use the update_layout function with the scene parameter. Example: fig.update_layout(scene=dict( xaxis=dict( title=\"x\", titlefont_size=16, tickfont_size=14, ), yaxis=dict( title=\"y\", titlefont_size=16, tickfont_size=14, ), zaxis=dict( title=\"z\", titlefont_size=16, tickfont_size=14, ), )) One thing that is not possible to customize idn 3D is the position of the axis . The x and y axes are always in the bottom, while the z axis is always on the left. Change position of the axis title \u00b6 Unfortunately, there is no way how to change the position of the axis title. The solution is to hide the title and add a new annotation with the title text. Example: fig.update_xaxes(title_text=\"\") fig.add_annotation( text=\"Comp. time relative to IH\", xref=\"paper\", yref=\"paper\", x=-0.09, y=0.5, showarrow=False, font=dict( size=14, ), textangle=270, ) Legend \u00b6 documentation reference The legend can be styled using the figure.update_layout function. The most important parameters are: legend_title : the title of the legend legend : dictionary containing many parameters orientation : h or v for horizontal or vertical legend x , y : the position of the in normalized coordinates of the whole plot. 'xref', yref : the coordinate system of the x and y coordinates. Can be \"container\" : the whole plot \"paper\" (default): the plotting area xanchor , yanchor : the position of the legend box relative to the x and y coordinates title : if string, the title of the legend (equivalent to the legend_title parameter). If dictionary, multiple legend title parameters can be set. bordercolor : the color of the legend border borderwidth : the width of the legend border Unfortunately, there is no way how to customize the padding between the legend items and the legend box . Also, it is not possible to set the legend border to have rounded corners . Hide legend \u00b6 To hide the legend, we can use the showlegend parameter. Example: fig.update_layout(showlegend=False) Legend items order \u00b6 The order of the legend items is determined by the order of the traces in the figure. However, we can change the order using the legend_traceorder parameter. Example: fig.update_layout(legend_traceorder=\"reversed\") Legend position \u00b6 The coordinates of the legend are normalized with respect to each axis. By default, the legend x and y coordinates are set so that the legend is outside the plot, in the top right corner. If the legend is positioned inside the plot, the plot expands to the whole width, but if the legend is positioned outside the plot, the plot width is smaller to leave space for the legend. Legend items text \u00b6 The legend item text is determined by the name parameter of the trace. Therefore, to customize the legend item text, we need to set the name parameter of the trace. For normal single trace functions, this is simple: fig.add_trace(go.Scatter(x=x, y=y, name=\"Custom name\")) However, it can be complicated for plotly express functions that plot multiple traces at once, as these determine the name parameter automatically from data. For example, when we use the color parameter, the name parameter is set to the color value. To overcome this, we have two options: set the name parameter manually for each trace after the figure is created change the data so that the name parameter is set automatically to the desired value. The first approach is usually preferable as we do not mix the data and appearance. To change the name parameter, we can use the update_traces function: for trace in fig.data: trace.name = process_name(trace.name) Figure Annotations \u00b6 documentation For adding annotations to to a figure, we can use the add_annotation function. The most compliated and also most limited aspect is the positioning of the annotation. There are two things we can position: the head of the annotation arrow, or the position of the text in case of no arrow the tail of position of the annotation arrow, if present Note that there are some important limitations of annotations : There is no way how to position the text of the annotation. If we need a specific position of the text, we need to add it as a separate annotation. This brings a huge burden, as we need to calculate the right offset from the arrow, which depends on the angle, but also the dimensions of the plot The textangle parameter of the text is in the coordinate system of the plot, not the coordinate system of the data. A recalculation is needed if we want to align a data based arrow annotation with a text annotation. These annotations only work for 2D plots. For 3D plots, custom solution with 3D line and cone plots is needed. Setting the head of the annotation \u00b6 There are two settings for head of the annotation for each axis: xref and yref : the mode of the positioning, and x and y : the coordinates The xref and yref parameters can be set to: <axis name> : The axis mode, where the axis coordinates are used for positioning. default for simple plots: xref=\"x\" and yref=\"y\" for subplots, we use \"x1\"-\"x<n>\" or \"y1\"-\"y<n>\" as <axis name> paper : The whole figure mode, where 0 means the left/bottom and 1 means the right/top of the figure. default for subplots Setting the tail of the annotation \u00b6 There are two settings for the tail of the annotation for each axis: axref and ayref : the mode of the positioning, and ax and ay : the coordinates The axref and ayref parameters can be set to: \"pixel\" : The pixel mode, where the coordinates are in pixels relative to the whole figure, where the origin is the bottom left corner of the figure. default <axis reference> : Same as for the head of the annotation, but must be equal to the xref / yref , ottherwise, the arrow will not be visible. e.g., setting xref to \"x1\" and axref to \"x2\" is not allowed Configuring the arrow \u00b6 There are many parameters for configuring the arrow. Morover, without configuring some of the parameters, the arrow head will not be visible at all . The most important parameters are: showarrow : toggl the whole arrow (head and tail). Default is True . arrowcolor : the color of the arrow line. arrowwidth : the width of the arrow line in pixels. Default is 1 . arrowhead : the number indicating the style of the arrow head. Possible values are: 0 : no arrow head (default) 1 : wide triangle arrow head 2 : narrow triangle arrow head 3 : open triangle arrow head (dart) 4 : open triangle arrow head (B2 spirit shape) 5 : open triangle arrow head (V shape) 6 : circle arrow head 7 : square arrow head arrowsize : the relative size of the arrow head to the arrowwidth . Default is 1 . Configuring the text \u00b6 By default, the annotation does not have any text, unless showarrow is set to False , in which case a default text is added to prevent the annotation from being completely empty. To add text, we use the text parameter. To style the text, we can use the following parameters: textangle : the angle of the text in degrees, font : dictionary containing font parameters size : the size of the font in pixels color : the color of the font weight : the weight of the font xanchor , yanchor : the position of the text relative to the text box. xanchor can be left , center or right yanchor can be top , middle or bottom Positinoing the text relative to the arrow \u00b6 There is no way how to position the text of the annotation with respect to the arrow . The text position is: the head of the annotation, if the arrow is not present, or the tail of the annotation, if the arrow is present If we want to, for example, need to position the text of the annotation above the arrow, so that the text is parallel to the arrow, we need to use two annotations. Other important parameters \u00b6 bgcolor : the background color of the annotation Markers \u00b6 documentation To style the markers, we can use the update_traces function. Example: fig.update_traces(marker=dict(size=10, line=dict(width=2, color='DarkSlateGrey'))) Marker parameters: size : the size of the marker in pixels line : dictionary containing border parameters width : the width of the border in pixels. The default is 0. This setting is ignored for 3d scatter plots, where the border width is always 1 (see github issue ) color : the color of the border color : the color of the marker. To make the marker transparent, it is best to use the rgba format. Example: rgba(255, 0, 0, 0.5) for a red marker with 50% transparency. Adding a marker to a hard-coded location \u00b6 To add a marker to a hard-coded location, we can add it as a new trace. Note that we can add new traces even to a figure created using plotly express. Lines \u00b6 Lines can be styled using the line parameter of the plotting functions or using the update_traces function. Important parameters: color : the color of the line width : the width of the line in pixels dash : the dash pattern. Can be \"solid\" (default) \"dot\" : dense dashed line \"dash\" : sparse dashed line ... Be aware that the dash pattern is proportional to the axis range . In result, it is practicaly impossible to use dash patterns on graphs with different axes ranges. Title \u00b6 documentation The title can be set using the plotly express functions or when creating the graph objects figure. To update the text or to customize the title layout, we can use the update_layout function with the title object parameter. Example: fig.update_layout( title={ 'x': 0.5, 'xanchor': 'center', 'y': 0.85, 'yanchor': 'top' } ) Important parameters: x , y : the x and y coordinates of the title. The origin is the bottom left corner of the figure. xanchor , yanchor : the position of the title relative to the x and y coordinates. Can be left , center or right for xanchor and top , middle or bottom for yanchor . Subplot titles \u00b6 Automatic subplot titles (facet plots) \u00b6 When using facet plots, the subplot titles are generated automatically as <facet column name>=<facet column value> . Usually, we want to remove the column name and keep only the value. To do this, we can use the for_each_annotation function. Example: fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1])) Individual subplot titles \u00b6 To set the title of an individual subplot, we can use the subplot_titles parameter of the make_subplots function. To position the subplot titles , we have to update all annotations. To prevent the need to filter the titles, it is best to update all annotations right after the make_subplots call. Example: fig = make_subplots(...) fig.update_annotations(yshift=10) # move the subplot titles 10 pixels up Z-Order of the Traces \u00b6 The z-order of the traces cannot be configured. Instead, the traces are drawn in the order they are added to the figure. This simple rule has an exception: the webGL traces are always drawn on top of the standard traces. Because all plotly express traces are WebGL traces, they are drawn on top of the graph objects traces added later if those are not WebGL. To overcome this limitation, we have two options: not combine plotly express and graph objects traces and convert everything to graph objects for figures with custom traces use the webGL versions of the graph objects traces. Example: python fig.add_trace(go.Scattergl(...)) Other Layout Parameters (background, borders, etc.) \u00b6 background color: plot_bgcolor border: borders are best drawn by showing and mirroring the axes (see the axis section above). Text \u00b6 documentation Subscript and superscript \u00b6 To add a subscript or superscript to a text, we can use HTML tags. Example: fig.add_annotation(x=0.5, y=0.5, text=\"Title<sub>subscript</sub>\", xref=\"x5\", yref=\"y5\", showarrow=False) Bold and italic \u00b6 To add bold or italic text, we can use the HTML tags <b> and <i> . Example: fig.add_annotation(x=0.5, y=0.5, text=\"Title<b>bold</b><i>italic</i>\", xref=\"x5\", yref=\"y5\", showarrow=False) Math symbols \u00b6 To add math symbols, we can use the LaTeX syntax. However, when using it, all text must be wrapped in math mode, not just the symbols. Example: fig.add_annotation(x=0.5, y=0.5, text=r\"$\\\\text{{Title: }} \\alpha \\\\text{{ and }} \\beta $\", xref=\"x5\", yref=\"y5\", showarrow=False) Note that the LaTeX syntax does not work in vscode out of the box [issue] . We need to add the following code: import plotly from IPython.display import display, HTML plotly.offline.init_notebook_mode() display(HTML( '<script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG\"></script>' )) Exporting the Figure \u00b6 documentation The static export is handeled using the figure's write_image function. Example: fig.write_image(\"figure.png\") The output format is determined by the extension. The margins of the figure should be set for the figure itself, not for the export. Troubleshooting \u00b6 Export hangs out \u00b6 It can be cause by kaleido. The solution si to install an older version, specifically 0.1.0.post1 . https://community.plotly.com/t/static-image-export-hangs-using-kaleido/61519/4 Exported size does not match the figure size \u00b6 Sometimes, the Kaleido engine resizes the figure before exporting. To overcome this, we can set the width and height parameters of the write_image function. Example: fig.write_image(\"figure.png\", width=1000, height=1000) Colors \u00b6 documentation For colors in plotly, we can use the color scales supplied by plotly express. These are: px.colors.sequential : sequential color scales px.colors.diverging : diverging color scales px.colors.qualitative : qualitative color scales","title":"Plotly Manual"},{"location":"Programming/Python/Plotly%20Manual/#plotly-express","text":"documentation Plotly express modul is loaded as: import plotly.express as px","title":"Plotly Express"},{"location":"Programming/Python/Plotly%20Manual/#common-parameters-for-all-types-of-plots","text":"data_frame : the dataframe to use. Mandatory, first positional parameter. x : the name of the column to use as x axis. Mandatory, second positional parameter. color : the name of the column to use as color. facet_col : the name of the column to use as facet column. facet_row : the name of the column to use as facet row. color_discrete_sequence : the list of colors in hexadecimal format to use for the color column. If the number of colors is less than the number of categories, the colors are reused. If the number of colors is greater, the colors are truncated. title : the title of the plot. hover_data : the list of columns to show in the hover tooltip. Axes columns are shown automatically. text : text labels for the data points.","title":"Common Parameters For All Types of Plots"},{"location":"Programming/Python/Plotly%20Manual/#automatic-color-assignment","text":"If we use the color parameter of a graph, plotly express plots a trace for each color value and assigns a color to the trace. To customize this color, we can use two parameters: color_discrete_sequence : list of the colors to be used color_discrete_map : dictionary mapping the color values to the colors. With the color_discrete_sequence parameter, plotly express iterates through the list of colors and assigns the colors to the color values in the order they appear in the data. If the number of colors is less than the number of color values, the colors are reused. If the number of colors is greater, the colors are truncated. Therefore, this parameter is useful only if: the number of colors is equal to the number of color values the number of colors is greater than the number of color values and we use categorical colors, so the truncation does not matter. The color_discrete_map parameter is more flexible. We can manually assign the colors to the color values, to use the color scale optimally.","title":"Automatic color assignment"},{"location":"Programming/Python/Plotly%20Manual/#histogram","text":"documentation Note that the Plotly histogram is only good for simple cases of small size . See below for more details. Plotly express has histogram function for creating histograms. The basic syntax is: px.histogram(<dataframe>, <xcol name>) The y is then the number of occurences of each value in the x column. Important parameters: nbins : number of bins.","title":"Histogram"},{"location":"Programming/Python/Plotly%20Manual/#plotly-histogram-limitations","text":"Plotly histogram is only good for simple cases of small size. This is because it first stores all data points in JSON and only computes the bins on the javascript size. As a result, the function is slow and the size of the Jupyter notebook cell can be enormous (hundreds of MBs). For more complex figures, it is better to generate the histogram manually (using numpy or pandas) and then plot it using the px.bar function.","title":"Plotly Histogram Limitations"},{"location":"Programming/Python/Plotly%20Manual/#bar-chart","text":"documentation reference For bar charts, we use the px.bar function. The basic syntax is: px.bar(<dataframe>, <xcol name>, <y col name>) Important parameters: barmode : how to combine the bars in case of multiple traces. Can be group (default), stack (default in facet plots ), relative or overlay . bargap : the gap between bars. bar_groupgap : the gap between the bars from the same group (only for barmode = group ). Unfortunately, there is no way how to represent missing values in the bar chart (they appear as y = 0 ). To mark the missing values, we can use annotations.","title":"Bar Chart"},{"location":"Programming/Python/Plotly%20Manual/#why-bar-charts-with-a-lot-of-records-appear-transparent","text":"If the number of records is large, the bar chart may appear transparent. This is because each bar has a border, which has a brighter color. To prevent this effect, we have to remove the border: fig.update_traces(marker_line_width=0)","title":"Why bar charts with a lot of records appear transparent?"},{"location":"Programming/Python/Plotly%20Manual/#numerical-vs-categorical-color","text":"The values in the color column are interpreted as numerical (continuous) if the column is numeric and as categorical if the column is of any other type. Even if the color column contains only integers, it is still interpreted as numerical with all consequences (color bar instead of categorical colors, color_discrete_sequence parameter is ignored, etc.). To force the categorical interpretation, we can convert the column to a string. Example: px.bar(df, x=\"x\", y=\"y\", color=df[\"color\"].astype(str))","title":"Numerical vs categorical color"},{"location":"Programming/Python/Plotly%20Manual/#scatter-plot","text":"documentation For scatter plots, we use the scatter function. The basic syntax is: px.scatter(<dataframe>, <xcol name>, <y col name>) Important parameters:","title":"Scatter Plot"},{"location":"Programming/Python/Plotly%20Manual/#line-chart","text":"documentation reference For line charts in plotly express, we use the px.line function. The basic syntax is: px.line(<dataframe>, <xcol name>, <y col name>)","title":"Line Chart"},{"location":"Programming/Python/Plotly%20Manual/#automatic-subplots-facet-plots","text":"Facet plots can be created using the same plot function as for normal plotly express plots and supplying the facet_col and/or facet_row parameters. Example: fig = px.histogram(df, x=\"column\", facet_row=\"<col 1>\", facet_col=\"<col 2>\") Here, the figure will be devided into subplotts. Each row will share the <column 1> values, and each column will share the <column 2> values. The number of rows and columns will be determined automatically as the number of unique values in <column 1> and <column 2> , respectively.","title":"Automatic Subplots: Facet plots"},{"location":"Programming/Python/Plotly%20Manual/#independent-axes-between-rows-and-columns","text":"It can happen that each row or column should have its own x or y axis due to a different scale. We can accomplish this by calling the update_xaxes and update_yaxes functions on the figure. Example: fig.update_xaxes(matches=None) fig.update_yaxes(matches=None)","title":"Independent axes between rows and columns"},{"location":"Programming/Python/Plotly%20Manual/#sharing-the-axes-titles-between-rows-and-columns","text":"Unfortunately, the axes titles cannot be easily shared between rows and columns. The only way is configure the axis text manually. Example: # remove y axis titles except for the first subplot for i in range(1, 4): fig.update_yaxes(title_text='', row=1, col=i) # set the text of the first y axis fig.update_yaxes(title_text=\"Comp. time relative to IH\", row=1, col=1)","title":"Sharing the axes titles between rows and columns"},{"location":"Programming/Python/Plotly%20Manual/#3d-scatter-plot","text":"documentation For 3D scatter plots, we use the scatter_3d function. The basic syntax is: fig = px.scatter_3d(<dataframe>, <xcol name>, <y col name>, <z col name>)","title":"3D Scatter Plot"},{"location":"Programming/Python/Plotly%20Manual/#plotly-graph-objects","text":"documentation If the plotly express is not enough, we can use the graph objects. We need to use the second option only for complex figures, for example: facet plots with more than one metric plots with custom traces behind the plotly express traces We can either add the graph objects to a plotly express figure or create a graph objects figure from scratch. Most of the time, we will use the first option, as using plotly express is easier. For adding traces to the figure, we can use the add_trace function. Important parameters: row , col : the row and column of the subplot where to add the trace. Both parameters have to be set. The numbering starts from 1. To make the figure from scratch , we can use the make_subplots function from the plotly.subplots module. Example: from plotly.subplots import make_subplots fig = make_subplots(rows=2, cols=2) Important parameters: rows , cols : the number of rows and columns in the figure shared_xaxes , shared_yaxes : configures the axes sharing. Possible values: False (default) : each subplot has its own axes True : only one axis per row (for shared_xaxes ) or column (for shared_yaxes ). row or col : equivalent to True , applicable only to shared_xaxes or shared_yaxes , respectively. all : all subplots share the same axes horizontal_spacing , vertical_spacing : the spacing between the subplots in relative units, values are in the range [0, 1] . The default is 0.2 for both, which means that the space between each subplot is 20% of the figure width/height. subplot_titles : the titles of the subplots.","title":"Plotly Graph Objects"},{"location":"Programming/Python/Plotly%20Manual/#common-parameters-for-all-types-of-plots_1","text":"The main difference between the plotly express and the graph objects is the data parameters. For plotly express, we pass the dataframe and the column names for the x and y axes. In contrast, we pass the data to the graph functions as iterables, one for each axis . Therefore, we have to use a single column from the dataframe, not the whole dataframe, even if it contains just one column.","title":"Common Parameters For All Types of Plots"},{"location":"Programming/Python/Plotly%20Manual/#legend","text":"The name in the legend is determined by the name parameter of the trace. To share the legend between multiple traces, the following steps are needed: set the name parameter of all traces to the same value set the showlegend parameter of one trace to True and to False for all other traces","title":"Legend"},{"location":"Programming/Python/Plotly%20Manual/#bar-chart_1","text":"The basic syntax is: go.Bar(x, y, ...) Some more complicated examples are in the documentation .","title":"Bar Chart"},{"location":"Programming/Python/Plotly%20Manual/#stacked-or-grouped-bars","text":"Unlike plotly express, the graph objects do not have the color parameter to set the column to use for determining the group to which the bar belongs. There are two options how to create stacked or grouped bars: create a trace for each group manually and add them all to the figure crete the figure with plotly express and then extract the traces from the figure python for trace in occ_fig.data: fig.add_trace(trace, row=1, col=i + 1) Also, to set the bar mode, we need to use the update_layout as the go.Bar function does not have the barmode parameter. Example: fig.update_layout(barmode=\"stack\")","title":"Stacked or grouped bars"},{"location":"Programming/Python/Plotly%20Manual/#line-and-scatter-plots","text":"line plot documentation Line plots, scatter plots and shapes, all of that can be created by the go.Scatter function. Example: go.Scatter(x, y, ...) Important parameters: mode : the mode of the line. Can be: lines , markers , lines+markers , text , lines+text , markers+text , lines+markers+text . The default is lines+markers if there are less than 20 data points and lines otherwise. line : dictionary containing the line parameters. The most important parameters are: color : the color of the line width : the width of the line","title":"Line and Scatter Plots"},{"location":"Programming/Python/Plotly%20Manual/#shapes","text":"documentation There are various ways how to create shapes in plotly: for simple higlight of regions on x or y axis, we can use the add_vrect and add_hrect functions ( introductory example , documentation ). only for recangles spaning the whole height or width of the plot The shapes can be created by supplying the coordinates of the shape to the go.Scatter function and setting the fill parameter to \"toself\" . ( documentation ) this method has a serious limitation: the filled scatter plots are always above the other traces finally, we can use the add_shape function to add a shape to the figure. There is no way how to fill shapes with a patern fill.","title":"Shapes"},{"location":"Programming/Python/Plotly%20Manual/#create-subplots","text":"documentation and examples To create a figure with multiple subplots, we use the make_subplots function. Example: fig = make_subplots(rows=2, cols=2, start_cell=\"bottom-left\") fig.add_trace(go.Scatter(x=[1, 2, 3], y=[4, 5, 6]), row=1, col=1) ... fig.show() Importanta parameters: shared_xaxes and shared_yaxes : if True , the subplots will share the same x and y axes. x_title and y_title : the titles of the x and y axes. horizontal_spacing and vertical_spacing : the spacing between the subplots.","title":"Create subplots"},{"location":"Programming/Python/Plotly%20Manual/#3d-plots","text":"","title":"3D plots"},{"location":"Programming/Python/Plotly%20Manual/#common-parameters","text":"","title":"Common Parameters"},{"location":"Programming/Python/Plotly%20Manual/#setting-color-of-the-plots-with-continuous-color-scale","text":"If the plot type is continuous (e.g., surface plot, cone plot), we cannot set a color for the whole trace as a continuous color scale is used. However, we can set the colorscale using the colorscale parameter. Example: fig.add_trace(go.Surface(z=z, colorscale=\"Viridis\")) The color scale can be also set manually as a list of colors. This way, we can overcome the limitation of the continuous color scale and set the color of the whole trace. Example: fig.add_trace(go.Surface(z=z, colorscale=[[0, \"red\"], [1, \"red\"]]))","title":"Setting color of the plots with continuous color scale"},{"location":"Programming/Python/Plotly%20Manual/#3d-scatter-plot_1","text":"documentation The 3D scatter plot is created using the go.Scatter3d function. Example: go.Scatter3d(x, y, z, ...)","title":"3D Scatter Plot"},{"location":"Programming/Python/Plotly%20Manual/#surface-plots","text":"There are multiple types of plots that may be considered as surface plots in plotly: 3D surface ( go.Surface ): a surface plot defined by a grid of x, y, and a 2D array of z values of the shape (|x|, |y|). The surface is plotted for all combinations of x and y, which makes this function suitable only for cases where the surface is defined for all combinations of x and y. 3D mesh ( go.Mesh3d ): a surface plot defined by vertices (points) and faces (connections between the points). The connections are defined by triangles, which can be either defined manually or computed automatically using a triangulation algorithm. Tri-surface ( figure_factory.create_trisurf ): a surface plot created by a triangulation of the data points. It accepts triangles in a single argument. I have to further investigate, how it differs from the 3D mesh plot.","title":"Surface Plots"},{"location":"Programming/Python/Plotly%20Manual/#3d-surface-plot","text":"documentation The 3D surface plot is created using the go.Surface function.","title":"3D Surface Plot"},{"location":"Programming/Python/Plotly%20Manual/#3d-mesh-plot","text":"documentation The 3D mesh plot is created using the go.Mesh3d function. Example: x = [1, 1, 1] y = [1, 2, 1] z = [0, 0, 1] fig.add_trace(go.Mesh3d(x=x, y=y, z=z)) This way, the triangulation is used to compute the connections between the points. If we want to use a different triangulation, we can use the i , j and k parameters. These parameters define the indices (in the source data used for x , y , and z ) of the vertices that form the triangles. For example, i[0] , j[0] and k[0] define the vertices of the first triangle. Example code: x = [1, 1, 1] y = [1, 2, 1] z = [0, 0, 1] i = [0] j = [1] k = [2] fig.add_trace(go.Mesh3d(x=x, y=y, z=z, i=i, j=j, k=k)) Note that here, the i , j and k parameters are redundant, as there is only one possible triangulation of the three points. However, for more complex surfaces, there can be multiple valid triangulations, and the i , j and k parameters can be used to select the triangulation manually.","title":"3D Mesh Plot"},{"location":"Programming/Python/Plotly%20Manual/#cone-plots","text":"documentation reference The main parameters of the cone plot are: x , y , z : the coordinates of the cone base u , v , w : the vector defining the direction and length of the cone","title":"Cone Plots"},{"location":"Programming/Python/Plotly%20Manual/#customizing-the-figure","text":"The figure object can be custommized in many ways.","title":"Customizing the Figure"},{"location":"Programming/Python/Plotly%20Manual/#size-and-margins","text":"The size and margins can be set using the figure.update_layout function. The specific parameters are: width : the width of the figure in pixels height : the height of the figure in pixels autosize : needs to be False if we want to set the width and height manually margin : dictionary containing the margins ( l , r , t , b ) and one another property: pad . All properties are in pixels. l , r , t , b : distance between the plot and the figure border. Note that titles are not included in the plot, so we have make space for the titles if we set margins manually. pad : distance between the plotting area (i.e., the plotting coordinates for data points) and the axis lines. Most of the time, this should be set to 0 (default) Unfortunately, there is no way how to set the margins automatically to fit all content like titles, annotations, etc. documentation reference","title":"Size and margins"},{"location":"Programming/Python/Plotly%20Manual/#customize-axes","text":"For customizing the axes, we can use the figure.update_xaxes and figure.update_yaxes functions. By default, the functions will update all axes. To update only a specific axis, we can use the row and col parameters. axis reference The range of the axis is determined automatically as the range of the data plus some margin. If we want any other range, we need to set it manually using the range parameter, e.g.: range=[0, 1] . Unfortunately, there is no way how to automatically set the range to match the data range exactly Another thing we usually want to customize are the ticks. Important tick parameters are: dtick : the distance between the ticks tickvals : the exact values of the ticks. This overrides the dtick parameter. ticks : the position of the ticks. Can be outside , inside or \"\" (no ticks, default). ticklen : the length of the ticks in pixels tickformat : the format of the tick labels. Depending on the axis datatype, we can use number formats (e.g., \".2f\" for two decimal places), datetime formats (e.g., \"%Y-%m-%d\" for dates) or scientific notation (e.g., \"e\" for scientific notation). for percentage, we can use \".0%\" for integer percentage and \".1%\" for one decimal place. Note that this way, the % sign is added automatically to each tick label. If we do not want this, we can either set the text manually using the ticktext parameter, or multiply the data by 100. tickangle : the angle of the tick labels in degrees Other important parameters are: title_text : the title of the axis. linecolor : the color of the axis line gridcolor : the color of the grid lines mirror : if True , the axis line will be mirrored to the other side of the plot","title":"Customize axes"},{"location":"Programming/Python/Plotly%20Manual/#drawing-borders-using-axes","text":"The borders can be drawn using the axes. To draw a black border around the plot, we can use the following code: fig.update_xaxes(linecolor=\"black\", mirror=True) fig.update_yaxes(linecolor=\"black\", mirror=True)","title":"Drawing borders using axes"},{"location":"Programming/Python/Plotly%20Manual/#customizing-datetime-axes","text":"Unfortunately, we cannot treat the datetime axes as expected, i.e., using the datetime objects. For example, to set the tick interval, we cannot use the datetime.timedelta object. Instead, we need to use the number of milliseconds. Example: fig.update_xaxes(dtick=1000 * 60 * 60 * 24 * 7) # one week interval","title":"Customizing datetime axes"},{"location":"Programming/Python/Plotly%20Manual/#customizing-shared-axes","text":"By default, only tick labels are shared between subplots with shared axes. To share other properties (tick markers, axis title) we need to hide them manually and add them to the first subplot. Example: fig.update_yaxes(showticklabels=False) fig.update_yaxes(showticklabels=True, row=1, col=1)","title":"Customizing shared axes"},{"location":"Programming/Python/Plotly%20Manual/#3d-axes","text":"documentation Unfortunately, the customization of the 3d axes works differently than for the 2d axes. The update_xaxes and update_yaxes functions do not work for the 3d axes. Instead, we need to use the update_layout function with the scene parameter. Example: fig.update_layout(scene=dict( xaxis=dict( title=\"x\", titlefont_size=16, tickfont_size=14, ), yaxis=dict( title=\"y\", titlefont_size=16, tickfont_size=14, ), zaxis=dict( title=\"z\", titlefont_size=16, tickfont_size=14, ), )) One thing that is not possible to customize idn 3D is the position of the axis . The x and y axes are always in the bottom, while the z axis is always on the left.","title":"3d axes"},{"location":"Programming/Python/Plotly%20Manual/#change-position-of-the-axis-title","text":"Unfortunately, there is no way how to change the position of the axis title. The solution is to hide the title and add a new annotation with the title text. Example: fig.update_xaxes(title_text=\"\") fig.add_annotation( text=\"Comp. time relative to IH\", xref=\"paper\", yref=\"paper\", x=-0.09, y=0.5, showarrow=False, font=dict( size=14, ), textangle=270, )","title":"Change position of the axis title"},{"location":"Programming/Python/Plotly%20Manual/#legend_1","text":"documentation reference The legend can be styled using the figure.update_layout function. The most important parameters are: legend_title : the title of the legend legend : dictionary containing many parameters orientation : h or v for horizontal or vertical legend x , y : the position of the in normalized coordinates of the whole plot. 'xref', yref : the coordinate system of the x and y coordinates. Can be \"container\" : the whole plot \"paper\" (default): the plotting area xanchor , yanchor : the position of the legend box relative to the x and y coordinates title : if string, the title of the legend (equivalent to the legend_title parameter). If dictionary, multiple legend title parameters can be set. bordercolor : the color of the legend border borderwidth : the width of the legend border Unfortunately, there is no way how to customize the padding between the legend items and the legend box . Also, it is not possible to set the legend border to have rounded corners .","title":"Legend"},{"location":"Programming/Python/Plotly%20Manual/#hide-legend","text":"To hide the legend, we can use the showlegend parameter. Example: fig.update_layout(showlegend=False)","title":"Hide legend"},{"location":"Programming/Python/Plotly%20Manual/#legend-items-order","text":"The order of the legend items is determined by the order of the traces in the figure. However, we can change the order using the legend_traceorder parameter. Example: fig.update_layout(legend_traceorder=\"reversed\")","title":"Legend items order"},{"location":"Programming/Python/Plotly%20Manual/#legend-position","text":"The coordinates of the legend are normalized with respect to each axis. By default, the legend x and y coordinates are set so that the legend is outside the plot, in the top right corner. If the legend is positioned inside the plot, the plot expands to the whole width, but if the legend is positioned outside the plot, the plot width is smaller to leave space for the legend.","title":"Legend position"},{"location":"Programming/Python/Plotly%20Manual/#legend-items-text","text":"The legend item text is determined by the name parameter of the trace. Therefore, to customize the legend item text, we need to set the name parameter of the trace. For normal single trace functions, this is simple: fig.add_trace(go.Scatter(x=x, y=y, name=\"Custom name\")) However, it can be complicated for plotly express functions that plot multiple traces at once, as these determine the name parameter automatically from data. For example, when we use the color parameter, the name parameter is set to the color value. To overcome this, we have two options: set the name parameter manually for each trace after the figure is created change the data so that the name parameter is set automatically to the desired value. The first approach is usually preferable as we do not mix the data and appearance. To change the name parameter, we can use the update_traces function: for trace in fig.data: trace.name = process_name(trace.name)","title":"Legend items text"},{"location":"Programming/Python/Plotly%20Manual/#figure-annotations","text":"documentation For adding annotations to to a figure, we can use the add_annotation function. The most compliated and also most limited aspect is the positioning of the annotation. There are two things we can position: the head of the annotation arrow, or the position of the text in case of no arrow the tail of position of the annotation arrow, if present Note that there are some important limitations of annotations : There is no way how to position the text of the annotation. If we need a specific position of the text, we need to add it as a separate annotation. This brings a huge burden, as we need to calculate the right offset from the arrow, which depends on the angle, but also the dimensions of the plot The textangle parameter of the text is in the coordinate system of the plot, not the coordinate system of the data. A recalculation is needed if we want to align a data based arrow annotation with a text annotation. These annotations only work for 2D plots. For 3D plots, custom solution with 3D line and cone plots is needed.","title":"Figure Annotations"},{"location":"Programming/Python/Plotly%20Manual/#setting-the-head-of-the-annotation","text":"There are two settings for head of the annotation for each axis: xref and yref : the mode of the positioning, and x and y : the coordinates The xref and yref parameters can be set to: <axis name> : The axis mode, where the axis coordinates are used for positioning. default for simple plots: xref=\"x\" and yref=\"y\" for subplots, we use \"x1\"-\"x<n>\" or \"y1\"-\"y<n>\" as <axis name> paper : The whole figure mode, where 0 means the left/bottom and 1 means the right/top of the figure. default for subplots","title":"Setting the head of the annotation"},{"location":"Programming/Python/Plotly%20Manual/#setting-the-tail-of-the-annotation","text":"There are two settings for the tail of the annotation for each axis: axref and ayref : the mode of the positioning, and ax and ay : the coordinates The axref and ayref parameters can be set to: \"pixel\" : The pixel mode, where the coordinates are in pixels relative to the whole figure, where the origin is the bottom left corner of the figure. default <axis reference> : Same as for the head of the annotation, but must be equal to the xref / yref , ottherwise, the arrow will not be visible. e.g., setting xref to \"x1\" and axref to \"x2\" is not allowed","title":"Setting the tail of the annotation"},{"location":"Programming/Python/Plotly%20Manual/#configuring-the-arrow","text":"There are many parameters for configuring the arrow. Morover, without configuring some of the parameters, the arrow head will not be visible at all . The most important parameters are: showarrow : toggl the whole arrow (head and tail). Default is True . arrowcolor : the color of the arrow line. arrowwidth : the width of the arrow line in pixels. Default is 1 . arrowhead : the number indicating the style of the arrow head. Possible values are: 0 : no arrow head (default) 1 : wide triangle arrow head 2 : narrow triangle arrow head 3 : open triangle arrow head (dart) 4 : open triangle arrow head (B2 spirit shape) 5 : open triangle arrow head (V shape) 6 : circle arrow head 7 : square arrow head arrowsize : the relative size of the arrow head to the arrowwidth . Default is 1 .","title":"Configuring the arrow"},{"location":"Programming/Python/Plotly%20Manual/#configuring-the-text","text":"By default, the annotation does not have any text, unless showarrow is set to False , in which case a default text is added to prevent the annotation from being completely empty. To add text, we use the text parameter. To style the text, we can use the following parameters: textangle : the angle of the text in degrees, font : dictionary containing font parameters size : the size of the font in pixels color : the color of the font weight : the weight of the font xanchor , yanchor : the position of the text relative to the text box. xanchor can be left , center or right yanchor can be top , middle or bottom","title":"Configuring the text"},{"location":"Programming/Python/Plotly%20Manual/#positinoing-the-text-relative-to-the-arrow","text":"There is no way how to position the text of the annotation with respect to the arrow . The text position is: the head of the annotation, if the arrow is not present, or the tail of the annotation, if the arrow is present If we want to, for example, need to position the text of the annotation above the arrow, so that the text is parallel to the arrow, we need to use two annotations.","title":"Positinoing the text relative to the arrow"},{"location":"Programming/Python/Plotly%20Manual/#other-important-parameters","text":"bgcolor : the background color of the annotation","title":"Other important parameters"},{"location":"Programming/Python/Plotly%20Manual/#markers","text":"documentation To style the markers, we can use the update_traces function. Example: fig.update_traces(marker=dict(size=10, line=dict(width=2, color='DarkSlateGrey'))) Marker parameters: size : the size of the marker in pixels line : dictionary containing border parameters width : the width of the border in pixels. The default is 0. This setting is ignored for 3d scatter plots, where the border width is always 1 (see github issue ) color : the color of the border color : the color of the marker. To make the marker transparent, it is best to use the rgba format. Example: rgba(255, 0, 0, 0.5) for a red marker with 50% transparency.","title":"Markers"},{"location":"Programming/Python/Plotly%20Manual/#adding-a-marker-to-a-hard-coded-location","text":"To add a marker to a hard-coded location, we can add it as a new trace. Note that we can add new traces even to a figure created using plotly express.","title":"Adding a marker to a hard-coded location"},{"location":"Programming/Python/Plotly%20Manual/#lines","text":"Lines can be styled using the line parameter of the plotting functions or using the update_traces function. Important parameters: color : the color of the line width : the width of the line in pixels dash : the dash pattern. Can be \"solid\" (default) \"dot\" : dense dashed line \"dash\" : sparse dashed line ... Be aware that the dash pattern is proportional to the axis range . In result, it is practicaly impossible to use dash patterns on graphs with different axes ranges.","title":"Lines"},{"location":"Programming/Python/Plotly%20Manual/#title","text":"documentation The title can be set using the plotly express functions or when creating the graph objects figure. To update the text or to customize the title layout, we can use the update_layout function with the title object parameter. Example: fig.update_layout( title={ 'x': 0.5, 'xanchor': 'center', 'y': 0.85, 'yanchor': 'top' } ) Important parameters: x , y : the x and y coordinates of the title. The origin is the bottom left corner of the figure. xanchor , yanchor : the position of the title relative to the x and y coordinates. Can be left , center or right for xanchor and top , middle or bottom for yanchor .","title":"Title"},{"location":"Programming/Python/Plotly%20Manual/#subplot-titles","text":"","title":"Subplot titles"},{"location":"Programming/Python/Plotly%20Manual/#automatic-subplot-titles-facet-plots","text":"When using facet plots, the subplot titles are generated automatically as <facet column name>=<facet column value> . Usually, we want to remove the column name and keep only the value. To do this, we can use the for_each_annotation function. Example: fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))","title":"Automatic subplot titles (facet plots)"},{"location":"Programming/Python/Plotly%20Manual/#individual-subplot-titles","text":"To set the title of an individual subplot, we can use the subplot_titles parameter of the make_subplots function. To position the subplot titles , we have to update all annotations. To prevent the need to filter the titles, it is best to update all annotations right after the make_subplots call. Example: fig = make_subplots(...) fig.update_annotations(yshift=10) # move the subplot titles 10 pixels up","title":"Individual subplot titles"},{"location":"Programming/Python/Plotly%20Manual/#z-order-of-the-traces","text":"The z-order of the traces cannot be configured. Instead, the traces are drawn in the order they are added to the figure. This simple rule has an exception: the webGL traces are always drawn on top of the standard traces. Because all plotly express traces are WebGL traces, they are drawn on top of the graph objects traces added later if those are not WebGL. To overcome this limitation, we have two options: not combine plotly express and graph objects traces and convert everything to graph objects for figures with custom traces use the webGL versions of the graph objects traces. Example: python fig.add_trace(go.Scattergl(...))","title":"Z-Order of the Traces"},{"location":"Programming/Python/Plotly%20Manual/#other-layout-parameters-background-borders-etc","text":"background color: plot_bgcolor border: borders are best drawn by showing and mirroring the axes (see the axis section above).","title":"Other Layout Parameters (background, borders, etc.)"},{"location":"Programming/Python/Plotly%20Manual/#text","text":"documentation","title":"Text"},{"location":"Programming/Python/Plotly%20Manual/#subscript-and-superscript","text":"To add a subscript or superscript to a text, we can use HTML tags. Example: fig.add_annotation(x=0.5, y=0.5, text=\"Title<sub>subscript</sub>\", xref=\"x5\", yref=\"y5\", showarrow=False)","title":"Subscript and superscript"},{"location":"Programming/Python/Plotly%20Manual/#bold-and-italic","text":"To add bold or italic text, we can use the HTML tags <b> and <i> . Example: fig.add_annotation(x=0.5, y=0.5, text=\"Title<b>bold</b><i>italic</i>\", xref=\"x5\", yref=\"y5\", showarrow=False)","title":"Bold and italic"},{"location":"Programming/Python/Plotly%20Manual/#math-symbols","text":"To add math symbols, we can use the LaTeX syntax. However, when using it, all text must be wrapped in math mode, not just the symbols. Example: fig.add_annotation(x=0.5, y=0.5, text=r\"$\\\\text{{Title: }} \\alpha \\\\text{{ and }} \\beta $\", xref=\"x5\", yref=\"y5\", showarrow=False) Note that the LaTeX syntax does not work in vscode out of the box [issue] . We need to add the following code: import plotly from IPython.display import display, HTML plotly.offline.init_notebook_mode() display(HTML( '<script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG\"></script>' ))","title":"Math symbols"},{"location":"Programming/Python/Plotly%20Manual/#exporting-the-figure","text":"documentation The static export is handeled using the figure's write_image function. Example: fig.write_image(\"figure.png\") The output format is determined by the extension. The margins of the figure should be set for the figure itself, not for the export.","title":"Exporting the Figure"},{"location":"Programming/Python/Plotly%20Manual/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Programming/Python/Plotly%20Manual/#export-hangs-out","text":"It can be cause by kaleido. The solution si to install an older version, specifically 0.1.0.post1 . https://community.plotly.com/t/static-image-export-hangs-using-kaleido/61519/4","title":"Export hangs out"},{"location":"Programming/Python/Plotly%20Manual/#exported-size-does-not-match-the-figure-size","text":"Sometimes, the Kaleido engine resizes the figure before exporting. To overcome this, we can set the width and height parameters of the write_image function. Example: fig.write_image(\"figure.png\", width=1000, height=1000)","title":"Exported size does not match the figure size"},{"location":"Programming/Python/Plotly%20Manual/#colors","text":"documentation For colors in plotly, we can use the color scales supplied by plotly express. These are: px.colors.sequential : sequential color scales px.colors.diverging : diverging color scales px.colors.qualitative : qualitative color scales","title":"Colors"},{"location":"Programming/Python/Python%20Debugging%20and%20Profiling/","text":"Using the debugger \u00b6 Pycharm has a build in debugger, however, there are some tricky problems described below. Breaking on Exception \u00b6 Breaking on exception is one of the most important debugger tools. However, there are some problems with Pycharm exception debugging. Breake on Termination vs on Raise \u00b6 By default, the program breakes on termination (unhandled exception). This is usually a correct configuration. However, in jupyter, all exceptions are caught to not break the jupyter itself. Therefore, in jupyter, all exceptions are ignored by the debugger if the breakpoins are set to break on termination. To break on exceptions in jupyter, we have to breake on raise. By this setting, however, we stop even on expected/handeled exceptions, stoping potentially on hundereds breakpoints in library code. Another issue is with the setting itself. To propagate the change between breaking on raise/termination, we have to deactivate and then activate again the exception breakpoints, otherwise, the setting is ignored. Profiling \u00b6 Pycharm has a built-in profiler with a very simple use: Select three dots next to the debug button and click on Profile <script name> Wait some time to warm up and get to the important parts of the code in the left panel, select create snapshot Explore the results The built-in profiler is statistical and provides only aggregated results for each function. To get line-by-line results we need a line_profiler package. Line Profiling \u00b6 The line_profiler package provides line-by-line profiling of the code. Unfortunately, the corresponding PyCharm plugin is currently broken and does not work with latest PyCharm version (2025.2) .","title":"Python Debugging and Profiling"},{"location":"Programming/Python/Python%20Debugging%20and%20Profiling/#using-the-debugger","text":"Pycharm has a build in debugger, however, there are some tricky problems described below.","title":"Using the debugger"},{"location":"Programming/Python/Python%20Debugging%20and%20Profiling/#breaking-on-exception","text":"Breaking on exception is one of the most important debugger tools. However, there are some problems with Pycharm exception debugging.","title":"Breaking on Exception"},{"location":"Programming/Python/Python%20Debugging%20and%20Profiling/#breake-on-termination-vs-on-raise","text":"By default, the program breakes on termination (unhandled exception). This is usually a correct configuration. However, in jupyter, all exceptions are caught to not break the jupyter itself. Therefore, in jupyter, all exceptions are ignored by the debugger if the breakpoins are set to break on termination. To break on exceptions in jupyter, we have to breake on raise. By this setting, however, we stop even on expected/handeled exceptions, stoping potentially on hundereds breakpoints in library code. Another issue is with the setting itself. To propagate the change between breaking on raise/termination, we have to deactivate and then activate again the exception breakpoints, otherwise, the setting is ignored.","title":"Breake on Termination vs on Raise"},{"location":"Programming/Python/Python%20Debugging%20and%20Profiling/#profiling","text":"Pycharm has a built-in profiler with a very simple use: Select three dots next to the debug button and click on Profile <script name> Wait some time to warm up and get to the important parts of the code in the left panel, select create snapshot Explore the results The built-in profiler is statistical and provides only aggregated results for each function. To get line-by-line results we need a line_profiler package.","title":"Profiling"},{"location":"Programming/Python/Python%20Debugging%20and%20Profiling/#line-profiling","text":"The line_profiler package provides line-by-line profiling of the code. Unfortunately, the corresponding PyCharm plugin is currently broken and does not work with latest PyCharm version (2025.2) .","title":"Line Profiling"},{"location":"Programming/Python/Python%20Manual/","text":"Basics \u00b6 Variables \u00b6 To check if a local variable is defined, we can use the locals function: if 'my_variable' in locals(): print('Variable exists') Conditions and boolean context \u00b6 Comparison operators \u00b6 Python uses the standard set of comparison operators ( == , != , < , > , <= , >= ). They are functionally similar to C++ operators: they can be overloaded and the semantic meaning of == is equality, not identity (in contrast to Java). Automatic conversion to bool \u00b6 Unlike in other languages, any expression can be used in boolean context in python, as there are rules how to convert any type to bool . The following statement is valid, foor example: s = 'hello' if s: print(s) The code above prints 'hello', as the variable s evaluates to True . Any object in Python evaluates to True , with exeption of: False None numerically zero values (e.g., 0 , 0.0 ) standard library types that are empty (e.g., empty string, list , dict ) The automatic conversion to bool in boolean context has some couner intuitive consequences. The following conditions are not equal: s = 'hello' if s: # s evaluates to True if s == True: # the result of s == True is False, then False evaluete to False Checking the type \u00b6 To check the exact type: if type(<VAR>) is <TYPE>: # e.g. if type(o) is str: To check the type in the polymorphic way, including the subtypes: if isinstance(<VAR>, <TYPE>): # e.g. if isinstance(o, str): Built-in data types \u00b6 Numbers \u00b6 Python has the following numeric types: int - integer float - floating point number The int type is unlimited, i.e., it can represent any integer number. The float type is limited by the machine precision, i.e., it can represent only a finite number of real numbers. Check if a float number is integer \u00b6 To check whether a float number is integer, we can use the is_integer function: Check if a number is NaN \u00b6 To check whether a number is NaN, we can use the math.isnan function or the numpyp.isnan function: Rounding \u00b6 To round a number, use the round function. For rounding up , use the math.ceil function. For rounding down , use the math.floor function. Strings \u00b6 Strings in Python can be enclosed in single or double quotes (equivalent). The triple quotes can be used for multiline strings. String formatting \u00b6 The string formatting can be done in several ways: using the f prefix to string literal: f'{<VAR>}' using the format method: '{}'.format(<VAR>) Each variable can be formatted for that, Python has a string formatting mini language . The format is specified after the : character (e.g., f'{47:4}' set the width of the number 47 to 4 characters). Most of the format specifiers have default values, so we can omit them (e.g., f'{47:4}' is equivalent to f'{47:4d}' ). The following are the most common options: To use the character { and } in the string, we have to escape them using double braces: {{ and }} . String methods \u00b6 capitalize : capitalize the first letter of the string lower : convert the string to lowercase upper : convert the string to uppercase strip : remove leading and trailing whitespaces lstrip : remove leading whitespaces rstrip : remove trailing whitespaces Enumerations \u00b6 For enumerations, we can use the enum module. The basic syntax is: from enum import Enum class MyEnum(Enum): VALUE1 = 1 VALUE2 = 2 Ranges \u00b6 Documentation Python has a special concept of ranges, that are objects representing immutable sequences of numbers. We can use ranges for iteration instead of initializing local variables like in other computer languages. In typical programming languages, a loop with known number of iterations is written as: for(int i = 0; i < 10; i++) { System.out.println(i); } However, in Python, there is no such a thing, and instead we use ranges: for i in range(10): print(i) The range() construct has the syntax of range([<start>], <stop>, [<step>]) . all parameters are integers the default <start> is 0 the default <step> is 1 we need to specify <start> to be able to set <step> . Floating point ranges \u00b6 For floating point ranges, we can use NumPy's arange function: import numpy as np for i in np.arange(0, 10, 0.1): print(i) Collections and generators \u00b6 Python has several built-in data structures, most notably list , tuple , dict , and set . These are less efficient then comparable structures in other languages, but they are very convenient to use. Also, there is a special generator type. It does not store the data it is only a convinient way how to access data generated by some function. Generator \u00b6 Python wiki Generators are mostly used in the iteration, we can iterte them the same way as lists. To get the first item of the generator, we can use the next function: g = (x for x in range(10)) first = next(g) # 0 To create a generator function (a function that returns a generator), we can use the yield keyword. The following function returns a generator that yields the numbers 1, 2, and 3: def gen(): yield 1 yield 2 yield 3 The length of the generator is not known in advance, to get the length, we have to iterate the generator first, for example using len(list(<generator>)) Tuple \u00b6 Tuples are meant to store a fixed sequence of values. They are immutable. The tuple literal is a comma-separated list of values in round braces: t = (1, 2, 3) Named tuples \u00b6 Official Manual Named tuples are tuples with named members. Example: from collections import namedtuple Point = namedtuple('Point', ['x', 'y']) p = Point(1, 2) print(p.x) # 1 If we want to use type hints, we can use the NamedTuple class: from typing import NamedTuple class Point(NamedTuple): x: int y: int p = Point(1, 2) print(p.x) # 1 Dictionary \u00b6 Official Manual Disctionaries are initialized using curly braces ( {} ) and the : operator: d = { 'key1': 'value1', 'key2': 'value2', ... } Two dictionaries can be merged using the | operator: d3 = d1 | d2 Get a default value if a key is not present \u00b6 To get a default value if a key is not present, we can use the get method: d = {'a': 1} d.get('b', 0) # 0 Set \u00b6 Documentation Sets are initialized using curly braces ( {} ) or the set function: s = {1, 2, 3} s = set([1, 2, 3]) To add elements to the set, we use either the add for a single element or the update for multiple elements. In both cases, a union of the set and the new elements is computed, i.e., no exception is raised if an element is already in the set. Comprehensions \u00b6 In addition to literals, Python has a convinient way of creating basic data structures: the comprehensions. The basic syntax is: <struct var> = <op. brace> <member var expr.> for <member var> in <iterable><cl. brace> As for literals, we use square braces ( [] ) for lists, curly braces ( {} ) for sets, and curly braces with colons for dictionaries. In contrast, we get a generator expression when using round braces ( () ), not a tuple. We can also use the if keyword to filter the elements: a = [it for it in range(10) if it % 2 == 0] # [0, 2, 4, 6, 8] Sorting \u00b6 Official Manual For sorting, you can use the sorted function. Instead of using comparators, Python has a different concept of key functions for custom sorting. The key function is a function that is applied to each element before sorting. For any expected object, the key function should return a value that can be compared. Complex sorting using tuples \u00b6 If we need to apply some complex sorting, we can use tuples as the key function return value. The tuples have comparison operator defined, the implementation is as follows: elements are compared one by one on first non-equal element, the comparison result is returned This way, we can implement a complex sorting that would normaly require several conditions by storing the condition results in the tuple. Slices \u00b6 Many Python data structures support slicing: selecting a subset of elements. The syntax is: <object>[<start>:<end>:<step>] The start and end are inclusive. The step is optional and defaults to 1. The start is also optional and defaults to 0. Instead of omitting the start and end , we can use the None keyword: a = [1, 2, 3, 4, 5] a[None:3] # [1, 2, 3] Sometimes, it is not possible to use the slice syntax: when we need to use a variable for the step or, when the object use the slice syntax for something else, e.g., for selecting columns in a Pandas dataframe. In such cases, we can use the slice object: a[0:10:2] s = slice(0, 10, 2) a[s] # equivalent Here, the parameters can be ommited as well. We can select everything by using slice(None) , which is equivalent to slice(None, None, None) . Copying collections \u00b6 If we copy a complex collection (e.g., a list of dictionaries), we typically want to create a deep copy so that the original collection is not modified. We can use the copy module for that: import copy a = [{'a': 1}, {'b': 2}] b = copy.deepcopy(a) Date and time \u00b6 Python documentation The base object for date and time is datetime datetime construction \u00b6 The datetime object can be directly constructed from the parts: from daterime import datetime d = datetime(2022, 12, 20, 22, 30, 0) # 2022-12-20 22:30:00 The time part can be ommited. We can load the datetime from string using the strptime function: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') For all possible time formats, check the strftime cheatsheet Accessing the parts of datetime \u00b6 The datetime object has the following attributes: year month day hour minute second We can also query the day of the week using the weekday() method. The day of the week is represented as an integer, where Monday is 0 and Sunday is 6. Intervals \u00b6 There is also a dedicated object for time interval named timedelta . It can be constructed from parts (seconds to days), all parts are optional. We can obtain a timedelta by substracting a datetime from another datetime : d1 = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') d2 = datetime.strptime('2022-05-20 18:30', '%Y-%m-%d %H:%M') interval = d2 - d1 # 30 minutes We can also add or substract a timedelta object from the datetime object: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') interval = timedelta(hours=1) d2 = d + interval # '2022-05-20 19:00' Converting to Unix timestamp \u00b6 To convert a datetime object to unix timestamp, we can use the timestamp method. It returns the number of seconds since the epoch (1.1.1970 00:00:00). Note however, that the timestamp is computed based on the datetime object's timezone, or your local timezone if the datetime object has no timezone information. Time and date objects \u00b6 Instead of using the datetime object, we can use the time and date objects if we need to work with time or date only. Analogously to the datetime object, these objects can be constructed from the parts : t = time(hour=12, minute=30, second=0) d = date(year=2022, month=12, day=20) To convert datetime to time or date , we can use the time and date methods. d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') t = d.time() # '18:00:00' d = d.date() # '2022-05-20' Similarly, we use the date and time objects to construct a datetime object using the combine method: d = date(year=2022, month=12, day=20) t = time(hour=12, minute=30, second=0) dt = datetime.combine(d, t) # '2022-12-20 12:30:00' Named tuples \u00b6 Apart from the standard tuple, Python has a named tuple class that can be created using the collections.namedtuple function. In named tuple, each member has a name and can be accessed using the dot operator: from collections import namedtuple Point = namedtuple('Point', ['x', 'y']) p = Point(1, 2) print(p.x) # 1 Functions \u00b6 Variable length arguments \u00b6 There are two types of variable length arguments in Python: *args - variable length positional arguments. **kwargs - variable length keyword arguments Argument unpacking \u00b6 if we need to conditionaly execute function with a different set of parameters (supposed the function has optional/default parameters), we can avoid multiple function calls inside the branching tree by using argument unpacking. Suppose we have a function with three optional parameters: a , b , c . If we skip only last n parameters, we can use a list for parameters and unpack it using * : def call_me(a, b, c): ... l = ['param A', True] call_me(*l) # calls the function with a = 'param A' and b = True If we need to skip some parameters in the middle, we have to use a dict and unpack it using ** : d = {'c': 142} call_me(**d) # calls the function with c = 142 String formatting \u00b6 To format python strings we can use the format function of the string or the equivalen fstring: a = 'world' message = \"Hello {} world\".format(a) message = f\"Hello {a}\" # equivalent If we need to a special formatting for a variable, we can specify it behind : as we can see in the following example that padds the number from left: uid = '47' message = \"Hello user {:0>4d}\".format(a) # prints \"Hello user 0047\" message = f\"Hello {a:0>4d}\" # equivalent More formating optios can be found in the Python string formatting cookbook . Classes \u00b6 Official Manual Classes in Python are defined using the class keyword: class MyClass: ... Unlike in other languages, we only declare the function members, other members are declared in the constructor or even later. Constructor \u00b6 The constructor is a special function named __init__ . Usually, non-function members are declared in the constructor: class MyClass: def __init__(self, a, b): self.a = a self.b = b self.c = 0 self.d = None Check if an object contains a member \u00b6 To check whether an object contains a member, we can use the hasattr function: if hasattr(obj, 'member'): ... Constructor overloading \u00b6 Python does not support function overloading, including the constructor. That is unfortunate as default arguments are less powerfull mechanism. For other functions, we can supplement overloading using a function with a different name. However, for the constructor, we need to use a different approach. The most clean way is to use a class method as a constructor. Example: class MyClass: def __init__(self, a, b = 0): self.a = a self.b = b self.c = 0 self.d = None @classmethod def from_a(cls, b): return cls(0, b) Importing \u00b6 In python, we can import whole modules as: import <module> Also, we can import specific functions, classes, or variables from the module: from <module> import <name> Note that when importing variable, we import the reference to the variable. Therefore, it will become out of sync with the original variable if the original variable is reassigned. Therefore, importing non-constant variables is not recommended. The module path can absolute or relative (starting with . ). Absolute imports are recommended, as they are more robust and less error-prone. Resolving absolute module paths \u00b6 If the path is absolute, it is resolved as follows: The already imported modules are searched The built-in modules are searched The module is searched in the import path which is a list of directories stored in the sys.path variable. The sys.path variable typically contains the following directories: the directory of the script that is executed ( '' in case of the interactive shell), the directories in the PYTHONPATH environment variable, the standard library directories (e.g., /usr/lib/python3.9 ), and the site-packages directory. Resolving relative module paths \u00b6 Relative imports can only be used in packages (directories with __init__.py file). The relative path may start with . : relative to the current module, .. relative to the parent module. Imports in tests \u00b6 The tests are located outside the main package, so we cannot use the absolute import starting with the package name. One option is to use relative imports. But a better option is to use absolute imports starting from the project root . We can do that, because test suites like pytest add the project root to the sys.path variable. The project root is typically determined automatically by the test suite, e.g. by searching for the setup.py file. Therefore, if the tests directory is located in the same directory as the setup.py file, we can import as follows: import tests/common Exceptions \u00b6 documentation Syntax: try: <code that can raise exception> except <ERROR TYPE> as <ERROR VAR>: <ERROR HANDELING> finally: <code that is executed always> The except and finally blocks are optional. In other words, we can handle errors without having any default cleanup code, and we can have cleanup code without handling errors. Raising exceptions \u00b6 To raise an exception, we can use the raise keyword: raise ValueError('message') Sometimes, we want just to re-raise an exception after some partial exception handling. In such cases, we can use the raise keyword without arguments: try: ... except: ... raise Assertions \u00b6 In Python, assertions are executed by defult. They can be disabled by running python with the -O or -OO flag. The syntax is: assert <condition>, <message> Filesystem \u00b6 There are three ways commonly used to work with filesystem in Python: manipulate paths as strings os.path pathlib The folowing code compares both approaches for path concatenation: # string path concatenation a = \"C:/workspace\" b = \"project/file.txt\" c = f\"{a}/{b}\" # os.path concatenation a = \"C:/workspace\" b = \"project/file.txt\" c = os.path.join(a, b) # pathlib concatentation a = Path(\"C:/workspace\") b = Path(\"project/file.txt\") c = a / b As the pathlib is the most modern approach, we will use it in the following examples. Appart from pathlib documentation, there is also a cheat sheet available on github . Path editing \u00b6 Computing relative path \u00b6 To prevent misetakes, it is better to compute relative paths beteen directories than to hard-code them. Fortunately, there are methods we can use for that. If the desired relative path is a child of the start path, we can simply use the relative_to method of the Path object: a = Path(\"C:/workspace\") b = Path(\"C:/workspace/project/file.txt\") rel = b.relative_to(a) # rel = 'project/file.txt' However, if we need to go back in the filetree, we need a more sophisticated method from os.path : a = Path(\"C:/Users\") b = Path(\"C:/workspace/project/file.txt\") rel = os.path.relpath(a, b) # rel = '../Workspaces/project/file.txt' Get parent directory \u00b6 We can use the parent property of the Path object: p = Path(\"C:/workspace/project/file.txt\") parent = p.parent # 'C:\\\\workspace\\\\project' Absolute and canonical path \u00b6 We can use the absolute method of the Path object to get the absolute path. To get the canonical path, we can use the resolve method. Always prefer the canonical path, because it can prevent many possible errors (with cloud folders, etc.). Splitting paths and working with path parts \u00b6 To read the file extension , we can use the suffix property of the Path object. The property returns the extension with the dot . To change the extension , we can use the with_suffix method: p = Path(\"C:/workspace/project/file.txt\") p = p.with_suffix('.csv') # 'C:\\\\workspace\\\\project\\\\file.csv' To remove the extension , just use the with_suffix method with an empty string. We can split the path into parts using the parts property: p = Path(\"C:/workspace/project/file.txt\") parts = p.parts # ('C:\\\\', 'workspace', 'project', 'file.txt') To find the index of some specific part, we can use the index method: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 Later, we can use the index to manipulate the path: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 p = Path(*p.parts[:index]) # 'C:\\\\workspace' Changing path separators \u00b6 To change the path separators to forward slashes, we can use the as_posix and method: p = Path(r\"C:\\workspace\\project\\file.txt\") p = p.as_posix() # 'C:/workspace/project/file.txt' Using ~ as the home directory in paths \u00b6 Normally, the ~ character is not recognized as the home directory in Python paths. To enable this, we can use the expanduser method: p = Path(\"~/project/file.txt\") p = p.expanduser() # 'C:\\\\Users\\\\user\\\\project\\\\file.txt' Working directory \u00b6 os.getcwd() - get the current working directory os.chdir(<path>) - set the current working directory Iterating over files \u00b6 The pathlib module provides a convenient way to iterate over files in a directory. The particular methods are: iterdir - iterate all files and directories in a directory glob - iterate over files in a single directory, using a filter rglob - iterate over files in a directory and all its subdirectories, using a filter In general, the files will be sorted alphabetically. When we need a different order, we have to store the results in a list and sort it. Single directory iteration \u00b6 Using pathlib, we can iterate over files using a filter with the glob method: p = Path(\"C:/workspace/project\") for filepath in p.glob('*.txt') # iterate over all txt files in the project directory The old way is to use the os.listdir method: p = Path(\"C:/workspace/project\") for filename in os.listdir(p): if filename.endswith('.txt'): filepath = p / filename Recursive iteration \u00b6 Using pathlib, we can iterate over files using a filter with the rglob method: p = Path(\"C:/workspace/project\") for filepath in p.rglob('*.txt') # iterate over all txt files in the project directory and all its subdirectories The old way is to use the os.walk method: p = Path(\"C:/workspace/project\") for root, dirs, files in os.walk(p): for filename in files: if filename.endswith('.txt'): filepath = Path(root) / filename Iterate only directories/files \u00b6 There is no specific filter for files/directories, but we can use the is_file or is_dir method to filter out directories: p = Path(\"C:/workspace/project\") for filepath in p.glob('*'): if filepath.is_file(): # do something Use more complex filters \u00b6 Unfortunately, the glob and rglob methods do not support more complex filters (like regex). However, we can easily apply the regex filter manually: p = Path(\"C:/workspace/project\") for filepath in p.glob('*'): if not re.match(r'^config.yaml$', filepath.name): # do something Get the path to the current script \u00b6 Path(__file__).resolve().parent Checking write permissions for a directory \u00b6 Unfortunatelly, most of the methods for checking write permissions are not reliable outside Unix systems. The most reliable way is to try to create a file in the directory: p = Path(\"C:/workspace/project\") try: with open(p / 'test.txt', 'w') as f: pass p.unlink() return True except PermissionError: return False except: raise # re-raise the exception Other methods like os.access or using tempfile module are not reliable on Windows (see e.g.: https://github.com/python/cpython/issues/66305). Creating directories \u00b6 To create a directory, we can use the mkdir method of the Path object: p = Path(\"C:/workspace/project\") p.mkdir() Important parameters: parents : if set to True , the directory will be created even if the parent directories do not exist. Default is False . exist_ok : if set to True , the directory will not be created if it already exists. Default is False . Copying files and directories \u00b6 For copying files and directories, we can use the shutil module. The most used method is copy2 , which copies the file with all metadata: import shutil p1 = Path(\"C:/workspace/project/file.txt\") p2 = Path(\"C:/workspace/project/file2.txt\") shutil.copy2(p1, p2) The copy2 method can also copy into a directory: p1 = Path(\"C:/workspace/project/file.txt\") p2 = Path(\"C:/workspace/project2\") shutil.copy2(p1, p2) # the new file will be 'C:/workspace/project2/file.txt' Other methods and the comparison are described in a SO question . Deleting files and directories \u00b6 To delete a file, we can use the unlink method of the Path object: p = Path(\"C:/workspace/project/file.txt\") p.unlink() for deleting directories, we can use the rmdir method: p = Path(\"C:/workspace/project\") p.rmdir() However, the rmdir method can delete only empty directories. To delete a directory with content, we can use the shutil module: p = Path(\"C:/workspace/project\") shutil.rmtree(p) Deleting Windows read-only files (i.e. Access Denied error) \u00b6 On Windows, all the delete methods can fail because lot of files and directories are read-only. This is not a problem for most application, but it breaks Python delete methods. One way to solve this is to handle the error and change the attribute in the habdler. Example for shutil: import os import stat import shutil p = Path(\"C:/workspace/project\") shutil.rmtree(p, onerror=lambda func, path, _: (os.chmod(path, stat.S_IWRITE), func(path))) Working with temporary files \u00b6 The tempfile module provides a convenient way to work with temporary files. To create a temporary file, we can use the NamedTemporaryFile function: import tempfile with tempfile.NamedTemporaryFile() as f: f.write(<data>) Unlike normal files, we can both read and write to the temporary file using a single file object. However, we must return the file pointer to the beginning of the file: with open('file.txt', 'rw') as f: f.write('data') f.seek(0) data = f.read() Troubleshooting Missing Files \u00b6 If Python cannot locate a file that we can locate in the file explorer, the problem can be cause by the cloud syncronization. Sometimes, relative paths are not resolved correctly when a part of the path is on the cloud folder. We can solve this by resolving the path: safe_path = safePath(<relative path>).resolve() . I/O \u00b6 For simple file operations, we can use the open function. A simple file read is done as follows: with open('file.txt', 'r') as f: data = f.read() A simple file write is done as follows: with open('file.txt', 'w') as f: f.write('data') By default, the open function opens the file in text mode. To open the file in binary mode, we have to use the b flag: with open('file.txt', 'rb') as f: data = f.read() CSV \u00b6 Official Manual The csv module provides a Python interface for working with CSV files. The basic usage is: import csv with open('file.csv', 'r') as f: reader = csv.reader(f) for row in reader: # do something Reader parameters: delimiter - the delimiter character JSON \u00b6 Official Manual TO read a JSON file: import json with open('file.json', 'r') as f: data = json.load(f) To write a JSON file: import json data = {'a': 1, 'b': 2} with open('file.json', 'w') as f: json.dump(data, f) Important parameters: indent : the number of spaces used for indentation. This also enables other pretty-printing functionalities, like newlines after each element. Custom serialization \u00b6 The json module can serialize only basic types. If we need to serialize custom objects, we have to provide a custom serialization class. We then supply the class to the cls parameter of the dump function. The serialization class is usually a subclass of the JSONEncoder class. The class has to implement the default method, which is called for each object that cannot be serialized by the standard serialization methods. Example: import json class MyEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, MyCustomClass): return obj.to_json() return super().default(obj) HDF5 \u00b6 HDF5 is a binary file format for storing large amounts of data. The h5py module provides a Python interface for working with HDF5 files. An example of reading a dataset from an HDF5 file on SO INI files \u00b6 The configparser module provides a Python interface for working with INI files . The basic usage is: import configparser config = configparser.ConfigParser() config.read('file.ini') value = config['section']['key'] If we do not have sections in the INI file, we have to: use the allow_unnamed_section argument of the ConfigParser: Python config = configparser.ConfigParser(allow_unnamed_section=True) use the configparser.UNNAMED_SECTION in place of the section name: Python value = config[configparser.UNNAMED_SECTION]['key'] Command line arguments \u00b6 The sys module provides access to the command line arguments. They are stored in the argv list with the first element being the name of the script. However, mostly, we use the argparse module to parse the command line arguments. Example: import argparse parser = argparse.ArgumentParser() parser.add_argument('input', type=str, help='input file') parser.add_argument('output', type=str, help='output file') args = parser.parse_args() print(args.input) The add_argument method has the following parameters: name : the name of the argument type : the type of the argument help : the help message for the argument required : if set to True , the argument is required default : the default value for the argument action : the action to be performed on the argument. Examples: store_true : store True if the argument is present, False otherwise Logging \u00b6 Official Manual The logging itself is then done using the logging module methods: logging.info(\"message\") logging.warning(\"message %s\", \"with parameter\") A simple logging configuration: logging.basicConfig( level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s', handlers=[ logging.FileHandler(\"log.txt\"), logging.StreamHandler() ] ) Note that this configuration can be done only once. Therefore, it should not be done in a library as it prevents the user from configuring the logging. To set the level for a specific logger, we use the setLevel method: logger.setLevel(logging.DEBUG) . We can also use a string representation of the level, e.g., logger.setLevel('DEBUG') . To check the level of the logger, we can use the isEnabledFor method: if logger.isEnabledFor(logging.DEBUG): ... This can be useful for avoiding expensive computations needed just for logging if the logging level is set to a higher level. Type hints \u00b6 Official Manual Type hints are a useful way to help the IDE and other tools to understand the code so that they can provide better support (autocompletion, type checking, refactoring, etc.). The type hints are not enforced at runtime, so they do not affect the performance of the code. We can specify the type of a variable using the : operator: a: int = 1 Apart from the basic types, we can also use the typing module to specify more complex types: from typing import List, Dict, Tuple, Set, Optional, Union, Any a: List[int] = [1, 2, 3] We can also specify the type of a function argument and return value: def foo(a: int, b: str) -> List[int]: return [a, b] Type hints in loops \u00b6 The type of the loop variable is usually inferred by IDE from the type of the iterable. However, this sometimes fails, e.g., for zip objects. In such cases, we need to specify the type of the loop variable. However, we cannot use the : directly in the loop, but instead, we have to declare the variable before the loop : for a: int in ... # error a: int for a in ... # ok Circular type hints \u00b6 Unfortunately, Python currently does not support circular type hints. However, it should be possible to use circular type hints since Python 3.14 . There are two types of circular type hints: we need to refer the type while defining it. For that, we use the Self type: Python class MyClass: def get_me(self) -> Self: return self two or more types refer to each other. For that, use a string representation of the type: ```Python class ClassA: def init (self, b: 'ClassB'): self.b = b class ClassB: def set_a(self, a: ClassA): self.a = a ``` Common type hints \u00b6 Language types \u00b6 None Numeric types: int , float , bool str Collection types: list , or List[<type>] , tuple , or Tuple[<type>, ...] , set , or Set[<type>] , dict , or Dict[<key type>, <value type>] , Iterables: Iterable[<type>] - any iterable Sequence[<type>] - iterable with random access ( [] operator) Any - any type Union[<type>, ...] - any of the specified types Optional[<type>] - the specified type or None Callable[[<arg type>, ...], <return type>] - a function with specified arguments and return type Pandas types \u00b6 Pandas does not provide type hints. We can use the types itself, but this is only partially useful. We can use Series as a hint, but we cannot specify the inner type (e.g.: Series[int] ). For that, we can use a wrapper library called pandera : from pandera.typing import Series a: Series[int] Officiall documentation for Pandera data types Calling external programs \u00b6 To call an external program, we use the subprocess module. Most of the time, we use the run function: import subprocess subprocess.run(['ls', '-l']) Important parameters: check : if set to True , the function raises an exception if the return code is not 0. Default is False . text , or universal_newlines : if set to True , the function returns the output as a string. Default is None . env : A dictionary with the environment variables. Note that the default environment obtained from the parent process is not extended, but replaced by the provided dictionary. Therefore, if we want to extend the environment, we have to initialize the dictionary with the parent environment: Python env = os.environ.copy() However, the subprocess.run method has some limitations. Notably, it cannot both capture and stream the output. To achieve this, and some other advanced features, we have to use the subprocess.Popen class. subprocess.Popen \u00b6 The subprocess.Popen class provides more control over the process. The basic usage is: p = subprocess.Popen(['ls', '-l']) # start the process # now we can communicate with the process, stream the output, etc. p.wait() # wait for the process to finish # now we can get the return code, continue with the code, etc. Loading resources \u00b6 Resources can be loaded using the importlib.resources module. This way, we can handle files but also resources stored in an archive. The basic usage is: import importlib.resources file = importlib.resources.files('package').joinpath('file.txt') # send the file to the function expecting a file-like object my_function(importlib.resources.as_file(file)) Numpy \u00b6 Data types \u00b6 Documentation of basic data types Date and time \u00b6 Documentation Numpy use the datetime64 data type for date and time. This type has its internal resolution, which can be anyting from nanoseconds to years. The resolution is displayed in the type name, e.g., datetime64[ns] and it is determined automatically from the input data if dtype is not specified or specified as datetime64 or by the dtype parameter if specified as datetime64[<resolution>] . Initialization \u00b6 We can create the new array as: zero-filled: np.zeros(<shape>, <dtype>) ones-filled: np.ones(<shape>, <dtype>) empty: np.empty(<shape>, <dtype>) filled with a constant: np.full(<shape>, <value>, <dtype>) Sorting \u00b6 for sorting, we use the sort function. There is no way how to set the sorting order, we have to use a trick for that: a = np.array([1, 2, 3, 4, 5]) a[::-1].sort() # sort in reverse order I/O \u00b6 Export to CSV \u00b6 To export the numpy array to CSV, we can use the savetxt function: np.savetxt('file.csv', a, delimiter=',') By default, the function saves values in the mathematical float format, even if the values are integers. To save the values as integers, we can use the fmt parameter: np.savetxt('file.csv', a, delimiter=',', fmt='%i') HDF5 \u00b6 To work with HDF5 files in Python, we use the h5py library. To import a dataset from an HDF5 file to a numpy array: import h5py with h5py.File('file.h5', 'r') as f: a = np.array(f['dataset']) To export a numpy array to an HDF5 file: with h5py.File('file.h5', 'w') as f: f.create_dataset('dataset', data=<numpy array>) Usefull array properties: \u00b6 size : number of array items unlike len, it counts all items in the mutli-dimensional array itemsize : memory (bytes) needed to store one item in the array nbytes : array size in bytes. Should be equal to size * itemsize . Masking \u00b6 A usefull technique in Numpy is masking: getting an array of boolean that indicate satisfaction of a condition. This array can be used to filter the elements of the original array (or any other array of the same shape). Example: a = np.array([1, 2, 3, 4, 5]) mask = a > 3 # mask is [False, False, False, True, True] b = a[mask] # b is [4, 5] For more complex masking, we can use the logical operators: & : logical and | : logical or ^ : logical xor ~ : logical not Aggregate masks \u00b6 Even more powerful technique is to use aggregate masks that are computed for a whole array or a single dimension. We create such a mask simply by applying an aggregation function to the mask. Example: a = np.array([[2, 3, 4], [5, 6, 7]]) mask = a < 3 # mask is [[True, True, False], [False, False, False]] column_mask = mask.any(axis=0) # column_mask is [True, True, False] row_mask = mask.any(axis=1) # row_mask is [True, False] Of course, we can use other aggregation functions than any , such as all , sum , mean , etc. Dedicated mask functions \u00b6 Apart from logical operations, we can use dedicated mask functions: nonzero : returns the indices of the non-zero elements Useful functions \u00b6 unique : returns the unique elements of the array. Important parameters: return_counts : if set to True , the function returns the counts of the unique elements (same as value_counts in pandas) Regular expressions \u00b6 In Python, the regex patterns are not compiled by default. Therefore we can use strings to store them. The basic syntax for regex search is: result = re.search(<pattern>, <string>) if result: # pattern matches group = result.group(<group index>) # print the first group The 0th group is the whole match, as usual. To substitute the matched pattern, we can use the sub function: pattern = re.compile(r'(\\d+)') result = pattern.sub(r'[\\1]', '123') # '[123]' Sometimes, we need to use numbers around the group index. In such cases, we have to use the \\<group index><<number>> notation: pattern = re.compile(r'(\\d+)') result = pattern.sub(r'\\g<1>2025', '123') # '1232025' Lambda functions \u00b6 Lambda functions in python have the following syntax: lambda <input parameters>: <return value> Example: f = lambda x: x**2 Only a single expression can be used in the lambda function, so we need standard functions for more complex logic (temporary variables, loops, etc.). Decorators \u00b6 Decorators are a special type of function that can be used to modify other functions. When we write an annotation with the name of a function above another function, the annotated function is decorated . It means that when we call the annotated function, a wrapper function is called instead. The wrapper function is the function returned by the decorater : the function with the same name as the annotation. If we want to also keep the original function functionality, we have to pass the function to the decorator and call it inside the wrapper function. In the following example, we create a dummy decorator that keeps the original function functionality: Example: def decorator(func): def wrapper(): result = func() return result return wrapper @decorator def my_func(): # do something return result Decorator with arguments \u00b6 If the original function has arguments, we have to pass them to the wrapper function. Example: def decorator(func): def wrapper(param_1, param_2): result = func(param_1, param_2) return result return wrapper @decorator def my_func(param_1, param_2): # do something return result Singletons \u00b6 SO question There are several ways how to implement a singleton in Python. The most common are: using a module-level variable using the __new__ method in combination with a base class use a metaclass using a decorator Module-level variable \u00b6 The most simple way is to use a module-level variable. Note taht if the singleton has to be initialized from the outside, the initialization has to be done in a singleton method, not in the constructor! class Singleton: def __init__(self): self.initialized = False def init(self, init_param): if not self.initialized: self.initialized = True # do the initialization singleton = Singleton() # initialization singleton.init(init_param) Testing with pytest \u00b6 Pytest is a simple testing framework for Python. It uses the assert statement for testing. The tests are defined in functions with the test_ prefix. Fixtures \u00b6 Fixtures are used to set up the environment for more than one test. If defined in the conftest.py file, they are available for all tests in the project. Fixtures are defined using the @pytest.fixture decorator. The fixture can be used in the test function by passing the fixture name as an argument. The fixture has the following structure: @pytest.fixture def my_fixture(): # code for setting up the environment yield # the test is executed here # clean up code Mocking \u00b6 For mocking, we can use the pytest-mock package. After installation, we can use the mocker fixture in any test function. Capturing output \u00b6 Documentation To capture the output, we can use the capsys fixture: def test_output(capsys): print('hello') captured = capsys.readouterr() assert captured.out == 'hello\\n' Similarly, we can inspect the standard error output using the captured.err attribute. Jupyter \u00b6 Memory \u00b6 Most of the time, when the memory allocated by the notebook is larger than expected, it is caused by some library objects (plots, tables...]). However sometimes, it can be forgotten user objects. To list all user objects, from the largest: # These are the usual ipython objects, including this one you are creating ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars'] # Get a sorted list of the objects and their sizes sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True) Reloding modules with autoreload \u00b6 When modules are imported they are not reloaded unless the kernel is restarted. In python scripts, this does not matter, we just execute the script again. However, when working with notebooks , it may be inconvenient to reload the kernel and all necessary cells just because of a small change in the imported module. Instead, we can use the autoreload extension . First, we have to load the extension: %load_ext autoreload Then, we configure the autoreload with %autoreload <mode> . The most common modes are: now (default): reload all modules immediately (if not excluded by the %aimport magic) This is useful especially if the automatic reloading does not work as expected. 0 , off : disable autoreload 1 , explicit : reload modules that were imported using the %aimport magic every time before executing the Python code 2 , all : reload all modules (except those excluded by %aimport ) every time before executing the Python code 3 , complete : same as 2 , but also add any new objects in the module Plotting \u00b6 There are several libraries for plotting in Python. The most common are: matplotlib plotly In the table below, we can see a comparison of the most common plotting libraries: Functionality Matplotlib Plotly real 3D plots no yes detail legend styling (padding, round corners...) yes no dash patterns for lines with different axes ranges yes no Matplotlib \u00b6 Official Manual Saving figures \u00b6 To save a figure, we can use the savefig function. The savefig function has to be called before the show function, otherwise the figure will be empty . Docstrings \u00b6 For documenting Python code, we use docstrings, special comments soroudned by three quotation marks: \"\"\" docstring \"\"\" Unlike in other languages, there are multiple styles for docstring content. The most common are: Epytext Python \"\"\" @param <param name>: <param description> @return: <return description> \"\"\" Google Python \"\"\" Args: <param name>: <param description> Returns: <return description> \"\"\" Numpy Python \"\"\" Parameters ---------- <param name> : <param type> <param description> Returns ------- <return type> <return description> \"\"\" reStructuredText Python \"\"\" :param <param name>: <param description> :return: <return description> \"\"\" Progress bars \u00b6 For displaying progress bars, we can use the tqdm library. It is very simple to use: from tqdm import tqdm for i in tqdm(range(100)): ... Important parameters: desc : description of the progress bar Custom progress updates \u00b6 When the progress is more complex (conditional updates, etc.), we can use the progress bar object to update the progress manually: with tqdm(total=<total number of steps>) as progress_bar: if(<condition>): progress_bar.update(<number of steps>) TQDM in Jupyter \u00b6 When using tqdm in Jupyter, the basic progress bar may not work (it may print other logs repeatedly). In such cases, we can change the import to: from tqdm.notebook import tqdm If the code can be called both from Jupyter and from console, we can use `autonotebook PostgreSQL \u00b6 When working with PostgreSQL databases, we usually use either the psycopg2 adapter or, the sqlalchemy . psycopg2 \u00b6 documentation To connect to a database: con = psycopg2.connect(<connection string>) After running this code a new session is created in the database, this session is handeled by the con object. The operation to the database is then done as follows: create a cursor object which represents a database transaction Python cur = con.cursor() execute any number of SQL commands Python cur.execute(<sql>) commit the transaction Python con.commit() SQLAlchemy \u00b6 Connection documentation SQLAlchemy works with engine objects that represent the application's connection to the database. The engine object is created using the create_engine function: from sqlalchemy import create_engine engine = create_engine('postgresql://user:password@localhost:5432/dbname') A simple SELECT query can be executed using using the following code: with engine.connect() as conneciton: result = conneciton.execute(\"SELECT * FROM table\") ... With modifying statements, the situation is more complicated as SQLAlchemy uses transactions by default. Therefore we need to commit the transaction. There are two ways how to do that: using the commit method of the connection object Python with engine.connect() as conneciton: conneciton.execute(\"INSERT INTO table VALUES (1, 2, 3)\") conneciton.commit() creating a new block for the transaction using the begin method of the connection object Python with engine.connect() as conneciton: with conneciton.begin(): conneciton.execute(\"INSERT INTO table VALUES (1, 2, 3)\") this option has also its shortcut: the begin method of the engine object Python with engine.begin() as conneciton: conneciton.execute(\"INSERT INTO table VALUES (1, 2, 3)\") Note that the old execute method of the engine object is not available anymore in newer versions of SQLAlchemy. Statements with parameters \u00b6 Sometimes it is desirable to use parameters in the SQL statements: it prevents SQL injection in case of user input, the provided parameters are automatically escaped, so we don't have to worry : in the SQL statement. The syntax is: connection.execute(\"INSERT INTO table VALUES (:param1, :param2)\", param1=1, param2=2) # or connection.execute(\"INSERT INTO table VALUES (:param1, :param2)\", {'param1': 1, 'param2': 2}) Executing statements without transaction \u00b6 By default, sqlalchemy executes sql statements in a transaction. However, some statements (e.g., CREATE DATABASE ) cannot be executed in a transaction. To execute such statements, we have to use the execution_options method: with sqlalchemy_engine.connect() as conn: conn.execution_options(isolation_level=\"AUTOCOMMIT\") conn.execute(\"<sql>\") conn.commit() Getting the affected rowcount \u00b6 The result object returned by the execute method has the rowcount attribute that contains the number of affected rows. Executing multiple statements at once \u00b6 To execute multiple statements at once, for example when executing a script, it is best to use the execute method of the psycopg2 connection object. Moreover, to safely handle errors, it is best to catch the exceptions and manually rollback the transaction in case of an error: conn = psycopg2.connect(<connection string>) cursor = conn.cursor() try: cursor.execute(<sql>) conn.commit() except Exception as e: conn.rollback() raise e finally: cursor.close() conn.close() Working with GIS \u00b6 When working with gis data, we usually change the pandas library for its GIS extension called geopandas . For more, see the pandas manual. Geocoding \u00b6 For geocoding, we can use the Geocoder library. Complex data structures \u00b6 KDTree \u00b6 documentation KDTree can be found in the scipy library. Geometry \u00b6 There are various libraries for working with geometry in Python: scipy.spatial : for basic geometry operations shapely geopandas : for gis data. See the pandas manual for more details. Shapely \u00b6 homepage To merge a multi-linestring into a single linestring, we can use the line_merge function. Downloading files \u00b6 To download files from the internet, we can use the requests library. The basic usage is: import requests response = requests.get(<url>) with open(<filename>, 'wb') as f: f.write(response.content) Graphs with NetworkX and OSMnx \u00b6 NetworkX documentation OSMnx documentation NetworkX is a library for working with graphs. OSMnx is a library that creates networkx graphs from OpenStreetMap data. Creating a graph from OpenStreetMap \u00b6 To create a graph from OpenStreetMap, we can use the ox.graph_from_place function. Limitations The are we want to must be some official name, and we have to know the name The elements are selected either by nodes inside the area, or edges, but not ways. If we need keeping the ways intact (if they cross the area boundary), we need to use a different approach. Example: import osmnx as ox graph = ox.graph_from_place(<place name>, network_type=<network type>) The network_type parameter can be one of the following: 'drive' : driving network 'walk' : walking network 'bike' : cycling network And some more. Other important parameters are: simplify : whether to contract the graph. Default is True . retain_all : whether to retain all nodes, or just the largest strongly connected component. Default is False . truncate_by_edge : If True, retain nodes outside the area if they have at least one neighbor inside the area. Default is False . To add speed information to the graph, we can use the OSMnx.add_edge_speeds function: graph = ox.add_edge_speeds(graph) Similarily, we can add travel times: graph = ox.add_edge_travel_times(graph) Caching \u00b6 OSMnx automatically caches the overpass API responses to the cache directory, each response as a separate json file. Importing OSM elements \u00b6 We can also import individual OSM elements (nodes, ways, relations) from OpenStreetMap using the ox.features_from_place function. This function returns a geodataframe with the features. Limitations: there is no way how to download related features. Ways, for example, are downloaded with full geometry, but the associated nodes are not downloaded. Creating a graph from pandas dataframe \u00b6 To create a graph from a pandas dataframe, we can use the NetworkX.from_pandas_edgelist function: graph = nx.from_pandas_edgelist(df, source='source', target='target', edge_attr=['weight', 'speed', 'travel_time'], create_using=nx.DiGraph()) Exporting graph to pandas dataframe \u00b6 To export a graph to a pandas dataframe, we have multiple options: osmnx.graph_to_gdfs : exports the graph to a pandas dataframe. Important parameters: edges : if True , the edges are exported. Default is True . nodes : if True , the nodes are exported. Default is True . node_geometry : if True , the node geometries are exported. Default is True . fill_edge_geometry : if True , the edge geometries are exported. Default is True . NetworkX.to_pandas_edgelist : exports the graph to a pandas dataframe. note that htis method does not preserve the edge order (unlike the osmnx.graph_to_gdfs function) another option is to use the nodes() and edges() methods of the graph object and manually create the dataframe. Reindexing the nodes \u00b6 Sometimes, we want to discard the node indices created from the input data and use a new index that has a range of (0, n-1). We can do this by using the NetworkX.convert_node_labels_to_integers function: graph = nx.convert_node_labels_to_integers(graph)","title":"Python Manual"},{"location":"Programming/Python/Python%20Manual/#basics","text":"","title":"Basics"},{"location":"Programming/Python/Python%20Manual/#variables","text":"To check if a local variable is defined, we can use the locals function: if 'my_variable' in locals(): print('Variable exists')","title":"Variables"},{"location":"Programming/Python/Python%20Manual/#conditions-and-boolean-context","text":"","title":"Conditions and boolean context"},{"location":"Programming/Python/Python%20Manual/#comparison-operators","text":"Python uses the standard set of comparison operators ( == , != , < , > , <= , >= ). They are functionally similar to C++ operators: they can be overloaded and the semantic meaning of == is equality, not identity (in contrast to Java).","title":"Comparison operators"},{"location":"Programming/Python/Python%20Manual/#automatic-conversion-to-bool","text":"Unlike in other languages, any expression can be used in boolean context in python, as there are rules how to convert any type to bool . The following statement is valid, foor example: s = 'hello' if s: print(s) The code above prints 'hello', as the variable s evaluates to True . Any object in Python evaluates to True , with exeption of: False None numerically zero values (e.g., 0 , 0.0 ) standard library types that are empty (e.g., empty string, list , dict ) The automatic conversion to bool in boolean context has some couner intuitive consequences. The following conditions are not equal: s = 'hello' if s: # s evaluates to True if s == True: # the result of s == True is False, then False evaluete to False","title":"Automatic conversion to bool"},{"location":"Programming/Python/Python%20Manual/#checking-the-type","text":"To check the exact type: if type(<VAR>) is <TYPE>: # e.g. if type(o) is str: To check the type in the polymorphic way, including the subtypes: if isinstance(<VAR>, <TYPE>): # e.g. if isinstance(o, str):","title":"Checking the type"},{"location":"Programming/Python/Python%20Manual/#built-in-data-types","text":"","title":"Built-in data types"},{"location":"Programming/Python/Python%20Manual/#numbers","text":"Python has the following numeric types: int - integer float - floating point number The int type is unlimited, i.e., it can represent any integer number. The float type is limited by the machine precision, i.e., it can represent only a finite number of real numbers.","title":"Numbers"},{"location":"Programming/Python/Python%20Manual/#check-if-a-float-number-is-integer","text":"To check whether a float number is integer, we can use the is_integer function:","title":"Check if a float number is integer"},{"location":"Programming/Python/Python%20Manual/#check-if-a-number-is-nan","text":"To check whether a number is NaN, we can use the math.isnan function or the numpyp.isnan function:","title":"Check if a number is NaN"},{"location":"Programming/Python/Python%20Manual/#rounding","text":"To round a number, use the round function. For rounding up , use the math.ceil function. For rounding down , use the math.floor function.","title":"Rounding"},{"location":"Programming/Python/Python%20Manual/#strings","text":"Strings in Python can be enclosed in single or double quotes (equivalent). The triple quotes can be used for multiline strings.","title":"Strings"},{"location":"Programming/Python/Python%20Manual/#string-formatting","text":"The string formatting can be done in several ways: using the f prefix to string literal: f'{<VAR>}' using the format method: '{}'.format(<VAR>) Each variable can be formatted for that, Python has a string formatting mini language . The format is specified after the : character (e.g., f'{47:4}' set the width of the number 47 to 4 characters). Most of the format specifiers have default values, so we can omit them (e.g., f'{47:4}' is equivalent to f'{47:4d}' ). The following are the most common options: To use the character { and } in the string, we have to escape them using double braces: {{ and }} .","title":"String formatting"},{"location":"Programming/Python/Python%20Manual/#string-methods","text":"capitalize : capitalize the first letter of the string lower : convert the string to lowercase upper : convert the string to uppercase strip : remove leading and trailing whitespaces lstrip : remove leading whitespaces rstrip : remove trailing whitespaces","title":"String methods"},{"location":"Programming/Python/Python%20Manual/#enumerations","text":"For enumerations, we can use the enum module. The basic syntax is: from enum import Enum class MyEnum(Enum): VALUE1 = 1 VALUE2 = 2","title":"Enumerations"},{"location":"Programming/Python/Python%20Manual/#ranges","text":"Documentation Python has a special concept of ranges, that are objects representing immutable sequences of numbers. We can use ranges for iteration instead of initializing local variables like in other computer languages. In typical programming languages, a loop with known number of iterations is written as: for(int i = 0; i < 10; i++) { System.out.println(i); } However, in Python, there is no such a thing, and instead we use ranges: for i in range(10): print(i) The range() construct has the syntax of range([<start>], <stop>, [<step>]) . all parameters are integers the default <start> is 0 the default <step> is 1 we need to specify <start> to be able to set <step> .","title":"Ranges"},{"location":"Programming/Python/Python%20Manual/#floating-point-ranges","text":"For floating point ranges, we can use NumPy's arange function: import numpy as np for i in np.arange(0, 10, 0.1): print(i)","title":"Floating point ranges"},{"location":"Programming/Python/Python%20Manual/#collections-and-generators","text":"Python has several built-in data structures, most notably list , tuple , dict , and set . These are less efficient then comparable structures in other languages, but they are very convenient to use. Also, there is a special generator type. It does not store the data it is only a convinient way how to access data generated by some function.","title":"Collections and generators"},{"location":"Programming/Python/Python%20Manual/#generator","text":"Python wiki Generators are mostly used in the iteration, we can iterte them the same way as lists. To get the first item of the generator, we can use the next function: g = (x for x in range(10)) first = next(g) # 0 To create a generator function (a function that returns a generator), we can use the yield keyword. The following function returns a generator that yields the numbers 1, 2, and 3: def gen(): yield 1 yield 2 yield 3 The length of the generator is not known in advance, to get the length, we have to iterate the generator first, for example using len(list(<generator>))","title":"Generator"},{"location":"Programming/Python/Python%20Manual/#tuple","text":"Tuples are meant to store a fixed sequence of values. They are immutable. The tuple literal is a comma-separated list of values in round braces: t = (1, 2, 3)","title":"Tuple"},{"location":"Programming/Python/Python%20Manual/#named-tuples","text":"Official Manual Named tuples are tuples with named members. Example: from collections import namedtuple Point = namedtuple('Point', ['x', 'y']) p = Point(1, 2) print(p.x) # 1 If we want to use type hints, we can use the NamedTuple class: from typing import NamedTuple class Point(NamedTuple): x: int y: int p = Point(1, 2) print(p.x) # 1","title":"Named tuples"},{"location":"Programming/Python/Python%20Manual/#dictionary","text":"Official Manual Disctionaries are initialized using curly braces ( {} ) and the : operator: d = { 'key1': 'value1', 'key2': 'value2', ... } Two dictionaries can be merged using the | operator: d3 = d1 | d2","title":"Dictionary"},{"location":"Programming/Python/Python%20Manual/#get-a-default-value-if-a-key-is-not-present","text":"To get a default value if a key is not present, we can use the get method: d = {'a': 1} d.get('b', 0) # 0","title":"Get a default value if a key is not present"},{"location":"Programming/Python/Python%20Manual/#set","text":"Documentation Sets are initialized using curly braces ( {} ) or the set function: s = {1, 2, 3} s = set([1, 2, 3]) To add elements to the set, we use either the add for a single element or the update for multiple elements. In both cases, a union of the set and the new elements is computed, i.e., no exception is raised if an element is already in the set.","title":"Set"},{"location":"Programming/Python/Python%20Manual/#comprehensions","text":"In addition to literals, Python has a convinient way of creating basic data structures: the comprehensions. The basic syntax is: <struct var> = <op. brace> <member var expr.> for <member var> in <iterable><cl. brace> As for literals, we use square braces ( [] ) for lists, curly braces ( {} ) for sets, and curly braces with colons for dictionaries. In contrast, we get a generator expression when using round braces ( () ), not a tuple. We can also use the if keyword to filter the elements: a = [it for it in range(10) if it % 2 == 0] # [0, 2, 4, 6, 8]","title":"Comprehensions"},{"location":"Programming/Python/Python%20Manual/#sorting","text":"Official Manual For sorting, you can use the sorted function. Instead of using comparators, Python has a different concept of key functions for custom sorting. The key function is a function that is applied to each element before sorting. For any expected object, the key function should return a value that can be compared.","title":"Sorting"},{"location":"Programming/Python/Python%20Manual/#complex-sorting-using-tuples","text":"If we need to apply some complex sorting, we can use tuples as the key function return value. The tuples have comparison operator defined, the implementation is as follows: elements are compared one by one on first non-equal element, the comparison result is returned This way, we can implement a complex sorting that would normaly require several conditions by storing the condition results in the tuple.","title":"Complex sorting using tuples"},{"location":"Programming/Python/Python%20Manual/#slices","text":"Many Python data structures support slicing: selecting a subset of elements. The syntax is: <object>[<start>:<end>:<step>] The start and end are inclusive. The step is optional and defaults to 1. The start is also optional and defaults to 0. Instead of omitting the start and end , we can use the None keyword: a = [1, 2, 3, 4, 5] a[None:3] # [1, 2, 3] Sometimes, it is not possible to use the slice syntax: when we need to use a variable for the step or, when the object use the slice syntax for something else, e.g., for selecting columns in a Pandas dataframe. In such cases, we can use the slice object: a[0:10:2] s = slice(0, 10, 2) a[s] # equivalent Here, the parameters can be ommited as well. We can select everything by using slice(None) , which is equivalent to slice(None, None, None) .","title":"Slices"},{"location":"Programming/Python/Python%20Manual/#copying-collections","text":"If we copy a complex collection (e.g., a list of dictionaries), we typically want to create a deep copy so that the original collection is not modified. We can use the copy module for that: import copy a = [{'a': 1}, {'b': 2}] b = copy.deepcopy(a)","title":"Copying collections"},{"location":"Programming/Python/Python%20Manual/#date-and-time","text":"Python documentation The base object for date and time is datetime","title":"Date and time"},{"location":"Programming/Python/Python%20Manual/#datetime-construction","text":"The datetime object can be directly constructed from the parts: from daterime import datetime d = datetime(2022, 12, 20, 22, 30, 0) # 2022-12-20 22:30:00 The time part can be ommited. We can load the datetime from string using the strptime function: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') For all possible time formats, check the strftime cheatsheet","title":"datetime construction"},{"location":"Programming/Python/Python%20Manual/#accessing-the-parts-of-datetime","text":"The datetime object has the following attributes: year month day hour minute second We can also query the day of the week using the weekday() method. The day of the week is represented as an integer, where Monday is 0 and Sunday is 6.","title":"Accessing the parts of datetime"},{"location":"Programming/Python/Python%20Manual/#intervals","text":"There is also a dedicated object for time interval named timedelta . It can be constructed from parts (seconds to days), all parts are optional. We can obtain a timedelta by substracting a datetime from another datetime : d1 = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') d2 = datetime.strptime('2022-05-20 18:30', '%Y-%m-%d %H:%M') interval = d2 - d1 # 30 minutes We can also add or substract a timedelta object from the datetime object: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') interval = timedelta(hours=1) d2 = d + interval # '2022-05-20 19:00'","title":"Intervals"},{"location":"Programming/Python/Python%20Manual/#converting-to-unix-timestamp","text":"To convert a datetime object to unix timestamp, we can use the timestamp method. It returns the number of seconds since the epoch (1.1.1970 00:00:00). Note however, that the timestamp is computed based on the datetime object's timezone, or your local timezone if the datetime object has no timezone information.","title":"Converting to Unix timestamp"},{"location":"Programming/Python/Python%20Manual/#time-and-date-objects","text":"Instead of using the datetime object, we can use the time and date objects if we need to work with time or date only. Analogously to the datetime object, these objects can be constructed from the parts : t = time(hour=12, minute=30, second=0) d = date(year=2022, month=12, day=20) To convert datetime to time or date , we can use the time and date methods. d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') t = d.time() # '18:00:00' d = d.date() # '2022-05-20' Similarly, we use the date and time objects to construct a datetime object using the combine method: d = date(year=2022, month=12, day=20) t = time(hour=12, minute=30, second=0) dt = datetime.combine(d, t) # '2022-12-20 12:30:00'","title":"Time and date objects"},{"location":"Programming/Python/Python%20Manual/#named-tuples_1","text":"Apart from the standard tuple, Python has a named tuple class that can be created using the collections.namedtuple function. In named tuple, each member has a name and can be accessed using the dot operator: from collections import namedtuple Point = namedtuple('Point', ['x', 'y']) p = Point(1, 2) print(p.x) # 1","title":"Named tuples"},{"location":"Programming/Python/Python%20Manual/#functions","text":"","title":"Functions"},{"location":"Programming/Python/Python%20Manual/#variable-length-arguments","text":"There are two types of variable length arguments in Python: *args - variable length positional arguments. **kwargs - variable length keyword arguments","title":"Variable length arguments"},{"location":"Programming/Python/Python%20Manual/#argument-unpacking","text":"if we need to conditionaly execute function with a different set of parameters (supposed the function has optional/default parameters), we can avoid multiple function calls inside the branching tree by using argument unpacking. Suppose we have a function with three optional parameters: a , b , c . If we skip only last n parameters, we can use a list for parameters and unpack it using * : def call_me(a, b, c): ... l = ['param A', True] call_me(*l) # calls the function with a = 'param A' and b = True If we need to skip some parameters in the middle, we have to use a dict and unpack it using ** : d = {'c': 142} call_me(**d) # calls the function with c = 142","title":"Argument unpacking"},{"location":"Programming/Python/Python%20Manual/#string-formatting_1","text":"To format python strings we can use the format function of the string or the equivalen fstring: a = 'world' message = \"Hello {} world\".format(a) message = f\"Hello {a}\" # equivalent If we need to a special formatting for a variable, we can specify it behind : as we can see in the following example that padds the number from left: uid = '47' message = \"Hello user {:0>4d}\".format(a) # prints \"Hello user 0047\" message = f\"Hello {a:0>4d}\" # equivalent More formating optios can be found in the Python string formatting cookbook .","title":"String formatting"},{"location":"Programming/Python/Python%20Manual/#classes","text":"Official Manual Classes in Python are defined using the class keyword: class MyClass: ... Unlike in other languages, we only declare the function members, other members are declared in the constructor or even later.","title":"Classes"},{"location":"Programming/Python/Python%20Manual/#constructor","text":"The constructor is a special function named __init__ . Usually, non-function members are declared in the constructor: class MyClass: def __init__(self, a, b): self.a = a self.b = b self.c = 0 self.d = None","title":"Constructor"},{"location":"Programming/Python/Python%20Manual/#check-if-an-object-contains-a-member","text":"To check whether an object contains a member, we can use the hasattr function: if hasattr(obj, 'member'): ...","title":"Check if an object contains a member"},{"location":"Programming/Python/Python%20Manual/#constructor-overloading","text":"Python does not support function overloading, including the constructor. That is unfortunate as default arguments are less powerfull mechanism. For other functions, we can supplement overloading using a function with a different name. However, for the constructor, we need to use a different approach. The most clean way is to use a class method as a constructor. Example: class MyClass: def __init__(self, a, b = 0): self.a = a self.b = b self.c = 0 self.d = None @classmethod def from_a(cls, b): return cls(0, b)","title":"Constructor overloading"},{"location":"Programming/Python/Python%20Manual/#importing","text":"In python, we can import whole modules as: import <module> Also, we can import specific functions, classes, or variables from the module: from <module> import <name> Note that when importing variable, we import the reference to the variable. Therefore, it will become out of sync with the original variable if the original variable is reassigned. Therefore, importing non-constant variables is not recommended. The module path can absolute or relative (starting with . ). Absolute imports are recommended, as they are more robust and less error-prone.","title":"Importing"},{"location":"Programming/Python/Python%20Manual/#resolving-absolute-module-paths","text":"If the path is absolute, it is resolved as follows: The already imported modules are searched The built-in modules are searched The module is searched in the import path which is a list of directories stored in the sys.path variable. The sys.path variable typically contains the following directories: the directory of the script that is executed ( '' in case of the interactive shell), the directories in the PYTHONPATH environment variable, the standard library directories (e.g., /usr/lib/python3.9 ), and the site-packages directory.","title":"Resolving absolute module paths"},{"location":"Programming/Python/Python%20Manual/#resolving-relative-module-paths","text":"Relative imports can only be used in packages (directories with __init__.py file). The relative path may start with . : relative to the current module, .. relative to the parent module.","title":"Resolving relative module paths"},{"location":"Programming/Python/Python%20Manual/#imports-in-tests","text":"The tests are located outside the main package, so we cannot use the absolute import starting with the package name. One option is to use relative imports. But a better option is to use absolute imports starting from the project root . We can do that, because test suites like pytest add the project root to the sys.path variable. The project root is typically determined automatically by the test suite, e.g. by searching for the setup.py file. Therefore, if the tests directory is located in the same directory as the setup.py file, we can import as follows: import tests/common","title":"Imports in tests"},{"location":"Programming/Python/Python%20Manual/#exceptions","text":"documentation Syntax: try: <code that can raise exception> except <ERROR TYPE> as <ERROR VAR>: <ERROR HANDELING> finally: <code that is executed always> The except and finally blocks are optional. In other words, we can handle errors without having any default cleanup code, and we can have cleanup code without handling errors.","title":"Exceptions"},{"location":"Programming/Python/Python%20Manual/#raising-exceptions","text":"To raise an exception, we can use the raise keyword: raise ValueError('message') Sometimes, we want just to re-raise an exception after some partial exception handling. In such cases, we can use the raise keyword without arguments: try: ... except: ... raise","title":"Raising exceptions"},{"location":"Programming/Python/Python%20Manual/#assertions","text":"In Python, assertions are executed by defult. They can be disabled by running python with the -O or -OO flag. The syntax is: assert <condition>, <message>","title":"Assertions"},{"location":"Programming/Python/Python%20Manual/#filesystem","text":"There are three ways commonly used to work with filesystem in Python: manipulate paths as strings os.path pathlib The folowing code compares both approaches for path concatenation: # string path concatenation a = \"C:/workspace\" b = \"project/file.txt\" c = f\"{a}/{b}\" # os.path concatenation a = \"C:/workspace\" b = \"project/file.txt\" c = os.path.join(a, b) # pathlib concatentation a = Path(\"C:/workspace\") b = Path(\"project/file.txt\") c = a / b As the pathlib is the most modern approach, we will use it in the following examples. Appart from pathlib documentation, there is also a cheat sheet available on github .","title":"Filesystem"},{"location":"Programming/Python/Python%20Manual/#path-editing","text":"","title":"Path editing"},{"location":"Programming/Python/Python%20Manual/#computing-relative-path","text":"To prevent misetakes, it is better to compute relative paths beteen directories than to hard-code them. Fortunately, there are methods we can use for that. If the desired relative path is a child of the start path, we can simply use the relative_to method of the Path object: a = Path(\"C:/workspace\") b = Path(\"C:/workspace/project/file.txt\") rel = b.relative_to(a) # rel = 'project/file.txt' However, if we need to go back in the filetree, we need a more sophisticated method from os.path : a = Path(\"C:/Users\") b = Path(\"C:/workspace/project/file.txt\") rel = os.path.relpath(a, b) # rel = '../Workspaces/project/file.txt'","title":"Computing relative path"},{"location":"Programming/Python/Python%20Manual/#get-parent-directory","text":"We can use the parent property of the Path object: p = Path(\"C:/workspace/project/file.txt\") parent = p.parent # 'C:\\\\workspace\\\\project'","title":"Get parent directory"},{"location":"Programming/Python/Python%20Manual/#absolute-and-canonical-path","text":"We can use the absolute method of the Path object to get the absolute path. To get the canonical path, we can use the resolve method. Always prefer the canonical path, because it can prevent many possible errors (with cloud folders, etc.).","title":"Absolute and canonical path"},{"location":"Programming/Python/Python%20Manual/#splitting-paths-and-working-with-path-parts","text":"To read the file extension , we can use the suffix property of the Path object. The property returns the extension with the dot . To change the extension , we can use the with_suffix method: p = Path(\"C:/workspace/project/file.txt\") p = p.with_suffix('.csv') # 'C:\\\\workspace\\\\project\\\\file.csv' To remove the extension , just use the with_suffix method with an empty string. We can split the path into parts using the parts property: p = Path(\"C:/workspace/project/file.txt\") parts = p.parts # ('C:\\\\', 'workspace', 'project', 'file.txt') To find the index of some specific part, we can use the index method: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 Later, we can use the index to manipulate the path: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 p = Path(*p.parts[:index]) # 'C:\\\\workspace'","title":"Splitting paths and working with path parts"},{"location":"Programming/Python/Python%20Manual/#changing-path-separators","text":"To change the path separators to forward slashes, we can use the as_posix and method: p = Path(r\"C:\\workspace\\project\\file.txt\") p = p.as_posix() # 'C:/workspace/project/file.txt'","title":"Changing path separators"},{"location":"Programming/Python/Python%20Manual/#using-as-the-home-directory-in-paths","text":"Normally, the ~ character is not recognized as the home directory in Python paths. To enable this, we can use the expanduser method: p = Path(\"~/project/file.txt\") p = p.expanduser() # 'C:\\\\Users\\\\user\\\\project\\\\file.txt'","title":"Using ~ as the home directory in paths"},{"location":"Programming/Python/Python%20Manual/#working-directory","text":"os.getcwd() - get the current working directory os.chdir(<path>) - set the current working directory","title":"Working directory"},{"location":"Programming/Python/Python%20Manual/#iterating-over-files","text":"The pathlib module provides a convenient way to iterate over files in a directory. The particular methods are: iterdir - iterate all files and directories in a directory glob - iterate over files in a single directory, using a filter rglob - iterate over files in a directory and all its subdirectories, using a filter In general, the files will be sorted alphabetically. When we need a different order, we have to store the results in a list and sort it.","title":"Iterating over files"},{"location":"Programming/Python/Python%20Manual/#single-directory-iteration","text":"Using pathlib, we can iterate over files using a filter with the glob method: p = Path(\"C:/workspace/project\") for filepath in p.glob('*.txt') # iterate over all txt files in the project directory The old way is to use the os.listdir method: p = Path(\"C:/workspace/project\") for filename in os.listdir(p): if filename.endswith('.txt'): filepath = p / filename","title":"Single directory iteration"},{"location":"Programming/Python/Python%20Manual/#recursive-iteration","text":"Using pathlib, we can iterate over files using a filter with the rglob method: p = Path(\"C:/workspace/project\") for filepath in p.rglob('*.txt') # iterate over all txt files in the project directory and all its subdirectories The old way is to use the os.walk method: p = Path(\"C:/workspace/project\") for root, dirs, files in os.walk(p): for filename in files: if filename.endswith('.txt'): filepath = Path(root) / filename","title":"Recursive iteration"},{"location":"Programming/Python/Python%20Manual/#iterate-only-directoriesfiles","text":"There is no specific filter for files/directories, but we can use the is_file or is_dir method to filter out directories: p = Path(\"C:/workspace/project\") for filepath in p.glob('*'): if filepath.is_file(): # do something","title":"Iterate only directories/files"},{"location":"Programming/Python/Python%20Manual/#use-more-complex-filters","text":"Unfortunately, the glob and rglob methods do not support more complex filters (like regex). However, we can easily apply the regex filter manually: p = Path(\"C:/workspace/project\") for filepath in p.glob('*'): if not re.match(r'^config.yaml$', filepath.name): # do something","title":"Use more complex filters"},{"location":"Programming/Python/Python%20Manual/#get-the-path-to-the-current-script","text":"Path(__file__).resolve().parent","title":"Get the path to the current script"},{"location":"Programming/Python/Python%20Manual/#checking-write-permissions-for-a-directory","text":"Unfortunatelly, most of the methods for checking write permissions are not reliable outside Unix systems. The most reliable way is to try to create a file in the directory: p = Path(\"C:/workspace/project\") try: with open(p / 'test.txt', 'w') as f: pass p.unlink() return True except PermissionError: return False except: raise # re-raise the exception Other methods like os.access or using tempfile module are not reliable on Windows (see e.g.: https://github.com/python/cpython/issues/66305).","title":"Checking write permissions for a directory"},{"location":"Programming/Python/Python%20Manual/#creating-directories","text":"To create a directory, we can use the mkdir method of the Path object: p = Path(\"C:/workspace/project\") p.mkdir() Important parameters: parents : if set to True , the directory will be created even if the parent directories do not exist. Default is False . exist_ok : if set to True , the directory will not be created if it already exists. Default is False .","title":"Creating directories"},{"location":"Programming/Python/Python%20Manual/#copying-files-and-directories","text":"For copying files and directories, we can use the shutil module. The most used method is copy2 , which copies the file with all metadata: import shutil p1 = Path(\"C:/workspace/project/file.txt\") p2 = Path(\"C:/workspace/project/file2.txt\") shutil.copy2(p1, p2) The copy2 method can also copy into a directory: p1 = Path(\"C:/workspace/project/file.txt\") p2 = Path(\"C:/workspace/project2\") shutil.copy2(p1, p2) # the new file will be 'C:/workspace/project2/file.txt' Other methods and the comparison are described in a SO question .","title":"Copying files and directories"},{"location":"Programming/Python/Python%20Manual/#deleting-files-and-directories","text":"To delete a file, we can use the unlink method of the Path object: p = Path(\"C:/workspace/project/file.txt\") p.unlink() for deleting directories, we can use the rmdir method: p = Path(\"C:/workspace/project\") p.rmdir() However, the rmdir method can delete only empty directories. To delete a directory with content, we can use the shutil module: p = Path(\"C:/workspace/project\") shutil.rmtree(p)","title":"Deleting files and directories"},{"location":"Programming/Python/Python%20Manual/#deleting-windows-read-only-files-ie-access-denied-error","text":"On Windows, all the delete methods can fail because lot of files and directories are read-only. This is not a problem for most application, but it breaks Python delete methods. One way to solve this is to handle the error and change the attribute in the habdler. Example for shutil: import os import stat import shutil p = Path(\"C:/workspace/project\") shutil.rmtree(p, onerror=lambda func, path, _: (os.chmod(path, stat.S_IWRITE), func(path)))","title":"Deleting Windows read-only files (i.e. Access Denied error)"},{"location":"Programming/Python/Python%20Manual/#working-with-temporary-files","text":"The tempfile module provides a convenient way to work with temporary files. To create a temporary file, we can use the NamedTemporaryFile function: import tempfile with tempfile.NamedTemporaryFile() as f: f.write(<data>) Unlike normal files, we can both read and write to the temporary file using a single file object. However, we must return the file pointer to the beginning of the file: with open('file.txt', 'rw') as f: f.write('data') f.seek(0) data = f.read()","title":"Working with temporary files"},{"location":"Programming/Python/Python%20Manual/#troubleshooting-missing-files","text":"If Python cannot locate a file that we can locate in the file explorer, the problem can be cause by the cloud syncronization. Sometimes, relative paths are not resolved correctly when a part of the path is on the cloud folder. We can solve this by resolving the path: safe_path = safePath(<relative path>).resolve() .","title":"Troubleshooting Missing Files"},{"location":"Programming/Python/Python%20Manual/#io","text":"For simple file operations, we can use the open function. A simple file read is done as follows: with open('file.txt', 'r') as f: data = f.read() A simple file write is done as follows: with open('file.txt', 'w') as f: f.write('data') By default, the open function opens the file in text mode. To open the file in binary mode, we have to use the b flag: with open('file.txt', 'rb') as f: data = f.read()","title":"I/O"},{"location":"Programming/Python/Python%20Manual/#csv","text":"Official Manual The csv module provides a Python interface for working with CSV files. The basic usage is: import csv with open('file.csv', 'r') as f: reader = csv.reader(f) for row in reader: # do something Reader parameters: delimiter - the delimiter character","title":"CSV"},{"location":"Programming/Python/Python%20Manual/#json","text":"Official Manual TO read a JSON file: import json with open('file.json', 'r') as f: data = json.load(f) To write a JSON file: import json data = {'a': 1, 'b': 2} with open('file.json', 'w') as f: json.dump(data, f) Important parameters: indent : the number of spaces used for indentation. This also enables other pretty-printing functionalities, like newlines after each element.","title":"JSON"},{"location":"Programming/Python/Python%20Manual/#custom-serialization","text":"The json module can serialize only basic types. If we need to serialize custom objects, we have to provide a custom serialization class. We then supply the class to the cls parameter of the dump function. The serialization class is usually a subclass of the JSONEncoder class. The class has to implement the default method, which is called for each object that cannot be serialized by the standard serialization methods. Example: import json class MyEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, MyCustomClass): return obj.to_json() return super().default(obj)","title":"Custom serialization"},{"location":"Programming/Python/Python%20Manual/#hdf5","text":"HDF5 is a binary file format for storing large amounts of data. The h5py module provides a Python interface for working with HDF5 files. An example of reading a dataset from an HDF5 file on SO","title":"HDF5"},{"location":"Programming/Python/Python%20Manual/#ini-files","text":"The configparser module provides a Python interface for working with INI files . The basic usage is: import configparser config = configparser.ConfigParser() config.read('file.ini') value = config['section']['key'] If we do not have sections in the INI file, we have to: use the allow_unnamed_section argument of the ConfigParser: Python config = configparser.ConfigParser(allow_unnamed_section=True) use the configparser.UNNAMED_SECTION in place of the section name: Python value = config[configparser.UNNAMED_SECTION]['key']","title":"INI files"},{"location":"Programming/Python/Python%20Manual/#command-line-arguments","text":"The sys module provides access to the command line arguments. They are stored in the argv list with the first element being the name of the script. However, mostly, we use the argparse module to parse the command line arguments. Example: import argparse parser = argparse.ArgumentParser() parser.add_argument('input', type=str, help='input file') parser.add_argument('output', type=str, help='output file') args = parser.parse_args() print(args.input) The add_argument method has the following parameters: name : the name of the argument type : the type of the argument help : the help message for the argument required : if set to True , the argument is required default : the default value for the argument action : the action to be performed on the argument. Examples: store_true : store True if the argument is present, False otherwise","title":"Command line arguments"},{"location":"Programming/Python/Python%20Manual/#logging","text":"Official Manual The logging itself is then done using the logging module methods: logging.info(\"message\") logging.warning(\"message %s\", \"with parameter\") A simple logging configuration: logging.basicConfig( level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s', handlers=[ logging.FileHandler(\"log.txt\"), logging.StreamHandler() ] ) Note that this configuration can be done only once. Therefore, it should not be done in a library as it prevents the user from configuring the logging. To set the level for a specific logger, we use the setLevel method: logger.setLevel(logging.DEBUG) . We can also use a string representation of the level, e.g., logger.setLevel('DEBUG') . To check the level of the logger, we can use the isEnabledFor method: if logger.isEnabledFor(logging.DEBUG): ... This can be useful for avoiding expensive computations needed just for logging if the logging level is set to a higher level.","title":"Logging"},{"location":"Programming/Python/Python%20Manual/#type-hints","text":"Official Manual Type hints are a useful way to help the IDE and other tools to understand the code so that they can provide better support (autocompletion, type checking, refactoring, etc.). The type hints are not enforced at runtime, so they do not affect the performance of the code. We can specify the type of a variable using the : operator: a: int = 1 Apart from the basic types, we can also use the typing module to specify more complex types: from typing import List, Dict, Tuple, Set, Optional, Union, Any a: List[int] = [1, 2, 3] We can also specify the type of a function argument and return value: def foo(a: int, b: str) -> List[int]: return [a, b]","title":"Type hints"},{"location":"Programming/Python/Python%20Manual/#type-hints-in-loops","text":"The type of the loop variable is usually inferred by IDE from the type of the iterable. However, this sometimes fails, e.g., for zip objects. In such cases, we need to specify the type of the loop variable. However, we cannot use the : directly in the loop, but instead, we have to declare the variable before the loop : for a: int in ... # error a: int for a in ... # ok","title":"Type hints in loops"},{"location":"Programming/Python/Python%20Manual/#circular-type-hints","text":"Unfortunately, Python currently does not support circular type hints. However, it should be possible to use circular type hints since Python 3.14 . There are two types of circular type hints: we need to refer the type while defining it. For that, we use the Self type: Python class MyClass: def get_me(self) -> Self: return self two or more types refer to each other. For that, use a string representation of the type: ```Python class ClassA: def init (self, b: 'ClassB'): self.b = b class ClassB: def set_a(self, a: ClassA): self.a = a ```","title":"Circular type hints"},{"location":"Programming/Python/Python%20Manual/#common-type-hints","text":"","title":"Common type hints"},{"location":"Programming/Python/Python%20Manual/#language-types","text":"None Numeric types: int , float , bool str Collection types: list , or List[<type>] , tuple , or Tuple[<type>, ...] , set , or Set[<type>] , dict , or Dict[<key type>, <value type>] , Iterables: Iterable[<type>] - any iterable Sequence[<type>] - iterable with random access ( [] operator) Any - any type Union[<type>, ...] - any of the specified types Optional[<type>] - the specified type or None Callable[[<arg type>, ...], <return type>] - a function with specified arguments and return type","title":"Language types"},{"location":"Programming/Python/Python%20Manual/#pandas-types","text":"Pandas does not provide type hints. We can use the types itself, but this is only partially useful. We can use Series as a hint, but we cannot specify the inner type (e.g.: Series[int] ). For that, we can use a wrapper library called pandera : from pandera.typing import Series a: Series[int] Officiall documentation for Pandera data types","title":"Pandas types"},{"location":"Programming/Python/Python%20Manual/#calling-external-programs","text":"To call an external program, we use the subprocess module. Most of the time, we use the run function: import subprocess subprocess.run(['ls', '-l']) Important parameters: check : if set to True , the function raises an exception if the return code is not 0. Default is False . text , or universal_newlines : if set to True , the function returns the output as a string. Default is None . env : A dictionary with the environment variables. Note that the default environment obtained from the parent process is not extended, but replaced by the provided dictionary. Therefore, if we want to extend the environment, we have to initialize the dictionary with the parent environment: Python env = os.environ.copy() However, the subprocess.run method has some limitations. Notably, it cannot both capture and stream the output. To achieve this, and some other advanced features, we have to use the subprocess.Popen class.","title":"Calling external programs"},{"location":"Programming/Python/Python%20Manual/#subprocesspopen","text":"The subprocess.Popen class provides more control over the process. The basic usage is: p = subprocess.Popen(['ls', '-l']) # start the process # now we can communicate with the process, stream the output, etc. p.wait() # wait for the process to finish # now we can get the return code, continue with the code, etc.","title":"subprocess.Popen"},{"location":"Programming/Python/Python%20Manual/#loading-resources","text":"Resources can be loaded using the importlib.resources module. This way, we can handle files but also resources stored in an archive. The basic usage is: import importlib.resources file = importlib.resources.files('package').joinpath('file.txt') # send the file to the function expecting a file-like object my_function(importlib.resources.as_file(file))","title":"Loading resources"},{"location":"Programming/Python/Python%20Manual/#numpy","text":"","title":"Numpy"},{"location":"Programming/Python/Python%20Manual/#data-types","text":"Documentation of basic data types","title":"Data types"},{"location":"Programming/Python/Python%20Manual/#date-and-time_1","text":"Documentation Numpy use the datetime64 data type for date and time. This type has its internal resolution, which can be anyting from nanoseconds to years. The resolution is displayed in the type name, e.g., datetime64[ns] and it is determined automatically from the input data if dtype is not specified or specified as datetime64 or by the dtype parameter if specified as datetime64[<resolution>] .","title":"Date and time"},{"location":"Programming/Python/Python%20Manual/#initialization","text":"We can create the new array as: zero-filled: np.zeros(<shape>, <dtype>) ones-filled: np.ones(<shape>, <dtype>) empty: np.empty(<shape>, <dtype>) filled with a constant: np.full(<shape>, <value>, <dtype>)","title":"Initialization"},{"location":"Programming/Python/Python%20Manual/#sorting_1","text":"for sorting, we use the sort function. There is no way how to set the sorting order, we have to use a trick for that: a = np.array([1, 2, 3, 4, 5]) a[::-1].sort() # sort in reverse order","title":"Sorting"},{"location":"Programming/Python/Python%20Manual/#io_1","text":"","title":"I/O"},{"location":"Programming/Python/Python%20Manual/#export-to-csv","text":"To export the numpy array to CSV, we can use the savetxt function: np.savetxt('file.csv', a, delimiter=',') By default, the function saves values in the mathematical float format, even if the values are integers. To save the values as integers, we can use the fmt parameter: np.savetxt('file.csv', a, delimiter=',', fmt='%i')","title":"Export to CSV"},{"location":"Programming/Python/Python%20Manual/#hdf5_1","text":"To work with HDF5 files in Python, we use the h5py library. To import a dataset from an HDF5 file to a numpy array: import h5py with h5py.File('file.h5', 'r') as f: a = np.array(f['dataset']) To export a numpy array to an HDF5 file: with h5py.File('file.h5', 'w') as f: f.create_dataset('dataset', data=<numpy array>)","title":"HDF5"},{"location":"Programming/Python/Python%20Manual/#usefull-array-properties","text":"size : number of array items unlike len, it counts all items in the mutli-dimensional array itemsize : memory (bytes) needed to store one item in the array nbytes : array size in bytes. Should be equal to size * itemsize .","title":"Usefull array properties:"},{"location":"Programming/Python/Python%20Manual/#masking","text":"A usefull technique in Numpy is masking: getting an array of boolean that indicate satisfaction of a condition. This array can be used to filter the elements of the original array (or any other array of the same shape). Example: a = np.array([1, 2, 3, 4, 5]) mask = a > 3 # mask is [False, False, False, True, True] b = a[mask] # b is [4, 5] For more complex masking, we can use the logical operators: & : logical and | : logical or ^ : logical xor ~ : logical not","title":"Masking"},{"location":"Programming/Python/Python%20Manual/#aggregate-masks","text":"Even more powerful technique is to use aggregate masks that are computed for a whole array or a single dimension. We create such a mask simply by applying an aggregation function to the mask. Example: a = np.array([[2, 3, 4], [5, 6, 7]]) mask = a < 3 # mask is [[True, True, False], [False, False, False]] column_mask = mask.any(axis=0) # column_mask is [True, True, False] row_mask = mask.any(axis=1) # row_mask is [True, False] Of course, we can use other aggregation functions than any , such as all , sum , mean , etc.","title":"Aggregate masks"},{"location":"Programming/Python/Python%20Manual/#dedicated-mask-functions","text":"Apart from logical operations, we can use dedicated mask functions: nonzero : returns the indices of the non-zero elements","title":"Dedicated mask functions"},{"location":"Programming/Python/Python%20Manual/#useful-functions","text":"unique : returns the unique elements of the array. Important parameters: return_counts : if set to True , the function returns the counts of the unique elements (same as value_counts in pandas)","title":"Useful functions"},{"location":"Programming/Python/Python%20Manual/#regular-expressions","text":"In Python, the regex patterns are not compiled by default. Therefore we can use strings to store them. The basic syntax for regex search is: result = re.search(<pattern>, <string>) if result: # pattern matches group = result.group(<group index>) # print the first group The 0th group is the whole match, as usual. To substitute the matched pattern, we can use the sub function: pattern = re.compile(r'(\\d+)') result = pattern.sub(r'[\\1]', '123') # '[123]' Sometimes, we need to use numbers around the group index. In such cases, we have to use the \\<group index><<number>> notation: pattern = re.compile(r'(\\d+)') result = pattern.sub(r'\\g<1>2025', '123') # '1232025'","title":"Regular expressions"},{"location":"Programming/Python/Python%20Manual/#lambda-functions","text":"Lambda functions in python have the following syntax: lambda <input parameters>: <return value> Example: f = lambda x: x**2 Only a single expression can be used in the lambda function, so we need standard functions for more complex logic (temporary variables, loops, etc.).","title":"Lambda functions"},{"location":"Programming/Python/Python%20Manual/#decorators","text":"Decorators are a special type of function that can be used to modify other functions. When we write an annotation with the name of a function above another function, the annotated function is decorated . It means that when we call the annotated function, a wrapper function is called instead. The wrapper function is the function returned by the decorater : the function with the same name as the annotation. If we want to also keep the original function functionality, we have to pass the function to the decorator and call it inside the wrapper function. In the following example, we create a dummy decorator that keeps the original function functionality: Example: def decorator(func): def wrapper(): result = func() return result return wrapper @decorator def my_func(): # do something return result","title":"Decorators"},{"location":"Programming/Python/Python%20Manual/#decorator-with-arguments","text":"If the original function has arguments, we have to pass them to the wrapper function. Example: def decorator(func): def wrapper(param_1, param_2): result = func(param_1, param_2) return result return wrapper @decorator def my_func(param_1, param_2): # do something return result","title":"Decorator with arguments"},{"location":"Programming/Python/Python%20Manual/#singletons","text":"SO question There are several ways how to implement a singleton in Python. The most common are: using a module-level variable using the __new__ method in combination with a base class use a metaclass using a decorator","title":"Singletons"},{"location":"Programming/Python/Python%20Manual/#module-level-variable","text":"The most simple way is to use a module-level variable. Note taht if the singleton has to be initialized from the outside, the initialization has to be done in a singleton method, not in the constructor! class Singleton: def __init__(self): self.initialized = False def init(self, init_param): if not self.initialized: self.initialized = True # do the initialization singleton = Singleton() # initialization singleton.init(init_param)","title":"Module-level variable"},{"location":"Programming/Python/Python%20Manual/#testing-with-pytest","text":"Pytest is a simple testing framework for Python. It uses the assert statement for testing. The tests are defined in functions with the test_ prefix.","title":"Testing with pytest"},{"location":"Programming/Python/Python%20Manual/#fixtures","text":"Fixtures are used to set up the environment for more than one test. If defined in the conftest.py file, they are available for all tests in the project. Fixtures are defined using the @pytest.fixture decorator. The fixture can be used in the test function by passing the fixture name as an argument. The fixture has the following structure: @pytest.fixture def my_fixture(): # code for setting up the environment yield # the test is executed here # clean up code","title":"Fixtures"},{"location":"Programming/Python/Python%20Manual/#mocking","text":"For mocking, we can use the pytest-mock package. After installation, we can use the mocker fixture in any test function.","title":"Mocking"},{"location":"Programming/Python/Python%20Manual/#capturing-output","text":"Documentation To capture the output, we can use the capsys fixture: def test_output(capsys): print('hello') captured = capsys.readouterr() assert captured.out == 'hello\\n' Similarly, we can inspect the standard error output using the captured.err attribute.","title":"Capturing output"},{"location":"Programming/Python/Python%20Manual/#jupyter","text":"","title":"Jupyter"},{"location":"Programming/Python/Python%20Manual/#memory","text":"Most of the time, when the memory allocated by the notebook is larger than expected, it is caused by some library objects (plots, tables...]). However sometimes, it can be forgotten user objects. To list all user objects, from the largest: # These are the usual ipython objects, including this one you are creating ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars'] # Get a sorted list of the objects and their sizes sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)","title":"Memory"},{"location":"Programming/Python/Python%20Manual/#reloding-modules-with-autoreload","text":"When modules are imported they are not reloaded unless the kernel is restarted. In python scripts, this does not matter, we just execute the script again. However, when working with notebooks , it may be inconvenient to reload the kernel and all necessary cells just because of a small change in the imported module. Instead, we can use the autoreload extension . First, we have to load the extension: %load_ext autoreload Then, we configure the autoreload with %autoreload <mode> . The most common modes are: now (default): reload all modules immediately (if not excluded by the %aimport magic) This is useful especially if the automatic reloading does not work as expected. 0 , off : disable autoreload 1 , explicit : reload modules that were imported using the %aimport magic every time before executing the Python code 2 , all : reload all modules (except those excluded by %aimport ) every time before executing the Python code 3 , complete : same as 2 , but also add any new objects in the module","title":"Reloding modules with autoreload"},{"location":"Programming/Python/Python%20Manual/#plotting","text":"There are several libraries for plotting in Python. The most common are: matplotlib plotly In the table below, we can see a comparison of the most common plotting libraries: Functionality Matplotlib Plotly real 3D plots no yes detail legend styling (padding, round corners...) yes no dash patterns for lines with different axes ranges yes no","title":"Plotting"},{"location":"Programming/Python/Python%20Manual/#matplotlib","text":"Official Manual","title":"Matplotlib"},{"location":"Programming/Python/Python%20Manual/#saving-figures","text":"To save a figure, we can use the savefig function. The savefig function has to be called before the show function, otherwise the figure will be empty .","title":"Saving figures"},{"location":"Programming/Python/Python%20Manual/#docstrings","text":"For documenting Python code, we use docstrings, special comments soroudned by three quotation marks: \"\"\" docstring \"\"\" Unlike in other languages, there are multiple styles for docstring content. The most common are: Epytext Python \"\"\" @param <param name>: <param description> @return: <return description> \"\"\" Google Python \"\"\" Args: <param name>: <param description> Returns: <return description> \"\"\" Numpy Python \"\"\" Parameters ---------- <param name> : <param type> <param description> Returns ------- <return type> <return description> \"\"\" reStructuredText Python \"\"\" :param <param name>: <param description> :return: <return description> \"\"\"","title":"Docstrings"},{"location":"Programming/Python/Python%20Manual/#progress-bars","text":"For displaying progress bars, we can use the tqdm library. It is very simple to use: from tqdm import tqdm for i in tqdm(range(100)): ... Important parameters: desc : description of the progress bar","title":"Progress bars"},{"location":"Programming/Python/Python%20Manual/#custom-progress-updates","text":"When the progress is more complex (conditional updates, etc.), we can use the progress bar object to update the progress manually: with tqdm(total=<total number of steps>) as progress_bar: if(<condition>): progress_bar.update(<number of steps>)","title":"Custom progress updates"},{"location":"Programming/Python/Python%20Manual/#tqdm-in-jupyter","text":"When using tqdm in Jupyter, the basic progress bar may not work (it may print other logs repeatedly). In such cases, we can change the import to: from tqdm.notebook import tqdm If the code can be called both from Jupyter and from console, we can use `autonotebook","title":"TQDM in Jupyter"},{"location":"Programming/Python/Python%20Manual/#postgresql","text":"When working with PostgreSQL databases, we usually use either the psycopg2 adapter or, the sqlalchemy .","title":"PostgreSQL"},{"location":"Programming/Python/Python%20Manual/#psycopg2","text":"documentation To connect to a database: con = psycopg2.connect(<connection string>) After running this code a new session is created in the database, this session is handeled by the con object. The operation to the database is then done as follows: create a cursor object which represents a database transaction Python cur = con.cursor() execute any number of SQL commands Python cur.execute(<sql>) commit the transaction Python con.commit()","title":"psycopg2"},{"location":"Programming/Python/Python%20Manual/#sqlalchemy","text":"Connection documentation SQLAlchemy works with engine objects that represent the application's connection to the database. The engine object is created using the create_engine function: from sqlalchemy import create_engine engine = create_engine('postgresql://user:password@localhost:5432/dbname') A simple SELECT query can be executed using using the following code: with engine.connect() as conneciton: result = conneciton.execute(\"SELECT * FROM table\") ... With modifying statements, the situation is more complicated as SQLAlchemy uses transactions by default. Therefore we need to commit the transaction. There are two ways how to do that: using the commit method of the connection object Python with engine.connect() as conneciton: conneciton.execute(\"INSERT INTO table VALUES (1, 2, 3)\") conneciton.commit() creating a new block for the transaction using the begin method of the connection object Python with engine.connect() as conneciton: with conneciton.begin(): conneciton.execute(\"INSERT INTO table VALUES (1, 2, 3)\") this option has also its shortcut: the begin method of the engine object Python with engine.begin() as conneciton: conneciton.execute(\"INSERT INTO table VALUES (1, 2, 3)\") Note that the old execute method of the engine object is not available anymore in newer versions of SQLAlchemy.","title":"SQLAlchemy"},{"location":"Programming/Python/Python%20Manual/#statements-with-parameters","text":"Sometimes it is desirable to use parameters in the SQL statements: it prevents SQL injection in case of user input, the provided parameters are automatically escaped, so we don't have to worry : in the SQL statement. The syntax is: connection.execute(\"INSERT INTO table VALUES (:param1, :param2)\", param1=1, param2=2) # or connection.execute(\"INSERT INTO table VALUES (:param1, :param2)\", {'param1': 1, 'param2': 2})","title":"Statements with parameters"},{"location":"Programming/Python/Python%20Manual/#executing-statements-without-transaction","text":"By default, sqlalchemy executes sql statements in a transaction. However, some statements (e.g., CREATE DATABASE ) cannot be executed in a transaction. To execute such statements, we have to use the execution_options method: with sqlalchemy_engine.connect() as conn: conn.execution_options(isolation_level=\"AUTOCOMMIT\") conn.execute(\"<sql>\") conn.commit()","title":"Executing statements without transaction"},{"location":"Programming/Python/Python%20Manual/#getting-the-affected-rowcount","text":"The result object returned by the execute method has the rowcount attribute that contains the number of affected rows.","title":"Getting the affected rowcount"},{"location":"Programming/Python/Python%20Manual/#executing-multiple-statements-at-once","text":"To execute multiple statements at once, for example when executing a script, it is best to use the execute method of the psycopg2 connection object. Moreover, to safely handle errors, it is best to catch the exceptions and manually rollback the transaction in case of an error: conn = psycopg2.connect(<connection string>) cursor = conn.cursor() try: cursor.execute(<sql>) conn.commit() except Exception as e: conn.rollback() raise e finally: cursor.close() conn.close()","title":"Executing multiple statements at once"},{"location":"Programming/Python/Python%20Manual/#working-with-gis","text":"When working with gis data, we usually change the pandas library for its GIS extension called geopandas . For more, see the pandas manual.","title":"Working with GIS"},{"location":"Programming/Python/Python%20Manual/#geocoding","text":"For geocoding, we can use the Geocoder library.","title":"Geocoding"},{"location":"Programming/Python/Python%20Manual/#complex-data-structures","text":"","title":"Complex data structures"},{"location":"Programming/Python/Python%20Manual/#kdtree","text":"documentation KDTree can be found in the scipy library.","title":"KDTree"},{"location":"Programming/Python/Python%20Manual/#geometry","text":"There are various libraries for working with geometry in Python: scipy.spatial : for basic geometry operations shapely geopandas : for gis data. See the pandas manual for more details.","title":"Geometry"},{"location":"Programming/Python/Python%20Manual/#shapely","text":"homepage To merge a multi-linestring into a single linestring, we can use the line_merge function.","title":"Shapely"},{"location":"Programming/Python/Python%20Manual/#downloading-files","text":"To download files from the internet, we can use the requests library. The basic usage is: import requests response = requests.get(<url>) with open(<filename>, 'wb') as f: f.write(response.content)","title":"Downloading files"},{"location":"Programming/Python/Python%20Manual/#graphs-with-networkx-and-osmnx","text":"NetworkX documentation OSMnx documentation NetworkX is a library for working with graphs. OSMnx is a library that creates networkx graphs from OpenStreetMap data.","title":"Graphs with NetworkX and OSMnx"},{"location":"Programming/Python/Python%20Manual/#creating-a-graph-from-openstreetmap","text":"To create a graph from OpenStreetMap, we can use the ox.graph_from_place function. Limitations The are we want to must be some official name, and we have to know the name The elements are selected either by nodes inside the area, or edges, but not ways. If we need keeping the ways intact (if they cross the area boundary), we need to use a different approach. Example: import osmnx as ox graph = ox.graph_from_place(<place name>, network_type=<network type>) The network_type parameter can be one of the following: 'drive' : driving network 'walk' : walking network 'bike' : cycling network And some more. Other important parameters are: simplify : whether to contract the graph. Default is True . retain_all : whether to retain all nodes, or just the largest strongly connected component. Default is False . truncate_by_edge : If True, retain nodes outside the area if they have at least one neighbor inside the area. Default is False . To add speed information to the graph, we can use the OSMnx.add_edge_speeds function: graph = ox.add_edge_speeds(graph) Similarily, we can add travel times: graph = ox.add_edge_travel_times(graph)","title":"Creating a graph from OpenStreetMap"},{"location":"Programming/Python/Python%20Manual/#caching","text":"OSMnx automatically caches the overpass API responses to the cache directory, each response as a separate json file.","title":"Caching"},{"location":"Programming/Python/Python%20Manual/#importing-osm-elements","text":"We can also import individual OSM elements (nodes, ways, relations) from OpenStreetMap using the ox.features_from_place function. This function returns a geodataframe with the features. Limitations: there is no way how to download related features. Ways, for example, are downloaded with full geometry, but the associated nodes are not downloaded.","title":"Importing OSM elements"},{"location":"Programming/Python/Python%20Manual/#creating-a-graph-from-pandas-dataframe","text":"To create a graph from a pandas dataframe, we can use the NetworkX.from_pandas_edgelist function: graph = nx.from_pandas_edgelist(df, source='source', target='target', edge_attr=['weight', 'speed', 'travel_time'], create_using=nx.DiGraph())","title":"Creating a graph from pandas dataframe"},{"location":"Programming/Python/Python%20Manual/#exporting-graph-to-pandas-dataframe","text":"To export a graph to a pandas dataframe, we have multiple options: osmnx.graph_to_gdfs : exports the graph to a pandas dataframe. Important parameters: edges : if True , the edges are exported. Default is True . nodes : if True , the nodes are exported. Default is True . node_geometry : if True , the node geometries are exported. Default is True . fill_edge_geometry : if True , the edge geometries are exported. Default is True . NetworkX.to_pandas_edgelist : exports the graph to a pandas dataframe. note that htis method does not preserve the edge order (unlike the osmnx.graph_to_gdfs function) another option is to use the nodes() and edges() methods of the graph object and manually create the dataframe.","title":"Exporting graph to pandas dataframe"},{"location":"Programming/Python/Python%20Manual/#reindexing-the-nodes","text":"Sometimes, we want to discard the node indices created from the input data and use a new index that has a range of (0, n-1). We can do this by using the NetworkX.convert_node_labels_to_integers function: graph = nx.convert_node_labels_to_integers(graph)","title":"Reindexing the nodes"},{"location":"Programming/Python/Python%20Workflow/","text":"Dev Stack \u00b6 I use the following stack: the latest Python, 64 bit pip as the package manager Pycharm IDE pytest test suite Visual Studio for deugging native code Python \u00b6 Python should be installed from the official web page , not using any package manager. Steps: Dowload the 64-bit installer Run the installer, choose advanced install Include the GUI tools (Tkinter, TK) Includ *.py launcher, but only if there is no newer python version installed. If this is checked and there is a newer vesrsion of Python installed, the setup will fail. Include documentation Check download debug symbols to enable native code debugging The source code for python can be inspected on GitHub Command line \u00b6 We execute python scripts from the command line as: python <path to the .py file> . Parameters: -m executes a module as a script, e.g. python -m venv . This is useful for executing scripts whithout knowing the path to the script. Pip \u00b6 Installing Packages \u00b6 Normal packages are installed using: pip install <package name> . However, if a package uses a C/C++ backend and does not contain the compiled wheel on PyPy, this approach will fail on Windows. Instead, you have to download the wheel from the Chris Gohlke page and install it: pip install <path to wheel> . Also, you have to install the dependencies mentioned on that page. Uninstalling packages \u00b6 To uninstall a package, use pip uninstall <package name> . There is no way how to uninstall more packages using some wildcard. To uninstall more packages efficiently (not one by one): create a file with the list of all installed packages: pip freeze > packages.txt edit the file and remove all packages you want to keep uninstall all packages from the file: pip uninstall -r packages.txt -y Troubleshooting package installation \u00b6 If the installation fails, check the following: if you installed the package by name, check for the wheel on the Chris Golthke page. if you installed the package from a wheel, check the notes/requirement info on Chris Golthke page Check the log. Specifically, no building should appear there whatsoever. If a build starts, it means that some dependency that should be installed as a prebuild wheel is missing. Possible reasons: you forget to install the dependency, go back to step 2 the dependency version does not correspond with the version required by the package you are installing. Check the log for the required version. Upgrading pip \u00b6 To upgrade pip, use python -m pip install --upgrade pip . Sometimes, this command end ith an error. There can be specific solutions to this, but what always seems to fix the pip is the get-pip script . Download the script and run it using python get-pip.py . Local packages \u00b6 A useful method how to develop and test packages is to have them installed locally. This way, each change in the source code is immediately reflected in the package and also in the code that uses the package. To install a package locally, use pip install -e <path to the package> . Note that the package needs to be properly initialized first, i.e.: at least one __init__.py file in the package root (optionally others in subpackages) a setup.py file in the parent directory of the package root IDE \u00b6 Pycharm \u00b6 Configuration \u00b6 Settings synchronization \u00b6 Same as in IDEA: Log in into JetBrains Toolbox or to the App Click on the gear icon on the top-right and choose Sync Check all categories and click on pull settings from the cloud Do Not Run scripts in Python Console by Default \u00b6 Run configuration select box -> Edit Configurations... -> Edit configuration templates -> Python -> uncheck the Run with Python Console Enable Progress Bars in output console \u00b6 Run configuration select box -> Edit Configurations... -> Select the configuration -> check the Emulate terminal in output console Setup the Docstring format \u00b6 In Tools -> Python Integrated Tools -> Docstring format . Project Configuration \u00b6 configure the correct test suite in File -> Settings -> Tools -> Python Integrated Tools -> testing Known problems & solutions \u00b6 Non deterministic output in the run window \u00b6 Problem: It can happen that the output printing/logging can be reordered randomly (not matching the order of calls in the source, neither the system console output). Solution: Edit Configurations... -> select configuration for the script -> check Emulate terminal in output console . Pycharm does not recognize a locally installed package \u00b6 It can happen that a locally installed package ( -e ) is not recognized by Pycharm. If that happens, first try to invalidate the cache by File -> Invalidate Caches... check Clear file system cache and Local History click Invalidate and Restart If this does not work, it can be solved by adding the path to the package to the interpreter paths: File -> Settings -> Python -> Interpreter Click on the arrow next to the interpreter name and choose Show All... Click on the desired interpreter and click on the filetree icon on the top of the window Add the path to the package to the list of paths No problems found in the code of a single project \u00b6 This can be due to a broken Idea settings. To fix: Close Pycharm Move the .idea folder from the project directory to a backup location Start Pycharm and open the project again VS Code \u00b6 Formatting \u00b6 For formatting, we can use the black formatter. To install it: pip install black install the black extension in VS Code Code Analysis \u00b6 The code analysis in VS Code is provided by the language server. The language server can be configured the settings is python.languageServer . By default, the language server is set to Pylance . Cursor \u00b6 In this section we provide the differences between Cursor and VS Code workflow, and also the limitations of Cursor for Python development. Limitations \u00b6 The main limitation is the static analysis tool. The cursor uses a built-in static analysis tool that is a fork of the basedpyright [source] . This tool have some limitations compared to Pylance or the code analysis available in Pycharm, and cannot be replaced (the classical VS Code setting python.languageServer is reverted to None on restart). A typical example is an object returned from a function. In Cursor, this object, when type hints are missing, is declared as Any , and its members cannot be accessed, searched or renamed. Project Structure \u00b6 A typical project structure is: <project root>/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 <package name> | \u251c\u2500\u2500 __init__.py | \u251c\u2500\u2500 <module files> | \u251c\u2500\u2500 resources/ | | \u251c\u2500\u2500 <resource files for main module> | \u251c\u2500\u2500 <submodule>/ | \u251c\u2500\u2500 __init__.py | \u251c\u2500\u2500 <submodule files> | \u251c\u2500\u2500 resources/ | \u251c\u2500\u2500 <resource files for submodule> \u251c\u2500\u2500 tests/ \u2502 \u251c\u2500\u2500 <test files> | \u251c\u2500\u2500 resources/ | \u251c\u2500\u2500 <resource files for tests> \u251c\u2500\u2500 docs/ | \u251c\u2500\u2500<documentation files> \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 README.md Sometimes, we skip the src directory and put the package directly into the project root. This is typical for projects with a single package. Jupyter \u00b6 Jupyter can be used both in Pycharm and in a web browser. Jupyter in Pycharm \u00b6 The key for the effectivity of Jupyter in Pycharm is using the command mode . To enter the command mode, press Esc . To enter the edit mode, pres enter. Command mode shortcuts \u00b6 m : change cell type to markdown Ctrl + C : copy cell Ctrl + V : paste cell Ctrl + Shift + Up : move cell up Ctrl + Shift + Down : move cell down Text mode shortcuts: Ctrl + Shift + - : split cell on cursor position Web Browser Configuration \u00b6 Install Extension Manager with basic extensions \u00b6 Best use the official installation guide . The extensions then can be toggled on in the Nbextensions tab in the jupyter homepage. Be sure to unselect the disable configuration for nbextensions without explicit compatibility (they may break your notebook environment, but can be useful to show for nbextension development) checkbox, otherwise, all extensions will be disabled. Jupyter in VS Code \u00b6 Documentation To move a cell just use drag and drop, on the left side of the cell. Debugging \u00b6 Pycharm contains a good debuger for python code. However, it cannot step into most standard library functions, as those are native, implemented in C/C++. For that, we need mixed python/native debugging. Testing \u00b6 Pycharm Configuration \u00b6 By default, all exceptions are handled by test frameworks, and therefore, the debugger does not stop on them. To stop on exceptions in test, we need to edit the breakpoint configuration -> Activation policy : check On raise check Ignore library files Pytest \u00b6 To run pytest, simply go to the folder and run pytest . Arguments: -x : stop on first failure For mocking, we need to install the pytest-mock package, otherwise we get an error fixture 'mocker' not found . Mixed Python-native debugging \u00b6 In theory, there are two ways how to debug python native code: use a tool that can step from python to C++ (only Visual Studio offers this possibility AFAIK) use two debuggers, start with a python debugger, and attach a native debugger to the python process. This way, the debuggers can be independent. However, one needs to put a breakpoint in the native code, and for that, we need to know the location of the code that will be executed from python (non-trivial for standard library) Python-native debugger in Visual Studio \u00b6 First check and install the requirements: Python 3.5-3.9, including debugging symbols Python 3.10 is not compatible yet in Visual Studio, a Python development component needs to be installed, including Python native development tools Then, configure the Visual studio: Go to Tools -> Options -> Python -> Debugging and check: Enable debugging of the Python standard library . Finally, create a new Python project (either clean, or from existing code) and configure it: use the interpreter directly, do not create a virtual environment enable native code debugging: Go to project properties -> Debug Check enable native code debugging use the -i flag to always see the debug console, even if the program ends without breaking Go to project properties -> Debug Add the -i flag to the Interpreter Arguments field Known issues \u00b6 The reference assemblies for .NETFramework,Version=v4.7.2 were not found. -> Install this component using the visual studio installer Other Sources \u00b6 Microsoft official documentation Python tools for Visual Studio GitHub page Releasing libraries to PyPi \u00b6 Steps \u00b6 licensing check that setup.py contains all requirements: pipreqs release update the min version in dependencies Release \u00b6 raise version in setup.py run sdist in Pycharm: Tools -> Run setup.py task -> sdist upload to pypi: twine upload dist/*","title":"Python Workflow"},{"location":"Programming/Python/Python%20Workflow/#dev-stack","text":"I use the following stack: the latest Python, 64 bit pip as the package manager Pycharm IDE pytest test suite Visual Studio for deugging native code","title":"Dev Stack"},{"location":"Programming/Python/Python%20Workflow/#python","text":"Python should be installed from the official web page , not using any package manager. Steps: Dowload the 64-bit installer Run the installer, choose advanced install Include the GUI tools (Tkinter, TK) Includ *.py launcher, but only if there is no newer python version installed. If this is checked and there is a newer vesrsion of Python installed, the setup will fail. Include documentation Check download debug symbols to enable native code debugging The source code for python can be inspected on GitHub","title":"Python"},{"location":"Programming/Python/Python%20Workflow/#command-line","text":"We execute python scripts from the command line as: python <path to the .py file> . Parameters: -m executes a module as a script, e.g. python -m venv . This is useful for executing scripts whithout knowing the path to the script.","title":"Command line"},{"location":"Programming/Python/Python%20Workflow/#pip","text":"","title":"Pip"},{"location":"Programming/Python/Python%20Workflow/#installing-packages","text":"Normal packages are installed using: pip install <package name> . However, if a package uses a C/C++ backend and does not contain the compiled wheel on PyPy, this approach will fail on Windows. Instead, you have to download the wheel from the Chris Gohlke page and install it: pip install <path to wheel> . Also, you have to install the dependencies mentioned on that page.","title":"Installing Packages"},{"location":"Programming/Python/Python%20Workflow/#uninstalling-packages","text":"To uninstall a package, use pip uninstall <package name> . There is no way how to uninstall more packages using some wildcard. To uninstall more packages efficiently (not one by one): create a file with the list of all installed packages: pip freeze > packages.txt edit the file and remove all packages you want to keep uninstall all packages from the file: pip uninstall -r packages.txt -y","title":"Uninstalling packages"},{"location":"Programming/Python/Python%20Workflow/#troubleshooting-package-installation","text":"If the installation fails, check the following: if you installed the package by name, check for the wheel on the Chris Golthke page. if you installed the package from a wheel, check the notes/requirement info on Chris Golthke page Check the log. Specifically, no building should appear there whatsoever. If a build starts, it means that some dependency that should be installed as a prebuild wheel is missing. Possible reasons: you forget to install the dependency, go back to step 2 the dependency version does not correspond with the version required by the package you are installing. Check the log for the required version.","title":"Troubleshooting package installation"},{"location":"Programming/Python/Python%20Workflow/#upgrading-pip","text":"To upgrade pip, use python -m pip install --upgrade pip . Sometimes, this command end ith an error. There can be specific solutions to this, but what always seems to fix the pip is the get-pip script . Download the script and run it using python get-pip.py .","title":"Upgrading pip"},{"location":"Programming/Python/Python%20Workflow/#local-packages","text":"A useful method how to develop and test packages is to have them installed locally. This way, each change in the source code is immediately reflected in the package and also in the code that uses the package. To install a package locally, use pip install -e <path to the package> . Note that the package needs to be properly initialized first, i.e.: at least one __init__.py file in the package root (optionally others in subpackages) a setup.py file in the parent directory of the package root","title":"Local packages"},{"location":"Programming/Python/Python%20Workflow/#ide","text":"","title":"IDE"},{"location":"Programming/Python/Python%20Workflow/#pycharm","text":"","title":"Pycharm"},{"location":"Programming/Python/Python%20Workflow/#configuration","text":"","title":"Configuration"},{"location":"Programming/Python/Python%20Workflow/#settings-synchronization","text":"Same as in IDEA: Log in into JetBrains Toolbox or to the App Click on the gear icon on the top-right and choose Sync Check all categories and click on pull settings from the cloud","title":"Settings synchronization"},{"location":"Programming/Python/Python%20Workflow/#do-not-run-scripts-in-python-console-by-default","text":"Run configuration select box -> Edit Configurations... -> Edit configuration templates -> Python -> uncheck the Run with Python Console","title":"Do Not Run scripts in Python Console by Default"},{"location":"Programming/Python/Python%20Workflow/#enable-progress-bars-in-output-console","text":"Run configuration select box -> Edit Configurations... -> Select the configuration -> check the Emulate terminal in output console","title":"Enable Progress Bars in output console"},{"location":"Programming/Python/Python%20Workflow/#setup-the-docstring-format","text":"In Tools -> Python Integrated Tools -> Docstring format .","title":"Setup the Docstring format"},{"location":"Programming/Python/Python%20Workflow/#project-configuration","text":"configure the correct test suite in File -> Settings -> Tools -> Python Integrated Tools -> testing","title":"Project Configuration"},{"location":"Programming/Python/Python%20Workflow/#known-problems-solutions","text":"","title":"Known problems &amp; solutions"},{"location":"Programming/Python/Python%20Workflow/#non-deterministic-output-in-the-run-window","text":"Problem: It can happen that the output printing/logging can be reordered randomly (not matching the order of calls in the source, neither the system console output). Solution: Edit Configurations... -> select configuration for the script -> check Emulate terminal in output console .","title":"Non deterministic output in the run window"},{"location":"Programming/Python/Python%20Workflow/#pycharm-does-not-recognize-a-locally-installed-package","text":"It can happen that a locally installed package ( -e ) is not recognized by Pycharm. If that happens, first try to invalidate the cache by File -> Invalidate Caches... check Clear file system cache and Local History click Invalidate and Restart If this does not work, it can be solved by adding the path to the package to the interpreter paths: File -> Settings -> Python -> Interpreter Click on the arrow next to the interpreter name and choose Show All... Click on the desired interpreter and click on the filetree icon on the top of the window Add the path to the package to the list of paths","title":"Pycharm does not recognize a locally installed package"},{"location":"Programming/Python/Python%20Workflow/#no-problems-found-in-the-code-of-a-single-project","text":"This can be due to a broken Idea settings. To fix: Close Pycharm Move the .idea folder from the project directory to a backup location Start Pycharm and open the project again","title":"No problems found in the code of a single project"},{"location":"Programming/Python/Python%20Workflow/#vs-code","text":"","title":"VS Code"},{"location":"Programming/Python/Python%20Workflow/#formatting","text":"For formatting, we can use the black formatter. To install it: pip install black install the black extension in VS Code","title":"Formatting"},{"location":"Programming/Python/Python%20Workflow/#code-analysis","text":"The code analysis in VS Code is provided by the language server. The language server can be configured the settings is python.languageServer . By default, the language server is set to Pylance .","title":"Code Analysis"},{"location":"Programming/Python/Python%20Workflow/#cursor","text":"In this section we provide the differences between Cursor and VS Code workflow, and also the limitations of Cursor for Python development.","title":"Cursor"},{"location":"Programming/Python/Python%20Workflow/#limitations","text":"The main limitation is the static analysis tool. The cursor uses a built-in static analysis tool that is a fork of the basedpyright [source] . This tool have some limitations compared to Pylance or the code analysis available in Pycharm, and cannot be replaced (the classical VS Code setting python.languageServer is reverted to None on restart). A typical example is an object returned from a function. In Cursor, this object, when type hints are missing, is declared as Any , and its members cannot be accessed, searched or renamed.","title":"Limitations"},{"location":"Programming/Python/Python%20Workflow/#project-structure","text":"A typical project structure is: <project root>/ \u251c\u2500\u2500 src/ \u2502 \u251c\u2500\u2500 <package name> | \u251c\u2500\u2500 __init__.py | \u251c\u2500\u2500 <module files> | \u251c\u2500\u2500 resources/ | | \u251c\u2500\u2500 <resource files for main module> | \u251c\u2500\u2500 <submodule>/ | \u251c\u2500\u2500 __init__.py | \u251c\u2500\u2500 <submodule files> | \u251c\u2500\u2500 resources/ | \u251c\u2500\u2500 <resource files for submodule> \u251c\u2500\u2500 tests/ \u2502 \u251c\u2500\u2500 <test files> | \u251c\u2500\u2500 resources/ | \u251c\u2500\u2500 <resource files for tests> \u251c\u2500\u2500 docs/ | \u251c\u2500\u2500<documentation files> \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 README.md Sometimes, we skip the src directory and put the package directly into the project root. This is typical for projects with a single package.","title":"Project Structure"},{"location":"Programming/Python/Python%20Workflow/#jupyter","text":"Jupyter can be used both in Pycharm and in a web browser.","title":"Jupyter"},{"location":"Programming/Python/Python%20Workflow/#jupyter-in-pycharm","text":"The key for the effectivity of Jupyter in Pycharm is using the command mode . To enter the command mode, press Esc . To enter the edit mode, pres enter.","title":"Jupyter in Pycharm"},{"location":"Programming/Python/Python%20Workflow/#command-mode-shortcuts","text":"m : change cell type to markdown Ctrl + C : copy cell Ctrl + V : paste cell Ctrl + Shift + Up : move cell up Ctrl + Shift + Down : move cell down Text mode shortcuts: Ctrl + Shift + - : split cell on cursor position","title":"Command mode shortcuts"},{"location":"Programming/Python/Python%20Workflow/#web-browser-configuration","text":"","title":"Web Browser Configuration"},{"location":"Programming/Python/Python%20Workflow/#install-extension-manager-with-basic-extensions","text":"Best use the official installation guide . The extensions then can be toggled on in the Nbextensions tab in the jupyter homepage. Be sure to unselect the disable configuration for nbextensions without explicit compatibility (they may break your notebook environment, but can be useful to show for nbextension development) checkbox, otherwise, all extensions will be disabled.","title":"Install Extension Manager with basic extensions"},{"location":"Programming/Python/Python%20Workflow/#jupyter-in-vs-code","text":"Documentation To move a cell just use drag and drop, on the left side of the cell.","title":"Jupyter in VS Code"},{"location":"Programming/Python/Python%20Workflow/#debugging","text":"Pycharm contains a good debuger for python code. However, it cannot step into most standard library functions, as those are native, implemented in C/C++. For that, we need mixed python/native debugging.","title":"Debugging"},{"location":"Programming/Python/Python%20Workflow/#testing","text":"","title":"Testing"},{"location":"Programming/Python/Python%20Workflow/#pycharm-configuration","text":"By default, all exceptions are handled by test frameworks, and therefore, the debugger does not stop on them. To stop on exceptions in test, we need to edit the breakpoint configuration -> Activation policy : check On raise check Ignore library files","title":"Pycharm Configuration"},{"location":"Programming/Python/Python%20Workflow/#pytest","text":"To run pytest, simply go to the folder and run pytest . Arguments: -x : stop on first failure For mocking, we need to install the pytest-mock package, otherwise we get an error fixture 'mocker' not found .","title":"Pytest"},{"location":"Programming/Python/Python%20Workflow/#mixed-python-native-debugging","text":"In theory, there are two ways how to debug python native code: use a tool that can step from python to C++ (only Visual Studio offers this possibility AFAIK) use two debuggers, start with a python debugger, and attach a native debugger to the python process. This way, the debuggers can be independent. However, one needs to put a breakpoint in the native code, and for that, we need to know the location of the code that will be executed from python (non-trivial for standard library)","title":"Mixed Python-native debugging"},{"location":"Programming/Python/Python%20Workflow/#python-native-debugger-in-visual-studio","text":"First check and install the requirements: Python 3.5-3.9, including debugging symbols Python 3.10 is not compatible yet in Visual Studio, a Python development component needs to be installed, including Python native development tools Then, configure the Visual studio: Go to Tools -> Options -> Python -> Debugging and check: Enable debugging of the Python standard library . Finally, create a new Python project (either clean, or from existing code) and configure it: use the interpreter directly, do not create a virtual environment enable native code debugging: Go to project properties -> Debug Check enable native code debugging use the -i flag to always see the debug console, even if the program ends without breaking Go to project properties -> Debug Add the -i flag to the Interpreter Arguments field","title":"Python-native debugger in Visual Studio"},{"location":"Programming/Python/Python%20Workflow/#known-issues","text":"The reference assemblies for .NETFramework,Version=v4.7.2 were not found. -> Install this component using the visual studio installer","title":"Known issues"},{"location":"Programming/Python/Python%20Workflow/#other-sources","text":"Microsoft official documentation Python tools for Visual Studio GitHub page","title":"Other Sources"},{"location":"Programming/Python/Python%20Workflow/#releasing-libraries-to-pypi","text":"","title":"Releasing libraries to PyPi"},{"location":"Programming/Python/Python%20Workflow/#steps","text":"licensing check that setup.py contains all requirements: pipreqs release update the min version in dependencies","title":"Steps"},{"location":"Programming/Python/Python%20Workflow/#release","text":"raise version in setup.py run sdist in Pycharm: Tools -> Run setup.py task -> sdist upload to pypi: twine upload dist/*","title":"Release"},{"location":"Programming/Web/CSS%20Manual/","text":"Selectors \u00b6 There are many CSS selectors: HTML element: e.g. div , span , p , a , img , etc. ID: #<id> Class: .<class> Attribute: [<attribute>] Any element: * Combining selectors \u00b6 Selectors are so powerful because we can combine them: <selector 1> <selector 2> : select elements that match <selector 2> inside <selector 1> <selector 1><selector 2> : select elements that match <selector 1> and <selector 2> div.class : select <div> with class class <selector 1> > <selector 2> : select elements that match <selector 2> as a direct child of <selector 1> Layout options \u00b6 Grid Layout \u00b6 For complex pages. See the tutorial . Flex Layout \u00b6 For simpler pages. See the tutorial . Note that setting max_width: 100% for child elements of flex items does not work frequenly, so it's better to specify max_witdth ( SO ). Oldschool Layout \u00b6 Oldschool layout use floats. Very Oldschool Layout \u00b6 With tables...","title":"CSS Manual"},{"location":"Programming/Web/CSS%20Manual/#selectors","text":"There are many CSS selectors: HTML element: e.g. div , span , p , a , img , etc. ID: #<id> Class: .<class> Attribute: [<attribute>] Any element: *","title":"Selectors"},{"location":"Programming/Web/CSS%20Manual/#combining-selectors","text":"Selectors are so powerful because we can combine them: <selector 1> <selector 2> : select elements that match <selector 2> inside <selector 1> <selector 1><selector 2> : select elements that match <selector 1> and <selector 2> div.class : select <div> with class class <selector 1> > <selector 2> : select elements that match <selector 2> as a direct child of <selector 1>","title":"Combining selectors"},{"location":"Programming/Web/CSS%20Manual/#layout-options","text":"","title":"Layout options"},{"location":"Programming/Web/CSS%20Manual/#grid-layout","text":"For complex pages. See the tutorial .","title":"Grid Layout"},{"location":"Programming/Web/CSS%20Manual/#flex-layout","text":"For simpler pages. See the tutorial . Note that setting max_width: 100% for child elements of flex items does not work frequenly, so it's better to specify max_witdth ( SO ).","title":"Flex Layout"},{"location":"Programming/Web/CSS%20Manual/#oldschool-layout","text":"Oldschool layout use floats.","title":"Oldschool Layout"},{"location":"Programming/Web/CSS%20Manual/#very-oldschool-layout","text":"With tables...","title":"Very Oldschool Layout"},{"location":"Programming/Web/Google%20API%20and%20Apps%20Script/","text":"Comments \u00b6 API reference Anchored Comments \u00b6 According to my experiments and SO . It is not possible to add achore comments to google docs. Also, it is not possible to get any readable anchors from existing comments. The only way to extract comments with anchors is to export the document as Microsoft Word. Written with StackEdit .","title":"Google API and Apps Script"},{"location":"Programming/Web/Google%20API%20and%20Apps%20Script/#comments","text":"API reference","title":"Comments"},{"location":"Programming/Web/Google%20API%20and%20Apps%20Script/#anchored-comments","text":"According to my experiments and SO . It is not possible to add achore comments to google docs. Also, it is not possible to get any readable anchors from existing comments. The only way to extract comments with anchors is to export the document as Microsoft Word. Written with StackEdit .","title":"Anchored Comments"},{"location":"Programming/Web/JavaScript/","text":"","title":"JavaScript"},{"location":"Programming/Web/Jekyll/","text":"Local testing \u00b6 Install Ruby Download dependencies using bundle command Run bundle exec jekyll serve to start local server Directory structure \u00b6 Jekyll has a predefined directory structure. The important directories are: _posts : Contains all the posts. The file name should be in the format YYYY-MM-DD-title.md . _pages : Contains all the pages. The file name should be in the format title.md . _config.yml : Contains the configuration of the site. Configuration \u00b6 The configuration can be set in _config.yml file. Impotant options are: Markdown \u00b6 Code blocks \u00b6 Markdown code blocks are supported by Jekyll. The syntax is: ```language code \\``` Note that for Jekyll, the language name is case sensitive . For example, java is correct, but Java is not. This is in contrast to GitHub markdown, where the language name is case insensitive. Plugins \u00b6 Jekyll functionality can be extended using plugins. Plugins are Ruby gems, which should be added to the Gemfile and also to the _config.yml file. Note that GitHub Pages supports only a limited number of plugins . The list of supported plugins can be found here .","title":"Jekyll"},{"location":"Programming/Web/Jekyll/#local-testing","text":"Install Ruby Download dependencies using bundle command Run bundle exec jekyll serve to start local server","title":"Local testing"},{"location":"Programming/Web/Jekyll/#directory-structure","text":"Jekyll has a predefined directory structure. The important directories are: _posts : Contains all the posts. The file name should be in the format YYYY-MM-DD-title.md . _pages : Contains all the pages. The file name should be in the format title.md . _config.yml : Contains the configuration of the site.","title":"Directory structure"},{"location":"Programming/Web/Jekyll/#configuration","text":"The configuration can be set in _config.yml file. Impotant options are:","title":"Configuration"},{"location":"Programming/Web/Jekyll/#markdown","text":"","title":"Markdown"},{"location":"Programming/Web/Jekyll/#code-blocks","text":"Markdown code blocks are supported by Jekyll. The syntax is: ```language code \\``` Note that for Jekyll, the language name is case sensitive . For example, java is correct, but Java is not. This is in contrast to GitHub markdown, where the language name is case insensitive.","title":"Code blocks"},{"location":"Programming/Web/Jekyll/#plugins","text":"Jekyll functionality can be extended using plugins. Plugins are Ruby gems, which should be added to the Gemfile and also to the _config.yml file. Note that GitHub Pages supports only a limited number of plugins . The list of supported plugins can be found here .","title":"Plugins"},{"location":"Programming/Web/Selenium/","text":"Introduction \u00b6 Selenium is a browser automation tool that allows you to automate browser actions such as clicking, typing, and scrolling. Basic usage: from selenium import webdriver driver = webdriver.Chrome() driver.get(\"https://www.google.com\") Selecting elements \u00b6 We can select any element on the page using the find_element(<method>, <selector>) method. The method can be: By.ID : DOM ID By.NAME : DOM name By.XPATH : XPath query Waiting \u00b6 Frequently, we need to wait for an element to be present on the page or active. We can do this using the WebDriverWait class. from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC wait = WebDriverWait(driver, 10) element = wait.until(EC.presence_of_element_located((By.ID, \"id\"))) The expected condition can be: presence_of_element_located : the element is present on the page element_to_be_clickable : the element is clickable Interacting with elements \u00b6 There are several methods to interact with, each described in its own section. Clicking \u00b6 We can click on buttons, links, but also inputs to activate them. To do this: element = driver.find_element(By.ID, \"id\") element.click() Sometimes, we may encounter the ElementClickInterceptedException error with a message like Element <element> is not clickable at point (x, y). Other element would receive the click: <element> . This happens when the element is not visible or not clickable. The solution depends on the case. If the element is overlapped by another element, the solution is to click on the parent element instead. Typing \u00b6 We can type text into inputs using the send_keys method: element = driver.find_element(By.ID, \"id\") element.send_keys(\"text\") However, this may fail to pass some special characters (e.g. @ , ^ , etc.). To overcome this, we can use the clipboard to copy and paste the text: import pyperclip from selenium.webdriver.common.keys import Keys ... pyperclip.copy(\"text ^^^\") element.send_keys(Keys.CONTROL + \"v\") Downloading files \u00b6 First it is useful to configure the browser to download files to a specific directory and not ask for confirmation: options = uc.ChromeOptions() ... download_prefs = { \"download.default_directory\": str(download_dir), # save here \"download.prompt_for_download\": False, # no Save As dialog \"download.directory_upgrade\": True, # use existing folder \"safebrowsing.enabled\": True # bypass safe browsing check } options.add_experimental_option(\"prefs\", download_prefs) driver = uc.Chrome( options=options, ... ) Next, we can just click any download link and the download will start. Next problem can be finding out whether the download is complete. One strategy is to wait till no changes are detected in the download directory: stable_since = time.time() files_before = list(download_dir.glob(\"*.*\")) while time.time() - stable_since < 1: # needs 1s of unchanging size files_after = list(download_dir.glob(\"*.*\")) changed = True if len(files_after) == len(files_before): for file_before, file_after in zip(files_before, files_after): if file_before.name != file_after.name: break if file_before.stat().st_size != file_after.stat().st_size: break changed = False if changed: files_before = files_after stable_since = time.time() time.sleep(0.2) Login \u00b6 Login is one of the most complicated things to automate, due to several layers of security and the fact that the login page usually serves as the guard point against automation. Using Undetected Chrome Driver to prevent automation detection \u00b6 Sometimes, login through chrome testing driver is detected as automation and the login fails. In this case, we can try to use the Undetected Chrome Driver to prevent detection. To use it, we need to: import the driver: Python import undetected_chromedriver as uc pass the path to the chrome browser to driver options: Python options = uc.ChromeOptions() # note the different options class! options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" use the new driver and pass the options to it: Python driver = uc.Chrome(options=options) on Windows, we also need to use the use_subprocess argument to avoid an error: Python driver = uc.Chrome(options=options, use_subprocess=True) Storing cookies to avoid re-login \u00b6 We can store the cookies so that we do not have to re-login every time. Example: ... cookies_file = Path(\"cookies.pkl\") if cookies_file.is_file(): cookies = pickle.load(cookies_file.open(\"rb\")) # go to the right domain first driver.get(\"https://www.example.com\") # add the cookies for cookie in cookies: driver.add_cookie(cookie) # go to the desired page driver.get(\"https://www.example.com/desired_page\") else: # normal login here ... # after login, store the cookies pickle.dump(driver.get_cookies(), open(cookies_file, \"wb\"))","title":"Selenium"},{"location":"Programming/Web/Selenium/#introduction","text":"Selenium is a browser automation tool that allows you to automate browser actions such as clicking, typing, and scrolling. Basic usage: from selenium import webdriver driver = webdriver.Chrome() driver.get(\"https://www.google.com\")","title":"Introduction"},{"location":"Programming/Web/Selenium/#selecting-elements","text":"We can select any element on the page using the find_element(<method>, <selector>) method. The method can be: By.ID : DOM ID By.NAME : DOM name By.XPATH : XPath query","title":"Selecting elements"},{"location":"Programming/Web/Selenium/#waiting","text":"Frequently, we need to wait for an element to be present on the page or active. We can do this using the WebDriverWait class. from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC wait = WebDriverWait(driver, 10) element = wait.until(EC.presence_of_element_located((By.ID, \"id\"))) The expected condition can be: presence_of_element_located : the element is present on the page element_to_be_clickable : the element is clickable","title":"Waiting"},{"location":"Programming/Web/Selenium/#interacting-with-elements","text":"There are several methods to interact with, each described in its own section.","title":"Interacting with elements"},{"location":"Programming/Web/Selenium/#clicking","text":"We can click on buttons, links, but also inputs to activate them. To do this: element = driver.find_element(By.ID, \"id\") element.click() Sometimes, we may encounter the ElementClickInterceptedException error with a message like Element <element> is not clickable at point (x, y). Other element would receive the click: <element> . This happens when the element is not visible or not clickable. The solution depends on the case. If the element is overlapped by another element, the solution is to click on the parent element instead.","title":"Clicking"},{"location":"Programming/Web/Selenium/#typing","text":"We can type text into inputs using the send_keys method: element = driver.find_element(By.ID, \"id\") element.send_keys(\"text\") However, this may fail to pass some special characters (e.g. @ , ^ , etc.). To overcome this, we can use the clipboard to copy and paste the text: import pyperclip from selenium.webdriver.common.keys import Keys ... pyperclip.copy(\"text ^^^\") element.send_keys(Keys.CONTROL + \"v\")","title":"Typing"},{"location":"Programming/Web/Selenium/#downloading-files","text":"First it is useful to configure the browser to download files to a specific directory and not ask for confirmation: options = uc.ChromeOptions() ... download_prefs = { \"download.default_directory\": str(download_dir), # save here \"download.prompt_for_download\": False, # no Save As dialog \"download.directory_upgrade\": True, # use existing folder \"safebrowsing.enabled\": True # bypass safe browsing check } options.add_experimental_option(\"prefs\", download_prefs) driver = uc.Chrome( options=options, ... ) Next, we can just click any download link and the download will start. Next problem can be finding out whether the download is complete. One strategy is to wait till no changes are detected in the download directory: stable_since = time.time() files_before = list(download_dir.glob(\"*.*\")) while time.time() - stable_since < 1: # needs 1s of unchanging size files_after = list(download_dir.glob(\"*.*\")) changed = True if len(files_after) == len(files_before): for file_before, file_after in zip(files_before, files_after): if file_before.name != file_after.name: break if file_before.stat().st_size != file_after.stat().st_size: break changed = False if changed: files_before = files_after stable_since = time.time() time.sleep(0.2)","title":"Downloading files"},{"location":"Programming/Web/Selenium/#login","text":"Login is one of the most complicated things to automate, due to several layers of security and the fact that the login page usually serves as the guard point against automation.","title":"Login"},{"location":"Programming/Web/Selenium/#using-undetected-chrome-driver-to-prevent-automation-detection","text":"Sometimes, login through chrome testing driver is detected as automation and the login fails. In this case, we can try to use the Undetected Chrome Driver to prevent detection. To use it, we need to: import the driver: Python import undetected_chromedriver as uc pass the path to the chrome browser to driver options: Python options = uc.ChromeOptions() # note the different options class! options.binary_location = r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\" use the new driver and pass the options to it: Python driver = uc.Chrome(options=options) on Windows, we also need to use the use_subprocess argument to avoid an error: Python driver = uc.Chrome(options=options, use_subprocess=True)","title":"Using Undetected Chrome Driver to prevent automation detection"},{"location":"Programming/Web/Selenium/#storing-cookies-to-avoid-re-login","text":"We can store the cookies so that we do not have to re-login every time. Example: ... cookies_file = Path(\"cookies.pkl\") if cookies_file.is_file(): cookies = pickle.load(cookies_file.open(\"rb\")) # go to the right domain first driver.get(\"https://www.example.com\") # add the cookies for cookie in cookies: driver.add_cookie(cookie) # go to the desired page driver.get(\"https://www.example.com/desired_page\") else: # normal login here ... # after login, store the cookies pickle.dump(driver.get_cookies(), open(cookies_file, \"wb\"))","title":"Storing cookies to avoid re-login"},{"location":"Programming/Web/css/","text":"Selectors \u00b6 basic selectors are: element : e.g., p selects all paragraphs class : e.g., .intro selects all elements with class intro id : e.g., #first selects the element with id first (ids are unique) These basic selectors can be combined to form complex selectors the combination operators are: inside ( <space> ): e.g., div p selects all paragraphs inside a div adjacent ( + ): e.g., div + p selects the first paragraph after a div child ( > ): e.g., div > p selects all paragraphs that are direct children of a div parent ( < ): e.g., div < p selects all divs that are direct parents of a paragraph contains ( :has() ) : e.g., div:has(p) selects all divs that contain a paragraph","title":"css"},{"location":"Programming/Web/css/#selectors","text":"basic selectors are: element : e.g., p selects all paragraphs class : e.g., .intro selects all elements with class intro id : e.g., #first selects the element with id first (ids are unique) These basic selectors can be combined to form complex selectors the combination operators are: inside ( <space> ): e.g., div p selects all paragraphs inside a div adjacent ( + ): e.g., div + p selects the first paragraph after a div child ( > ): e.g., div > p selects all paragraphs that are direct children of a div parent ( < ): e.g., div < p selects all divs that are direct parents of a paragraph contains ( :has() ) : e.g., div:has(p) selects all divs that contain a paragraph","title":"Selectors"},{"location":"Programming/Web/htaccess/","text":"kakpsatweb","title":"htaccess"},{"location":"Programming/Web/mkdocs/","text":"https://www.mkdocs.org/ update steps: generate navigation: `python .\\generate_nav.py generate site: mkdocs build deploy: mkdocs gh-deploy Known limitations \u00b6 MkDocs use the Python-Markdown parser which does not follow the CommonMark specification. Therefore, some markdown elements may not work as expected: lists has to be separated by an empty line, otherwise they will be rendered as simple text. Troubleshooting \u00b6 Left sidebar overlaps the main content \u00b6 This can be caused by too long words in the left sidebar. To fix this, find the problematic header and split the long word.","title":"mkdocs"},{"location":"Programming/Web/mkdocs/#known-limitations","text":"MkDocs use the Python-Markdown parser which does not follow the CommonMark specification. Therefore, some markdown elements may not work as expected: lists has to be separated by an empty line, otherwise they will be rendered as simple text.","title":"Known limitations"},{"location":"Programming/Web/mkdocs/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Programming/Web/mkdocs/#left-sidebar-overlaps-the-main-content","text":"This can be caused by too long words in the left sidebar. To fix this, find the problematic header and split the long word.","title":"Left sidebar overlaps the main content"},{"location":"Windows/Comand%20shell/","text":"Introduction \u00b6 wiki cmd.exe Windows Commands Overview and Reference Wikibook Command shell is the basic shell in Windows and one of the two preinstalled shells (the other is PowerShell). The commands available in the Command shell are refered to as Windows commands . As the PowerShell was designed as an extension of the Command shell, the Windows commands can be executed in PowerShell . Script files for the Command shell are often called batch files , and have the .bat or .cmd extension. Comments \u00b6 Instead of some conventional comment syntax, the Command shell uses the rem command. Example: rem This is a comment. We print Hello, World! below it. echo Hello, World! echo : Displaying text and automatic command printing \u00b6 The echo command serve two important purposes: displaying text, like in PowerShell and switching on/off the automatic command printing feature of the Command shell. The automatic command printing basically prints every command that is executed. We can skip the printing by prefixing the command with @ . Example: echo Hello, World! # prints echo Hello, World!\\nHello, World! @echo Hello, World! # prints Hello, World! However, typically, we want to disable the automatic command printing for the whole script, this can be done by adding call echo with parameter off . To turn it back on, we can call echo with parameter on . So the typical first command in a batch file is: @echo off Which disables the automatic command printing and also skips it for the disabeling command itself. Executable Execution \u00b6 Executable path resolution \u00b6 Old reference for Windows NT Unfortunatelly, there is no up-to-date reference for Command Shell execution logic. However, using the information from the old reference, the execution logic is as follows: If the specified executable is a path, the path is evaluated. If the path is valid, the executable is executed. Otherwise, error is raised. If the specified executable is a name, the name is searched in the locations below (in order): Windows Commands Current directory directories listed in the PATH environment variable, in the order they are listed When we mention the executable name, we mean the name of the executable file with the extension. Executables names can be also specified without the extension, but only if the extension is listed in the PATHEXT environment variable. Variables \u00b6 Variables are set using the set command. Example: set myVar=Hello To use a variable , we use its name surrounded by % . Example: echo %myVar% rem prints Hello To display all variables , use the set command without any arguments. Note that unlike in PowerShell, there is no distinction between local and environment variables . For example, to set the value of the PATH environment variable, we can use the set command: set PATH=C:\\Program Files\\Java\\jdk1.8.0_181\\bin;%PATH% The set command and its important nuances \u00b6 official documentation ss64 documentation The variable setting has several surprising nuances with important consequences: all whitespaces are interpreted and not discarded: ```batch set myVar=Hello rem variable 'myVar' is set to 'Hello' set myVar= Hello rem variable 'myVar' is set to ' Hello' set myVar =Hello rem variable 'myVar ' is set to 'Hello' ``` The quoting for set use a different syntax than expected: ```batch set myVar=\"Hello, World!\" rem myVar is set to '\"Hello, World!\"' set \"myVar=Hello, World!\" rem myVar is set to 'Hello, World!' - no quoting is needed for spaces in the value: batch set myVar=Hello World! rem myVar is set to 'Hello World!' ``` String Manipulation \u00b6 Unlike in PowerShell or programming languages, there is no concatenation operator in Command Shell. Instead, we just place the strings/variables next to each other. Example: set myVar=Hello, set myVar=%myVar% World! echo %myVar% # prints \"Hello, World!\" Control Statements \u00b6 if \u00b6 documentation The if statement has the following syntax: if <condition> <then command> [else <command>] There are three types of conditions: <string 1>==<string 2> : most useful, ERRORLEVEL <number> : test the return value of the last command, and exists <file> : test if the file exists Any of the conditions can be negated by using if not instead of if . goto \u00b6 documentation The goto statement has the following syntax: goto <label> The <label> is then prese in the script as :<label> . Example: goto label ... :label echo Hello, World! Output redirection \u00b6 unofficial documentation The | operator is used to redirect the standard output of a command to the input of another command as usual. The > is used to redirect the standard output of a command to a file. By prependind 2 ( 2> ), we redirect the standard error output instead. When changing > to >> , we write to the file in the append mode. Using < we can redirect a file to the standard input of a command. Finally, 2>&1 redirects the standard error output to the standard output. This can be combined with the output to file: <command> > <file> 2>&1 writes both the standard output and the standard error output to the file. As any <file> we can use NUL which basically discards the output. Command Separation \u00b6 We can execute multiple commands in the same line. We can use <command 1> & <command 2> : commands are executed independently, or <command 1> && <command 2> : <command 2> is executed only if <command 1> exits with code 0. Batch Script Arguments \u00b6 wikibook ss64 documentation When we call commands from a batch script, we can refer to the called arguments called replacement parameters . These are %0 (the name of the script), %1 -- %9 (the arguments of the script). To refer all script arguments ( %1 -- %9 ), we can use %* . Contrary to a typical programming language behavior, the arguments are not split just by spaces but also commas ( , ), semi-colons ( ; ), equal signs ( = ), and by a horizontal tab. Therefore, one has to be careful if some of these characters are used in the arguments. Parameter extension for files \u00b6 If the argument is a file name, we can get several additional information about the file by applying parameters extensions. These extensions are in format ~<extension> where <extension> is a single letter long and are placed between the % and the parameter number. Example: echo %~f1 The above command prints the full path of the file passed as the first argument. Most commonly used extensions are: ~ : remove any surrounding quotes ~f : full path ~d : drive letter ~p : path, without the drive letter and the file name We can also combine some extensions: echo %~dp1 The above command prints the directory of the file passed as the first argument. Usefull Commands \u00b6 Get the path of the executable/command \u00b6 To get the path of the executable/command, we can use the where command. Example: where java If there are more than one executable/command with the same name, all are listed (one per line), from the one to be executed, to the one with the lowest priority. findstr : Select lines containing a pattern \u00b6 The findstr command is used to select lines containing a pattern. It is similar to the grep or Select-String in PowerShell. However, it can only ever select the whole line, not parts of it. The command can either: read from a file: findstr <pattern> <file> , or read from the standard input: <command> | findstr <pattern> . By default, the pattern is interpreted as a regular expression. However, findstr has a plenty of options to configure the matching: /i : ignore case /c:<pattern> : use a literal pattern, instead of a regular expression mklink : Create a symlink \u00b6 The mklink command is used to create a symlink. Example: mklink <source> <target> Limitations \u00b6 The command shell is very limited compared to any other shell that is used in practice. Some of the limitations are: no command for selecting line by a number like head/tail no command for matching substrings, only whole line can be matched","title":"Comand shell"},{"location":"Windows/Comand%20shell/#introduction","text":"wiki cmd.exe Windows Commands Overview and Reference Wikibook Command shell is the basic shell in Windows and one of the two preinstalled shells (the other is PowerShell). The commands available in the Command shell are refered to as Windows commands . As the PowerShell was designed as an extension of the Command shell, the Windows commands can be executed in PowerShell . Script files for the Command shell are often called batch files , and have the .bat or .cmd extension.","title":"Introduction"},{"location":"Windows/Comand%20shell/#comments","text":"Instead of some conventional comment syntax, the Command shell uses the rem command. Example: rem This is a comment. We print Hello, World! below it. echo Hello, World!","title":"Comments"},{"location":"Windows/Comand%20shell/#echo-displaying-text-and-automatic-command-printing","text":"The echo command serve two important purposes: displaying text, like in PowerShell and switching on/off the automatic command printing feature of the Command shell. The automatic command printing basically prints every command that is executed. We can skip the printing by prefixing the command with @ . Example: echo Hello, World! # prints echo Hello, World!\\nHello, World! @echo Hello, World! # prints Hello, World! However, typically, we want to disable the automatic command printing for the whole script, this can be done by adding call echo with parameter off . To turn it back on, we can call echo with parameter on . So the typical first command in a batch file is: @echo off Which disables the automatic command printing and also skips it for the disabeling command itself.","title":"echo: Displaying text and automatic command printing"},{"location":"Windows/Comand%20shell/#executable-execution","text":"","title":"Executable Execution"},{"location":"Windows/Comand%20shell/#executable-path-resolution","text":"Old reference for Windows NT Unfortunatelly, there is no up-to-date reference for Command Shell execution logic. However, using the information from the old reference, the execution logic is as follows: If the specified executable is a path, the path is evaluated. If the path is valid, the executable is executed. Otherwise, error is raised. If the specified executable is a name, the name is searched in the locations below (in order): Windows Commands Current directory directories listed in the PATH environment variable, in the order they are listed When we mention the executable name, we mean the name of the executable file with the extension. Executables names can be also specified without the extension, but only if the extension is listed in the PATHEXT environment variable.","title":"Executable path resolution"},{"location":"Windows/Comand%20shell/#variables","text":"Variables are set using the set command. Example: set myVar=Hello To use a variable , we use its name surrounded by % . Example: echo %myVar% rem prints Hello To display all variables , use the set command without any arguments. Note that unlike in PowerShell, there is no distinction between local and environment variables . For example, to set the value of the PATH environment variable, we can use the set command: set PATH=C:\\Program Files\\Java\\jdk1.8.0_181\\bin;%PATH%","title":"Variables"},{"location":"Windows/Comand%20shell/#the-set-command-and-its-important-nuances","text":"official documentation ss64 documentation The variable setting has several surprising nuances with important consequences: all whitespaces are interpreted and not discarded: ```batch set myVar=Hello rem variable 'myVar' is set to 'Hello' set myVar= Hello rem variable 'myVar' is set to ' Hello' set myVar =Hello rem variable 'myVar ' is set to 'Hello' ``` The quoting for set use a different syntax than expected: ```batch set myVar=\"Hello, World!\" rem myVar is set to '\"Hello, World!\"' set \"myVar=Hello, World!\" rem myVar is set to 'Hello, World!' - no quoting is needed for spaces in the value: batch set myVar=Hello World! rem myVar is set to 'Hello World!' ```","title":"The set command and its important nuances"},{"location":"Windows/Comand%20shell/#string-manipulation","text":"Unlike in PowerShell or programming languages, there is no concatenation operator in Command Shell. Instead, we just place the strings/variables next to each other. Example: set myVar=Hello, set myVar=%myVar% World! echo %myVar% # prints \"Hello, World!\"","title":"String Manipulation"},{"location":"Windows/Comand%20shell/#control-statements","text":"","title":"Control Statements"},{"location":"Windows/Comand%20shell/#if","text":"documentation The if statement has the following syntax: if <condition> <then command> [else <command>] There are three types of conditions: <string 1>==<string 2> : most useful, ERRORLEVEL <number> : test the return value of the last command, and exists <file> : test if the file exists Any of the conditions can be negated by using if not instead of if .","title":"if"},{"location":"Windows/Comand%20shell/#goto","text":"documentation The goto statement has the following syntax: goto <label> The <label> is then prese in the script as :<label> . Example: goto label ... :label echo Hello, World!","title":"goto"},{"location":"Windows/Comand%20shell/#output-redirection","text":"unofficial documentation The | operator is used to redirect the standard output of a command to the input of another command as usual. The > is used to redirect the standard output of a command to a file. By prependind 2 ( 2> ), we redirect the standard error output instead. When changing > to >> , we write to the file in the append mode. Using < we can redirect a file to the standard input of a command. Finally, 2>&1 redirects the standard error output to the standard output. This can be combined with the output to file: <command> > <file> 2>&1 writes both the standard output and the standard error output to the file. As any <file> we can use NUL which basically discards the output.","title":"Output redirection"},{"location":"Windows/Comand%20shell/#command-separation","text":"We can execute multiple commands in the same line. We can use <command 1> & <command 2> : commands are executed independently, or <command 1> && <command 2> : <command 2> is executed only if <command 1> exits with code 0.","title":"Command Separation"},{"location":"Windows/Comand%20shell/#batch-script-arguments","text":"wikibook ss64 documentation When we call commands from a batch script, we can refer to the called arguments called replacement parameters . These are %0 (the name of the script), %1 -- %9 (the arguments of the script). To refer all script arguments ( %1 -- %9 ), we can use %* . Contrary to a typical programming language behavior, the arguments are not split just by spaces but also commas ( , ), semi-colons ( ; ), equal signs ( = ), and by a horizontal tab. Therefore, one has to be careful if some of these characters are used in the arguments.","title":"Batch Script Arguments"},{"location":"Windows/Comand%20shell/#parameter-extension-for-files","text":"If the argument is a file name, we can get several additional information about the file by applying parameters extensions. These extensions are in format ~<extension> where <extension> is a single letter long and are placed between the % and the parameter number. Example: echo %~f1 The above command prints the full path of the file passed as the first argument. Most commonly used extensions are: ~ : remove any surrounding quotes ~f : full path ~d : drive letter ~p : path, without the drive letter and the file name We can also combine some extensions: echo %~dp1 The above command prints the directory of the file passed as the first argument.","title":"Parameter extension for files"},{"location":"Windows/Comand%20shell/#usefull-commands","text":"","title":"Usefull Commands"},{"location":"Windows/Comand%20shell/#get-the-path-of-the-executablecommand","text":"To get the path of the executable/command, we can use the where command. Example: where java If there are more than one executable/command with the same name, all are listed (one per line), from the one to be executed, to the one with the lowest priority.","title":"Get the path of the executable/command"},{"location":"Windows/Comand%20shell/#findstr-select-lines-containing-a-pattern","text":"The findstr command is used to select lines containing a pattern. It is similar to the grep or Select-String in PowerShell. However, it can only ever select the whole line, not parts of it. The command can either: read from a file: findstr <pattern> <file> , or read from the standard input: <command> | findstr <pattern> . By default, the pattern is interpreted as a regular expression. However, findstr has a plenty of options to configure the matching: /i : ignore case /c:<pattern> : use a literal pattern, instead of a regular expression","title":"findstr: Select lines containing a pattern"},{"location":"Windows/Comand%20shell/#mklink-create-a-symlink","text":"The mklink command is used to create a symlink. Example: mklink <source> <target>","title":"mklink: Create a symlink"},{"location":"Windows/Comand%20shell/#limitations","text":"The command shell is very limited compared to any other shell that is used in practice. Some of the limitations are: no command for selecting line by a number like head/tail no command for matching substrings, only whole line can be matched","title":"Limitations"},{"location":"Windows/Excel%20Manual/","text":"Move a column: hold Shift and drag the column to the new location","title":"Excel Manual"},{"location":"Windows/PowerPoint/","text":"Slide design \u00b6 Slide design should not be set separately for each slide but globaly for the whole presentation. This can be done by configuring the theme in the slide master view: View > Master Views > Slide Master . On the left, we can select a type of the slide to edit, while on the right, we edit the style for that style type. On the left, the top slide, larger than the others is the master slide . It represents the default design for the whole theme. Shareing the theme \u00b6 The theme can be exported in the slide master view by clicking: Edit theme > Save current theme . The theme can be imported in the slide master view by clicking: Edit theme > Browse for themes . Edit the master slide \u00b6 The master slide is the default design for the whole theme. We cannot add any content to it (it makes no sense, as this would be added to all slides.) However, we can set up some properties of predefined basic elements here: Title : The title of the slide. Text : The text for different levels of indentation Date Slide number : page numbering. Footer Note that the configuration here applies to all slides. Therefore, if we want to have for example date only on the first slide, we should not configure the date here, but on the first slide itself. To select which of the elements should be configured for all slides, click on Master Layout -> Master Layout and select the elements to be configured. Edit the layout slide \u00b6 Object design \u00b6 Sometimes, it is usefull to copy the design of an object. To do so: select the object Go to Home and click on the Format Painter button. Click on the object to which the design should be copied. If we want multiple objects to have the same design, we use the same procedure, but double-click on the Format Painter button. Symbols \u00b6 arrow: write ==> and press space. Tables \u00b6 Tables are treated specially in PowerPoint. They can be created using the Insert > Table menu. To edit the table style, we have to select it and go to the special Table Design tab. To edit the table layaut, add rows or columns, we have to select the table and go to the Layout tab. Insert table from csv \u00b6 Unfortunately, csv text cannot be pasted directly into a PowerPoint table. However, there is a workaround: Copy the csv text into Excel. Select the cells in Excel and copy them. Paste in the powerpoint slide. This way a new table is created. Text formatting \u00b6 Bullets and numbering \u00b6 There is a limitation in PowerPoint that the bullets and numbering have only one level, no indentation is possible. The exception is the placeholder text, which can have multiple levels. Subscript and superscript \u00b6 First, select the text to adjust, then: subscript: Ctrl + = superscript: Ctrl + Shift + +","title":"PowerPoint"},{"location":"Windows/PowerPoint/#slide-design","text":"Slide design should not be set separately for each slide but globaly for the whole presentation. This can be done by configuring the theme in the slide master view: View > Master Views > Slide Master . On the left, we can select a type of the slide to edit, while on the right, we edit the style for that style type. On the left, the top slide, larger than the others is the master slide . It represents the default design for the whole theme.","title":"Slide design"},{"location":"Windows/PowerPoint/#shareing-the-theme","text":"The theme can be exported in the slide master view by clicking: Edit theme > Save current theme . The theme can be imported in the slide master view by clicking: Edit theme > Browse for themes .","title":"Shareing the theme"},{"location":"Windows/PowerPoint/#edit-the-master-slide","text":"The master slide is the default design for the whole theme. We cannot add any content to it (it makes no sense, as this would be added to all slides.) However, we can set up some properties of predefined basic elements here: Title : The title of the slide. Text : The text for different levels of indentation Date Slide number : page numbering. Footer Note that the configuration here applies to all slides. Therefore, if we want to have for example date only on the first slide, we should not configure the date here, but on the first slide itself. To select which of the elements should be configured for all slides, click on Master Layout -> Master Layout and select the elements to be configured.","title":"Edit the master slide"},{"location":"Windows/PowerPoint/#edit-the-layout-slide","text":"","title":"Edit the layout slide"},{"location":"Windows/PowerPoint/#object-design","text":"Sometimes, it is usefull to copy the design of an object. To do so: select the object Go to Home and click on the Format Painter button. Click on the object to which the design should be copied. If we want multiple objects to have the same design, we use the same procedure, but double-click on the Format Painter button.","title":"Object design"},{"location":"Windows/PowerPoint/#symbols","text":"arrow: write ==> and press space.","title":"Symbols"},{"location":"Windows/PowerPoint/#tables","text":"Tables are treated specially in PowerPoint. They can be created using the Insert > Table menu. To edit the table style, we have to select it and go to the special Table Design tab. To edit the table layaut, add rows or columns, we have to select the table and go to the Layout tab.","title":"Tables"},{"location":"Windows/PowerPoint/#insert-table-from-csv","text":"Unfortunately, csv text cannot be pasted directly into a PowerPoint table. However, there is a workaround: Copy the csv text into Excel. Select the cells in Excel and copy them. Paste in the powerpoint slide. This way a new table is created.","title":"Insert table from csv"},{"location":"Windows/PowerPoint/#text-formatting","text":"","title":"Text formatting"},{"location":"Windows/PowerPoint/#bullets-and-numbering","text":"There is a limitation in PowerPoint that the bullets and numbering have only one level, no indentation is possible. The exception is the placeholder text, which can have multiple levels.","title":"Bullets and numbering"},{"location":"Windows/PowerPoint/#subscript-and-superscript","text":"First, select the text to adjust, then: subscript: Ctrl + = superscript: Ctrl + Shift + +","title":"Subscript and superscript"},{"location":"Windows/Powershell%20Manual/","text":"Introduction \u00b6 PowerShell is the new command line interface for Windows that replaces the old command prompt. It is superior in almost every aspect so it is recommended to use it instead of the old command prompt. The PowerShell script files have the .ps1 extension. In addition to system commands, PowerShell can also execute native PowerShell commands called cmdlets . New PowerShell \u00b6 The PowerShell Integrated in Windows is version 5. You can recognize it by the iconical blue background color. This old version has some important limitations (e. g. it cannot pass arguments containing arguments with spaces ). Therefore, it is best to install the new PowerShell first. Quick Edit / Insert Mode \u00b6 PowerShell enables copy/pase of commands. The downside is that every time you click inside PowerShell, the execution (if PowerShell is currently executing somethig) stops. To resume the execution, hit enter . Script Blocks: scripts, functions, and standalone blocks \u00b6 documentation A basic unit of execution in PowerShell is a script block . A script block can be: a piece of code enclosed in curly braces {} , a function, a script Resources \u00b6 Microsoft documentation SS64 pwsh documentation (arguments of pwsh.exe , etc) Reading script block arguments \u00b6 Each script block can be called with parameters. The arguments are first parsed by PowerShell and then passed to the script block. See Parameter parsing for details. Note that this parsing happens even if the script was invoked from outside PowerShell. We have two ways how to read the arguments passed to the script: using the $args variable: simple old syntax, deprecated using the parameter block ( param() ): complex, but more powerful Here, we describe the first option. For the second option, check the Parameter blocks section . To read the arguments, we just access the $args variable . The following example prints the second and third argument: echo $args[1] echo $args[2] Parametr parsing \u00b6 documentation In PowerShell, all arguments are named. However, the name may be set as non-required, in which case we can omit it. The syntax is: -<name> <value> # or -<name>:<value> Thic creates an inherent incompatibility with most command line tools, as : is a valid argument separator in PowerShell. Therefore, any argument passed to a PowerShell script block that contains : has to be quoted . This is a known limitation of PowerShell [ source ]. Parameter blocks \u00b6 defined using the param keyword. Example: $myFunction = { param($param1, $param2) # do something } # or in a function function MyFunction { param($param1, $param2) # do something } # or in a script param($param1, $param2) Parameter blocks \u00b6 The parameter block defines the parameters of the script block. By default ( param() ), only the build in parameters are available. In the parameter block, individual parameters are divided by commas. Example: $myFunction = { param($param1, $param2) # do something } Parameters can be typed . Example: $myFunction = { param([int]$param1, [string]$param2, [switch]$param3) # do something } The switch type is a boolean parameter that does not require a value. It is set to True if the parameter is present. Parameters can be mandatory . Example: $myFunction = { param( [Parameter(Mandatory)][int]$param1, [Parameter(Mandatory)][string]$param2, [switch]$param3 ) # do something } We can also validate the parameters. Example: $myFunction = { param( [Parameter(Mandatory)][ValidateRange(0, 100)][int]$param1, [Parameter(Mandatory)][ValidateSet(\"a\", \"b\", \"c\")][string]$param2, [Parameter(Mandatory)][ValidateScript({$_ -eq \"a\" -or $_ -eq \"b\"})][string]$param3 ) # do something } Also with custom error message: $myFunction = { param( [Parameter(Mandatory)][ValidateScript({ Test-Path $_ }, ErrorMessage=\"Path does not exists: {0}\")][string]$path ) # do something } The advanced usage of parameters is described in the documentation . Command parsing \u00b6 documentation Quoting \u00b6 documentation In PowerShell, there are two types of quoting: \" (double quotes): for expandable strings. These strings can contain variables and expressions that are evaluated. non-basic variables need to be wrapped by expression $() , e.g. $($PSVersionTable.PSVersion) To separate the variable from the following text, use ${} . Example: ${myVar}Text to escape (not evaluate) the $ sign, use the backtick: echo \"`$myVar\" prints $myVar ' (single quotes): for literal strings. These strings are not evaluated. Note that PowerShell consumes the first level of quoting, so that \"a b\" is passed as a b and the same goes for 'a b' . If we need to preserve the quoting, we need to use a second level of quoting: echo \"a b\" # prints a b echo 'a b' # prints a b echo \"a 'b'\" # prints a 'b' echo 'a \"b\"' # prints a \"b\" In case we need the same quoting for both levels, we can escape the first level of quoting with the backtick: \\\" . Example: echo \"a \\\"b\\\"\" # prints a \"b\" Arguments starting with - and containing . \u00b6 If a program argument starts with - , and contains . it needs to be wrapped by ' . Otherwise, the argument will be split on the dot. Example: mvn exec:java -Dexec.mainClass=com.example.MyExample -Dfile.encoding=UTF-8 In Powershell, this needs to be converted to: mvn exec:java '-Dexec.mainClass=com.example.MyExample' '-Dfile.encoding=UTF-8' The problem may arise if the argument or its part needs to be quotted as well. Then: for first level of quoting, use \" (double quotes) for second level of quoting, use '' (two single quotes) Example: mvn exec:exec '-Dexec.executable=\"java\"' '-Dexec.args=\"-Xmx30g -Djava.library.path=''C:\\Program Files\\HDF_Group\\HDF5\\1.14.3\\bin'' -classpath %classpath cz.cvut.fel.aic.simod.OnDemandVehiclesSimulation\"' Escaping \" and ' in Arguments \u00b6 Double quotes \" contained in arguments can be preserved by escaping with backslash: \\\" . Example for that can be passing an argument list to some executable: 'args=\\\"arg1 arg2\\\"' Single quotes ' are esceped by duble single quote: '' . Example can be passing a list of args, where some of them contains space: 'args=\\\"''arg1 with space'' arg2\\\"' Executable Execution \u00b6 There are several ways how to run an executable in PowerShell: The standard way is to just type the command. Another option is the & operator: & <command> <arguments> . This way, we can execute a program with spaces in the path : \"C:/Program Files/a/a\" does not work, but & \"C:/Program Files/a/a\" does prepare the command or arguments dynamically: PowerShell $command = 'ls' & $command # prints the content of the current directory $command $ # prints ls The third option is to use the Invoke-Expression command. This command executes a string as a command with arguments. Example: PowerShell $argument = \"--version\" Invoke-Expression \"java $argument\" Using the call ( & ) operator \u00b6 documentation ss64 documentation The syntax is & <command> <arguments> . Here: <command> is the command to be executed. If the path to the command contains spaces, it has to be wrapped in quotes. <arguments> are either inline arguments, e.g.: & vcpkg list x64-windows-static array of arguments, e.g.: & vcpkg @(\"list\", \"x64-windows-static\") Note that passing the array of arguments to a cmdlet does not work as expected . Cmdlets uses special named parameters. For example the ls (alias for Get-ChildItem cmdlet) has the -Depth parameter that can have an integer value. However, the cmdlet expects such arguments to be passed as key-value pairs. For inline parameters, the key-value pairs are automatically created from the positionally passed arguments. But this is not the case for the array of arguments. So: & ls -Depth 2 # works & ls @(\"-Depth\", 2) # does not work. Each element of the array is treated as a separate positional argument -> a path is expected Unfortunatelly, there is no nice solution for this, if we want to use the call operator (there is no such problem with Invoke-Expression ). There are two options: pass the arguments as key-value pairs. Example: PowerShell $params = @{\"Depth\" = 2} & ls @params # this unrolls the dictionary into key-value pairs do not mistake the second line with & ls $params . This would pass the dictionary as a single argument. use the code block and pass the arguments inside it. Example: PowerShell & {ls -Depth 2} Using the Invoke-Expression command \u00b6 The Invoke-Expression command executes a string as a command with arguments. Example: $argument = \"--version\" Invoke-Expression \"java $argument\" A big caution is needed when using the Invoke-Expression as its typical usage with double-quoted strings triggers expression evaluation twice , which can lead to unexpected behavior. Example: # we want to print $test echo $test # prints nothing as variable is not defined echo `$test # prints $test standard escape solution Invoke-Expression \"echo `$test\" # prints nothing as variable is not defined. The problem is that the escape character is evaluated during the first string evaluation, and then, during the Invoke-Expression evaluation, there is no escape character. Invoke-Expression 'echo `$test' # prints $test. The single quotes prevent the evaluation of the escape character during the first string evaluation. Print the exit code \u00b6 To print the exit code of the last command, there are two options: $? : returns True if the last command was successful, False otherwise $LastExitCode variable contains the exit code of the last command No Output for EXE file \u00b6 Some errors are unfortunatelly not reported by powershell (e.g. missing dll ). The solution is to run such program in cmd, which reports the error. Variables \u00b6 Variables are defined by the $ sign. Example: $myVar = \"Hello, World!\" To print the variable, just type its name. Example: $myVar Environment variables \u00b6 They are accessed by the $env: prefix. Example: $env:PATH Operations on Variables \u00b6 The variables can be used in expressions. Example: $myVar = 5 $myVar + 3 String Operations \u00b6 Strings can be concatenated using the + operator. Examples: $myVar = \"Hello, \" + \"World!\" # append to a path: $env:PATH += \";C:\\Program Files\\Java\\jdk1.8.0_181\\bin\" # prepend to a path: $env:PATH = \"C:\\Program Files\\Java\\jdk1.8.0_181\\bin;\" + $env:PATH Operators \u00b6 documentation The additive operator \u00b6 The additive operator ( + ) is overloaded for different types: for numeric types, it performs the arithmetic addition for string types, it performs the string concatenation for arrays and hashtables, it performs the array/hashtable concatenation Comparison and Matching Operators \u00b6 help page documentation Equality operators: -eq : equal -ne : not equal -gt : greater than -lt : less than -ge : greater or equal -le : less or equal Matching operators: -match : match Logical Operators \u00b6 documentation -and : logical and -or : logical or -not : logical not Control Structures \u00b6 Conditions \u00b6 if \u00b6 The if statement is used for conditional execution. The syntax is: if ($condition) { # do something } elseif ($anotherCondition) { # do something else } else { # do something else } The if structure is also available as a cmdlet If . Example: If ($condition) { \"True\" } Else { \"False\" } The If cmdlet is also available as an alias if and ? . Loops \u00b6 foreach \u00b6 The foreach cycle iterates over a collection. The syntax is: foreach ($item in $collection) { # do something with $item } The foreach structure is also available as a cmdlet ForEach-Object . In this case, we access the current item using the $_ variable. Example: Get-ChildItem | ForEach-Object { $_.Name } The above command lists the names of all files in the current directory. The alias for the ForEach-Object cmdlet is foreach and % . Inputs and Outputs \u00b6 Inputs \u00b6 To read a file, use the Get-Content cmdlet. Example: Get-Content \"C:\\Users\\user\\file.txt\" Outputs \u00b6 There are many output streams in PowerShell. We can use: Write-Output : for standard output Write-Error : for standard error Write-Warning : for warnings Write-Verbose : for verbose output Write-Debug : for debug output Write-Information : for information output Write-Host : for writing directly to the console. No output stream is used. By default, only the standard and error output streams are displayed. To display the other streams, we have several options: manually set the $VerbosePreference , $DebugPreference , $WarningPreference , $InformationPreference variables to Continue (default is SilentlyContinue ), or make the function or script we are running an Advanced Function or Script and use the -Verbose , -Debug , -WarningAction , -InformationAction parameters Pipes and Redirection \u00b6 pipe documentation redirection documentation Output forwading is done using | (pipe) operator, just like in Linux. For redirecting the output to a file, there are the following operators: > : redirect to file, overwrite if exists >> : redirect to file, append if exists >&1 : redirect to standard output stream When using any of these operators, by default, the standard output stream is redirected. If we want to redirect the standard error stream, we have to prepend 2 to the operator. Example: dir > out.txt # redirect standard output stream to out.txt dir 2> err.txt # redirect standard error stream to err.txt If we want both see the output and redirect it to a file, we can use the Tee-Object command which is the equivalent of the tee command from Linux In new PowerShell, we have even more options: 3> : redirect Warning stream 4> : redirect Verbose stream 5> : redirect Debug stream 6> : redirect Information stream *> : redirect all streams Data Types \u00b6 Powershell uses .NET types for data types. We can encounter nubers, characters strings, or boolean values. Additionally, we can have composite types, like arrays, or objects. Dates \u00b6 The date is represented by the System.DateTime type. To create a date, we can use the Get-Date command. Example: $date = Get-Date # current date $date = Get-Date -Date \"2025-01-01\" # date from a string Dates can be compared using the standard comparison operators. Example: if ($date1 -lt $date2) { ... } Arrays \u00b6 To create an array, use the @() operator: a = @() . To add an element to an array, use the += operator: a += 1 . The same operator can be used to append one array to another: a += @(\"1\", \"bar\") . The arrays can be iterated over using the foreach loop. To join the elements of an array into a space-separated string, we can just use the array in a string context. Example: $a = @(\"foo\", \"bar\") \"$a\" # prints \"foo bar\" String Manipulation \u00b6 Concatenation \u00b6 The concatenation operator is the + operator, see The additive ( + ) operator section . Replace \u00b6 For replacing a substring in a string, we have two options: Replace method -replace operator The Replace method replaces all occurences of a string with another string. The syntax is: $myString.Replace(\"oldString\", \"newString\") The -replace operator uses regular expressions for replacing. The syntax is: $myString -replace \"pattern\", \"newString\" Multiple replacements can be done using chained -replace operators. Example: $myString -replace \"pattern1\", \"newString1\" -replace \"pattern2\", \"newString2\" Match \u00b6 To test if a string matches a regular expression, use the Match method. The syntax is: $myString -match \"pattern\" Select-String \u00b6 The Select-String is the grep equivalent for PowerShell. The alias for the comand is sls . Parameters: -Content <before>[, <after>] : Select also <before> lines before the matched line and <after> lines after the matched line -Pattern : If we want to use a regular expression for searching, not a plain string Selecting the matched string \u00b6 If we use the Select-String with the -Pattern parameter, the matching lines are returned with the matching string highlighted. If we want to get only the matching string, we have to access the Matches.Value property for each line. Example: Select-String -Pattern \"pattern\" | ForEach-Object { $_.Matches.Value } Select-Object \u00b6 The Select-Object selects the specified properties of an object. Example: Get-Process | Select-Object -Property Name, Id File System \u00b6 Listing files \u00b6 Documentation For listing files, we can use the Get-ChildItem command, which has an alias ls . The result will print the files with various properties out of which the Mode needs to be explained: a : archive d : directory h : hidden l : symbolic link r : read-only s : system Symlinks \u00b6 To create a symlink , use the New-Item command with the -ItemType parameter set to SymbolicLink . Example: New-Item -ItemType SymbolicLink -Path <source> -Target <target> To modify a symlink , we use the same command, we just need to add the -Force parameter. Note that administrator privileges are required to create or modify symlinks . Network \u00b6 netstat \u00b6 The netstat is the basic network monitoring program. It displays TCP connections. It originated on Linux, so the usage and parameters are described in the Linux manual. Below, we discuss the differencies in the command interface and behavior of the Windows version of the command. Winndows manual . Lot of kubernetes entries in the log \u00b6 By default, the netstat command translates IPs into names. Unfortunatelly, on Windows, it also uses the information from the hosts file ( C:\\Windows\\System32\\drivers\\etc\\hosts ). This is a problem, because some services, like Docker, can use the hosts file to redirect some adddress to localhost. Therefore, at the end, all localhost entries are named kubernetes. Solution options: use the -n parameter for netstat or remove the redirection to localhost in the hosts file Display executable for connection \u00b6 To display executable for all connection, just use the -b swith. For filtering out only some, you have to use the -Context parameter in the Select-String command, as the executable is printed one line below the connection: netstat -b | sls <search pattern> -Context 0,1 System Information \u00b6 CIM documentation WMI classes There are several interfaces for getting system information in PowerShell: Using the Common Information Model (CIM) Using the Windows Management Instrumentation (WMI) By reading from the registry CIM and WMI \u00b6 Because the CIM and WMI interfaces are very similar, we will discuss them together. The main difference is that the CIM is the newer interface, which is more powerful and more user-friendly. The CIM is also cross-platform, while the WMI is Windows-only. The advantage of the CIM and WMI interfaces is that they are clear and object-oriented. The information can be queried using database-like operations. The disadvantage is that they are slow. The main command for getting system information is: Get-CimInstance : for CIM Get-WmiObject : for WMI For both commands, we need to specify the class of the object we want to get using the -ClassName (CIM) or -Class (WMI) parameter. The classes are the same for both interfaces. Typical classes are: Win32_ComputerSystem : information about the computer Win32_OperatingSystem : information about the operating system Win32_InstalledWin32Program : information about installed programs Win32_Product : information about programs installed using the MSI installer contains more information than Win32_InstalledWin32Program Windows Registry \u00b6 To access the Windows registry, we use the same commands that are used for file system: Get-ItemProperty : to get the value of a registry key Get-ChilItem to get a list of child keys Advanced Script blocks \u00b6 Any script block can be made an advanced script block. Advanced script blocks are run similarly to the compiled cmdlets. To make a script block an advanced script block, we have to use the [CmdletBinding()] attribute at the beginning of the script block. Example: function MyFunction { [CmdletBinding()] # do something } The advanced functions and scripts have the following features: parameters are not available as $args (have to be parsed using the param block) builtin parameters like -Verbose , -Debug , -WarningAction , -InformationAction are parsed automatically support for pipeline input Elevation \u00b6 Some commands may require administrator privileges. Therefore, it is wise to check if the script is running with administrator privileges so that the execution is not interrupted. To check if the script is running with administrator privileges, use the following code: $myWindowsID=[Security.Principal.WindowsIdentity]::GetCurrent() $myWindowsPrincipal=new-object Security.Principal.WindowsPrincipal($myWindowsID) if (!$myWindowsPrincipal.IsInRole([Security.Principal.WindowsBuiltInRole]::Administrator)) { Write-Host \"This script requires elevated privileges. Please run it as an administrator.\" exit } Another solution may be to restart the script with administrator privileges. However, this has several limitations: when the script is run with administrator privileges a new terminal window is opened parameters are not passed automatically to the new script. We can manually pass them using the -ArgumentList parameter. However, this is problematic for non-string parameters, especially in an advanced script block. To restart the script with administrator privileges, use the following code: $argString = $args -join ' ' Start-Process \"pwsh\" -Verb RunAs -ArgumentList \"-noexit -File `\"$PSCommandPath`\" $argString\" exit PowerShell History \u00b6 To iterate over the commands used in the past, we use the up arrow and down arrow keys. To list the history, use the Get-History command. To clear the history, use the Clear-History command: Clear-History -Id <id> : clear the command with the specified id (use Get-History to get the id) Clear-History -Count <count> : clear the last count commands Clear-History -CommandLine <pattern> : clear the commands that match the pattern. The pattern use the simple matching (e.g. * is the wildcard), and have to be wrapped in quotes if it contains spaces. PowerShell Scripts \u00b6 Including other scripts \u00b6 documentation Script including in PowerShell is called dot sourcing . To include a script, use the . operator. Example: . \"C:\\Users\\user\\script.ps1\" To include a script from the current directory , use the . operator: PowerShell . .\\script.ps1 To include a relative to the script directory , use the $PSScriptRoot variable. Example: PowerShell . \"$PSScriptRoot\\script.ps1\" Usefull Commands \u00b6 File Encoding Conversion \u00b6 gc [INPUT PATH] | Out-File -en [ENCODING] [OUTPUT PATH] example: gc \"C:\\AIC data\\Roadmaptools big data test/map-filtered.osm\" | Out-File -en utf8 \"C:\\AIC data\\Roadmaptools big data test/map-filtered-utf8.osm\" NOTE: it is not very fast :) Delete all files with a certain extension \u00b6 ls *.extension -Recurse | foreach {rm $_} to try it, add the -WhatIf parameter to rm Batch rename \u00b6 Example dir . | % { $newName = $_.Name -replace '^DSC_0(.*)', 'DSC_1$1'; rename-item -newname $newName -literalPath $_.Fullname -whatif} Count Lines in large file \u00b6 switch -File FILE { default { ++$count } } Get Help \u00b6 Mirosoft documentation To get help about a command, use the Get-Help (alias man ) command. Example: Get-Help Get-ChildItem If the output is the list of articles, it means that there is no help for the command. Translate alias to command \u00b6 To translate an alias to a command, use the Get-Alias command. Example: Get-Alias ls # returns Get-ChildItem Get the path of the current script \u00b6 To get the path of the current script, use the $PSCommandPath variable. Compute the hash of a file \u00b6 To compute the hash of a file, use the Get-FileHash command. Example: Get-FileHash \"C:\\Users\\user\\file.txt\" Important parameters: -Algorithm : the algorithm used for hashing. The default is SHA256 . Other options are SHA1 , SHA256 , SHA384 , SHA512 , MD5 Get path of the executable/command \u00b6 If we need to know the installed path of an executable or command, similar to the which command in Linux, we can use the Get-Command command, used as a Get-Command <command> . Important parameters: -CommandType , -Type : the type of the command we are looking for (e. g. Application , Cmdlet , ...). Note that this argument works differently than how it is described in the documentation ! without this argument : just first command is returned (best match, e.g., first executable in the path) with this argument : all commands that matches the command name, type and other criteria are returned Possible values: Alias : aliases All : all types Application : executable files Cmdlet : PowerShell commands ExternalScript : All .ps1 files listed in the $env:Path Function : PowerShell functions Filter : PowerShell filters Script : PowerShell scripts blocks","title":"Powershell Manual"},{"location":"Windows/Powershell%20Manual/#introduction","text":"PowerShell is the new command line interface for Windows that replaces the old command prompt. It is superior in almost every aspect so it is recommended to use it instead of the old command prompt. The PowerShell script files have the .ps1 extension. In addition to system commands, PowerShell can also execute native PowerShell commands called cmdlets .","title":"Introduction"},{"location":"Windows/Powershell%20Manual/#new-powershell","text":"The PowerShell Integrated in Windows is version 5. You can recognize it by the iconical blue background color. This old version has some important limitations (e. g. it cannot pass arguments containing arguments with spaces ). Therefore, it is best to install the new PowerShell first.","title":"New PowerShell"},{"location":"Windows/Powershell%20Manual/#quick-edit-insert-mode","text":"PowerShell enables copy/pase of commands. The downside is that every time you click inside PowerShell, the execution (if PowerShell is currently executing somethig) stops. To resume the execution, hit enter .","title":"Quick Edit / Insert Mode"},{"location":"Windows/Powershell%20Manual/#script-blocks-scripts-functions-and-standalone-blocks","text":"documentation A basic unit of execution in PowerShell is a script block . A script block can be: a piece of code enclosed in curly braces {} , a function, a script","title":"Script Blocks: scripts, functions, and standalone blocks"},{"location":"Windows/Powershell%20Manual/#resources","text":"Microsoft documentation SS64 pwsh documentation (arguments of pwsh.exe , etc)","title":"Resources"},{"location":"Windows/Powershell%20Manual/#reading-script-block-arguments","text":"Each script block can be called with parameters. The arguments are first parsed by PowerShell and then passed to the script block. See Parameter parsing for details. Note that this parsing happens even if the script was invoked from outside PowerShell. We have two ways how to read the arguments passed to the script: using the $args variable: simple old syntax, deprecated using the parameter block ( param() ): complex, but more powerful Here, we describe the first option. For the second option, check the Parameter blocks section . To read the arguments, we just access the $args variable . The following example prints the second and third argument: echo $args[1] echo $args[2]","title":"Reading script block arguments"},{"location":"Windows/Powershell%20Manual/#parametr-parsing","text":"documentation In PowerShell, all arguments are named. However, the name may be set as non-required, in which case we can omit it. The syntax is: -<name> <value> # or -<name>:<value> Thic creates an inherent incompatibility with most command line tools, as : is a valid argument separator in PowerShell. Therefore, any argument passed to a PowerShell script block that contains : has to be quoted . This is a known limitation of PowerShell [ source ].","title":"Parametr parsing"},{"location":"Windows/Powershell%20Manual/#parameter-blocks","text":"defined using the param keyword. Example: $myFunction = { param($param1, $param2) # do something } # or in a function function MyFunction { param($param1, $param2) # do something } # or in a script param($param1, $param2)","title":"Parameter blocks"},{"location":"Windows/Powershell%20Manual/#parameter-blocks_1","text":"The parameter block defines the parameters of the script block. By default ( param() ), only the build in parameters are available. In the parameter block, individual parameters are divided by commas. Example: $myFunction = { param($param1, $param2) # do something } Parameters can be typed . Example: $myFunction = { param([int]$param1, [string]$param2, [switch]$param3) # do something } The switch type is a boolean parameter that does not require a value. It is set to True if the parameter is present. Parameters can be mandatory . Example: $myFunction = { param( [Parameter(Mandatory)][int]$param1, [Parameter(Mandatory)][string]$param2, [switch]$param3 ) # do something } We can also validate the parameters. Example: $myFunction = { param( [Parameter(Mandatory)][ValidateRange(0, 100)][int]$param1, [Parameter(Mandatory)][ValidateSet(\"a\", \"b\", \"c\")][string]$param2, [Parameter(Mandatory)][ValidateScript({$_ -eq \"a\" -or $_ -eq \"b\"})][string]$param3 ) # do something } Also with custom error message: $myFunction = { param( [Parameter(Mandatory)][ValidateScript({ Test-Path $_ }, ErrorMessage=\"Path does not exists: {0}\")][string]$path ) # do something } The advanced usage of parameters is described in the documentation .","title":"Parameter blocks"},{"location":"Windows/Powershell%20Manual/#command-parsing","text":"documentation","title":"Command parsing"},{"location":"Windows/Powershell%20Manual/#quoting","text":"documentation In PowerShell, there are two types of quoting: \" (double quotes): for expandable strings. These strings can contain variables and expressions that are evaluated. non-basic variables need to be wrapped by expression $() , e.g. $($PSVersionTable.PSVersion) To separate the variable from the following text, use ${} . Example: ${myVar}Text to escape (not evaluate) the $ sign, use the backtick: echo \"`$myVar\" prints $myVar ' (single quotes): for literal strings. These strings are not evaluated. Note that PowerShell consumes the first level of quoting, so that \"a b\" is passed as a b and the same goes for 'a b' . If we need to preserve the quoting, we need to use a second level of quoting: echo \"a b\" # prints a b echo 'a b' # prints a b echo \"a 'b'\" # prints a 'b' echo 'a \"b\"' # prints a \"b\" In case we need the same quoting for both levels, we can escape the first level of quoting with the backtick: \\\" . Example: echo \"a \\\"b\\\"\" # prints a \"b\"","title":"Quoting"},{"location":"Windows/Powershell%20Manual/#arguments-starting-with-and-containing","text":"If a program argument starts with - , and contains . it needs to be wrapped by ' . Otherwise, the argument will be split on the dot. Example: mvn exec:java -Dexec.mainClass=com.example.MyExample -Dfile.encoding=UTF-8 In Powershell, this needs to be converted to: mvn exec:java '-Dexec.mainClass=com.example.MyExample' '-Dfile.encoding=UTF-8' The problem may arise if the argument or its part needs to be quotted as well. Then: for first level of quoting, use \" (double quotes) for second level of quoting, use '' (two single quotes) Example: mvn exec:exec '-Dexec.executable=\"java\"' '-Dexec.args=\"-Xmx30g -Djava.library.path=''C:\\Program Files\\HDF_Group\\HDF5\\1.14.3\\bin'' -classpath %classpath cz.cvut.fel.aic.simod.OnDemandVehiclesSimulation\"'","title":"Arguments starting with - and containing ."},{"location":"Windows/Powershell%20Manual/#escaping-and-in-arguments","text":"Double quotes \" contained in arguments can be preserved by escaping with backslash: \\\" . Example for that can be passing an argument list to some executable: 'args=\\\"arg1 arg2\\\"' Single quotes ' are esceped by duble single quote: '' . Example can be passing a list of args, where some of them contains space: 'args=\\\"''arg1 with space'' arg2\\\"'","title":"Escaping \" and ' in Arguments"},{"location":"Windows/Powershell%20Manual/#executable-execution","text":"There are several ways how to run an executable in PowerShell: The standard way is to just type the command. Another option is the & operator: & <command> <arguments> . This way, we can execute a program with spaces in the path : \"C:/Program Files/a/a\" does not work, but & \"C:/Program Files/a/a\" does prepare the command or arguments dynamically: PowerShell $command = 'ls' & $command # prints the content of the current directory $command $ # prints ls The third option is to use the Invoke-Expression command. This command executes a string as a command with arguments. Example: PowerShell $argument = \"--version\" Invoke-Expression \"java $argument\"","title":"Executable Execution"},{"location":"Windows/Powershell%20Manual/#using-the-call-operator","text":"documentation ss64 documentation The syntax is & <command> <arguments> . Here: <command> is the command to be executed. If the path to the command contains spaces, it has to be wrapped in quotes. <arguments> are either inline arguments, e.g.: & vcpkg list x64-windows-static array of arguments, e.g.: & vcpkg @(\"list\", \"x64-windows-static\") Note that passing the array of arguments to a cmdlet does not work as expected . Cmdlets uses special named parameters. For example the ls (alias for Get-ChildItem cmdlet) has the -Depth parameter that can have an integer value. However, the cmdlet expects such arguments to be passed as key-value pairs. For inline parameters, the key-value pairs are automatically created from the positionally passed arguments. But this is not the case for the array of arguments. So: & ls -Depth 2 # works & ls @(\"-Depth\", 2) # does not work. Each element of the array is treated as a separate positional argument -> a path is expected Unfortunatelly, there is no nice solution for this, if we want to use the call operator (there is no such problem with Invoke-Expression ). There are two options: pass the arguments as key-value pairs. Example: PowerShell $params = @{\"Depth\" = 2} & ls @params # this unrolls the dictionary into key-value pairs do not mistake the second line with & ls $params . This would pass the dictionary as a single argument. use the code block and pass the arguments inside it. Example: PowerShell & {ls -Depth 2}","title":"Using the call (&amp;) operator"},{"location":"Windows/Powershell%20Manual/#using-the-invoke-expression-command","text":"The Invoke-Expression command executes a string as a command with arguments. Example: $argument = \"--version\" Invoke-Expression \"java $argument\" A big caution is needed when using the Invoke-Expression as its typical usage with double-quoted strings triggers expression evaluation twice , which can lead to unexpected behavior. Example: # we want to print $test echo $test # prints nothing as variable is not defined echo `$test # prints $test standard escape solution Invoke-Expression \"echo `$test\" # prints nothing as variable is not defined. The problem is that the escape character is evaluated during the first string evaluation, and then, during the Invoke-Expression evaluation, there is no escape character. Invoke-Expression 'echo `$test' # prints $test. The single quotes prevent the evaluation of the escape character during the first string evaluation.","title":"Using the Invoke-Expression command"},{"location":"Windows/Powershell%20Manual/#print-the-exit-code","text":"To print the exit code of the last command, there are two options: $? : returns True if the last command was successful, False otherwise $LastExitCode variable contains the exit code of the last command","title":"Print the exit code"},{"location":"Windows/Powershell%20Manual/#no-output-for-exe-file","text":"Some errors are unfortunatelly not reported by powershell (e.g. missing dll ). The solution is to run such program in cmd, which reports the error.","title":"No Output for EXE file"},{"location":"Windows/Powershell%20Manual/#variables","text":"Variables are defined by the $ sign. Example: $myVar = \"Hello, World!\" To print the variable, just type its name. Example: $myVar","title":"Variables"},{"location":"Windows/Powershell%20Manual/#environment-variables","text":"They are accessed by the $env: prefix. Example: $env:PATH","title":"Environment variables"},{"location":"Windows/Powershell%20Manual/#operations-on-variables","text":"The variables can be used in expressions. Example: $myVar = 5 $myVar + 3","title":"Operations on Variables"},{"location":"Windows/Powershell%20Manual/#string-operations","text":"Strings can be concatenated using the + operator. Examples: $myVar = \"Hello, \" + \"World!\" # append to a path: $env:PATH += \";C:\\Program Files\\Java\\jdk1.8.0_181\\bin\" # prepend to a path: $env:PATH = \"C:\\Program Files\\Java\\jdk1.8.0_181\\bin;\" + $env:PATH","title":"String Operations"},{"location":"Windows/Powershell%20Manual/#operators","text":"documentation","title":"Operators"},{"location":"Windows/Powershell%20Manual/#the-additive-operator","text":"The additive operator ( + ) is overloaded for different types: for numeric types, it performs the arithmetic addition for string types, it performs the string concatenation for arrays and hashtables, it performs the array/hashtable concatenation","title":"The additive operator"},{"location":"Windows/Powershell%20Manual/#comparison-and-matching-operators","text":"help page documentation Equality operators: -eq : equal -ne : not equal -gt : greater than -lt : less than -ge : greater or equal -le : less or equal Matching operators: -match : match","title":"Comparison and Matching Operators"},{"location":"Windows/Powershell%20Manual/#logical-operators","text":"documentation -and : logical and -or : logical or -not : logical not","title":"Logical Operators"},{"location":"Windows/Powershell%20Manual/#control-structures","text":"","title":"Control Structures"},{"location":"Windows/Powershell%20Manual/#conditions","text":"","title":"Conditions"},{"location":"Windows/Powershell%20Manual/#if","text":"The if statement is used for conditional execution. The syntax is: if ($condition) { # do something } elseif ($anotherCondition) { # do something else } else { # do something else } The if structure is also available as a cmdlet If . Example: If ($condition) { \"True\" } Else { \"False\" } The If cmdlet is also available as an alias if and ? .","title":"if"},{"location":"Windows/Powershell%20Manual/#loops","text":"","title":"Loops"},{"location":"Windows/Powershell%20Manual/#foreach","text":"The foreach cycle iterates over a collection. The syntax is: foreach ($item in $collection) { # do something with $item } The foreach structure is also available as a cmdlet ForEach-Object . In this case, we access the current item using the $_ variable. Example: Get-ChildItem | ForEach-Object { $_.Name } The above command lists the names of all files in the current directory. The alias for the ForEach-Object cmdlet is foreach and % .","title":"foreach"},{"location":"Windows/Powershell%20Manual/#inputs-and-outputs","text":"","title":"Inputs and Outputs"},{"location":"Windows/Powershell%20Manual/#inputs","text":"To read a file, use the Get-Content cmdlet. Example: Get-Content \"C:\\Users\\user\\file.txt\"","title":"Inputs"},{"location":"Windows/Powershell%20Manual/#outputs","text":"There are many output streams in PowerShell. We can use: Write-Output : for standard output Write-Error : for standard error Write-Warning : for warnings Write-Verbose : for verbose output Write-Debug : for debug output Write-Information : for information output Write-Host : for writing directly to the console. No output stream is used. By default, only the standard and error output streams are displayed. To display the other streams, we have several options: manually set the $VerbosePreference , $DebugPreference , $WarningPreference , $InformationPreference variables to Continue (default is SilentlyContinue ), or make the function or script we are running an Advanced Function or Script and use the -Verbose , -Debug , -WarningAction , -InformationAction parameters","title":"Outputs"},{"location":"Windows/Powershell%20Manual/#pipes-and-redirection","text":"pipe documentation redirection documentation Output forwading is done using | (pipe) operator, just like in Linux. For redirecting the output to a file, there are the following operators: > : redirect to file, overwrite if exists >> : redirect to file, append if exists >&1 : redirect to standard output stream When using any of these operators, by default, the standard output stream is redirected. If we want to redirect the standard error stream, we have to prepend 2 to the operator. Example: dir > out.txt # redirect standard output stream to out.txt dir 2> err.txt # redirect standard error stream to err.txt If we want both see the output and redirect it to a file, we can use the Tee-Object command which is the equivalent of the tee command from Linux In new PowerShell, we have even more options: 3> : redirect Warning stream 4> : redirect Verbose stream 5> : redirect Debug stream 6> : redirect Information stream *> : redirect all streams","title":"Pipes and Redirection"},{"location":"Windows/Powershell%20Manual/#data-types","text":"Powershell uses .NET types for data types. We can encounter nubers, characters strings, or boolean values. Additionally, we can have composite types, like arrays, or objects.","title":"Data Types"},{"location":"Windows/Powershell%20Manual/#dates","text":"The date is represented by the System.DateTime type. To create a date, we can use the Get-Date command. Example: $date = Get-Date # current date $date = Get-Date -Date \"2025-01-01\" # date from a string Dates can be compared using the standard comparison operators. Example: if ($date1 -lt $date2) { ... }","title":"Dates"},{"location":"Windows/Powershell%20Manual/#arrays","text":"To create an array, use the @() operator: a = @() . To add an element to an array, use the += operator: a += 1 . The same operator can be used to append one array to another: a += @(\"1\", \"bar\") . The arrays can be iterated over using the foreach loop. To join the elements of an array into a space-separated string, we can just use the array in a string context. Example: $a = @(\"foo\", \"bar\") \"$a\" # prints \"foo bar\"","title":"Arrays"},{"location":"Windows/Powershell%20Manual/#string-manipulation","text":"","title":"String Manipulation"},{"location":"Windows/Powershell%20Manual/#concatenation","text":"The concatenation operator is the + operator, see The additive ( + ) operator section .","title":"Concatenation"},{"location":"Windows/Powershell%20Manual/#replace","text":"For replacing a substring in a string, we have two options: Replace method -replace operator The Replace method replaces all occurences of a string with another string. The syntax is: $myString.Replace(\"oldString\", \"newString\") The -replace operator uses regular expressions for replacing. The syntax is: $myString -replace \"pattern\", \"newString\" Multiple replacements can be done using chained -replace operators. Example: $myString -replace \"pattern1\", \"newString1\" -replace \"pattern2\", \"newString2\"","title":"Replace"},{"location":"Windows/Powershell%20Manual/#match","text":"To test if a string matches a regular expression, use the Match method. The syntax is: $myString -match \"pattern\"","title":"Match"},{"location":"Windows/Powershell%20Manual/#select-string","text":"The Select-String is the grep equivalent for PowerShell. The alias for the comand is sls . Parameters: -Content <before>[, <after>] : Select also <before> lines before the matched line and <after> lines after the matched line -Pattern : If we want to use a regular expression for searching, not a plain string","title":"Select-String"},{"location":"Windows/Powershell%20Manual/#selecting-the-matched-string","text":"If we use the Select-String with the -Pattern parameter, the matching lines are returned with the matching string highlighted. If we want to get only the matching string, we have to access the Matches.Value property for each line. Example: Select-String -Pattern \"pattern\" | ForEach-Object { $_.Matches.Value }","title":"Selecting the matched string"},{"location":"Windows/Powershell%20Manual/#select-object","text":"The Select-Object selects the specified properties of an object. Example: Get-Process | Select-Object -Property Name, Id","title":"Select-Object"},{"location":"Windows/Powershell%20Manual/#file-system","text":"","title":"File System"},{"location":"Windows/Powershell%20Manual/#listing-files","text":"Documentation For listing files, we can use the Get-ChildItem command, which has an alias ls . The result will print the files with various properties out of which the Mode needs to be explained: a : archive d : directory h : hidden l : symbolic link r : read-only s : system","title":"Listing files"},{"location":"Windows/Powershell%20Manual/#symlinks","text":"To create a symlink , use the New-Item command with the -ItemType parameter set to SymbolicLink . Example: New-Item -ItemType SymbolicLink -Path <source> -Target <target> To modify a symlink , we use the same command, we just need to add the -Force parameter. Note that administrator privileges are required to create or modify symlinks .","title":"Symlinks"},{"location":"Windows/Powershell%20Manual/#network","text":"","title":"Network"},{"location":"Windows/Powershell%20Manual/#netstat","text":"The netstat is the basic network monitoring program. It displays TCP connections. It originated on Linux, so the usage and parameters are described in the Linux manual. Below, we discuss the differencies in the command interface and behavior of the Windows version of the command. Winndows manual .","title":"netstat"},{"location":"Windows/Powershell%20Manual/#lot-of-kubernetes-entries-in-the-log","text":"By default, the netstat command translates IPs into names. Unfortunatelly, on Windows, it also uses the information from the hosts file ( C:\\Windows\\System32\\drivers\\etc\\hosts ). This is a problem, because some services, like Docker, can use the hosts file to redirect some adddress to localhost. Therefore, at the end, all localhost entries are named kubernetes. Solution options: use the -n parameter for netstat or remove the redirection to localhost in the hosts file","title":"Lot of kubernetes entries in the log"},{"location":"Windows/Powershell%20Manual/#display-executable-for-connection","text":"To display executable for all connection, just use the -b swith. For filtering out only some, you have to use the -Context parameter in the Select-String command, as the executable is printed one line below the connection: netstat -b | sls <search pattern> -Context 0,1","title":"Display executable for connection"},{"location":"Windows/Powershell%20Manual/#system-information","text":"CIM documentation WMI classes There are several interfaces for getting system information in PowerShell: Using the Common Information Model (CIM) Using the Windows Management Instrumentation (WMI) By reading from the registry","title":"System Information"},{"location":"Windows/Powershell%20Manual/#cim-and-wmi","text":"Because the CIM and WMI interfaces are very similar, we will discuss them together. The main difference is that the CIM is the newer interface, which is more powerful and more user-friendly. The CIM is also cross-platform, while the WMI is Windows-only. The advantage of the CIM and WMI interfaces is that they are clear and object-oriented. The information can be queried using database-like operations. The disadvantage is that they are slow. The main command for getting system information is: Get-CimInstance : for CIM Get-WmiObject : for WMI For both commands, we need to specify the class of the object we want to get using the -ClassName (CIM) or -Class (WMI) parameter. The classes are the same for both interfaces. Typical classes are: Win32_ComputerSystem : information about the computer Win32_OperatingSystem : information about the operating system Win32_InstalledWin32Program : information about installed programs Win32_Product : information about programs installed using the MSI installer contains more information than Win32_InstalledWin32Program","title":"CIM and WMI"},{"location":"Windows/Powershell%20Manual/#windows-registry","text":"To access the Windows registry, we use the same commands that are used for file system: Get-ItemProperty : to get the value of a registry key Get-ChilItem to get a list of child keys","title":"Windows Registry"},{"location":"Windows/Powershell%20Manual/#advanced-script-blocks","text":"Any script block can be made an advanced script block. Advanced script blocks are run similarly to the compiled cmdlets. To make a script block an advanced script block, we have to use the [CmdletBinding()] attribute at the beginning of the script block. Example: function MyFunction { [CmdletBinding()] # do something } The advanced functions and scripts have the following features: parameters are not available as $args (have to be parsed using the param block) builtin parameters like -Verbose , -Debug , -WarningAction , -InformationAction are parsed automatically support for pipeline input","title":"Advanced Script blocks"},{"location":"Windows/Powershell%20Manual/#elevation","text":"Some commands may require administrator privileges. Therefore, it is wise to check if the script is running with administrator privileges so that the execution is not interrupted. To check if the script is running with administrator privileges, use the following code: $myWindowsID=[Security.Principal.WindowsIdentity]::GetCurrent() $myWindowsPrincipal=new-object Security.Principal.WindowsPrincipal($myWindowsID) if (!$myWindowsPrincipal.IsInRole([Security.Principal.WindowsBuiltInRole]::Administrator)) { Write-Host \"This script requires elevated privileges. Please run it as an administrator.\" exit } Another solution may be to restart the script with administrator privileges. However, this has several limitations: when the script is run with administrator privileges a new terminal window is opened parameters are not passed automatically to the new script. We can manually pass them using the -ArgumentList parameter. However, this is problematic for non-string parameters, especially in an advanced script block. To restart the script with administrator privileges, use the following code: $argString = $args -join ' ' Start-Process \"pwsh\" -Verb RunAs -ArgumentList \"-noexit -File `\"$PSCommandPath`\" $argString\" exit","title":"Elevation"},{"location":"Windows/Powershell%20Manual/#powershell-history","text":"To iterate over the commands used in the past, we use the up arrow and down arrow keys. To list the history, use the Get-History command. To clear the history, use the Clear-History command: Clear-History -Id <id> : clear the command with the specified id (use Get-History to get the id) Clear-History -Count <count> : clear the last count commands Clear-History -CommandLine <pattern> : clear the commands that match the pattern. The pattern use the simple matching (e.g. * is the wildcard), and have to be wrapped in quotes if it contains spaces.","title":"PowerShell History"},{"location":"Windows/Powershell%20Manual/#powershell-scripts","text":"","title":"PowerShell Scripts"},{"location":"Windows/Powershell%20Manual/#including-other-scripts","text":"documentation Script including in PowerShell is called dot sourcing . To include a script, use the . operator. Example: . \"C:\\Users\\user\\script.ps1\" To include a script from the current directory , use the . operator: PowerShell . .\\script.ps1 To include a relative to the script directory , use the $PSScriptRoot variable. Example: PowerShell . \"$PSScriptRoot\\script.ps1\"","title":"Including other scripts"},{"location":"Windows/Powershell%20Manual/#usefull-commands","text":"","title":"Usefull Commands"},{"location":"Windows/Powershell%20Manual/#file-encoding-conversion","text":"gc [INPUT PATH] | Out-File -en [ENCODING] [OUTPUT PATH] example: gc \"C:\\AIC data\\Roadmaptools big data test/map-filtered.osm\" | Out-File -en utf8 \"C:\\AIC data\\Roadmaptools big data test/map-filtered-utf8.osm\" NOTE: it is not very fast :)","title":"File Encoding Conversion"},{"location":"Windows/Powershell%20Manual/#delete-all-files-with-a-certain-extension","text":"ls *.extension -Recurse | foreach {rm $_} to try it, add the -WhatIf parameter to rm","title":"Delete all files with a certain extension"},{"location":"Windows/Powershell%20Manual/#batch-rename","text":"Example dir . | % { $newName = $_.Name -replace '^DSC_0(.*)', 'DSC_1$1'; rename-item -newname $newName -literalPath $_.Fullname -whatif}","title":"Batch rename"},{"location":"Windows/Powershell%20Manual/#count-lines-in-large-file","text":"switch -File FILE { default { ++$count } }","title":"Count Lines in large file"},{"location":"Windows/Powershell%20Manual/#get-help","text":"Mirosoft documentation To get help about a command, use the Get-Help (alias man ) command. Example: Get-Help Get-ChildItem If the output is the list of articles, it means that there is no help for the command.","title":"Get Help"},{"location":"Windows/Powershell%20Manual/#translate-alias-to-command","text":"To translate an alias to a command, use the Get-Alias command. Example: Get-Alias ls # returns Get-ChildItem","title":"Translate alias to command"},{"location":"Windows/Powershell%20Manual/#get-the-path-of-the-current-script","text":"To get the path of the current script, use the $PSCommandPath variable.","title":"Get the path of the current script"},{"location":"Windows/Powershell%20Manual/#compute-the-hash-of-a-file","text":"To compute the hash of a file, use the Get-FileHash command. Example: Get-FileHash \"C:\\Users\\user\\file.txt\" Important parameters: -Algorithm : the algorithm used for hashing. The default is SHA256 . Other options are SHA1 , SHA256 , SHA384 , SHA512 , MD5","title":"Compute the hash of a file"},{"location":"Windows/Powershell%20Manual/#get-path-of-the-executablecommand","text":"If we need to know the installed path of an executable or command, similar to the which command in Linux, we can use the Get-Command command, used as a Get-Command <command> . Important parameters: -CommandType , -Type : the type of the command we are looking for (e. g. Application , Cmdlet , ...). Note that this argument works differently than how it is described in the documentation ! without this argument : just first command is returned (best match, e.g., first executable in the path) with this argument : all commands that matches the command name, type and other criteria are returned Possible values: Alias : aliases All : all types Application : executable files Cmdlet : PowerShell commands ExternalScript : All .ps1 files listed in the $env:Path Function : PowerShell functions Filter : PowerShell filters Script : PowerShell scripts blocks","title":"Get path of the executable/command"},{"location":"Windows/Windows%20Manual/","text":"Shells and Terminals \u00b6 Official Shell Overview Using shell to manage the operating system is an essential task and therefore, we start with the terminals and shells . In the history of Windows and DOS, there were many shell or built-in script interpreters, and also numerous terminal applications. Here, we only cover the shells and terminals relevant to the most recent versions of Windows. There are two shells in Windows: command prompt (Command shell) ( cmd.exe ) very simple and limited functionality successor of COMMAND.COM For Command shell solutions/guides, check the Command shell manual . PowerShell Modern shell with many features Supports all the Windows commands (commands from the Command shell) For PowerShell solutions/guides, check the PoweShell manual . There are two terminal applications in Windows: Windows Console Host ( conhost.exe ) : an old terminal application present in Windows for years startup program is the command prompt default terminal application in Windows 10 and older no longer relevant in Windows 11 Windows Terminal ( wt.exe ) : a new terminal application that is still in development. startup program can be configured (PowerShell by default) default terminal application in Windows 11, available in the Microsoft Store in older versions of Windows modern terminal, almost Linux-like experience Run a non-default terminal application \u00b6 It is not an easy task to manually run a shell in a non-default terminal application. To do it, we need to execute the terminal application executable: Windows Terminal: wt.exe Windows Console Host: conhost.exe Windows Terminal \u00b6 The Windows Terminal is a new terminal application with many great features: multiple tabs smart text selection To configure Windows Terminal, click the v button in the top bar and select Settings . There are two kinds of settings: Global settings sorted in categories in the left panel Profile settings for each profile. The profiles are listed in the left panel. After selecting a profile, the settings are displayed in the right panel. Nonbasic settings are sorted in categories under the Additonal settings header. Windows terminal pass stdout to text editor instead displaying it \u00b6 This can happen if the output is too long or wide for the terminal. If a command outputs a text in Windows console host but passes it to a text editor in Windows Terminal, the possible cause is the font size. The default size in Windows Terminal is 12, which is much larger than the 16 in Windows Console Host (default). To fix it, change the font size in Windows Terminal to 10 which corresponds to the 16 in Windows Console Host. Command Prompt \u00b6 Compared to PowerShell or Linux shells, the Command Prompt is very limited. Therefore, we describe only the most important commands. For more advanced commands, use PowerShell. To change the current directory , use the cd command. However, in the Command Prompt, this command cannot be used to change the drive. To change the drive, use the drive letter followed by a colon, e.g., command D: changes the drive to D: . Character Encoding \u00b6 By default, Windows use the local ANSI code page for the character encoding, both in Command Prompt and PowerShell. We can change this encoding in an active console window, for example, command chcp 65001 changes the encoding to UTF-8 in Command Prompt. To do a permanent change, we have to edit Windows settings: Open the Windows settings Go to Time & language -> Language & region -> Administrative language settings Click on the Change system locale... button Check the Beta: Use Unicode UTF-8 for worldwide language support checkbox Execution of Programs, Scripts, Commands and similar \u00b6 The execution of programs, scripts, commands and other executable files is a common, yet mysterious task in almost every operating system. To prevent confusion, it is absolutely essential to know what, and under what conditions is executed. The most important lesson is that the two Windows shells, Command Prompt and PowerShell, has a different approach to execution, and also, the executables can be run outside a shell with, yet different, mechanisms. The table below ( source ) presents the basics of the Windows execution logic when we execute the program by its name : .ritz .waffle a { color: inherit; }.ritz .waffle .s0{background-color:#d9d9d9;text-align:left;font-weight:bold;color:#000000;font-family:Arial;font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s4{background-color:#f4cccc;text-align:left;color:#000000;font-family:Arial;font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s1{background-color:#d9d9d9;text-align:center;font-weight:bold;color:#000000;font-family:Arial;font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s2{background-color:#ffffff;text-align:left;font-weight:bold;color:#000000;font-family:Arial;font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s3{background-color:#b7e1cd;text-align:left;color:#000000;font-family:Arial;font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;} Executable type Local Directory on PATH with extension without extension with extension without extension sys\u00b9 cmd pwsh\u00b2 sys\u00b9 cmd pwsh\u00b2 sys\u00b9 cmd pwsh sys\u00b9 cmd pwsh application (*.exe) yes yes no yes yes no yes yes yes yes yes yes batch file (*.bat, *.cmd) no yes no no yes no no yes yes no yes yes PowerShell script (*.ps1)\u00b3 no no no no no no no no yes no no yes Notes: The system ( CreateProcessW function) only considers native executables ( .exe files). PowerShell never search local directory when executing program by name, due to security reasons. This can be changed by adding . (current directory) to the PATH environment variable (not recommended). PowerShell scripts are not considered as executables by Command Shell, again, due to security reasons. This can be changed by modifying the PATHEXT environment variable that lists the executable file extensions considered by cmd.exe and system (not recommended). The table above cover only what can and what cannot be executed by the name. However, when duplicate names are present, the resolution order becomes important. Also, there may be various ways how to execute the same executable in the same shell. For details, check the respective sections of the manual for each execution environment: System (Out of the shell) execution Command shell execution PowerShell shell execution If we execute using the path , the execution is consistent among all three execution environments (cmd, pwsh and system). After the executable is found by the name or path, it is executed in an environment determinded by its file type association, which is by default: program: execute the program itself (there is some bootstrapping logic, but it is not important for us) batch script: cmd.exe PowerShell script: notepad.exe . It is very useful to reconfiure this association to PowerShell. System (Out of the shell) execution \u00b6 The system execution is realized by calling the CreateProcessW function. The executable path resolution is as follows: If the executable is a path, the path is evaluated. Otherwise, the name is searched in the locations below (in order): The directory of the calling process Current working directory of the calling process System directories 32-bit system directory 16-bit system directory Windows directory Directories listed in the PATH environment variable, in the order they are listed Unlike Command Shell and PowerShell, the system execution does not consider script files, only native executables. Only the .exe extension is considered if the file name is specified without the extension. Keyboard Shortcuts \u00b6 Alt + Shift : change input language Win + Space : change keyboard input method Wireless Network \u00b6 Problem: Can't connect to this network \u00b6 Solution: Forget the connection and connect to the network manually Connect to a Network Manually \u00b6 Control Panel -> Network and Internet -> Network and Sharing Center Set up a new connection or network Manually connect to a wireless network Fill the credentials: Network name: SSID Security type: depends, try WPA2 personal Security key: password Click next Close the dialog Click the wifi icon in the taskbar and connect to the network There are various usefull comands. For most of the commands, you need to open PowerShell as admin. Various Commands Related to the Wifi \u00b6 Show All Network Profiles \u00b6 This command show network configurations stored on the device. netsh wlan show profile Various Wifi Reports in HTML \u00b6 netsh wlan show wlanreport Bluetooth \u00b6 Troiubleshooting \u00b6 Cannot connect to the device \u00b6 Try to remove the device and pair it with the PC again If it does not help, proceeed to the next section (even if the pairing is successfull) Cannot pair with the device \u00b6 Turn off the device and unplug it from the electricity/remove batteries. Then plug it back after ~10 seconds, power it of, and try to pair with it again. Bluetooth Command Line Tools \u00b6 https://bluetoothinstaller.com/bluetooth-command-line-tools Bluetooth Command Line Tools is a set off tools that enables command line interaction with blootooth services. Basic usage: discover and list available devices: btdiscovery -s Filesytem \u00b6 Standard folder structure \u00b6 In Windows, the standard folder structure is completely different for system and user instalation. Details are listed below, but the main difference is that the system instalations are stored in a single root folder for each application (similarly to Android), while the user instalations' files are distributed among multiple folders, depending on the type of the file (similarly to Linux). The standard folders can be quick-accessed by aliases written in the form %alias% . These aliases works in Windows Explorer and Command Prompt. To use them in PowerShell, we have to access them as environment variables, e.g. $env:alias . User home folder \u00b6 The user home folder is located in C:\\Users\\<username> by default. It is aliased as %userprofile% . System instalation folders \u00b6 If an application is installed for all users, all its files are usually installed in a single folder per application. The location of the folder depends on the type of the application: C:\\Program Files : 64-bit applications C:\\Program Files (x86) : 32-bit applications If the application needs to store some data, they are usually stored in the C:\\ProgramData (aliased as %programdata% ) folder. User instalation folders \u00b6 User instalations are stored in multiple folders, depending on the type of the file. All these folders are located in the user's home folder, which is C:\\Users\\<username> by default. The folders are: ~\\AppData\\Local : Program data and sometimes also executables ~\\AppData\\Local\\Promgrams : program files and executables ~\\AppData\\LocalLow : ~\\AppData\\Roaming (aliased as %appdata% ): Start Menu folder \u00b6 The user specific shortcuts are stored in: %appdata%\\Microsoft\\Windows\\Start Menu\\Programs . The system wide shortcuts are stored in: %programdata%\\Microsoft\\Windows\\Start Menu\\Programs . Read Only Files and Folders \u00b6 An ancient form of file protection on Windows is the read only flag that can be set on files and folders. It is not a real protection, as it can be easily removed by the user, but it can be used to prevent accidental changes. Most of the programs can ignore this flag and work with the file anyway. However, some programs (e.g. Python) can have problems with it. Formatting a drive/partition or disk \u00b6 When we want to delete all data from a drive/partition or disk and start over, we use the procedure called formatting. There are various tools for that, with different trade-offs between capabilities and complexity. In all tools, there are two variants of formatting: Quick Format: All files are forgotten, but they data is not overwritten. Best choice most of the time but: other people can still recover the data using special software, and when bootable disk is formatted this way, the BIOS can still boot from it, confusing the user. Full Format: All files are forgotten and the data is overwritten with zeros. This is the safe option, but it can take a long time. There are several tools for formatting: properties window of the drive in File Explorer: can only format mounted drives Disk Management tool in Computer Management diskpart command line tool Sugarsync \u00b6 Quick orientation in the desktop app: for file changes, check left menu -> Activity for deleted files, check left menu -> Deleted Items Solving sync problems \u00b6 check if the file is updated in cloud using web browser if not, check the activity log on the computer with the updated file if the change is not in the log, a simple hack can help: copy the file outside SugarSync folder and back. Useful Commands \u00b6 Get Motherboard Info \u00b6 wmic baseboard get product,Manufacturer,version,serialnumber Copy multiple files/dirs \u00b6 robocopy is the best command for that. Usefull params: /e : Copy subdirectories, including empty ones /b : Copy in the backup mode, so that even files with a different owner can be copied /xc : Excludes changed files. /xn : Excludes newer files. /xo : Excludes older files. /r:<n> : Specifies the number of retries on failed copies. The default value of n is 1,000,000 (one million retries). /w:<n> : Specifies the wait time between retries, in seconds. The default value of n is 30 (wait time 30 seconds). echoargs : Print arguments as passed to the script \u00b6 Sometimes, it is hard to see what are the exact arguments passed to an executable. To debug it quickly, we can use the echoargs.exe tool present in Portable Programs or at ss64 . Usage: echoargs <command> <arguments> User info \u00b6 Information about users can be obtained with the Get-LocalUser command. By default, the command lists all users. Some useful params: -Name : Specifies the user account names of the users to get. -SID : Specifies the security identifier (SID) of the users to get. Installation \u00b6 Windows can be installed from bootable USB created by a tool downloaded from the official Miccosoft website . There is a single image for all Windows editions, a particular version is choosen based on the license key. The license can be purchased online either from Microsoft (be sure to buy it on CZ website, it cannot be bought on the US website) or from a retailer (e.g., Alza ). Licenses are transferable, not OEM, unless specified otherwise. The Home version is typically sufficient. Installation Steps: Get a license key Download the install tool from Microsoft Run the tool and create a bootable USB Start the installation Fill in the licence key Choose where to install Windows Complete the installation guide Post Installation Steps \u00b6 Import the disks from previous installations. Import disks \u00b6 After installation, only the main disk and newly installed disk are initialized. Other disks will be marked as foreign in the disk management tool, and the drives on them will not show up in File Explorer. To import the disks: Go to Computer Management -> Storage -> Disk Management Right click on disk marked as foreign and select Import foreign disks Installation Problems \u00b6 We couldn\u2019t create a partition or locate an existing one \u00b6 Ensure that the boot priority of the drive where the Windows should be installed is right behind the installation USB priority. windows installation encountered an unexpected error 0x80042444 - 0x4002F \u00b6 This error is triggered by SATA drives from a different OS installation. There are two solutions: Use the legacy Windows installer that can be started from a small link on one of the initial installation screens. disconnect the SATA drives from previous installations, either physically, or in BIOS/UEFI Configuration \u00b6 Changing the input method \u00b6 It is possible to let the system have a different input method for each app. It is not possible however, to remember the input method (after app/OS restart). Troubleshooting \u00b6 Nothing happens after clicking on the input method in the taskbar (windows 10) \u00b6 restrat the computer :) Right Click Menu \u00b6 Bring the old right click menu back to Windows 11 \u00b6 There is a new right click menu in Windows 11, which is much less practical than the old one. To bring the old one back, we have to edit the registry: reg.exe add \"HKCU\\Software\\Classes\\CLSID\\{86ca1aa0-34aa-4e8b-a509-50c905bae2a2}\\InprocServer32\" /f /ve Configure the items in the right click menu \u00b6 Unfortunatelly, the right click menu is not directly configurable in Windows. Usually, the actions are enabled by the application installation and sometimes, this can be disabled in the installation process To disable context menu entries after installation, we usually need to do one of the four things: configure the context menu in the application settings : if possible, easiest way add the item to blocked list in the registry : for applications that cannot configured the context menu in the settings uninstall the application : for applications that we do not need at all delete the item from the registry : for certain built-in Windows items To configure the context menu in the registry, we need to add a new key to the registry under the following path: HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Shell Extensions\\Blocked . Under this path, we need to add a new string value matching the context menu entry we want to disable. For example, in PowerShell, we can run: REG ADD \"HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Shell Extensions\\Blocked\" /v \"<context meny entry name>\" /d <any value, can be empty> Example for Skype: REG ADD \"HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Shell Extensions\\Blocked\" /v \"{776DBC8D-7347-478C-8D71-791E12EF49D8}\" /d Skype After editing the registry, restart the Explorer. For deleting the item from the registry, just run REG DELETE <path> Context menu that can be disabled by configuring the application settings \u00b6 PowerToys modules: These can be removed by deactivating the specific modules in the PowerToys settings. Context menu items to be disabled by editing the registry \u00b6 Context menu item Registry key Ask Copilot {CB3B0003-8088-4EDE-8769-8B354AB2FF8C} Move to OneDrive {1FA0E654-C9F2-4A1F-9800-B9A75D744B00} Restore previous version {596AB062-B4D2-4215-9F74-E9109B0A8153} Share with Skype {776DBC8D-7347-478C-8D71-791E12EF49D8} Context menu items to be disabled by uninstalling the application \u00b6 Edit with Notepad Context menu items to be disabled by deleting the registry item \u00b6 Open with Visual Studio : HKEY_CLASSES_ROOT\\Directory\\shell\\AnyCode Scan with Microsoft Defender ( Source ): \"HKEY_CLASSES_ROOT\\*\\shellex\\ContextMenuHandlers\\EPP\" \"HKEY_CLASSES_ROOT\\CLSID\\{09A47860-11B0-4DA5-AFA5-26D86198A780}\" \"HKEY_CLASSES_ROOT\\Directory\\shellex\\ContextMenuHandlers\\EPP\" \"HKEY_CLASSES_ROOT\\Drive\\shellex\\ContextMenuHandlers\\EPP\" Send to : \"HKEY_CLASSES_ROOT\\AllFilesystemObjects\\shellex\\ContextMenuHandlers\\SendTo\" \"HKEY_CLASSES_ROOT\\UserLibraryFolder\\shellex\\ContextMenuHandlers\\SendTo\" VLC entries: \"HKEY_CLASSES_ROOT\\Directory\\shell\\AddtoPlaylistVLC\" \"HKEY_CLASSES_ROOT\\Directory\\shell\\PlayWithVLC\" Translate with Deepl \u00b6 Haven't found a way to remove it yet. Even uninstalling the Deepl does not help. Diskpart \u00b6 documentation Diskpart is a useful command line tool for work with disks, partitions, etc. We start it by running diskpart command, and then, we use other commands to manage the disks. Basic general commands: list disk : list all disks exit : exit diskpart select disk <disk number> : select the disk to manage When a disk is selected, we can use other commands: list partition : list all partitions on the selected disk list volume : list all volumes on the selected disk. A volume is a mount point for a partition. select partition <partition number> : select the partition to manage Find out wheteher a disk is MBR or GPT \u00b6 Open Command Promp from the Windows 10 USB \u00b6 Insert the USB stick Wait till the first installation screen shift + F10 Firewall \u00b6 Generate firewall logs \u00b6 Go to Windows firewall and select properties on the right At the top, choose the profile corresponding to the current network profile In the logging section, click to customizze set both DROP and ACCEPT to yes Do not forgot to turn of the logging after the investigation! SSH \u00b6 For ssh, we can use the standard ssh commannd available in Windows (check Linux manual for more info). If the command is not available, it can be installed in Apps & Features -> Optional features . One thing that differs from Linux is that the Windows ssh does not support the <addres>:<port> syntax. To specify the port, it is necessary to use the -p parameter For more features, we can use other programs KiTTY for credentials storage, automatic reconection, etc. WinSCP for file manipulation KiTTY \u00b6 It is best to use the portable version, so that nothing is stored in the Windows registry. Configurtation: copy the PuTTY credentials : .\\kitty_portable-0.76.1.3.exe -convert-dir auto reconnect: Connection -> auto reconnect on connection failure and auto reconnect on system wakeup WinSCP \u00b6 WinSCP is a graphical tool for file manipulation. Ii can be used both for local and remote files, and it supports various protocols (FTP, SFTP, SCP, WebDAV, etc.). Adding a new connection \u00b6 There is a simple New Site button on the left, which opens a straightforward dialog. The only complicated thing can be the SSH key. To add it, click on the Advanced button and go to the SSH -> Authentication tab. There, we can select the private key file. Bookmarks \u00b6 To add bookmarks, go to Local / Remote -> Add Path to Bookmarks or press Ctrl + B . To open a bookmark, go to Local / Remote -> Go To -> Open Drirectory/bookmark or press Ctrl + O . SSH key agent \u00b6 To enable the ssh-agent on Windows, one extra step is needed: we need to start the ssh-agent service. To do that, open the services manager and start the OpenSSH Authentication Agent service. Git \u00b6 Most of the git functionality is the same as in Linux, so check the Linux manual for more info. However, there are some important differences mostly resulting from the fact that Git is not a native Windows application, but it runs in MinGW. Git on Windows and SSH \u00b6 As the git on Windows runs in MinGW, it does not use the Windows SSH command. That can be problematic if we want to debug the SSH connection using the env variable or configuring an ssh key agent. To force git to use the Windows SSH, we need to set the sshCommand config variable to the path to the Windows SSH: git config --global core.sshCommand C:/Windows/System32/OpenSSH/ssh.exe VPN \u00b6 VPN is natively supported in Windows. It can be set up in two ways: using a system dialog in PowerShell If the system dialog does not work, try the PowerShell method. WSL \u00b6 Windows Subsystem for Linux (WSL) is basically a virtual machine running Linux. To install WSL , run wsl --install command. To list installed WSL distributions , run wsl --list To list available WSL distributions , run wsl --list --online To install a WSL distribution , run wsl --install -d <distribution name> Finally, to remove a WSL distribution , run wsl --unregister <distribution name> Translate Windows Path to Linux Path \u00b6 To translate a Windows path to a Linux path, we can use the wslpath command, which is available in Ubuntu WSL distributions. Execute commands from the Windows \u00b6 We can directly execute commands in the WSL from the Windows command line without logging into the WSL with the wsl command. The syntax is: wsl <command> Important arguments: -u <user>, --user <user> : run the command as the specified user. This can be useful for running the command as the root user. --shutdown : shutdown the WSL after the command is executed. Dual Boot \u00b6 Make Windows Work after Linux Uninstall if the Bootloader is Grub \u00b6 Get rid of the Grub bootloader Set the Windows bootloader as the primary partition it is the small partition at the beginning of the main disk Shims \u00b6 Shims are small programs that are used to replace the original program, typically to run it with some additional parameters. On Windows, the shims can be easilly integrated into the system by adding the folder with the shims to the beginning of the PATH environment variable. As a shim, we can use: any executable file ( *.exe ) a batch file ( *.bat , *.cmd ) a PowerShell script ( *.ps1 ) note that the PowerShell script shims works only for PowerShell. To make it work outside PowerShell, we need to accommpany it with a batch file that runs the PowerShell script: batch @echo off powershell -ExecutionPolicy Bypass -File \"%~dp0\\script.ps1\" %* Notepad++ \u00b6 Configuration \u00b6 Usually, it is a good idea to configure autosave. This functionality is available in a plugin called Autosave. Problems \u00b6 Folder Sharing Problems \u00b6 Note that updated Windows 10 disabled anonymous sharing , so password protected sharing has to be turned on. To login, use the credentials for the computer with the shared folder . Below is a list of possible problems, together with solutions. The user name or password is incorrect \u00b6 Check whether the computer can be seen in the network. If not, resolve this issue first. quick check by running net view <IP address> Check that you are using the right username. You need to use the username and password of the computer you are connecting to . Check that the user name is correct by running net user on the target computer Check that the folder is shared with you in: right click on the folder -> Properties -> Sharing -> Advanced Sharing... -> Permisions . Note that your full name can be there instead of your username, which is OK. Check that you are using the right password. You have to use the password associated with your microsoft account. Note that it can differ from the password (PIN) you are using to log in to the computer! check it on the command line: net use * \\\\<IP address>\\<drive letter>$ /use:<username> <password> Folder right and ownership cannot be read \u00b6 Try to clear the windows filecache (CCcleaner or restart) Computer does not see itself in Network section in File Explorer \u00b6 Solution to this problem is to restart the service called Function Discovery Resource Publication . Either restart it in Computer Management -> Services, or by: net stop FDResPub net start FDResPub PC wakes up or cannot enter sleep \u00b6 1 Find the source \u00b6 Using the Event viwer open the event viewer go to windows logs -> system In case of wake up inspect the logs when the wake up happened and search for the Information log with the message \"The system has returned from a low power state.\" There is a wake up source in the end of the log message. If the soure is Unknown go to the command line section In case of not entering sleep Search for the any kernel power event If there is an event stating: The system is entering connected standby , it means that the modern fake sleep is present in the system, replacing the real sleep mode. Using command line (admin): Try powercfg -lastwake If the results are not know, try to call powercfg -devicequery wake_armed to get the list of devices that can wake the computer 2 Solve the problem \u00b6 Device waken up by network adapter \u00b6 Open device manager and search for the specific network adapter right click -> Properties -> Power Management Check Only allow a magic packet to wake up the computer The real sleep mode is not available on the system \u00b6 If this is the case, use the hibernate mode instead. To add it to the start menu: go to Control panel -> Hardware and sound -> Power options click on the left panel to Choose what the power buttons does click on Change settings that are currently unavailable check the hibernate checkbox below Camera problem \u00b6 Symptoms: the screen is blank, black, single color, in all apps and there are no problems reported in device manager Cause: it can be caused by some external cameras (now disconnected) that are still selected in the apps using the camera. Go Solution: Go to the app setting and select the correct camera Phone app cannot see the connected cell phone \u00b6 It can be due to the fucked up Windows N edition. Just install the normal edition. vmmem process uses a lot of CPU \u00b6 This process represents all virtual systems. One cultprit is therefore WSL. Try to shutdown the WSL using wsl --shutdown Computer Restarts without User Intervention \u00b6 The first thing is to go to the event viewer and check the error eventlogs. If the Dump file generation event is in the log, we can further explore the Dump file using the WinDbg tool. To analyze the dump: run WinDbg as administrator File -> Open Dump File and select the dump file (stored in the C:\\Windows\\Minidump folder) After opening the dump file, run !analyze -v to analyze the dump (or click to the link in the main window) The most Important information in the log is the BUGCHECK_CODE . The meaning of each code is documented in the Microsoft Learn . Encountered codes: 0x00000050 : PAGE_FAULT_IN_NONPAGED_AREA : invalid memory pointer (nonexisting or freed memory access) 0x0000003B : SYSTEM_SERVICE_EXCEPTION : Exeption during a routine for transitioning between user and kernel mode. This is a very nonspecific error, to see what is really going on, we have to inspect the firts parameter of the exception ( BUGCHECK_P1 in the log). The description of this error code can be found in the Microsoft Learn . Encountered code: c0000005 : STATUS_ACCESS_VIOLATION : This means invalid memory access.","title":"Windows Manual"},{"location":"Windows/Windows%20Manual/#shells-and-terminals","text":"Official Shell Overview Using shell to manage the operating system is an essential task and therefore, we start with the terminals and shells . In the history of Windows and DOS, there were many shell or built-in script interpreters, and also numerous terminal applications. Here, we only cover the shells and terminals relevant to the most recent versions of Windows. There are two shells in Windows: command prompt (Command shell) ( cmd.exe ) very simple and limited functionality successor of COMMAND.COM For Command shell solutions/guides, check the Command shell manual . PowerShell Modern shell with many features Supports all the Windows commands (commands from the Command shell) For PowerShell solutions/guides, check the PoweShell manual . There are two terminal applications in Windows: Windows Console Host ( conhost.exe ) : an old terminal application present in Windows for years startup program is the command prompt default terminal application in Windows 10 and older no longer relevant in Windows 11 Windows Terminal ( wt.exe ) : a new terminal application that is still in development. startup program can be configured (PowerShell by default) default terminal application in Windows 11, available in the Microsoft Store in older versions of Windows modern terminal, almost Linux-like experience","title":"Shells and Terminals"},{"location":"Windows/Windows%20Manual/#run-a-non-default-terminal-application","text":"It is not an easy task to manually run a shell in a non-default terminal application. To do it, we need to execute the terminal application executable: Windows Terminal: wt.exe Windows Console Host: conhost.exe","title":"Run a non-default terminal application"},{"location":"Windows/Windows%20Manual/#windows-terminal","text":"The Windows Terminal is a new terminal application with many great features: multiple tabs smart text selection To configure Windows Terminal, click the v button in the top bar and select Settings . There are two kinds of settings: Global settings sorted in categories in the left panel Profile settings for each profile. The profiles are listed in the left panel. After selecting a profile, the settings are displayed in the right panel. Nonbasic settings are sorted in categories under the Additonal settings header.","title":"Windows Terminal"},{"location":"Windows/Windows%20Manual/#windows-terminal-pass-stdout-to-text-editor-instead-displaying-it","text":"This can happen if the output is too long or wide for the terminal. If a command outputs a text in Windows console host but passes it to a text editor in Windows Terminal, the possible cause is the font size. The default size in Windows Terminal is 12, which is much larger than the 16 in Windows Console Host (default). To fix it, change the font size in Windows Terminal to 10 which corresponds to the 16 in Windows Console Host.","title":"Windows terminal pass stdout to text editor instead displaying it"},{"location":"Windows/Windows%20Manual/#command-prompt","text":"Compared to PowerShell or Linux shells, the Command Prompt is very limited. Therefore, we describe only the most important commands. For more advanced commands, use PowerShell. To change the current directory , use the cd command. However, in the Command Prompt, this command cannot be used to change the drive. To change the drive, use the drive letter followed by a colon, e.g., command D: changes the drive to D: .","title":"Command Prompt"},{"location":"Windows/Windows%20Manual/#character-encoding","text":"By default, Windows use the local ANSI code page for the character encoding, both in Command Prompt and PowerShell. We can change this encoding in an active console window, for example, command chcp 65001 changes the encoding to UTF-8 in Command Prompt. To do a permanent change, we have to edit Windows settings: Open the Windows settings Go to Time & language -> Language & region -> Administrative language settings Click on the Change system locale... button Check the Beta: Use Unicode UTF-8 for worldwide language support checkbox","title":"Character Encoding"},{"location":"Windows/Windows%20Manual/#execution-of-programs-scripts-commands-and-similar","text":"The execution of programs, scripts, commands and other executable files is a common, yet mysterious task in almost every operating system. To prevent confusion, it is absolutely essential to know what, and under what conditions is executed. The most important lesson is that the two Windows shells, Command Prompt and PowerShell, has a different approach to execution, and also, the executables can be run outside a shell with, yet different, mechanisms. The table below ( source ) presents the basics of the Windows execution logic when we execute the program by its name : .ritz .waffle a { color: inherit; }.ritz .waffle .s0{background-color:#d9d9d9;text-align:left;font-weight:bold;color:#000000;font-family:Arial;font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s4{background-color:#f4cccc;text-align:left;color:#000000;font-family:Arial;font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s1{background-color:#d9d9d9;text-align:center;font-weight:bold;color:#000000;font-family:Arial;font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s2{background-color:#ffffff;text-align:left;font-weight:bold;color:#000000;font-family:Arial;font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;}.ritz .waffle .s3{background-color:#b7e1cd;text-align:left;color:#000000;font-family:Arial;font-size:10pt;vertical-align:bottom;white-space:nowrap;direction:ltr;padding:2px 3px 2px 3px;} Executable type Local Directory on PATH with extension without extension with extension without extension sys\u00b9 cmd pwsh\u00b2 sys\u00b9 cmd pwsh\u00b2 sys\u00b9 cmd pwsh sys\u00b9 cmd pwsh application (*.exe) yes yes no yes yes no yes yes yes yes yes yes batch file (*.bat, *.cmd) no yes no no yes no no yes yes no yes yes PowerShell script (*.ps1)\u00b3 no no no no no no no no yes no no yes Notes: The system ( CreateProcessW function) only considers native executables ( .exe files). PowerShell never search local directory when executing program by name, due to security reasons. This can be changed by adding . (current directory) to the PATH environment variable (not recommended). PowerShell scripts are not considered as executables by Command Shell, again, due to security reasons. This can be changed by modifying the PATHEXT environment variable that lists the executable file extensions considered by cmd.exe and system (not recommended). The table above cover only what can and what cannot be executed by the name. However, when duplicate names are present, the resolution order becomes important. Also, there may be various ways how to execute the same executable in the same shell. For details, check the respective sections of the manual for each execution environment: System (Out of the shell) execution Command shell execution PowerShell shell execution If we execute using the path , the execution is consistent among all three execution environments (cmd, pwsh and system). After the executable is found by the name or path, it is executed in an environment determinded by its file type association, which is by default: program: execute the program itself (there is some bootstrapping logic, but it is not important for us) batch script: cmd.exe PowerShell script: notepad.exe . It is very useful to reconfiure this association to PowerShell.","title":"Execution of Programs, Scripts, Commands and similar"},{"location":"Windows/Windows%20Manual/#system-out-of-the-shell-execution","text":"The system execution is realized by calling the CreateProcessW function. The executable path resolution is as follows: If the executable is a path, the path is evaluated. Otherwise, the name is searched in the locations below (in order): The directory of the calling process Current working directory of the calling process System directories 32-bit system directory 16-bit system directory Windows directory Directories listed in the PATH environment variable, in the order they are listed Unlike Command Shell and PowerShell, the system execution does not consider script files, only native executables. Only the .exe extension is considered if the file name is specified without the extension.","title":"System (Out of the shell) execution"},{"location":"Windows/Windows%20Manual/#keyboard-shortcuts","text":"Alt + Shift : change input language Win + Space : change keyboard input method","title":"Keyboard Shortcuts"},{"location":"Windows/Windows%20Manual/#wireless-network","text":"","title":"Wireless Network"},{"location":"Windows/Windows%20Manual/#problem-cant-connect-to-this-network","text":"Solution: Forget the connection and connect to the network manually","title":"Problem: Can't connect to this network"},{"location":"Windows/Windows%20Manual/#connect-to-a-network-manually","text":"Control Panel -> Network and Internet -> Network and Sharing Center Set up a new connection or network Manually connect to a wireless network Fill the credentials: Network name: SSID Security type: depends, try WPA2 personal Security key: password Click next Close the dialog Click the wifi icon in the taskbar and connect to the network There are various usefull comands. For most of the commands, you need to open PowerShell as admin.","title":"Connect to a Network Manually"},{"location":"Windows/Windows%20Manual/#various-commands-related-to-the-wifi","text":"","title":"Various Commands Related to the Wifi"},{"location":"Windows/Windows%20Manual/#show-all-network-profiles","text":"This command show network configurations stored on the device. netsh wlan show profile","title":"Show All Network Profiles"},{"location":"Windows/Windows%20Manual/#various-wifi-reports-in-html","text":"netsh wlan show wlanreport","title":"Various Wifi Reports in HTML"},{"location":"Windows/Windows%20Manual/#bluetooth","text":"","title":"Bluetooth"},{"location":"Windows/Windows%20Manual/#troiubleshooting","text":"","title":"Troiubleshooting"},{"location":"Windows/Windows%20Manual/#cannot-connect-to-the-device","text":"Try to remove the device and pair it with the PC again If it does not help, proceeed to the next section (even if the pairing is successfull)","title":"Cannot connect to the device"},{"location":"Windows/Windows%20Manual/#cannot-pair-with-the-device","text":"Turn off the device and unplug it from the electricity/remove batteries. Then plug it back after ~10 seconds, power it of, and try to pair with it again.","title":"Cannot pair with the device"},{"location":"Windows/Windows%20Manual/#bluetooth-command-line-tools","text":"https://bluetoothinstaller.com/bluetooth-command-line-tools Bluetooth Command Line Tools is a set off tools that enables command line interaction with blootooth services. Basic usage: discover and list available devices: btdiscovery -s","title":"Bluetooth Command Line Tools"},{"location":"Windows/Windows%20Manual/#filesytem","text":"","title":"Filesytem"},{"location":"Windows/Windows%20Manual/#standard-folder-structure","text":"In Windows, the standard folder structure is completely different for system and user instalation. Details are listed below, but the main difference is that the system instalations are stored in a single root folder for each application (similarly to Android), while the user instalations' files are distributed among multiple folders, depending on the type of the file (similarly to Linux). The standard folders can be quick-accessed by aliases written in the form %alias% . These aliases works in Windows Explorer and Command Prompt. To use them in PowerShell, we have to access them as environment variables, e.g. $env:alias .","title":"Standard folder structure"},{"location":"Windows/Windows%20Manual/#user-home-folder","text":"The user home folder is located in C:\\Users\\<username> by default. It is aliased as %userprofile% .","title":"User home folder"},{"location":"Windows/Windows%20Manual/#system-instalation-folders","text":"If an application is installed for all users, all its files are usually installed in a single folder per application. The location of the folder depends on the type of the application: C:\\Program Files : 64-bit applications C:\\Program Files (x86) : 32-bit applications If the application needs to store some data, they are usually stored in the C:\\ProgramData (aliased as %programdata% ) folder.","title":"System instalation folders"},{"location":"Windows/Windows%20Manual/#user-instalation-folders","text":"User instalations are stored in multiple folders, depending on the type of the file. All these folders are located in the user's home folder, which is C:\\Users\\<username> by default. The folders are: ~\\AppData\\Local : Program data and sometimes also executables ~\\AppData\\Local\\Promgrams : program files and executables ~\\AppData\\LocalLow : ~\\AppData\\Roaming (aliased as %appdata% ):","title":"User instalation folders"},{"location":"Windows/Windows%20Manual/#start-menu-folder","text":"The user specific shortcuts are stored in: %appdata%\\Microsoft\\Windows\\Start Menu\\Programs . The system wide shortcuts are stored in: %programdata%\\Microsoft\\Windows\\Start Menu\\Programs .","title":"Start Menu folder"},{"location":"Windows/Windows%20Manual/#read-only-files-and-folders","text":"An ancient form of file protection on Windows is the read only flag that can be set on files and folders. It is not a real protection, as it can be easily removed by the user, but it can be used to prevent accidental changes. Most of the programs can ignore this flag and work with the file anyway. However, some programs (e.g. Python) can have problems with it.","title":"Read Only Files and Folders"},{"location":"Windows/Windows%20Manual/#formatting-a-drivepartition-or-disk","text":"When we want to delete all data from a drive/partition or disk and start over, we use the procedure called formatting. There are various tools for that, with different trade-offs between capabilities and complexity. In all tools, there are two variants of formatting: Quick Format: All files are forgotten, but they data is not overwritten. Best choice most of the time but: other people can still recover the data using special software, and when bootable disk is formatted this way, the BIOS can still boot from it, confusing the user. Full Format: All files are forgotten and the data is overwritten with zeros. This is the safe option, but it can take a long time. There are several tools for formatting: properties window of the drive in File Explorer: can only format mounted drives Disk Management tool in Computer Management diskpart command line tool","title":"Formatting a drive/partition or disk"},{"location":"Windows/Windows%20Manual/#sugarsync","text":"Quick orientation in the desktop app: for file changes, check left menu -> Activity for deleted files, check left menu -> Deleted Items","title":"Sugarsync"},{"location":"Windows/Windows%20Manual/#solving-sync-problems","text":"check if the file is updated in cloud using web browser if not, check the activity log on the computer with the updated file if the change is not in the log, a simple hack can help: copy the file outside SugarSync folder and back.","title":"Solving sync problems"},{"location":"Windows/Windows%20Manual/#useful-commands","text":"","title":"Useful Commands"},{"location":"Windows/Windows%20Manual/#get-motherboard-info","text":"wmic baseboard get product,Manufacturer,version,serialnumber","title":"Get Motherboard Info"},{"location":"Windows/Windows%20Manual/#copy-multiple-filesdirs","text":"robocopy is the best command for that. Usefull params: /e : Copy subdirectories, including empty ones /b : Copy in the backup mode, so that even files with a different owner can be copied /xc : Excludes changed files. /xn : Excludes newer files. /xo : Excludes older files. /r:<n> : Specifies the number of retries on failed copies. The default value of n is 1,000,000 (one million retries). /w:<n> : Specifies the wait time between retries, in seconds. The default value of n is 30 (wait time 30 seconds).","title":"Copy multiple files/dirs"},{"location":"Windows/Windows%20Manual/#echoargs-print-arguments-as-passed-to-the-script","text":"Sometimes, it is hard to see what are the exact arguments passed to an executable. To debug it quickly, we can use the echoargs.exe tool present in Portable Programs or at ss64 . Usage: echoargs <command> <arguments>","title":"echoargs: Print arguments as passed to the script"},{"location":"Windows/Windows%20Manual/#user-info","text":"Information about users can be obtained with the Get-LocalUser command. By default, the command lists all users. Some useful params: -Name : Specifies the user account names of the users to get. -SID : Specifies the security identifier (SID) of the users to get.","title":"User info"},{"location":"Windows/Windows%20Manual/#installation","text":"Windows can be installed from bootable USB created by a tool downloaded from the official Miccosoft website . There is a single image for all Windows editions, a particular version is choosen based on the license key. The license can be purchased online either from Microsoft (be sure to buy it on CZ website, it cannot be bought on the US website) or from a retailer (e.g., Alza ). Licenses are transferable, not OEM, unless specified otherwise. The Home version is typically sufficient. Installation Steps: Get a license key Download the install tool from Microsoft Run the tool and create a bootable USB Start the installation Fill in the licence key Choose where to install Windows Complete the installation guide","title":"Installation"},{"location":"Windows/Windows%20Manual/#post-installation-steps","text":"Import the disks from previous installations.","title":"Post Installation Steps"},{"location":"Windows/Windows%20Manual/#import-disks","text":"After installation, only the main disk and newly installed disk are initialized. Other disks will be marked as foreign in the disk management tool, and the drives on them will not show up in File Explorer. To import the disks: Go to Computer Management -> Storage -> Disk Management Right click on disk marked as foreign and select Import foreign disks","title":"Import disks"},{"location":"Windows/Windows%20Manual/#installation-problems","text":"","title":"Installation Problems"},{"location":"Windows/Windows%20Manual/#we-couldnt-create-a-partition-or-locate-an-existing-one","text":"Ensure that the boot priority of the drive where the Windows should be installed is right behind the installation USB priority.","title":"We couldn\u2019t create a partition or locate an existing one"},{"location":"Windows/Windows%20Manual/#windows-installation-encountered-an-unexpected-error-0x80042444-0x4002f","text":"This error is triggered by SATA drives from a different OS installation. There are two solutions: Use the legacy Windows installer that can be started from a small link on one of the initial installation screens. disconnect the SATA drives from previous installations, either physically, or in BIOS/UEFI","title":"windows installation encountered an unexpected error 0x80042444 - 0x4002F"},{"location":"Windows/Windows%20Manual/#configuration","text":"","title":"Configuration"},{"location":"Windows/Windows%20Manual/#changing-the-input-method","text":"It is possible to let the system have a different input method for each app. It is not possible however, to remember the input method (after app/OS restart).","title":"Changing the input method"},{"location":"Windows/Windows%20Manual/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Windows/Windows%20Manual/#nothing-happens-after-clicking-on-the-input-method-in-the-taskbar-windows-10","text":"restrat the computer :)","title":"Nothing happens after clicking on the input method in the taskbar (windows 10)"},{"location":"Windows/Windows%20Manual/#right-click-menu","text":"","title":"Right Click Menu"},{"location":"Windows/Windows%20Manual/#bring-the-old-right-click-menu-back-to-windows-11","text":"There is a new right click menu in Windows 11, which is much less practical than the old one. To bring the old one back, we have to edit the registry: reg.exe add \"HKCU\\Software\\Classes\\CLSID\\{86ca1aa0-34aa-4e8b-a509-50c905bae2a2}\\InprocServer32\" /f /ve","title":"Bring the old right click menu back to Windows 11"},{"location":"Windows/Windows%20Manual/#configure-the-items-in-the-right-click-menu","text":"Unfortunatelly, the right click menu is not directly configurable in Windows. Usually, the actions are enabled by the application installation and sometimes, this can be disabled in the installation process To disable context menu entries after installation, we usually need to do one of the four things: configure the context menu in the application settings : if possible, easiest way add the item to blocked list in the registry : for applications that cannot configured the context menu in the settings uninstall the application : for applications that we do not need at all delete the item from the registry : for certain built-in Windows items To configure the context menu in the registry, we need to add a new key to the registry under the following path: HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Shell Extensions\\Blocked . Under this path, we need to add a new string value matching the context menu entry we want to disable. For example, in PowerShell, we can run: REG ADD \"HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Shell Extensions\\Blocked\" /v \"<context meny entry name>\" /d <any value, can be empty> Example for Skype: REG ADD \"HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Shell Extensions\\Blocked\" /v \"{776DBC8D-7347-478C-8D71-791E12EF49D8}\" /d Skype After editing the registry, restart the Explorer. For deleting the item from the registry, just run REG DELETE <path>","title":"Configure the items in the right click menu"},{"location":"Windows/Windows%20Manual/#context-menu-that-can-be-disabled-by-configuring-the-application-settings","text":"PowerToys modules: These can be removed by deactivating the specific modules in the PowerToys settings.","title":"Context menu that can be disabled by configuring the application settings"},{"location":"Windows/Windows%20Manual/#context-menu-items-to-be-disabled-by-editing-the-registry","text":"Context menu item Registry key Ask Copilot {CB3B0003-8088-4EDE-8769-8B354AB2FF8C} Move to OneDrive {1FA0E654-C9F2-4A1F-9800-B9A75D744B00} Restore previous version {596AB062-B4D2-4215-9F74-E9109B0A8153} Share with Skype {776DBC8D-7347-478C-8D71-791E12EF49D8}","title":"Context menu items to be disabled by editing the registry"},{"location":"Windows/Windows%20Manual/#context-menu-items-to-be-disabled-by-uninstalling-the-application","text":"Edit with Notepad","title":"Context menu items to be disabled by uninstalling the application"},{"location":"Windows/Windows%20Manual/#context-menu-items-to-be-disabled-by-deleting-the-registry-item","text":"Open with Visual Studio : HKEY_CLASSES_ROOT\\Directory\\shell\\AnyCode Scan with Microsoft Defender ( Source ): \"HKEY_CLASSES_ROOT\\*\\shellex\\ContextMenuHandlers\\EPP\" \"HKEY_CLASSES_ROOT\\CLSID\\{09A47860-11B0-4DA5-AFA5-26D86198A780}\" \"HKEY_CLASSES_ROOT\\Directory\\shellex\\ContextMenuHandlers\\EPP\" \"HKEY_CLASSES_ROOT\\Drive\\shellex\\ContextMenuHandlers\\EPP\" Send to : \"HKEY_CLASSES_ROOT\\AllFilesystemObjects\\shellex\\ContextMenuHandlers\\SendTo\" \"HKEY_CLASSES_ROOT\\UserLibraryFolder\\shellex\\ContextMenuHandlers\\SendTo\" VLC entries: \"HKEY_CLASSES_ROOT\\Directory\\shell\\AddtoPlaylistVLC\" \"HKEY_CLASSES_ROOT\\Directory\\shell\\PlayWithVLC\"","title":"Context menu items to be disabled by deleting the registry item"},{"location":"Windows/Windows%20Manual/#translate-with-deepl","text":"Haven't found a way to remove it yet. Even uninstalling the Deepl does not help.","title":"Translate with Deepl"},{"location":"Windows/Windows%20Manual/#diskpart","text":"documentation Diskpart is a useful command line tool for work with disks, partitions, etc. We start it by running diskpart command, and then, we use other commands to manage the disks. Basic general commands: list disk : list all disks exit : exit diskpart select disk <disk number> : select the disk to manage When a disk is selected, we can use other commands: list partition : list all partitions on the selected disk list volume : list all volumes on the selected disk. A volume is a mount point for a partition. select partition <partition number> : select the partition to manage","title":"Diskpart"},{"location":"Windows/Windows%20Manual/#find-out-wheteher-a-disk-is-mbr-or-gpt","text":"","title":"Find out wheteher a disk is MBR or GPT"},{"location":"Windows/Windows%20Manual/#open-command-promp-from-the-windows-10-usb","text":"Insert the USB stick Wait till the first installation screen shift + F10","title":"Open Command Promp from the Windows 10 USB"},{"location":"Windows/Windows%20Manual/#firewall","text":"","title":"Firewall"},{"location":"Windows/Windows%20Manual/#generate-firewall-logs","text":"Go to Windows firewall and select properties on the right At the top, choose the profile corresponding to the current network profile In the logging section, click to customizze set both DROP and ACCEPT to yes Do not forgot to turn of the logging after the investigation!","title":"Generate firewall logs"},{"location":"Windows/Windows%20Manual/#ssh","text":"For ssh, we can use the standard ssh commannd available in Windows (check Linux manual for more info). If the command is not available, it can be installed in Apps & Features -> Optional features . One thing that differs from Linux is that the Windows ssh does not support the <addres>:<port> syntax. To specify the port, it is necessary to use the -p parameter For more features, we can use other programs KiTTY for credentials storage, automatic reconection, etc. WinSCP for file manipulation","title":"SSH"},{"location":"Windows/Windows%20Manual/#kitty","text":"It is best to use the portable version, so that nothing is stored in the Windows registry. Configurtation: copy the PuTTY credentials : .\\kitty_portable-0.76.1.3.exe -convert-dir auto reconnect: Connection -> auto reconnect on connection failure and auto reconnect on system wakeup","title":"KiTTY"},{"location":"Windows/Windows%20Manual/#winscp","text":"WinSCP is a graphical tool for file manipulation. Ii can be used both for local and remote files, and it supports various protocols (FTP, SFTP, SCP, WebDAV, etc.).","title":"WinSCP"},{"location":"Windows/Windows%20Manual/#adding-a-new-connection","text":"There is a simple New Site button on the left, which opens a straightforward dialog. The only complicated thing can be the SSH key. To add it, click on the Advanced button and go to the SSH -> Authentication tab. There, we can select the private key file.","title":"Adding a new connection"},{"location":"Windows/Windows%20Manual/#bookmarks","text":"To add bookmarks, go to Local / Remote -> Add Path to Bookmarks or press Ctrl + B . To open a bookmark, go to Local / Remote -> Go To -> Open Drirectory/bookmark or press Ctrl + O .","title":"Bookmarks"},{"location":"Windows/Windows%20Manual/#ssh-key-agent","text":"To enable the ssh-agent on Windows, one extra step is needed: we need to start the ssh-agent service. To do that, open the services manager and start the OpenSSH Authentication Agent service.","title":"SSH key agent"},{"location":"Windows/Windows%20Manual/#git","text":"Most of the git functionality is the same as in Linux, so check the Linux manual for more info. However, there are some important differences mostly resulting from the fact that Git is not a native Windows application, but it runs in MinGW.","title":"Git"},{"location":"Windows/Windows%20Manual/#git-on-windows-and-ssh","text":"As the git on Windows runs in MinGW, it does not use the Windows SSH command. That can be problematic if we want to debug the SSH connection using the env variable or configuring an ssh key agent. To force git to use the Windows SSH, we need to set the sshCommand config variable to the path to the Windows SSH: git config --global core.sshCommand C:/Windows/System32/OpenSSH/ssh.exe","title":"Git on Windows and SSH"},{"location":"Windows/Windows%20Manual/#vpn","text":"VPN is natively supported in Windows. It can be set up in two ways: using a system dialog in PowerShell If the system dialog does not work, try the PowerShell method.","title":"VPN"},{"location":"Windows/Windows%20Manual/#wsl","text":"Windows Subsystem for Linux (WSL) is basically a virtual machine running Linux. To install WSL , run wsl --install command. To list installed WSL distributions , run wsl --list To list available WSL distributions , run wsl --list --online To install a WSL distribution , run wsl --install -d <distribution name> Finally, to remove a WSL distribution , run wsl --unregister <distribution name>","title":"WSL"},{"location":"Windows/Windows%20Manual/#translate-windows-path-to-linux-path","text":"To translate a Windows path to a Linux path, we can use the wslpath command, which is available in Ubuntu WSL distributions.","title":"Translate Windows Path to Linux Path"},{"location":"Windows/Windows%20Manual/#execute-commands-from-the-windows","text":"We can directly execute commands in the WSL from the Windows command line without logging into the WSL with the wsl command. The syntax is: wsl <command> Important arguments: -u <user>, --user <user> : run the command as the specified user. This can be useful for running the command as the root user. --shutdown : shutdown the WSL after the command is executed.","title":"Execute commands from the Windows"},{"location":"Windows/Windows%20Manual/#dual-boot","text":"","title":"Dual Boot"},{"location":"Windows/Windows%20Manual/#make-windows-work-after-linux-uninstall-if-the-bootloader-is-grub","text":"Get rid of the Grub bootloader Set the Windows bootloader as the primary partition it is the small partition at the beginning of the main disk","title":"Make Windows Work after Linux Uninstall if the Bootloader is Grub"},{"location":"Windows/Windows%20Manual/#shims","text":"Shims are small programs that are used to replace the original program, typically to run it with some additional parameters. On Windows, the shims can be easilly integrated into the system by adding the folder with the shims to the beginning of the PATH environment variable. As a shim, we can use: any executable file ( *.exe ) a batch file ( *.bat , *.cmd ) a PowerShell script ( *.ps1 ) note that the PowerShell script shims works only for PowerShell. To make it work outside PowerShell, we need to accommpany it with a batch file that runs the PowerShell script: batch @echo off powershell -ExecutionPolicy Bypass -File \"%~dp0\\script.ps1\" %*","title":"Shims"},{"location":"Windows/Windows%20Manual/#notepad","text":"","title":"Notepad++"},{"location":"Windows/Windows%20Manual/#configuration_1","text":"Usually, it is a good idea to configure autosave. This functionality is available in a plugin called Autosave.","title":"Configuration"},{"location":"Windows/Windows%20Manual/#problems","text":"","title":"Problems"},{"location":"Windows/Windows%20Manual/#folder-sharing-problems","text":"Note that updated Windows 10 disabled anonymous sharing , so password protected sharing has to be turned on. To login, use the credentials for the computer with the shared folder . Below is a list of possible problems, together with solutions.","title":"Folder Sharing Problems"},{"location":"Windows/Windows%20Manual/#the-user-name-or-password-is-incorrect","text":"Check whether the computer can be seen in the network. If not, resolve this issue first. quick check by running net view <IP address> Check that you are using the right username. You need to use the username and password of the computer you are connecting to . Check that the user name is correct by running net user on the target computer Check that the folder is shared with you in: right click on the folder -> Properties -> Sharing -> Advanced Sharing... -> Permisions . Note that your full name can be there instead of your username, which is OK. Check that you are using the right password. You have to use the password associated with your microsoft account. Note that it can differ from the password (PIN) you are using to log in to the computer! check it on the command line: net use * \\\\<IP address>\\<drive letter>$ /use:<username> <password>","title":"The user name or password is incorrect"},{"location":"Windows/Windows%20Manual/#folder-right-and-ownership-cannot-be-read","text":"Try to clear the windows filecache (CCcleaner or restart)","title":"Folder right and ownership cannot be read"},{"location":"Windows/Windows%20Manual/#computer-does-not-see-itself-in-network-section-in-file-explorer","text":"Solution to this problem is to restart the service called Function Discovery Resource Publication . Either restart it in Computer Management -> Services, or by: net stop FDResPub net start FDResPub","title":"Computer does not see itself in Network section in File Explorer"},{"location":"Windows/Windows%20Manual/#pc-wakes-up-or-cannot-enter-sleep","text":"","title":"PC wakes up or cannot enter sleep"},{"location":"Windows/Windows%20Manual/#1-find-the-source","text":"Using the Event viwer open the event viewer go to windows logs -> system In case of wake up inspect the logs when the wake up happened and search for the Information log with the message \"The system has returned from a low power state.\" There is a wake up source in the end of the log message. If the soure is Unknown go to the command line section In case of not entering sleep Search for the any kernel power event If there is an event stating: The system is entering connected standby , it means that the modern fake sleep is present in the system, replacing the real sleep mode. Using command line (admin): Try powercfg -lastwake If the results are not know, try to call powercfg -devicequery wake_armed to get the list of devices that can wake the computer","title":"1 Find the source"},{"location":"Windows/Windows%20Manual/#2-solve-the-problem","text":"","title":"2 Solve the problem"},{"location":"Windows/Windows%20Manual/#device-waken-up-by-network-adapter","text":"Open device manager and search for the specific network adapter right click -> Properties -> Power Management Check Only allow a magic packet to wake up the computer","title":"Device waken up by network adapter"},{"location":"Windows/Windows%20Manual/#the-real-sleep-mode-is-not-available-on-the-system","text":"If this is the case, use the hibernate mode instead. To add it to the start menu: go to Control panel -> Hardware and sound -> Power options click on the left panel to Choose what the power buttons does click on Change settings that are currently unavailable check the hibernate checkbox below","title":"The real sleep mode is not available on the system"},{"location":"Windows/Windows%20Manual/#camera-problem","text":"Symptoms: the screen is blank, black, single color, in all apps and there are no problems reported in device manager Cause: it can be caused by some external cameras (now disconnected) that are still selected in the apps using the camera. Go Solution: Go to the app setting and select the correct camera","title":"Camera problem"},{"location":"Windows/Windows%20Manual/#phone-app-cannot-see-the-connected-cell-phone","text":"It can be due to the fucked up Windows N edition. Just install the normal edition.","title":"Phone app cannot see the connected cell phone"},{"location":"Windows/Windows%20Manual/#vmmem-process-uses-a-lot-of-cpu","text":"This process represents all virtual systems. One cultprit is therefore WSL. Try to shutdown the WSL using wsl --shutdown","title":"vmmem process uses a lot of CPU"},{"location":"Windows/Windows%20Manual/#computer-restarts-without-user-intervention","text":"The first thing is to go to the event viewer and check the error eventlogs. If the Dump file generation event is in the log, we can further explore the Dump file using the WinDbg tool. To analyze the dump: run WinDbg as administrator File -> Open Dump File and select the dump file (stored in the C:\\Windows\\Minidump folder) After opening the dump file, run !analyze -v to analyze the dump (or click to the link in the main window) The most Important information in the log is the BUGCHECK_CODE . The meaning of each code is documented in the Microsoft Learn . Encountered codes: 0x00000050 : PAGE_FAULT_IN_NONPAGED_AREA : invalid memory pointer (nonexisting or freed memory access) 0x0000003B : SYSTEM_SERVICE_EXCEPTION : Exeption during a routine for transitioning between user and kernel mode. This is a very nonspecific error, to see what is really going on, we have to inspect the firts parameter of the exception ( BUGCHECK_P1 in the log). The description of this error code can be found in the Microsoft Learn . Encountered code: c0000005 : STATUS_ACCESS_VIOLATION : This means invalid memory access.","title":"Computer Restarts without User Intervention"}]}