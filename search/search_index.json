{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the manuals pages!","title":"index"},{"location":"Arxiv/","text":"Submiting a paper to Arxiv \u00b6 Before submitting a preprint to arxiv, you should check whether the journal allows it, and if so, what version of the paper can be submitted to arxiv. Links: Taylor & Francis Steps on arxiv: First page: details Select arxive as the license type Second page: files Papers written in latex have to be uploaded as source files Third page: check the printed pdf check that the pdf looks good. Check for missing references ( ? ), or figures. Fourth page: Metadata keep authors in the LaTeX encoding, this is the way how arxiv wants it Uploading LaTeX source to Arxiv \u00b6 The procedure should be as follows: delete any images not used in the paper check that the pdf looks good commit and push the changes to master branch delete all journal classes, style files, etc. write a title footnote about how the article is a preprint/accepted manuscript example: This is an Accepted Manuscript of an article published by Taylor & Francis in the Journal of Intelligent Transportation Systems on 4th December 2022, available online: https://www.tandfonline.com/doi/abs/10.1080/15472450.2022.2121651 compile the paper till no errors are present pack source files into a zip archive do not include any temporary files include the compiled bibliography file ( .bbl ) create a new git branch commit to the new branch, push it to the remote Creating new version of a paper \u00b6 In the menu, click Replace . Then, the process is the same as the new submission.","title":"Arxiv"},{"location":"Arxiv/#submiting-a-paper-to-arxiv","text":"Before submitting a preprint to arxiv, you should check whether the journal allows it, and if so, what version of the paper can be submitted to arxiv. Links: Taylor & Francis Steps on arxiv: First page: details Select arxive as the license type Second page: files Papers written in latex have to be uploaded as source files Third page: check the printed pdf check that the pdf looks good. Check for missing references ( ? ), or figures. Fourth page: Metadata keep authors in the LaTeX encoding, this is the way how arxiv wants it","title":"Submiting a paper to Arxiv"},{"location":"Arxiv/#uploading-latex-source-to-arxiv","text":"The procedure should be as follows: delete any images not used in the paper check that the pdf looks good commit and push the changes to master branch delete all journal classes, style files, etc. write a title footnote about how the article is a preprint/accepted manuscript example: This is an Accepted Manuscript of an article published by Taylor & Francis in the Journal of Intelligent Transportation Systems on 4th December 2022, available online: https://www.tandfonline.com/doi/abs/10.1080/15472450.2022.2121651 compile the paper till no errors are present pack source files into a zip archive do not include any temporary files include the compiled bibliography file ( .bbl ) create a new git branch commit to the new branch, push it to the remote","title":"Uploading LaTeX source to Arxiv"},{"location":"Arxiv/#creating-new-version-of-a-paper","text":"In the menu, click Replace . Then, the process is the same as the new submission.","title":"Creating new version of a paper"},{"location":"Markdown/","text":"Available elements with syntax Linter VSCode Linter specifics CommonMark specification Elements \u00b6 Links \u00b6 Links can be added to a markdown file by using the following syntax: [Link text](http://example.com) There are also simple links where the URL is used as the link text, these are called autolinks <http://example.com> Images \u00b6 Images can be added to a markdown file by using the following syntax: ![Alt text](/path/to/image.png) Standardization \u00b6 The only standard for markdown is the CommonMark specification . However, this specification was created ten years after the original markdown was introduced and therefore there are many implementations of markdown does not follow the CommonMark specification. The following implementations follow the CommonMark specification: Markdown-it The following implementations do not follow the CommonMark specification: Python-Markdown Linter \u00b6 There is a linter for markdown available also as a VSCode extension . The linter can be configured: in the .markdownlint.jsonc or markdownlint.yaml file in the root of the project a file with the same name in the home directory in the settings of the VSCode extension ( markdownlint.config ) By default, the linter will check more than 50 rules. Not all of them are based on the CommonMark specification and therefore it may be useful to disable some of them. We can do this by adding the following to the configuration file: { \"MD013\": false }","title":"Markdown"},{"location":"Markdown/#elements","text":"","title":"Elements"},{"location":"Markdown/#links","text":"Links can be added to a markdown file by using the following syntax: [Link text](http://example.com) There are also simple links where the URL is used as the link text, these are called autolinks <http://example.com>","title":"Links"},{"location":"Markdown/#images","text":"Images can be added to a markdown file by using the following syntax: ![Alt text](/path/to/image.png)","title":"Images"},{"location":"Markdown/#standardization","text":"The only standard for markdown is the CommonMark specification . However, this specification was created ten years after the original markdown was introduced and therefore there are many implementations of markdown does not follow the CommonMark specification. The following implementations follow the CommonMark specification: Markdown-it The following implementations do not follow the CommonMark specification: Python-Markdown","title":"Standardization"},{"location":"Markdown/#linter","text":"There is a linter for markdown available also as a VSCode extension . The linter can be configured: in the .markdownlint.jsonc or markdownlint.yaml file in the root of the project a file with the same name in the home directory in the settings of the VSCode extension ( markdownlint.config ) By default, the linter will check more than 50 rules. Not all of them are based on the CommonMark specification and therefore it may be useful to disable some of them. We can do this by adding the following to the configuration file: { \"MD013\": false }","title":"Linter"},{"location":"Network/","text":"Setup second router as a switch \u00b6 Tools \u00b6 Router 1 - master - connected to the internet Router 2 - Slave Laptop Setup Router 1 \u00b6 In the wireless security settings of the router, disable Automatic Channel selection and manually set the channel some channel. add an IP address reservation for router 2 to routers 1 setting, e.g. 192.168.0.2 Setup Router 2 \u00b6 On the laptop, log off from wifi and connect the cable to router 2 Disable the DHCP server on this router to prevent IP conflicts or network configuration issues allowing only Router 1 to manage the network. Restart the router Set the IPv4 address on the laptop manually for both laptop and gateway, the gateway has to be the fixed address of the router 2 (e.g. 192.168.0.1) Manually set the IP Address of this router to the IP reserved in step 2. Set the internet connection to static IP. Restart the router Set the IP on the laptop again (to the new address) if the network does not work Connect the two routers using a cable from any of LAN port in router 1 to any LAN port in router 2. In the wireless security settings of this router, disable Automatic Channel selection and manually set the channel to channel a different channel than the one set in step 1 Set up wireless security to be identical in router 2 as it is in router 1.","title":"Network"},{"location":"Network/#setup-second-router-as-a-switch","text":"","title":"Setup second router as a switch"},{"location":"Network/#tools","text":"Router 1 - master - connected to the internet Router 2 - Slave Laptop","title":"Tools"},{"location":"Network/#setup-router-1","text":"In the wireless security settings of the router, disable Automatic Channel selection and manually set the channel some channel. add an IP address reservation for router 2 to routers 1 setting, e.g. 192.168.0.2","title":"Setup Router 1"},{"location":"Network/#setup-router-2","text":"On the laptop, log off from wifi and connect the cable to router 2 Disable the DHCP server on this router to prevent IP conflicts or network configuration issues allowing only Router 1 to manage the network. Restart the router Set the IPv4 address on the laptop manually for both laptop and gateway, the gateway has to be the fixed address of the router 2 (e.g. 192.168.0.1) Manually set the IP Address of this router to the IP reserved in step 2. Set the internet connection to static IP. Restart the router Set the IP on the laptop again (to the new address) if the network does not work Connect the two routers using a cable from any of LAN port in router 1 to any LAN port in router 2. In the wireless security settings of this router, disable Automatic Channel selection and manually set the channel to channel a different channel than the one set in step 1 Set up wireless security to be identical in router 2 as it is in router 1.","title":"Setup Router 2"},{"location":"Security/","text":"This manual is focused on asymmetric encryption . As the principles are more or less the same for all technologies, we will focus on the technical aspects that differs. SSH \u00b6 The SSH is a protocol for secure communication between two computers. The basic usage is to connect to a remote computer: ssh <username>@<address> To close the connection, type exit to the console and press enter. If we do not want to establish a connection, but just run a single command, we can add the command at the end of the ssh command: ssh <username>@<address> <command> Authentication \u00b6 There are two ways to authenticate to the server: password private key Note that the server needs to be properly configured to accept your credentials. Specifically: for password authentication, the PasswordAuthentication option in /etc/ssh/sshd_config has to be set to yes . This is often disabled for security reasons. for private key authentication, the public key has to be added to your user account on the server. Generating a key pair \u00b6 To generate a key pair, use the ssh-keygen command. Setting up the private key to be used for ssh connection \u00b6 To use a private key for ssh connection, two conditions have to be met: the private key has to have the right permissions: in Linux, the permissions have to be read/write only for the owner ( 600 ) you have to specify that the private key should be used for the connection. This can be done in multiple ways: using the -i parameter of the ssh command specifying the key in the ~/.ssh/config file using ssh agent (see the SSH Agent section below) selecting the key in an application with GUI SSH Tunneling \u00b6 An SSH tunnel can be created by the ssh command. The usuall syntax is following: ssh -L <local port>:<remote machine>:<remote port> <ssh server username>@<ssh server address> The -L argument stands for local port forwarding , i.e., we forward a local port to some remote port. Example: ssh -L 1111:localhost:5432 fiedler@its.fel.cvut.cz The local port (here 1111 ) is arbitrary, we can use any free port. The aplications using the tunnel should then be configured as: host= localhost port:= 1111 The remote machine is the address of the machine we want to access relative to the ssh server . If the ssh server is running on the computer we want to access through the tunnel, we can use localhost . Analogously, the remote port is the port we wan to use on the remote machine (here 5432 is a PostgreSQL db port). The ssh server username and ssh server address are then the username/address of the remote machine. On top of that, we need password or priveta key to validate our identity. Note that the credential here are the credential for the server, not the credentials of the service we are accessing through the ssh connection. Those credentials has to be usually supplied by the application accessing the service through the ssh tunnel. The connection can be canceled any time byping exit to the console. More information Debugging a SSH tunnel \u00b6 This guide suppose that the tunnel creation comman run without any error message. If the tunnel seems to not work, first use a command line tool to be sure: web browser for HTTP tunnels (remote port 80) psql for postgeSQL tunnels (remote port 5432) telnet for telnet tunnels (reote port 23) If the test is unsucessful, try to kill all ssh connections to the server by shutting down all applications with ssh connections tunnels, untill there will be only one connection at the server (the console). The number of ssh connections can be checked with: sudo netstat -nap | grep :22 Enabnling SSH Access on Server \u00b6 install openssh: sudo apt update sudo apt install openssh-server configure access password: open /etc/ssh/sshd_config set PasswordAuothentication yes after restrat you can log in with the user and password used in Ubuntu keys: TODO restart the ssh server: sudo service ssh restart Enabling key authentication for a user \u00b6 The user can only use a private key for authentication if the corresponding public key is assigned to the user on server. This is done by adding the public key to the ~/.ssh/authorized_keys file. Note that the authorized_keys file has to have the right permissions , which is read/write only for the owner, and read only for the group and others ( 644 ). WSL configuration \u00b6 port 22 can be used on Windows, so change the port to 2222 in sshd_config when loging from Windows use 127.0.0.1 as a host SSH Agent \u00b6 Normally, the SSH client process runs only while the SSH session is active, then, it is terminated. That means that we have to reenter the passphrase for the private key every time we want to connect to the server. To overcome this limitation, we can use the SSH agent programe. An SSH agent is a process for storing decrypted SSH keys in memory. This means that we have to enter the passphrase only once per each OS login. The agent can be configured to automatically run on OS startup. The default SSH agent is ssh-agent , the rest of the section is dedicated to this agent. To successfully use the agent, we need to: start the agent, either manually or automatically on OS startup add the keys to the agent (only once) Starting the agent \u00b6 The agent can be started manually by running: eval `ssh-agent` We need to evaluate this command as it sets some environment variables. As the process cannot set the environment variables of the parent process due to security reasons, the ssh-agent prints the necessary commands to the console. By using eval, the ssh-agent is executed first, it prints the environment setup commands to stdout, which is captured by the eval command and executed. Listing keys \u00b6 To list the keys stored in the agent, run: ssh-add -l Adding keys \u00b6 To add a key to the agent, run: ssh-add <path to key> Debuging \u00b6 If the agent is running and the key is listed, the first thing to try is to connect via ssh to see whether it is an agent/ssh issue or an issue of the program using the SSH (like git, IDE, file manager...) Configuration \u00b6 For configuration, we can use the git config command. There are three levels of configuration: system : the configuration is applied to all users on the system. This configuration is set during the installation of git. global : the configuration is applied to all repositories of the current user. This configuration is set by the --global parameter. local : the configuration is applied only to the current repository. This configuration is set by the --local parameter. To list the configuration, use the -l / --list parameter of the git config command. To list the configuration for a specific level, use the --system , --global , --local parameters. To see the default value of a configuration, search in the git config documentation . known_hosts file \u00b6 To know that a connection leads to the desired server and not to some impersonator, the server sends its public key to the client. The client then checks the public key against the list of keys previously set as valid. This list is stored in the .ssh/known_hosts file. The format of the file is: <server address> <key type> <key> Each line contains one server record. What is confusing here is that each server can have multiple records, due to: different key type (e.g., RSA, ECDSA) key for both host name and IP address (e.g., github.com and 140.82.121.3 ) It is important to delete/updete all of these recorsds in case the server change its keys. More info is at this SO answer . Screen: executing a long running process over SSH \u00b6 When the SSH connection to a server is disconnected (either manually, or by network failure or timeout), the process running in the console is canceled. To overcome this limitation, we can use the screen command, which is especially usefull for long running processes. A typical workflow can look like this: execute screen to start the screen session run the long running command disconnect connect to the server again run screen -r to recconect to the session and see the results of the command. after the command is finished, exit the screen session with exit Sometimes, the server does not detect the connection failure and do not allow you to resume the session (step 5). In this way, we need to find the screen session ID and perform a detach and atach: screen -ls read the ID from the output and exec screen -rd <ID> Copying files over SSH using scp \u00b6 The scp command is used to copy files over SSH. The syntax is: scp <source> <destination> The <source> and <destination> can be either local or remote. The remote files are specified using the <username>@<address>:<path> syntax. Note that if the remote path contains spaces, double quoting is necessary , one for local and one for remote: scp <source> \"<username>@<address>:'<path with spaces>'\" Problems \u00b6 protocol error: filename does not match request : This error is triggered if the path contains unexpected characters. Sometimes, it can be triggered even for correct path, if the local console does not match the remote console. In that case, the solution is to use the -T parameter to disable the security check. List all active connections on a server \u00b6 To list all active connections on a server, we can use the lsof command and filter the output for the ssh connections: lsof -i -n | grep ssh WSL configuration \u00b6 port 22 can be used on Windows, so change port to 2222 in sshd_config when loging from Windows use 127.0.0.1 as a host Debugging \u00b6 If the server does not respond: check the ssh status with: service ssh status check the ssh port with sudo netstat -tpln If the key is not accepted: Check the log file: sudo tail -c 5000 /var/log/auth.log GNU Privacy Guard (GPG) \u00b6 GPG is a second most popular tool for encryption, after the SSH keys. Apart from encryption capabilities, it offers also a management of subkeys, and a possibility to revoke the key. It can be downloaded from the GNU-PG website . GPG comes with a build in key agent. To list the keys added to the agent, use gpg --list-keys . To import a key, call gpg --import <keyfile> . Key expiration \u00b6 There is a mechanism for key expiration in GPG. However, it is important to understand that the expiration date is mostly not a security feature! It can be useful in the following cases: you lose access to the key, and nobody else can access it as well. In that case, you cannot revoke the key, but you can just wait until the key expires. you set the expiration date for subkeys. For subkeys, the expiration date is a security feature, as it cannot be changed without the main key. To prolong the expiration date, we can use the gpg --edit-key <key-id> command. After using it: choose the key you want to edit by number now the chosen key should be marked with an asterisk * . Enter expire choose the new expiration date save the changes by save Troubleshooting \u00b6 partial length invalid for packet type 63 \u00b6 This can happen if the private key has a wrong encoding. It can be fixed by cnverting the key file to ASCII encoding.","title":"Security"},{"location":"Security/#ssh","text":"The SSH is a protocol for secure communication between two computers. The basic usage is to connect to a remote computer: ssh <username>@<address> To close the connection, type exit to the console and press enter. If we do not want to establish a connection, but just run a single command, we can add the command at the end of the ssh command: ssh <username>@<address> <command>","title":"SSH"},{"location":"Security/#authentication","text":"There are two ways to authenticate to the server: password private key Note that the server needs to be properly configured to accept your credentials. Specifically: for password authentication, the PasswordAuthentication option in /etc/ssh/sshd_config has to be set to yes . This is often disabled for security reasons. for private key authentication, the public key has to be added to your user account on the server.","title":"Authentication"},{"location":"Security/#generating-a-key-pair","text":"To generate a key pair, use the ssh-keygen command.","title":"Generating a key pair"},{"location":"Security/#setting-up-the-private-key-to-be-used-for-ssh-connection","text":"To use a private key for ssh connection, two conditions have to be met: the private key has to have the right permissions: in Linux, the permissions have to be read/write only for the owner ( 600 ) you have to specify that the private key should be used for the connection. This can be done in multiple ways: using the -i parameter of the ssh command specifying the key in the ~/.ssh/config file using ssh agent (see the SSH Agent section below) selecting the key in an application with GUI","title":"Setting up the private key to be used for ssh connection"},{"location":"Security/#ssh-tunneling","text":"An SSH tunnel can be created by the ssh command. The usuall syntax is following: ssh -L <local port>:<remote machine>:<remote port> <ssh server username>@<ssh server address> The -L argument stands for local port forwarding , i.e., we forward a local port to some remote port. Example: ssh -L 1111:localhost:5432 fiedler@its.fel.cvut.cz The local port (here 1111 ) is arbitrary, we can use any free port. The aplications using the tunnel should then be configured as: host= localhost port:= 1111 The remote machine is the address of the machine we want to access relative to the ssh server . If the ssh server is running on the computer we want to access through the tunnel, we can use localhost . Analogously, the remote port is the port we wan to use on the remote machine (here 5432 is a PostgreSQL db port). The ssh server username and ssh server address are then the username/address of the remote machine. On top of that, we need password or priveta key to validate our identity. Note that the credential here are the credential for the server, not the credentials of the service we are accessing through the ssh connection. Those credentials has to be usually supplied by the application accessing the service through the ssh tunnel. The connection can be canceled any time byping exit to the console. More information","title":"SSH Tunneling"},{"location":"Security/#debugging-a-ssh-tunnel","text":"This guide suppose that the tunnel creation comman run without any error message. If the tunnel seems to not work, first use a command line tool to be sure: web browser for HTTP tunnels (remote port 80) psql for postgeSQL tunnels (remote port 5432) telnet for telnet tunnels (reote port 23) If the test is unsucessful, try to kill all ssh connections to the server by shutting down all applications with ssh connections tunnels, untill there will be only one connection at the server (the console). The number of ssh connections can be checked with: sudo netstat -nap | grep :22","title":"Debugging a SSH tunnel"},{"location":"Security/#enabnling-ssh-access-on-server","text":"install openssh: sudo apt update sudo apt install openssh-server configure access password: open /etc/ssh/sshd_config set PasswordAuothentication yes after restrat you can log in with the user and password used in Ubuntu keys: TODO restart the ssh server: sudo service ssh restart","title":"Enabnling SSH Access on Server"},{"location":"Security/#enabling-key-authentication-for-a-user","text":"The user can only use a private key for authentication if the corresponding public key is assigned to the user on server. This is done by adding the public key to the ~/.ssh/authorized_keys file. Note that the authorized_keys file has to have the right permissions , which is read/write only for the owner, and read only for the group and others ( 644 ).","title":"Enabling key authentication for a user"},{"location":"Security/#wsl-configuration","text":"port 22 can be used on Windows, so change the port to 2222 in sshd_config when loging from Windows use 127.0.0.1 as a host","title":"WSL configuration"},{"location":"Security/#ssh-agent","text":"Normally, the SSH client process runs only while the SSH session is active, then, it is terminated. That means that we have to reenter the passphrase for the private key every time we want to connect to the server. To overcome this limitation, we can use the SSH agent programe. An SSH agent is a process for storing decrypted SSH keys in memory. This means that we have to enter the passphrase only once per each OS login. The agent can be configured to automatically run on OS startup. The default SSH agent is ssh-agent , the rest of the section is dedicated to this agent. To successfully use the agent, we need to: start the agent, either manually or automatically on OS startup add the keys to the agent (only once)","title":"SSH Agent"},{"location":"Security/#starting-the-agent","text":"The agent can be started manually by running: eval `ssh-agent` We need to evaluate this command as it sets some environment variables. As the process cannot set the environment variables of the parent process due to security reasons, the ssh-agent prints the necessary commands to the console. By using eval, the ssh-agent is executed first, it prints the environment setup commands to stdout, which is captured by the eval command and executed.","title":"Starting the agent"},{"location":"Security/#listing-keys","text":"To list the keys stored in the agent, run: ssh-add -l","title":"Listing keys"},{"location":"Security/#adding-keys","text":"To add a key to the agent, run: ssh-add <path to key>","title":"Adding keys"},{"location":"Security/#debuging","text":"If the agent is running and the key is listed, the first thing to try is to connect via ssh to see whether it is an agent/ssh issue or an issue of the program using the SSH (like git, IDE, file manager...)","title":"Debuging"},{"location":"Security/#configuration","text":"For configuration, we can use the git config command. There are three levels of configuration: system : the configuration is applied to all users on the system. This configuration is set during the installation of git. global : the configuration is applied to all repositories of the current user. This configuration is set by the --global parameter. local : the configuration is applied only to the current repository. This configuration is set by the --local parameter. To list the configuration, use the -l / --list parameter of the git config command. To list the configuration for a specific level, use the --system , --global , --local parameters. To see the default value of a configuration, search in the git config documentation .","title":"Configuration"},{"location":"Security/#known_hosts-file","text":"To know that a connection leads to the desired server and not to some impersonator, the server sends its public key to the client. The client then checks the public key against the list of keys previously set as valid. This list is stored in the .ssh/known_hosts file. The format of the file is: <server address> <key type> <key> Each line contains one server record. What is confusing here is that each server can have multiple records, due to: different key type (e.g., RSA, ECDSA) key for both host name and IP address (e.g., github.com and 140.82.121.3 ) It is important to delete/updete all of these recorsds in case the server change its keys. More info is at this SO answer .","title":"known_hosts file"},{"location":"Security/#screen-executing-a-long-running-process-over-ssh","text":"When the SSH connection to a server is disconnected (either manually, or by network failure or timeout), the process running in the console is canceled. To overcome this limitation, we can use the screen command, which is especially usefull for long running processes. A typical workflow can look like this: execute screen to start the screen session run the long running command disconnect connect to the server again run screen -r to recconect to the session and see the results of the command. after the command is finished, exit the screen session with exit Sometimes, the server does not detect the connection failure and do not allow you to resume the session (step 5). In this way, we need to find the screen session ID and perform a detach and atach: screen -ls read the ID from the output and exec screen -rd <ID>","title":"Screen: executing a long running process over SSH"},{"location":"Security/#copying-files-over-ssh-using-scp","text":"The scp command is used to copy files over SSH. The syntax is: scp <source> <destination> The <source> and <destination> can be either local or remote. The remote files are specified using the <username>@<address>:<path> syntax. Note that if the remote path contains spaces, double quoting is necessary , one for local and one for remote: scp <source> \"<username>@<address>:'<path with spaces>'\"","title":"Copying files over SSH using scp"},{"location":"Security/#problems","text":"protocol error: filename does not match request : This error is triggered if the path contains unexpected characters. Sometimes, it can be triggered even for correct path, if the local console does not match the remote console. In that case, the solution is to use the -T parameter to disable the security check.","title":"Problems"},{"location":"Security/#list-all-active-connections-on-a-server","text":"To list all active connections on a server, we can use the lsof command and filter the output for the ssh connections: lsof -i -n | grep ssh","title":"List all active connections on a server"},{"location":"Security/#wsl-configuration_1","text":"port 22 can be used on Windows, so change port to 2222 in sshd_config when loging from Windows use 127.0.0.1 as a host","title":"WSL configuration"},{"location":"Security/#debugging","text":"If the server does not respond: check the ssh status with: service ssh status check the ssh port with sudo netstat -tpln If the key is not accepted: Check the log file: sudo tail -c 5000 /var/log/auth.log","title":"Debugging"},{"location":"Security/#gnu-privacy-guard-gpg","text":"GPG is a second most popular tool for encryption, after the SSH keys. Apart from encryption capabilities, it offers also a management of subkeys, and a possibility to revoke the key. It can be downloaded from the GNU-PG website . GPG comes with a build in key agent. To list the keys added to the agent, use gpg --list-keys . To import a key, call gpg --import <keyfile> .","title":"GNU Privacy Guard (GPG)"},{"location":"Security/#key-expiration","text":"There is a mechanism for key expiration in GPG. However, it is important to understand that the expiration date is mostly not a security feature! It can be useful in the following cases: you lose access to the key, and nobody else can access it as well. In that case, you cannot revoke the key, but you can just wait until the key expires. you set the expiration date for subkeys. For subkeys, the expiration date is a security feature, as it cannot be changed without the main key. To prolong the expiration date, we can use the gpg --edit-key <key-id> command. After using it: choose the key you want to edit by number now the chosen key should be marked with an asterisk * . Enter expire choose the new expiration date save the changes by save","title":"Key expiration"},{"location":"Security/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Security/#partial-length-invalid-for-packet-type-63","text":"This can happen if the private key has a wrong encoding. It can be fixed by cnverting the key file to ASCII encoding.","title":"partial length invalid for packet type 63"},{"location":"Web%20Tricks/","text":"Download all photos from rajce.cz \u00b6 $(\"#photoList img\").each(function () {var a = $(this).closest(\"a\"); if (a.length){$(document.body).append(\"<img src='\" + a.attr(\"href\") + \"' \\/>\");}});$(\"body>*:not(img)\").remove();","title":"Web Tricks"},{"location":"Web%20Tricks/#download-all-photos-from-rajcecz","text":"$(\"#photoList img\").each(function () {var a = $(this).closest(\"a\"); if (a.length){$(document.body).append(\"<img src='\" + a.attr(\"href\") + \"' \\/>\");}});$(\"body>*:not(img)\").remove();","title":"Download all photos from rajce.cz"},{"location":"vscode/","text":"Show special characters \u00b6 To show whitespace characters, click on View > Apppearance > Render Whitespace . Line endings cannot be shown in the editor, but the status bar shows the line ending of the current file, and by clicking on it, you can change it.","title":"vscode"},{"location":"vscode/#show-special-characters","text":"To show whitespace characters, click on View > Apppearance > Render Whitespace . Line endings cannot be shown in the editor, but the status bar shows the line ending of the current file, and by clicking on it, you can change it.","title":"Show special characters"},{"location":"Audio%20%26%20Video/FFMpeg/","text":"First download binaries for Windows . Converting a Video to Gif Image \u00b6 Example: ffmpeg -ss 00:00:03 -to 00:00:06 -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=512:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" simod_showcase.gif Used options: -ss 00:00:03 : start at 3 seconds -to 00:00:06 : end at 6 seconds -r 15 : set the frame rate to 15 fps -vf \"scale=512:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" : set the size of the image to 512px width, and split the video into two streams: one for generating the palette, and one for using the palette Note that the order of the options is important. For example, if we put -ss after -i , then the video will be cut after the specified time, but the whole video will be loaded into memory before that. To change the speed, we can use an -itsscale inut option: ffmpeg -itsscale 0.2 -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=838:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -ss 00:00:00 -to 00:00:15 simod_showcase.gif Detailed description on SE A text can be then added to the video using online tools, e.g. ezgif .","title":"FFMpeg"},{"location":"Audio%20%26%20Video/FFMpeg/#converting-a-video-to-gif-image","text":"Example: ffmpeg -ss 00:00:03 -to 00:00:06 -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=512:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" simod_showcase.gif Used options: -ss 00:00:03 : start at 3 seconds -to 00:00:06 : end at 6 seconds -r 15 : set the frame rate to 15 fps -vf \"scale=512:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" : set the size of the image to 512px width, and split the video into two streams: one for generating the palette, and one for using the palette Note that the order of the options is important. For example, if we put -ss after -i , then the video will be cut after the specified time, but the whole video will be loaded into memory before that. To change the speed, we can use an -itsscale inut option: ffmpeg -itsscale 0.2 -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=838:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -ss 00:00:00 -to 00:00:15 simod_showcase.gif Detailed description on SE A text can be then added to the video using online tools, e.g. ezgif .","title":"Converting a Video to Gif Image"},{"location":"Audio%20%26%20Video/VLC/","text":"Showing subtitles under video \u00b6 This can be done using the video cropping filter ( croppadd ) check how much of the black bars are in the video, and how much of it is an empty space (use codec information tool) Options -> Advenced/All -> Filters -> croppadd a. to decrease the black bars contained in the video, set crop from top b. to decrease the black bars not contained in the video, set the padding from bottom Options -> Advenced/All -> Filters -> check the video cropping filter Change the subtitle position Restart VLC","title":"VLC"},{"location":"Audio%20%26%20Video/VLC/#showing-subtitles-under-video","text":"This can be done using the video cropping filter ( croppadd ) check how much of the black bars are in the video, and how much of it is an empty space (use codec information tool) Options -> Advenced/All -> Filters -> croppadd a. to decrease the black bars contained in the video, set crop from top b. to decrease the black bars not contained in the video, set the padding from bottom Options -> Advenced/All -> Filters -> check the video cropping filter Change the subtitle position Restart VLC","title":"Showing subtitles under video"},{"location":"Cloud%20computing/Lmod/","text":"Lmod is a module system that simplify the package management on the Linux cloud computing clusters. It is used on the RCI cluster. home user guide RCi cluster documentation module ( ml ) command \u00b6 The module performs different operations based on its first argument. The default operation (if the first/main argument is omited) is load . load <module name> : loads the module. Specific version can be used by adding / and the version number. If the version is not specified, the latest version is loaded. list : lists all loaded modules","title":"Lmod"},{"location":"Cloud%20computing/Lmod/#module-ml-command","text":"The module performs different operations based on its first argument. The default operation (if the first/main argument is omited) is load . load <module name> : loads the module. Specific version can be used by adding / and the version number. If the version is not specified, the latest version is loaded. list : lists all loaded modules","title":"module (ml) command"},{"location":"Cloud%20computing/RCI%20cluster/","text":"General \u00b6 To get access to the RCI cluster: https://docs.google.com/forms/d/e/1FAIpQLSewws_V6-D567fkp6QZmr0GQlkzQrEoB6QquAgQkZu8so818Q/viewform Official instructions: https://login.rci.cvut.cz/wiki/how_to_start Usual command Usage \u00b6 You can watch your jobs with squeue -u username . You test/debug your program when running it with srun command you usually don\u2019t have to allocate resources when testing To start an interactive shell, run: srun -p cpufast --pty bash -i Set your main script file (sh) executable via chmod +x <filename> command Test your script in console Cancel the job: scancel <JOB ID> You run your job with sbatch command with allocated resources Example sbatch: sbatch --mem=30G -t 60 -n30 -o /home/fiedlda1/Amodsim/log/ih.log /home/fiedlda1/Amodsim/rci_launchers/ih.sh How to clone projects \u00b6 Usually, you need to clone some of the SUM projects to start working on the RCI cluster. To do that: Copy your key to ~/.ssh/ Set file permissions to your ssh key safely Modify ~/.ssh/config IdentityFile to point to your key Clone your project Specifics for java projects \u00b6 Clone project on RCI cluster Download binary maven from: http://maven.apache.org/download.cgi and export it to your home folder on the RCI cluster. Prepare your bash script Add #!/bin/bash to the first line Change the environment variable PATH for your maven location with this command: PATH=$PATH:/home/$USER/apache-maven-3.6.1/bin/ Load all required software via ml command, definitely ml Java Build, compile, run your project via mvn commands Set your file executable via chmod +x filename command Example run command: mvn exec:exec -Dexec.executable=java -Dexec.args=\u2019-classpath %classpath -Xmx30g cz.cvut.fel.aic.amodsim.OnDemandVehiclesSimulation /home/kholkolg/amod-to-agentpolis/local_config_files/olga_vga_RCI.cfg\u2019 -Dfile.encoding=UTF-8 Example - Bash script for amodsim project: Run your script with srun, sbatch etc. commands, I recommend first use srun, to check everything is set up ok and then use sbatch command, because if computational nodes are busy, your job will be added to the queue and you can do other work. Specifics for python projects \u00b6 First load the appropriate version of python, e. G.: ml Python/3.6.6-foss-2018b You can\u2019t just install the packages with sudo , you have to install them to the user space instead: Run pip install --user packagename Specifics for C++ projects \u00b6 Workflow options \u00b6 As linux binaries are usually not portable. They are not compatible with older linux versions due to the infamous glibc incompatibility. There are three solutions to this problem: Method Setup Program Upgrade Compile the code on the RCI Setup the compilation on RCI. Copy the source code to RCI and recompile after every change Use a Singularity container learn with singularity, create the container Generate new container and copy it to the RCI Build a compatible binary using a modified toolchain learn with a toolchain generator, configure and generate the right toolchain Copy the updated binary Building on RCI \u00b6 In general the workflow is the same as on a local machine. The difference is that we do not have root access, so for all needed tools, we have to either load them via ml command, or, if not available, install them in the user space. Typically, we need to load: git: ml git GCC: ml GCC CMake: ml CMake Specific for projects with gurobi \u00b6 Load Gurobi with ml Gurobi or ml Gurobi/8.1.1-foss-2018b-Python-3.6.6 for a specific version Be aware that this operation can reload other packages Gurobi and Java \u00b6 It is necessary to install Gurobi to maven: mvn install:install-file -Dfile=/mnt/appl/software/Gurobi/9.0.3-GCCcore-8.3.0-Python-3.7.4/lib/gurobi.jar -DgroupId=com.gurobi -DartifactId=gurobi -Dversion=1.0 -Dpackaging=jar Gurobi and C++ \u00b6 As RCI use Linux as OS, we need to compile the Gurobi C++ libs with the same compiler as the one we use for compilation of our code (see C++ Workflow for more details). Note that this is necessary even if the Gurobi seems to be compiled with the same copiler we use for compilation . Unlike in Linux installation we controll, we cannot build the C++ lib in the Gurobi installation folder. To make the Linking work, foloow these steps: copy the src dir from the RCI Gurobi module located at mnt/appl/software/Gurobi/<desired version> to our home run make located in src/build copy the libgurobi_c++.a to the lib subfolder of your project configure the searching for the C++ lib in FindGUROVI.cmake file: find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED ) if the CMake cache is already generated, delete it. Generate the CMake cache and build the project Commands \u00b6 For command description, see Slurm manual. Main params --mem=<required memory> <required memory> is in megabytes by default, for gigabytes, we need to add G -t <time> Time in minutes Task Explanations \u00b6 srun --pty bash -i - --pty runs the first task and close output and error stream for everything except the first task - bash : what we want to run - -i Input setting, here followed by no param indicating that the input stream is closed osed","title":"RCI cluster"},{"location":"Cloud%20computing/RCI%20cluster/#general","text":"To get access to the RCI cluster: https://docs.google.com/forms/d/e/1FAIpQLSewws_V6-D567fkp6QZmr0GQlkzQrEoB6QquAgQkZu8so818Q/viewform Official instructions: https://login.rci.cvut.cz/wiki/how_to_start","title":"General"},{"location":"Cloud%20computing/RCI%20cluster/#usual-command-usage","text":"You can watch your jobs with squeue -u username . You test/debug your program when running it with srun command you usually don\u2019t have to allocate resources when testing To start an interactive shell, run: srun -p cpufast --pty bash -i Set your main script file (sh) executable via chmod +x <filename> command Test your script in console Cancel the job: scancel <JOB ID> You run your job with sbatch command with allocated resources Example sbatch: sbatch --mem=30G -t 60 -n30 -o /home/fiedlda1/Amodsim/log/ih.log /home/fiedlda1/Amodsim/rci_launchers/ih.sh","title":"Usual command Usage"},{"location":"Cloud%20computing/RCI%20cluster/#how-to-clone-projects","text":"Usually, you need to clone some of the SUM projects to start working on the RCI cluster. To do that: Copy your key to ~/.ssh/ Set file permissions to your ssh key safely Modify ~/.ssh/config IdentityFile to point to your key Clone your project","title":"How to clone projects"},{"location":"Cloud%20computing/RCI%20cluster/#specifics-for-java-projects","text":"Clone project on RCI cluster Download binary maven from: http://maven.apache.org/download.cgi and export it to your home folder on the RCI cluster. Prepare your bash script Add #!/bin/bash to the first line Change the environment variable PATH for your maven location with this command: PATH=$PATH:/home/$USER/apache-maven-3.6.1/bin/ Load all required software via ml command, definitely ml Java Build, compile, run your project via mvn commands Set your file executable via chmod +x filename command Example run command: mvn exec:exec -Dexec.executable=java -Dexec.args=\u2019-classpath %classpath -Xmx30g cz.cvut.fel.aic.amodsim.OnDemandVehiclesSimulation /home/kholkolg/amod-to-agentpolis/local_config_files/olga_vga_RCI.cfg\u2019 -Dfile.encoding=UTF-8 Example - Bash script for amodsim project: Run your script with srun, sbatch etc. commands, I recommend first use srun, to check everything is set up ok and then use sbatch command, because if computational nodes are busy, your job will be added to the queue and you can do other work.","title":"Specifics for java projects"},{"location":"Cloud%20computing/RCI%20cluster/#specifics-for-python-projects","text":"First load the appropriate version of python, e. G.: ml Python/3.6.6-foss-2018b You can\u2019t just install the packages with sudo , you have to install them to the user space instead: Run pip install --user packagename","title":"Specifics for python projects"},{"location":"Cloud%20computing/RCI%20cluster/#specifics-for-c-projects","text":"","title":"Specifics for C++ projects"},{"location":"Cloud%20computing/RCI%20cluster/#workflow-options","text":"As linux binaries are usually not portable. They are not compatible with older linux versions due to the infamous glibc incompatibility. There are three solutions to this problem: Method Setup Program Upgrade Compile the code on the RCI Setup the compilation on RCI. Copy the source code to RCI and recompile after every change Use a Singularity container learn with singularity, create the container Generate new container and copy it to the RCI Build a compatible binary using a modified toolchain learn with a toolchain generator, configure and generate the right toolchain Copy the updated binary","title":"Workflow options"},{"location":"Cloud%20computing/RCI%20cluster/#building-on-rci","text":"In general the workflow is the same as on a local machine. The difference is that we do not have root access, so for all needed tools, we have to either load them via ml command, or, if not available, install them in the user space. Typically, we need to load: git: ml git GCC: ml GCC CMake: ml CMake","title":"Building on RCI"},{"location":"Cloud%20computing/RCI%20cluster/#specific-for-projects-with-gurobi","text":"Load Gurobi with ml Gurobi or ml Gurobi/8.1.1-foss-2018b-Python-3.6.6 for a specific version Be aware that this operation can reload other packages","title":"Specific for projects with gurobi"},{"location":"Cloud%20computing/RCI%20cluster/#gurobi-and-java","text":"It is necessary to install Gurobi to maven: mvn install:install-file -Dfile=/mnt/appl/software/Gurobi/9.0.3-GCCcore-8.3.0-Python-3.7.4/lib/gurobi.jar -DgroupId=com.gurobi -DartifactId=gurobi -Dversion=1.0 -Dpackaging=jar","title":"Gurobi and Java"},{"location":"Cloud%20computing/RCI%20cluster/#gurobi-and-c","text":"As RCI use Linux as OS, we need to compile the Gurobi C++ libs with the same compiler as the one we use for compilation of our code (see C++ Workflow for more details). Note that this is necessary even if the Gurobi seems to be compiled with the same copiler we use for compilation . Unlike in Linux installation we controll, we cannot build the C++ lib in the Gurobi installation folder. To make the Linking work, foloow these steps: copy the src dir from the RCI Gurobi module located at mnt/appl/software/Gurobi/<desired version> to our home run make located in src/build copy the libgurobi_c++.a to the lib subfolder of your project configure the searching for the C++ lib in FindGUROVI.cmake file: find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED ) if the CMake cache is already generated, delete it. Generate the CMake cache and build the project","title":"Gurobi and C++"},{"location":"Cloud%20computing/RCI%20cluster/#commands","text":"For command description, see Slurm manual. Main params --mem=<required memory> <required memory> is in megabytes by default, for gigabytes, we need to add G -t <time> Time in minutes","title":"Commands"},{"location":"Cloud%20computing/RCI%20cluster/#task-explanations","text":"srun --pty bash -i - --pty runs the first task and close output and error stream for everything except the first task - bash : what we want to run - -i Input setting, here followed by no param indicating that the input stream is closed osed","title":"Task Explanations"},{"location":"Cloud%20computing/Slurm/","text":"Commands \u00b6 srun \u00b6 Run the task in the current shell in blocking mode, i.e., the console will be blocked till the task finishes. This command is only useful if we expect that the resources will be available immediatelly and the task will finish quickly. Otherwise, we should use sbatch . Params: --pty runs the in terminal mode. Output and error streams are closed for everything except the first task. -i Input setting. If followed by no param, it indicates that the input stream is closed. sbatch \u00b6 Request the execution of a task, with the required resources specified as sbatch parameters. The plain call with all resources defaulted is: sbatch <bash script> Note that the <bash script> here realy needs to be a bash script, it cannot be an arbitrary command or executable. Important parameters: -n, --ntasks : maximum number of tasks/threads that will be allocated by the job default is one task per node -N, --nodes : number of allocated nodes. default: minimum nodes that are needed to allocate resources according to other parameters (e.g., --ntasks , --mem ). --mem maximum memory that will be allocated by the job. The suffix G stands for gigabytes, by default, it uses megabytes. Example: --mem=40G . -t, --time : time limit. possible formats are <minutes> , <minutes:seconds> , <hours:minutes:seconds> , <days-hours> , <days-hours:minutes> , and <days-hours:minutes:seconds> . default: partition time limit -p , --partition= : partition name -o , --output= : job's output file name. The default name is slurm-<JOB ID>.out squeue \u00b6 --me filter just my jobs -u <username> filter just a specific user --start print the expected start time and the nodes planed to run the task -w --nodelist filter jobs running (but not planned) on specific nodes. The format for nodelist is <name>[<range>] , e.g., n[05-06] . sinfo \u00b6 Prints information about the computer cluster. scancel \u00b6 The scancel command cancels the execution of a job specified by the ID (firsta argument). To instead cancel jobs by name, use the --name option. Note however, that full name has to be specified and no wildcards are allowed . To cancel all jobs with a certain name, we have to mess with various linux commands instead: squeue --me | awk '/smod_cha/ {print $1}' | xargs scancel sacctmgr \u00b6 For viewing and modifying Slurm account information. The most important command for users is show (or list , which is equivalent). Baset on the parameter, it shows different information: show associations : associations between users and accounts show qos : quality of service: limits and priorities for each group-queue combination Determining why the job was killed \u00b6 Usually, the error message is at the end of the output file. Message meaning: Detected 1 oom_kill event in ... : oom stands for out of memory. The job was killed because it exceeded the memory limit.","title":"Slurm"},{"location":"Cloud%20computing/Slurm/#commands","text":"","title":"Commands"},{"location":"Cloud%20computing/Slurm/#srun","text":"Run the task in the current shell in blocking mode, i.e., the console will be blocked till the task finishes. This command is only useful if we expect that the resources will be available immediatelly and the task will finish quickly. Otherwise, we should use sbatch . Params: --pty runs the in terminal mode. Output and error streams are closed for everything except the first task. -i Input setting. If followed by no param, it indicates that the input stream is closed.","title":"srun"},{"location":"Cloud%20computing/Slurm/#sbatch","text":"Request the execution of a task, with the required resources specified as sbatch parameters. The plain call with all resources defaulted is: sbatch <bash script> Note that the <bash script> here realy needs to be a bash script, it cannot be an arbitrary command or executable. Important parameters: -n, --ntasks : maximum number of tasks/threads that will be allocated by the job default is one task per node -N, --nodes : number of allocated nodes. default: minimum nodes that are needed to allocate resources according to other parameters (e.g., --ntasks , --mem ). --mem maximum memory that will be allocated by the job. The suffix G stands for gigabytes, by default, it uses megabytes. Example: --mem=40G . -t, --time : time limit. possible formats are <minutes> , <minutes:seconds> , <hours:minutes:seconds> , <days-hours> , <days-hours:minutes> , and <days-hours:minutes:seconds> . default: partition time limit -p , --partition= : partition name -o , --output= : job's output file name. The default name is slurm-<JOB ID>.out","title":"sbatch"},{"location":"Cloud%20computing/Slurm/#squeue","text":"--me filter just my jobs -u <username> filter just a specific user --start print the expected start time and the nodes planed to run the task -w --nodelist filter jobs running (but not planned) on specific nodes. The format for nodelist is <name>[<range>] , e.g., n[05-06] .","title":"squeue"},{"location":"Cloud%20computing/Slurm/#sinfo","text":"Prints information about the computer cluster.","title":"sinfo"},{"location":"Cloud%20computing/Slurm/#scancel","text":"The scancel command cancels the execution of a job specified by the ID (firsta argument). To instead cancel jobs by name, use the --name option. Note however, that full name has to be specified and no wildcards are allowed . To cancel all jobs with a certain name, we have to mess with various linux commands instead: squeue --me | awk '/smod_cha/ {print $1}' | xargs scancel","title":"scancel"},{"location":"Cloud%20computing/Slurm/#sacctmgr","text":"For viewing and modifying Slurm account information. The most important command for users is show (or list , which is equivalent). Baset on the parameter, it shows different information: show associations : associations between users and accounts show qos : quality of service: limits and priorities for each group-queue combination","title":"sacctmgr"},{"location":"Cloud%20computing/Slurm/#determining-why-the-job-was-killed","text":"Usually, the error message is at the end of the output file. Message meaning: Detected 1 oom_kill event in ... : oom stands for out of memory. The job was killed because it exceeded the memory limit.","title":"Determining why the job was killed"},{"location":"GIS/Overpass%20Manual/","text":"Sources \u00b6 wiki/Overpass QL Strucutre \u00b6 Every statement ents with ; . Sets \u00b6 Almost all statements works with sets of data. Unless specified, we work with a default set ( _ ). To wrrito to a specific set, we can use ->. operator: <statement>->.<set> writes the result of the <statement> to the . The default set can be ommited: <statement> is equal to <statement>->._ . out statement \u00b6 All queries should contain an out statement that determines the output format. out is used for data only request out geom returns data with all elements associated with their geometry. Note that while the output format can be specified, we cannot filter the output (e.g., we cannot filter the ralation members ). Area specification \u00b6 We select an area by name as: area[\"name\"=\"Letkov\"]; And then we used it as: node(area); If we need more areas in a query, we can store them in variable: area[\"name\"=\"Letkov\"]->.let; And: node(area.let); Important note: If an area needs to be used repetadly, it has to be named, as the value of area ( area._ ) is replaced by the output of any statement. Filtering \u00b6 filters are specified in brackets: rel[\"admin_level\"=8]; It is also possible to use a regex filtering, we just neeed to replace = with ~ : rel[\"admin_level\"~\".*\"]; Selecting Multiple Data Sets \u00b6 Implicitely, all filters are aplied to a default dataset called _ and also written to it. Therefore, we cannot do: rel[\"admin_level\"~\".*\"]; node; because we are basically selecting nodes from a set of relations. Solutions: 1) Union statement 2) Named sets Union Utatement \u00b6 Union statement is surounded by parantheses. We can achieve the intended behaviour by: ( rel[\"admin_level\"~\".*\"]; node; ); Select Area Boundary \u00b6 Sometimes, it is usefull to check the boundary of the named area. However, the administrative level differ for various areas cities, countries, so the admin_level relation cannot be used for that. Luckilly, there is an option to use the pivot filter. Example: area[\"name\"=\"Praha\"]; node(pivot); out geom; Discover the full name of an area \u00b6 If we want to know the full name of the area while the above query return multiple results, we can do that in openstreet map: Move the map to see the area Click the button with cusor and question mark to select the exploration tool Click inside the area Scroll down to area relations Click on the proper region The name property is what we are looking for Filter areas with duplicite names \u00b6 Sometimes, even the full name of the area is not specific enough. In that case, we can use two approaches: select the area by the area relation id specify the area by the higher level area (state, country) Select area by ID \u00b6 select the requested area copy the id and add 3 600 000 000 to it (start with 36 and fill zerose till there are 10 digits in total) replace area[\"name\"=\"<NAME>] with area(<ID>) . Note that round brackets are used instead of square brackets! Specify area with higher level area \u00b6 In this exaple, we select the Coo County, Illinois. area[name=\"Illinois\"]; rel[name=\"Cook County\"](area); map_to_area; node(pivot); out geom; more info Get historical data \u00b6 To get historical data, prepend the query with a date statement. Example: [date:\"2016-01-01T00:00:00Z\"]; area[name=\"City of New York\"]; node(pivot); out geom;","title":"Overpass Manual"},{"location":"GIS/Overpass%20Manual/#sources","text":"wiki/Overpass QL","title":"Sources"},{"location":"GIS/Overpass%20Manual/#strucutre","text":"Every statement ents with ; .","title":"Strucutre"},{"location":"GIS/Overpass%20Manual/#sets","text":"Almost all statements works with sets of data. Unless specified, we work with a default set ( _ ). To wrrito to a specific set, we can use ->. operator: <statement>->.<set> writes the result of the <statement> to the . The default set can be ommited: <statement> is equal to <statement>->._ .","title":"Sets"},{"location":"GIS/Overpass%20Manual/#out-statement","text":"All queries should contain an out statement that determines the output format. out is used for data only request out geom returns data with all elements associated with their geometry. Note that while the output format can be specified, we cannot filter the output (e.g., we cannot filter the ralation members ).","title":"out statement"},{"location":"GIS/Overpass%20Manual/#area-specification","text":"We select an area by name as: area[\"name\"=\"Letkov\"]; And then we used it as: node(area); If we need more areas in a query, we can store them in variable: area[\"name\"=\"Letkov\"]->.let; And: node(area.let); Important note: If an area needs to be used repetadly, it has to be named, as the value of area ( area._ ) is replaced by the output of any statement.","title":"Area specification"},{"location":"GIS/Overpass%20Manual/#filtering","text":"filters are specified in brackets: rel[\"admin_level\"=8]; It is also possible to use a regex filtering, we just neeed to replace = with ~ : rel[\"admin_level\"~\".*\"];","title":"Filtering"},{"location":"GIS/Overpass%20Manual/#selecting-multiple-data-sets","text":"Implicitely, all filters are aplied to a default dataset called _ and also written to it. Therefore, we cannot do: rel[\"admin_level\"~\".*\"]; node; because we are basically selecting nodes from a set of relations. Solutions: 1) Union statement 2) Named sets","title":"Selecting Multiple Data Sets"},{"location":"GIS/Overpass%20Manual/#union-utatement","text":"Union statement is surounded by parantheses. We can achieve the intended behaviour by: ( rel[\"admin_level\"~\".*\"]; node; );","title":"Union Utatement"},{"location":"GIS/Overpass%20Manual/#select-area-boundary","text":"Sometimes, it is usefull to check the boundary of the named area. However, the administrative level differ for various areas cities, countries, so the admin_level relation cannot be used for that. Luckilly, there is an option to use the pivot filter. Example: area[\"name\"=\"Praha\"]; node(pivot); out geom;","title":"Select Area Boundary"},{"location":"GIS/Overpass%20Manual/#discover-the-full-name-of-an-area","text":"If we want to know the full name of the area while the above query return multiple results, we can do that in openstreet map: Move the map to see the area Click the button with cusor and question mark to select the exploration tool Click inside the area Scroll down to area relations Click on the proper region The name property is what we are looking for","title":"Discover the full name of an area"},{"location":"GIS/Overpass%20Manual/#filter-areas-with-duplicite-names","text":"Sometimes, even the full name of the area is not specific enough. In that case, we can use two approaches: select the area by the area relation id specify the area by the higher level area (state, country)","title":"Filter areas with duplicite names"},{"location":"GIS/Overpass%20Manual/#select-area-by-id","text":"select the requested area copy the id and add 3 600 000 000 to it (start with 36 and fill zerose till there are 10 digits in total) replace area[\"name\"=\"<NAME>] with area(<ID>) . Note that round brackets are used instead of square brackets!","title":"Select area by ID"},{"location":"GIS/Overpass%20Manual/#specify-area-with-higher-level-area","text":"In this exaple, we select the Coo County, Illinois. area[name=\"Illinois\"]; rel[name=\"Cook County\"](area); map_to_area; node(pivot); out geom; more info","title":"Specify area with higher level area"},{"location":"GIS/Overpass%20Manual/#get-historical-data","text":"To get historical data, prepend the query with a date statement. Example: [date:\"2016-01-01T00:00:00Z\"]; area[name=\"City of New York\"]; node(pivot); out geom;","title":"Get historical data"},{"location":"GIS/QGIS/","text":"Adding points from coordinates \u00b6 The easiest way is to use QuickWKT Count the number of features in area \u00b6 select the right layer on the top panel, third row, select the tool Select Features by area or single click select features Open the attribute table for layer. In the heder, there should be the number of selected features Postgis Layers \u00b6 Adding layers \u00b6 Layer -> Add Layer Temporary layer \u00b6 Temporary layers are useful for experimenting. Whether we want to add a point, line, or area, we need a layer for that even if we do not intend to save the result. The right layer for this is the temporary scratch layer we can add it by: Layer -> Create layer -> New Temporary Scratch Layer... Invalid layer \u00b6 The layer can be marked as invalid because of a missing or invalid index column. Each Postgis layer needs an id column consisting of unique integer values .","title":"QGIS"},{"location":"GIS/QGIS/#adding-points-from-coordinates","text":"The easiest way is to use QuickWKT","title":"Adding points from coordinates"},{"location":"GIS/QGIS/#count-the-number-of-features-in-area","text":"select the right layer on the top panel, third row, select the tool Select Features by area or single click select features Open the attribute table for layer. In the heder, there should be the number of selected features","title":"Count the number of features in area"},{"location":"GIS/QGIS/#postgis-layers","text":"","title":"Postgis Layers"},{"location":"GIS/QGIS/#adding-layers","text":"Layer -> Add Layer","title":"Adding layers"},{"location":"GIS/QGIS/#temporary-layer","text":"Temporary layers are useful for experimenting. Whether we want to add a point, line, or area, we need a layer for that even if we do not intend to save the result. The right layer for this is the temporary scratch layer we can add it by: Layer -> Create layer -> New Temporary Scratch Layer...","title":"Temporary layer"},{"location":"GIS/QGIS/#invalid-layer","text":"The layer can be marked as invalid because of a missing or invalid index column. Each Postgis layer needs an id column consisting of unique integer values .","title":"Invalid layer"},{"location":"Images/Midjourney/","text":"quick start The basic command is /imagine . The format is: \\imagine <description> <parameters> The <description describes the content of the image in human language. The <parameters> of the comand determines what we expect from the image. /imagine parameters \u00b6 Aspect ratio \u00b6 official documentation For aspect ratio, we use --ar or --aspect parameter. The format is <width>:<height> . Both <width> and <height> must be integers. Version \u00b6 official documentation For version, we use --version parameter. Using image input \u00b6 official documentation The format for impage input prompt is: \\imagine <image url> <prompt> <params> We can also adjust the weight of the image input with --iw parameter. Choosing the right variant to vary or upscale \u00b6 After successful generation, emojis can be use to either upscale one of the four generated images, or to vary the image. The order is top-left, top-right, bottom-left, bottom-right. Showing user information \u00b6 official documentation To show user information, use the /info command in any channel (the response will be private). Showing all generated images \u00b6 All generated images can be seen at https://www.midjourney.com/imagine","title":"Midjourney"},{"location":"Images/Midjourney/#imagine-parameters","text":"","title":"/imagine parameters"},{"location":"Images/Midjourney/#aspect-ratio","text":"official documentation For aspect ratio, we use --ar or --aspect parameter. The format is <width>:<height> . Both <width> and <height> must be integers.","title":"Aspect ratio"},{"location":"Images/Midjourney/#version","text":"official documentation For version, we use --version parameter.","title":"Version"},{"location":"Images/Midjourney/#using-image-input","text":"official documentation The format for impage input prompt is: \\imagine <image url> <prompt> <params> We can also adjust the weight of the image input with --iw parameter.","title":"Using image input"},{"location":"Images/Midjourney/#choosing-the-right-variant-to-vary-or-upscale","text":"After successful generation, emojis can be use to either upscale one of the four generated images, or to vary the image. The order is top-left, top-right, bottom-left, bottom-right.","title":"Choosing the right variant to vary or upscale"},{"location":"Images/Midjourney/#showing-user-information","text":"official documentation To show user information, use the /info command in any channel (the response will be private).","title":"Showing user information"},{"location":"Images/Midjourney/#showing-all-generated-images","text":"All generated images can be seen at https://www.midjourney.com/imagine","title":"Showing all generated images"},{"location":"Images/Photopea/","text":"Fill a selection with a color \u00b6 Edit > Fill...","title":"Photopea"},{"location":"Images/Photopea/#fill-a-selection-with-a-color","text":"Edit > Fill...","title":"Fill a selection with a color"},{"location":"LaTeX/LaTeX%20workflow/","text":"So far we have three LaTeX toolchains that has proven to work well: Overleaf : Cloud tool that is stable and very good for collaboration. Texmaker + MiKTex : Traditional desktop setup. VSCode + Latex Workshop + MikTeX/Tinytex : Modern desktop setup. The main advantage is that VSCode has the best Copilot support from all the editors, which is a huge time saver. VSCode + Latex Workshop + MikTeX/Tinytex \u00b6 Installation \u00b6 The installation of VSCode and Latex Workshop (VSCode extension) is straightforward, so we cover only the installation of MikTeX/ Tinytex here. MikTeX \u00b6 The installation of MikTeX is straightforward. MikTeX installs all the required packages on the fly, so there is no need to install them manually. The only thing that we need to do manually is to install Perl which is needed for the latexmk tool. There are two Perl distributions for Windows: ActivePerl and Strawberry Perl . This LaTeX toolchain has been only tested with Strawberry Perl. The installation of Strawberry Perl is straightforward: there is an executable installer. Do not forget to add it to the PATH variable. Do not forget to restart VSCode after the installation of MikTeX and Perl. Tinytex \u00b6 Official installation guide Install Tinytex using the shell script for the respective OS. The links to the scripts are in the installation guide. Add the executable path of Tinytex to the PATH variable. Installing additional packages \u00b6 Unlike MiKTex, Tinytex does not install required packages on the fly. Instead, it only shows an error in the log. To install a missing package, run the following command: tlmgr install <package name> Latex Workshop Configuration and Usage \u00b6 wiki Syncing between PDF and source \u00b6 To jump from PDF to source, use the binding configured in: Settings > Latex Workshop > View > Pdf > Internal > SyncTeX: Keybinding . To jump from source to PDF, use the binding configured in: Keyboad Shortcuts > Latex Workshop: SynTeX from cursor . Unfortunately, mouse cannot be used here due to VSCode limitations .","title":"LaTeX workflow"},{"location":"LaTeX/LaTeX%20workflow/#vscode-latex-workshop-miktextinytex","text":"","title":"VSCode + Latex Workshop + MikTeX/Tinytex"},{"location":"LaTeX/LaTeX%20workflow/#installation","text":"The installation of VSCode and Latex Workshop (VSCode extension) is straightforward, so we cover only the installation of MikTeX/ Tinytex here.","title":"Installation"},{"location":"LaTeX/LaTeX%20workflow/#miktex","text":"The installation of MikTeX is straightforward. MikTeX installs all the required packages on the fly, so there is no need to install them manually. The only thing that we need to do manually is to install Perl which is needed for the latexmk tool. There are two Perl distributions for Windows: ActivePerl and Strawberry Perl . This LaTeX toolchain has been only tested with Strawberry Perl. The installation of Strawberry Perl is straightforward: there is an executable installer. Do not forget to add it to the PATH variable. Do not forget to restart VSCode after the installation of MikTeX and Perl.","title":"MikTeX"},{"location":"LaTeX/LaTeX%20workflow/#tinytex","text":"Official installation guide Install Tinytex using the shell script for the respective OS. The links to the scripts are in the installation guide. Add the executable path of Tinytex to the PATH variable.","title":"Tinytex"},{"location":"LaTeX/LaTeX%20workflow/#installing-additional-packages","text":"Unlike MiKTex, Tinytex does not install required packages on the fly. Instead, it only shows an error in the log. To install a missing package, run the following command: tlmgr install <package name>","title":"Installing additional packages"},{"location":"LaTeX/LaTeX%20workflow/#latex-workshop-configuration-and-usage","text":"wiki","title":"Latex Workshop Configuration and Usage"},{"location":"LaTeX/LaTeX%20workflow/#syncing-between-pdf-and-source","text":"To jump from PDF to source, use the binding configured in: Settings > Latex Workshop > View > Pdf > Internal > SyncTeX: Keybinding . To jump from source to PDF, use the binding configured in: Keyboad Shortcuts > Latex Workshop: SynTeX from cursor . Unfortunately, mouse cannot be used here due to VSCode limitations .","title":"Syncing between PDF and source"},{"location":"LaTeX/Latex%20manual/","text":"Document structure \u00b6 The document structure is well documented on wikibooks . The basic structure is: \\documentclass[<options>]{<class>} ... \\begin{document} ... \\end{document} Lot of content is usually put in the preamble, i.e., between the \\documentclass and \\begin{document} commands. The preamble usual content is: loading packages with \\usepackage{<package>} providing new commands or redefining existing commands configuring packages supplying metadata for the document (title, author, etc.) Title page \u00b6 The title page typically contains the title, authors, and potentially other metadata like date or keywords. The standard way how to create the title page is to first define the metadata and then use the \\maketitle command. To print the whole title page. The metadata available for standard article class: \\title{<title>} \\author{<author>} \\date{<date>} . \\today can be used to print the current date. This is the default value if we omit the \\date command. A special non-numbered footnote can be added to most fields using the \\thanks{<text>} command. Authors specification \u00b6 By default, all authors should be filled within a single \\author command, separated by the \\and command. If we need to specify the affiliations, we can do it inside the \\author command. This way, each author have the affiliation printed right after/bellow the author name. However, if there are many authors with shared affiliations, this approach is unsustainable. Instead, we can use the authblk package which lets us specify the authors and affiliations separately and connect them to authors using footnotes. Example: \\usepackage{authblk} \\author[1]{Author 1} \\author[2]{Author 2} \\author[1]{Author 3} \\affil[1]{Affiliation 1} \\affil[2]{Affiliation 2} Keywords \u00b6 Keywords are not part of the standard article class. If we need to include them when using the standard article class, we can provide the command ourselves. Example: \\providecommand{\\keywords}[1]{\\textbf{\\textit{Index terms---}} #1} ... \\keywords{keyword1, keyword2, ...} Appendices \u00b6 Appendicies are started with the command appendix . Then, each chapter started with the \\chapter command is considered an appendix. Book class structuring \u00b6 Appart from sections and chapters, the book class also supports top level parts for marking special areas of the document. The parts are: \\frontmatter : the front matter of the document, i.e., the title page, abstract, table of contents, etc. The page numbering is in roman numerals. \\mainmatter : the main matter of the document, i.e., the main content. The page numbering is in arabic numerals, and the page numbering is reset to 1. \\backmatter : the back matter of the document, i.e., the bibliography, appendices, etc. The page numbering is in arabic numerals. Chapters are not numbered. Escape characters \u00b6 LaTeX uses man6y special characters which needs to be escaped. Unfortunatelly, there is no single escape character, instead, there are many. The following table lists the most common escape characters: Character Escape sequence [ {[} ] {]} Special characters \u00b6 Non-breaking characters \u00b6 ~ : non-breaking space \\nobreakdash- : non-breaking dash Text and paragramph formatting \u00b6 wiki Fonts \u00b6 Determining used font \u00b6 To determine the font (size, type) used in a particular place put the following command there: \\showthe\\font Then compile the document. The font information will be printed in the log (e.g.: \\OT1/cmr/m/n/10 ). Changing font size \u00b6 The default font size is 10pt . To change the default font size, set the documentclass option 12pt or 11pt (other sizes are not avialable). See the wiki for more information. The size can be also changed for a specific part of the document. We can use some predefined sizes e.g.: Normal {\\tiny tiny} normal or we can use the \\fontsize command to set arbitrary size. Example: {\\fontsize{<size>}{<line spacing>}\\selectfont <text>} Font color \u00b6 wiki To change the font color, we need to use the xcolor package. First, we need to define the color using the \\definecolor command. Example: \\definecolor{my_color}{RGB}{255,0,0} # RGB color \\definecolor{my_color}{HTML}{FF0000} # HTML color \\definecolor{my_color}{cmyk}{0,1,1,0} # CMYK color Then, we can use various commands to change the font color: \\textcolor{<color>}{<text>} : change the color of the text {\\color{<color>}<text>} : change the color of the text. Can span multiple paragraphs. note that other commands that have color options usually depend on the xcolor package, and accept the same color definitions. Horizontal and vertical spacing \u00b6 Most of the time, the spacing should be handled by LaTeX automatically. However, there are cases when we need to adjust the spacing manually, either in a single place or globally. To adjust the spacing in a single place, we can use the following commands: \\hspace{<length>} : horizontal space \\vspace{<length>} : vertical space Note that we can use negative values for the <length> parameter, so we can use these commands to relatively positioning. Horizontal and vertical alignment \u00b6 Horizontal alignment \u00b6 wiki By default, the text is fully justified. To change the justification (alignment), to left or right, we can use either environments: flushleft for left alignment, flushright for right alignment, or commands: \\raggedright for left alignment, \\raggedleft for right alignment. Vertical alignment \u00b6 By default, the text starts from the top of the page. To align the text to the bottom of the page, we can use the vfill command. Example: Some text \\vfill Some text at the bottom Note that the vfill works only if there is something before it, it does not work if it is the first command on the page. To make it work, we can use the null command. Example: \\null \\vfill Some text at the bottom Subscript and superscript \u00b6 In math mode, the subscript and superscript are created using the _ and ^ characters. In text mode, we need to use a special commands: \\textsubscript and \\textsuperscript . Example: H\\textsubscript{2}O Lists \u00b6 The list enviroments have the following syntax: \\begin{<list type>} \\item <item 1> \\item <item 2> ... \\end{<list type>} The following list types are available: itemize : bullet points enumerate : numbered list description : description list. Items can have a label, which is specified as an optional argument of the \\item command. More typoes of lists like questions or checklists can be created using external packages. Units, numbers, and currency \u00b6 Units are usually typeset with a small space between the number and the unit. Normal space should be avoidedas it is too wide. Also, we sometimes want a special separator in large numbers. For these purposes, the best practice is to use the siunitx package. Eith siunitx , the units are typeset using the \\SI command. Example: \\SI{10}{\\meter} The numbers should be typeset using the \\num command. Example: \\num{1000000} The currecnies that have the currency sign before the number are also typeset using the \\SI command, but we need to use the optional argument for prefix: \\SI{10}[\\$]{} Quotes \u00b6 The quotes are typeset using the csquotes package. The quotes are typeset using the \\enquote command. Example: \\enquote{This is a quote} Supported units \u00b6 \\s : second \\km : kilometer \\km\\per\\hour : kilometer per hour \\percent : percent Boxes \u00b6 wiki Everythign in LaTeX is a box. Each character is a box, stored in a larger box for each word, and analogically for each line, paragraph, etc. Most of the time we just set properties for the boxes, but sometimes we need to create a box manually to format or position the content. The following table presents the most common box commands: Command paragraph witdth \\parbox single fixed \\pbox single flexible \\minipage multiple fixed The first two parameters are shared for these commands: pos : the position of the box, e.g., t for top, b for bottom, c for center. The position refers to the part of the box that is aligned with the surrounding text. height : the height of the box. The parbox and minipage share another two parameters that follows: contentpos : the position of the content inside the box, e.g., t for top, b for bottom, c for center. width : the width of the box. Make a box wider than the text width \u00b6 To make a box wider than the text width, we can use the \\adjustwidth command from the changepage package. Example: \\begin{adjustwidth}{-1cm}{-1cm} ... content \\end{adjustwidth} The box above will be 1cm wider on each side than the text width. Floats \u00b6 The following environments are floats: figure table algorithm Placement \u00b6 Any float takes the position as a first argument. The following positions are available: h : here t : top b : bottom p : special dedicated page per float ! : ignore nice positioning and put it according to the float specifiers The placement algorithm then iterate pages starting from the page where the float is placed. For each page, it tries to find a place for the float according to the float specifiers (in the same order as they appear in the float position argument). In case of success, the procedure stops and place the float. If the procedure fails for all pages, the float is placed at the end. Note that by specifying the float position, we only add constraints to the placement algorithm, but do not guarantee the placement. By omitting the float position, we can accually make the float appear closer to the place where it is defined. Sources: LaTeX Wikibook Overleaf Default placement \u00b6 The default placement differs between environments and also classes. For example for article class, the default placement for figure and table is tbp ( see SO ). Figures \u00b6 The float environment for figures is figure . The image itself is included using the \\includegraphics command. The mandatory argument of the \\includegraphics command is the path to the image file. This path can be relative or absolute. If the path is relative, it is relative to the location of the main .tex file. The file extension can be omitted. If the file extension is omitted, the compiler will try to find the file with various extensions. Therefore, it is only recommended to omit the file extension if there is only one file with the same name. Optional arguments of the \\includegraphics command are the following: width : the width of the image. example: \\includegraphics[width=0.5\\textwidth]{image.png} scale : the scale of the image with respect to the original size. example: \\includegraphics[scale=0.5]{image.png} Subfigures \u00b6 For more images in one float, we can use the subfigure environment from the subcaption package. The subfigure environment is used as follows: \\begin{figure}[h] \\centering \\begin{subfigure}{0.3\\textwidth} \\includegraphics[width=\\textwidth]{image1.png} \\caption{Image 1} \\label{fig:image1} \\end{subfigure} \\begin{subfigure}{0.3\\textwidth} \\includegraphics[width=\\textwidth]{image2.png} \\caption{Image 2} \\label{fig:image2} \\end{subfigure} \\caption{My figure} \\label{fig:my_figure} \\end{figure} If the figures are not of the same height, they will be aligned at the bottom. To align them at the top, we can use the T option for the subfigure environment. Example: \\begin{subfigure}[T]{0.3\\textwidth} ... \\end{subfigure} Tables \u00b6 The float environment for tables is table . However, the rows and columns are wrapped in another environment. The default inner enviroment is tabular , however, there are many other packages that extends the functionality. In practice, there are currently three inner environments to consider: tabular : the default environment that is sufficient for simple tables tabulary : the environment that allows to create columns with automatic width. If the main or only issue of the table is that it needs to fit a specific width, this is the environment to use. tblr : the tblr environment from the tabulararray package is the most up to date tabular environment that support many features. Also, it splits the table presentation from the table content, which can make generating tables from code easier. The only downside is that it does not support automatic column width . Column types \u00b6 The column types are specified in the argument of the tabular or equivalent environment. The following column types are available by default: l : left aligned c : centered r : right aligned p{width} : paragraph column with specified width Other column types can be provided by the inner environment package or by the user. Simple tables with tabular environment \u00b6 The usual way to create a table in the tabular environment is: \\begin{table}[h] \\centering % center the content of the table environment \\begin{tabular}{|c|c|} ... rows and columns \\end{tabular} \\caption{My table} \\label{tab:my_table} \\end{table} Columns with automatic width: tabulary \u00b6 By default, laTeX does not support automatic width for columns, i.e., sizing the columns by their content. To enable this feature, we can use the tabulary package, which provides the tabulary environment (which is a replacement for the tabular environment). The columns with automatic width are specified by the L , C , R column types. Note that the new column types can be combined with the standard column types. In that case, the standard columns will have width according to their content, and the rest of the space will be distributed among the new column types. Complex tables with tabulararray package \u00b6 The tabulararray package provides : full control over the table, most features among all packages separation of the table content and the table presentation simpler code for basic tasks like multirows and multicolumns, wrapping text in cells, etc. Notable features present in the tabulararray package missing in other packages: footnotes in tables (otherwise, it requires a threeparttable environment wrapper) As other tabular packages, there are some incompatibilities related to the tabulararray package. So far, I observed only incompatibilities with the tabulararray talltblr environment, not with the standard tblr environment. The following table summarizes the incompatibilities I found so far: cases : Obviously, the cases environment uses table components internally. When using together with talltblr , the cases environment compilation results with the following error: latex \"\\begin{cases}\" package array empty preamble l' used . The solution is to use the use the new +cases environment provided by the tabulararray package. As a bonus, the +cases environment also fixes some visual glitches. Steps: enable the +cases environment by adding \\UseTblrLibrary{amsmath} to the preamble replace the cases environment with the +cases environment tabular : yes, the talltblr environment is incompatible with the default tabular environment. The solution is simple: replace all tabular environments with tblr environments. Styling \u00b6 Table grid (visual lines) \u00b6 Usually, tables contain some horizontal and vertical lines to separate the cells. Unfortunately, the way how to create these lines differs completely between horizontal and vertical lines. Horizontal lines \u00b6 For horizontal lines, we can: use various commands to create a line in a specific row: \\hline : creates a line in the current row specify the lines outside the data with the tabularray package: Specify the lines outside the data with the tabularray package \u00b6 The format is hline{<list of rows>} = {<line style>} . Example: \\begin{tblr}{ colspec={|c|c|c|}, hline{1,Z} = {1pt, solid}, } ... \\end{tblr} The <list of rows> can be specified as: single row: hline{1} = {1pt, solid} multiple rows: hline{1,3,5} = {1pt, solid} range of rows: hline{1-Z} = {1pt, solid} Rows are numbered from 1. The Z character is used to specify the last row. Styling the specific rows or columns \u00b6 With the tabularray package, we can style the table header differently than the rest of the table. In fact, we can style each specific row or column differently. for that, we use the row and column keys. Example \\begin{tblr}{ colspec={|c|c|c|}, row{1} = {bg=gray9, fg=white}, column{1} = {bg=gray9, fg=white}, } The syntax is the same for both keys: row{<row number>} = {<style>} . For the <row number> specification, see the grid section. The <style> specification is a list of key-value pairs. The keys are the following: font : the font style Configure the space between columns \u00b6 In most packages, the space between columns is configured using the \\tabcolsep variable. Example: \\setlength{\\tabcolsep}{10pt} However, in the tblr environment, the space between columns is configured using the leftsep and rightsep keys. Example:, \\begin{tblr} { colspec={llllrr}, leftsep=2pt, rightsep=2pt } By default, the leftsep and rightsep are set to 6pt . Define table styles with tabularray package \u00b6 The tabularray package provides a way to define table styles. First, we define the new environment with the \\NewTblrEnviron command. Later, we use the SetTblrInner and SetTblrOuter commands to define the style. Example: \\NewTblrEnviron{mytblr}{ \\SetTblrInner{rowsep=1pt} \\SetTblrOuter{hspan=minimal} } Rotated text \u00b6 To rotate some text, we can use the \\rotatebox command: \\rotatebox{90}{Rotated text} Multirows and multicolumns \u00b6 Depending on the inner environment, the multirows and multicolumns are created using different commands. tabular environment \u00b6 In the tabular environment, the multirows and multicolumns are created using the \\multicolumn and \\multirow commands. Example: \\begin{tabular}{cc} \\multicolumn{2}{c}{multi column} \\\\ \\multirow{2}{*}{multi row} & 1 \\\\ & 2 \\\\ \\end{tabular} tabulararray environment \u00b6 In the tabulararray environment, the multirows and multicolumns are created using the \\SetCell command. Example: \\begin{tblr}{cc} \\SetCell[c=2]{c} multi column & \\\\ \\SetCell[r=2]{c} multi row & 1 \\\\ & 2 \\\\ \\end{tblr} Note that for multicolumns, we need to add the column divider ( & ) after the \\SetCell command for each column that is spanned by the multicolumn . Export google sheets to latex tables \u00b6 There is ann addon called LatexKit which can be used for that. Footnotes in tables \u00b6 In tables and table captions, the \\footnote command does not work correctly. Also, it is not desirable to have the footnote at the bottom of page, instead, we want the footnote to be at the bottom of the table. To achieve this, we use a special environment: threeparttable : if we are using the tabular or tabulary environment talltblr : if we are using the tblr environment Using the threeparttable \u00b6 The threeparttable environment is used as follows: \\begin{table}[h] \\centering \\caption{My table} \\label{tab:my_table} \\begin{threeparttable} \\begin{tabular}{|c|c|} one$^a$ & two$^b$ \\\\ ... other rows and columns \\end{tabular} \\begin{tablenotes} \\item $^a$footnote 1 \\item $^b$footnote 2 \\end{tablenotes} \\end{threeparttable} \\end{table} Using the talltblr \u00b6 The talltblr environment is used as follows: \\begin{table}[h] \\centering \\caption{My table} \\label{tab:my_table} \\begin{talltblr}[ label = none, note{a} = {footnote 1}, note{b} = {footnote 2} ]{ colspec={|c|c|}, } one\\TblrNote{a} & two\\TblrNote{b} \\\\ ... other rows and columns \\end{talltblr} \\end{table} Notice the label = none option. Without it, the table numbering is raised again, resulting in the table being numbered twice. Rotated text \u00b6 To rotate some text, we can use the \\rotatebox command: \\rotatebox{90}{Rotated text} If we need to limit the width of the rotated text, we can use the \\parbox command. Example: \\rotatebox{90}{\\parbox{2cm}{Rotated text}} Algorithms \u00b6 wiki There are two types of enviroments for, depending on the purpose: algorithm : for pseudocode lstlisting : for code in a specific language Pseudocode \u00b6 The floating environment for pseudocode is algorithm . It is an equivalent of the table environment for tables. Same as with tables, there is also an inner environment for the pseudocode. The options for the inner environment are: algorithmic : basic environment for pseudocode algorithmicx : extension of the algorithmic environment, supports custom keywords algorithm2e : algpseudocodex : extension of the algorithmicx package, various improvements pseudo : a package for writing pseudocode directly, not using any special commands Their properties are summarized in the following table: | Environment | Package | Updated | Custom keywords | |--- | --- | --- | --- | | algorithmic | algorithms | 2009 | no | | algorithmicx | algorithmicx | 2005 | yes | | algorithm2e | algorithm2e | 2017 | yes | | algpseudocodex | algpseudocodex | 2023 | yes | | program | program | 2017 | no | | pseudo | pseudo | 2023 | yes | If the algorithm requires a complex description and cannot be expressed using typical pseudocode, we can resort to using human language directly in the algorithm environment. There is such example at SO Algoritmicx and Algpseudocodex \u00b6 Normal code lines are written using the \\State command: \\State $x \\gets 0$ Conditions \u00b6 Conditions are written using the \\If , \\ElseIf , and \\Else commands. Example: \\If{$x < 0$} \\State $x \\gets 0$ \\ElsIf{$x > 100$} \\State $x \\gets 100$ \\Else \\State $x \\gets x$ \\EndIf Loops \u00b6 For loop: \\For{$ i = 1, \\dots, n $} ... \\EndFor Boolean operators \u00b6 Boolean operators are not defined by default. Either use plain text, or define them (see the Additional keywords section below). Additional keywords \u00b6 We can define additional keywords using the \\algnonewcommand command. The format is \\algnonewcommand<custom command>{<keyword>} . Example: \\algnewcommand\\Not{\\textbf{not}} Empty lines \u00b6 To add an empty line, use the \\State command without any argument. If the numbering is on and we want to skip the numbering for the empty line, we can use the \\Statex command. Functions and procedures \u00b6 Functions and procedures are defined using the \\Function and \\Procedure commands. Example: \\Function{my_function}{a, b} \\State $x \\gets a + b$ \\State \\Return $x$ \\EndFunction We can call the function or procedure using the \\Call command. Example: \\Call{my_function}{1, 2} Line numbering \u00b6 To add line numbering, add an optional argument to the algorithmic environment. The argument is the number determining the frequency of the line numbering. Example: \\begin{algorithmic}[3] % number every 3rd line ... \\end{algorithmic} Centering of floats wider than text width \u00b6 If the float is wider than the text width, it is not centered, but instead it is left-aligned with the text and it overflows on the right side. To fix this, we can wrap the too-wide float content in the \\makebox command. Example: \\begin{figure}[h] \\makebox[\\textwidth]{ \\includegraphics[width=1.2\\textwidth]{my_image.png} } \\caption{My figure} \\label{fig:my_figure} \\end{figure} Boxes \u00b6 wiki Everythign in LaTeX is a box. Each character is a box, stored in a larger box for each word, and analogically for each line, paragraph, etc. Most of the time we just set properties for the boxes, but sometimes we need to create a box manually to format or position the content. The following table presents the most common box commands: Command paragraph witdth \\parbox single fixed \\pbox single flexible \\minipage multiple fixed The first two parameters are shared for these commands: pos : the position of the box, e.g., t for top, b for bottom, c for center. The position refers to the part of the box that is aligned with the surrounding text. height : the height of the box. The parbox and minipage share another two parameters that follows: contentpos : the position of the content inside the box, e.g., t for top, b for bottom, c for center. width : the width of the box. Make a box wider than the text width \u00b6 To make a box wider than the text width, we can use the \\adjustwidth command from the changepage package. Example: \\begin{adjustwidth}{-1cm}{-1cm} ... content \\end{adjustwidth} The box above will be 1cm wider on each side than the text width. Math \u00b6 wiki To use math, we need the amsmath package. The math commands only works in math mode which can be entered in one of the many math environments. Common math constructs \u00b6 The following table lists the most common math constructs: fractions : \\frac{<numerator>}{<denominator>} binomial coefficients : \\binom{n}{k} Subscript and superscript \u00b6 Subscript and superscript are created using the _ and ^ characters. If the subscript or superscript is longer than one character, we need to wrap it in curly braces. There are aslo some special characters that result in superscript: ' : prime * : star However, these characters alone works only in normal math text. If we want to use them in a subscript or superscript, we need to use the ^ and _ characters. Example: x* % correct print a_{x*} % inccorrect print - the star is not in superscript a_{x^*} % correct print If/else variants \u00b6 For that, we use the cases environment. Example: \\begin{equation} f(x) = \\begin{cases} 0 & \\quad \\text{if } x < 0 \\\\ 1 & \\quad \\text{if } x \\geq 0 \\end{cases} \\end{equation} Equations and similar environments \u00b6 The following environments are available for equations: equation : for single equations align : for multiple equations aligned at a specific character alignat : for multiple equations aligned at multiple characters aligned : for multiple equations inside another math environment (e.g., equation ) gather : for multiple equations not aligned, each line numbered separately The new line is created using the \\\\ command. Nothe that the \\\\ command is not allowed in the equation environment . To align the equations in the align and alignat environments, we use the & character. Problem and similar environments \u00b6 wiki The environments for special math text blocks are not included in the amsmath package. We can define them manually using the \\newtheorem command. Example: \\newtheorem{problem}{Problem} \\begin{problem} This is a problem. \\end{problem} Here, the first argument of the \\newtheorem command is the name of the environment, and the second argument is the name of the environment in the output. There can be also an optional third argument, which is the name of the counter that is used for numbering the environment. For example, if we want to use another environment for propositions with the same numbering as the problems, we can use the following command: \\newtheorem{proposition}[problem]{Proposition} Some environments are already defined in the amsthm package, e.g., proof . Common environments names and their meaning \u00b6 theorem : a statement to be proved lemma : a minor theorem, with a limited applicability outside the context of the main theorem corollary : a theorem that follows directly from another theorem proposition : a less important theorem, usually used for something elementary, obvious, so that it does not require a proof premise : a statement that is assumed to be true and it represents a fact that is used in the proof of the theorem. For example: \"We only consider sets where half of the numbers are even...\" assumption : a statement that is assumed to be true and it represents a condition under which the theorem is true. For example: \"Assuming half of the numbers in the set are even...\" More can be fount on proofwiki Math fonts \u00b6 wiki the default math font is typed as common math italic. To use a different font, we need to use special commands: \\mathrm{} : for normal font in math mode, e.g., for multi-letter subscripts and superscripts \\mathbb{} : for blackboard bold font, e.g., for special sets (R, N, ...). This font requires the amsfonts package. Bold math font \u00b6 To use a bold font, we can use the \\bm command from the bm package: \\usepackage{bm} \\begin{equation} \\bm{a} = \\bm{b} + \\bm{c} \\end{equation} Correct size of braces \u00b6 To size any type of braces correctly, if the content is larger than the braces, we can use the \\left and \\right commands. Example: \\left( \\frac{a}{b} \\right) Links \u00b6 wiki For links, we need the hyperref package. Typical usage: \\url{https://www.google.com} \\href{https://www.google.com}{Google} Footnotes \u00b6 The footnote is created using the \\footnote{} command. Override footnote numbering \u00b6 To override the footnote numbering (e.g. to repeat the same number twice), we can use the \\setcounter command. Example: \\setcounter{footnote}{1} # set the footnote counter to 1 Disallow footnote splitting \u00b6 To disallow footnote splitting, we can increase the \\interfootnotelinepenalty command. Example: \\interfootnotelinepenalty=10000 Bibliography \u00b6 See more on SE . For bibliography management, whole toolchain is usually needed, including: a tool that generates the bibliography file (e.g. Zotero, Mendeley, ...) a latex package that cares about the citations style (e.g. biblatex, natbib, or default style) the real bibliography processer that generates and sorts the bibliography (e.g. bibtex, biber, ...) However, not all combinations of theses tools are possible. For understanding the pipeline and the possible combinations, see the following figure: When choosing what package to use in latex, we have to take care that we: have the bibliography file in the right format ( .bib for all pipelines, but the content differs) have the style in the right format ( .bst for default or natbib, .bbx for biblatex) By default, we should use the biblatex - Biber pipeline. Howevver, there are some circumstances where we need to use bibtex, for example, if we need to use a style that is not available for biblatex (as there is no conversion tool ). The styles available for biblatex are listed on CTAN . Latex document configuration \u00b6 Biblatex styling \u00b6 Basic setup: \\usepackage[style=numeric]{biblatex} ... \\addbibresource{bibliography.bib} ... \\printbibliography The style parameter is optional. The styles available for biblatex are listed on CTAN . Handle overflowing URLs in bibliography \u00b6 Sometimes, the links overflow the bibliography. To fix this, we can use the following commands: \\setcounter{biburllcpenalty}{100} \\setcounter{biburlucpenalty}{100} \\setcounter{biburlnumpenalty}{100} \\biburlnumskip=0mu plus 1mu\\relax \\biburlucskip=0mu plus 1mu\\relax \\biburllcskip=0mu plus 1mu\\relax Default and natbib styling \u00b6 Basic setup: \\bibliographystyle{plain} ... \\bibliography{bibliography} Note that we do not have to use any package to use basic cite commands. Also note, that the \\bibliographystyle command is mandatory . Finally, we do not need to specify the extension of the bibliography file. Natbib \u00b6 The bibtex bibliography management system is quite old and does not support many features. To overcome this, we can use the natbib package: \\usepackage{natbib} Commands for citing \u00b6 There are multiple commands for citing, each resulting in a different output. The two most important variants are in-text citation and parenthetical citation: In-text citation: the citation is part of the text. IEEE: this was proven by Smith et al. [1] APA: this was proven by Smith et al., 2019 Parenthetical citation: the citation is not part of the text. IEEE: this has been proven before [1] APA: this has been proven before (Smith et al., 2019) Unfortunately, the commands for these two variants are not consistent across the bibliography packages. The following table summarizes the commands for the two variants: | Package | In-text citation | Parenthetical citation | Full citation | | --- | --- | --- | --- | | Biblatex | \\textcite{<key>} | \\cite{<key>} ( \\parencite for APA) | \\fullcite{<key>} | | Natbib | \\cite{<key>} | \\citep{<key>} | \\bibentry{<key>} (requires the bibentry package) | There are more citation commands resulting in different styles for each bibliography styling package, and each of these packages can be also configurated for even more customized look. For more information, see the following links: Natbib styles Adittional details for citation (page number, chapter, ... ) \u00b6 To add additional details to the citation, we can use the optional argument of the citation command: \\cite[page~123]{key} Adding a reference to the bibliography without citing it \u00b6 For this, we use the \\nocite command. Example: \\nocite{key} Bibliography entries \u00b6 There are many types of bibliography entries, each of them with different fields. to make things even more complicated, these entries does not match the entry types in Zotero. To make it easier, many use-cases are covered in the table below: Use-case Biblatex Zotero Book @book Book Book chapter @incollection Book Section Conference paper @inproceedings Conference Paper Journal article @article Journal Article Report @report Report Thesis @thesis Thesis Web page @online Web Page Legal document @legal Unavailable. Use Report instead. Other sources: Biblatex documentation Zotero documentation Zotero legal types Custom commands and variables \u00b6 wiki Basic syntax for defining a new command or variable is: \\newcommand{\\<command name>}[<number of arguments>]{<command definition>} The number of arguments is optional. If it is not specified, the command does not take any arguments. The command is then used as follows: \\<command name>{<argument 1>}{<argument 2>}...{<argument n>} Providing default values for arguments \u00b6 Latex support default values for the first argument. The syntax is: \\newcommand{\\<command name>}[<number of arguments>][<default value>]{<command definition>} The command can than be used both with and without the optional argument: \\newcommand{\\mycommand}[2][0]{...} \\mycommand{<argument 2>} \\mycommand[<argument 1>]{<argument 2>} Note that that if we want to supply the optional argument, we use the square brackets. For two optional arguments, we have to use the twoopt package. Example: \\usepackage{twoopt} ... \\newcommandtwoopt{\\mycommand}[2][default1][default2]{...} Here again, both optioanl arguments, if supplied, must be supplied in the square brackets. Splitting the document into multiple files \u00b6 There are two ways to split the document into multiple files: \\input{file} \\include{file} The \\include is intended for chapters or other large parts of the document. It has the following properties: it starts a new page before and after the included file it does not allow nesting there is a special command \\includeonly{file1,file2,...} which allows to include only the specified files. This is useful for large documents where we want to compile only a part of the document. Without this command we would need to search for the include command and comment it out. The \\input command is intended for smaller parts of the document. Contrary to the \\include command, there is no special behavior involved. Instead, the content of the file is simply pasted at the place of the \\input command. Speedup Techniques \u00b6 The compilation of large documents can be slow. There are several techniques to speed up the compilation: split the document into multiple files and use \\includeonly to include only the relevant files precompiling the preamble using draft mode Precompiling the preamble \u00b6 The preamble is the part of the document before the \\begin{document} command. It contains the document configuration, packages, etc. Because the included packages are usually large, the compilation of the preamble can be slow. To speed up the compilation, we can precompile the preamble and use the precompiled preamble in the main document. This can be done using the mylatexformat package. The usage is as follows: At the beginning of the preamble, add the following comment: %&<format name> . This will tell the compiler to use the specified format. The <format name> can be arbitrary, but it is recommended to use the same name as the main document. To spare some preamble content from being precompiled (dynamic content), add a command \\endofdump after the content that should not be precompiled. run the following command: PowerShell pdflatex --ini -jobname=\"<format name>\" \"&pdflatex\" mylatexformat.ltx <format name>.tex Afther this, the compilation of the main document should be faster. For more information, see the package documentation or the SO question . Miscelaneous tasks \u00b6 Balancing columns in two-column documents \u00b6 To balance the columns at the end of the document, we can use the flushend package. Just add \\usepackage{flushend} to the preamble. Common problems \u00b6 Ugly font in pdf \u00b6 This can be cause by the missing vector fonts. If the vector fonts are missing, the bitmap fonts are used instead. To check if this is the cause, zoom in on the pdf. If the text is blurry, the bitmap fonts are used. To fix this, install the vector fonts. On Windows, install the cm-super package through MikTeX.","title":"Latex manual"},{"location":"LaTeX/Latex%20manual/#document-structure","text":"The document structure is well documented on wikibooks . The basic structure is: \\documentclass[<options>]{<class>} ... \\begin{document} ... \\end{document} Lot of content is usually put in the preamble, i.e., between the \\documentclass and \\begin{document} commands. The preamble usual content is: loading packages with \\usepackage{<package>} providing new commands or redefining existing commands configuring packages supplying metadata for the document (title, author, etc.)","title":"Document structure"},{"location":"LaTeX/Latex%20manual/#title-page","text":"The title page typically contains the title, authors, and potentially other metadata like date or keywords. The standard way how to create the title page is to first define the metadata and then use the \\maketitle command. To print the whole title page. The metadata available for standard article class: \\title{<title>} \\author{<author>} \\date{<date>} . \\today can be used to print the current date. This is the default value if we omit the \\date command. A special non-numbered footnote can be added to most fields using the \\thanks{<text>} command.","title":"Title page"},{"location":"LaTeX/Latex%20manual/#authors-specification","text":"By default, all authors should be filled within a single \\author command, separated by the \\and command. If we need to specify the affiliations, we can do it inside the \\author command. This way, each author have the affiliation printed right after/bellow the author name. However, if there are many authors with shared affiliations, this approach is unsustainable. Instead, we can use the authblk package which lets us specify the authors and affiliations separately and connect them to authors using footnotes. Example: \\usepackage{authblk} \\author[1]{Author 1} \\author[2]{Author 2} \\author[1]{Author 3} \\affil[1]{Affiliation 1} \\affil[2]{Affiliation 2}","title":"Authors specification"},{"location":"LaTeX/Latex%20manual/#keywords","text":"Keywords are not part of the standard article class. If we need to include them when using the standard article class, we can provide the command ourselves. Example: \\providecommand{\\keywords}[1]{\\textbf{\\textit{Index terms---}} #1} ... \\keywords{keyword1, keyword2, ...}","title":"Keywords"},{"location":"LaTeX/Latex%20manual/#appendices","text":"Appendicies are started with the command appendix . Then, each chapter started with the \\chapter command is considered an appendix.","title":"Appendices"},{"location":"LaTeX/Latex%20manual/#book-class-structuring","text":"Appart from sections and chapters, the book class also supports top level parts for marking special areas of the document. The parts are: \\frontmatter : the front matter of the document, i.e., the title page, abstract, table of contents, etc. The page numbering is in roman numerals. \\mainmatter : the main matter of the document, i.e., the main content. The page numbering is in arabic numerals, and the page numbering is reset to 1. \\backmatter : the back matter of the document, i.e., the bibliography, appendices, etc. The page numbering is in arabic numerals. Chapters are not numbered.","title":"Book class structuring"},{"location":"LaTeX/Latex%20manual/#escape-characters","text":"LaTeX uses man6y special characters which needs to be escaped. Unfortunatelly, there is no single escape character, instead, there are many. The following table lists the most common escape characters: Character Escape sequence [ {[} ] {]}","title":"Escape characters"},{"location":"LaTeX/Latex%20manual/#special-characters","text":"","title":"Special characters"},{"location":"LaTeX/Latex%20manual/#non-breaking-characters","text":"~ : non-breaking space \\nobreakdash- : non-breaking dash","title":"Non-breaking characters"},{"location":"LaTeX/Latex%20manual/#text-and-paragramph-formatting","text":"wiki","title":"Text and paragramph formatting"},{"location":"LaTeX/Latex%20manual/#fonts","text":"","title":"Fonts"},{"location":"LaTeX/Latex%20manual/#determining-used-font","text":"To determine the font (size, type) used in a particular place put the following command there: \\showthe\\font Then compile the document. The font information will be printed in the log (e.g.: \\OT1/cmr/m/n/10 ).","title":"Determining used font"},{"location":"LaTeX/Latex%20manual/#changing-font-size","text":"The default font size is 10pt . To change the default font size, set the documentclass option 12pt or 11pt (other sizes are not avialable). See the wiki for more information. The size can be also changed for a specific part of the document. We can use some predefined sizes e.g.: Normal {\\tiny tiny} normal or we can use the \\fontsize command to set arbitrary size. Example: {\\fontsize{<size>}{<line spacing>}\\selectfont <text>}","title":"Changing font size"},{"location":"LaTeX/Latex%20manual/#font-color","text":"wiki To change the font color, we need to use the xcolor package. First, we need to define the color using the \\definecolor command. Example: \\definecolor{my_color}{RGB}{255,0,0} # RGB color \\definecolor{my_color}{HTML}{FF0000} # HTML color \\definecolor{my_color}{cmyk}{0,1,1,0} # CMYK color Then, we can use various commands to change the font color: \\textcolor{<color>}{<text>} : change the color of the text {\\color{<color>}<text>} : change the color of the text. Can span multiple paragraphs. note that other commands that have color options usually depend on the xcolor package, and accept the same color definitions.","title":"Font color"},{"location":"LaTeX/Latex%20manual/#horizontal-and-vertical-spacing","text":"Most of the time, the spacing should be handled by LaTeX automatically. However, there are cases when we need to adjust the spacing manually, either in a single place or globally. To adjust the spacing in a single place, we can use the following commands: \\hspace{<length>} : horizontal space \\vspace{<length>} : vertical space Note that we can use negative values for the <length> parameter, so we can use these commands to relatively positioning.","title":"Horizontal and vertical spacing"},{"location":"LaTeX/Latex%20manual/#horizontal-and-vertical-alignment","text":"","title":"Horizontal and vertical alignment"},{"location":"LaTeX/Latex%20manual/#horizontal-alignment","text":"wiki By default, the text is fully justified. To change the justification (alignment), to left or right, we can use either environments: flushleft for left alignment, flushright for right alignment, or commands: \\raggedright for left alignment, \\raggedleft for right alignment.","title":"Horizontal alignment"},{"location":"LaTeX/Latex%20manual/#vertical-alignment","text":"By default, the text starts from the top of the page. To align the text to the bottom of the page, we can use the vfill command. Example: Some text \\vfill Some text at the bottom Note that the vfill works only if there is something before it, it does not work if it is the first command on the page. To make it work, we can use the null command. Example: \\null \\vfill Some text at the bottom","title":"Vertical alignment"},{"location":"LaTeX/Latex%20manual/#subscript-and-superscript","text":"In math mode, the subscript and superscript are created using the _ and ^ characters. In text mode, we need to use a special commands: \\textsubscript and \\textsuperscript . Example: H\\textsubscript{2}O","title":"Subscript and superscript"},{"location":"LaTeX/Latex%20manual/#lists","text":"The list enviroments have the following syntax: \\begin{<list type>} \\item <item 1> \\item <item 2> ... \\end{<list type>} The following list types are available: itemize : bullet points enumerate : numbered list description : description list. Items can have a label, which is specified as an optional argument of the \\item command. More typoes of lists like questions or checklists can be created using external packages.","title":"Lists"},{"location":"LaTeX/Latex%20manual/#units-numbers-and-currency","text":"Units are usually typeset with a small space between the number and the unit. Normal space should be avoidedas it is too wide. Also, we sometimes want a special separator in large numbers. For these purposes, the best practice is to use the siunitx package. Eith siunitx , the units are typeset using the \\SI command. Example: \\SI{10}{\\meter} The numbers should be typeset using the \\num command. Example: \\num{1000000} The currecnies that have the currency sign before the number are also typeset using the \\SI command, but we need to use the optional argument for prefix: \\SI{10}[\\$]{}","title":"Units, numbers, and currency"},{"location":"LaTeX/Latex%20manual/#quotes","text":"The quotes are typeset using the csquotes package. The quotes are typeset using the \\enquote command. Example: \\enquote{This is a quote}","title":"Quotes"},{"location":"LaTeX/Latex%20manual/#supported-units","text":"\\s : second \\km : kilometer \\km\\per\\hour : kilometer per hour \\percent : percent","title":"Supported units"},{"location":"LaTeX/Latex%20manual/#boxes","text":"wiki Everythign in LaTeX is a box. Each character is a box, stored in a larger box for each word, and analogically for each line, paragraph, etc. Most of the time we just set properties for the boxes, but sometimes we need to create a box manually to format or position the content. The following table presents the most common box commands: Command paragraph witdth \\parbox single fixed \\pbox single flexible \\minipage multiple fixed The first two parameters are shared for these commands: pos : the position of the box, e.g., t for top, b for bottom, c for center. The position refers to the part of the box that is aligned with the surrounding text. height : the height of the box. The parbox and minipage share another two parameters that follows: contentpos : the position of the content inside the box, e.g., t for top, b for bottom, c for center. width : the width of the box.","title":"Boxes"},{"location":"LaTeX/Latex%20manual/#make-a-box-wider-than-the-text-width","text":"To make a box wider than the text width, we can use the \\adjustwidth command from the changepage package. Example: \\begin{adjustwidth}{-1cm}{-1cm} ... content \\end{adjustwidth} The box above will be 1cm wider on each side than the text width.","title":"Make a box wider than the text width"},{"location":"LaTeX/Latex%20manual/#floats","text":"The following environments are floats: figure table algorithm","title":"Floats"},{"location":"LaTeX/Latex%20manual/#placement","text":"Any float takes the position as a first argument. The following positions are available: h : here t : top b : bottom p : special dedicated page per float ! : ignore nice positioning and put it according to the float specifiers The placement algorithm then iterate pages starting from the page where the float is placed. For each page, it tries to find a place for the float according to the float specifiers (in the same order as they appear in the float position argument). In case of success, the procedure stops and place the float. If the procedure fails for all pages, the float is placed at the end. Note that by specifying the float position, we only add constraints to the placement algorithm, but do not guarantee the placement. By omitting the float position, we can accually make the float appear closer to the place where it is defined. Sources: LaTeX Wikibook Overleaf","title":"Placement"},{"location":"LaTeX/Latex%20manual/#default-placement","text":"The default placement differs between environments and also classes. For example for article class, the default placement for figure and table is tbp ( see SO ).","title":"Default placement"},{"location":"LaTeX/Latex%20manual/#figures","text":"The float environment for figures is figure . The image itself is included using the \\includegraphics command. The mandatory argument of the \\includegraphics command is the path to the image file. This path can be relative or absolute. If the path is relative, it is relative to the location of the main .tex file. The file extension can be omitted. If the file extension is omitted, the compiler will try to find the file with various extensions. Therefore, it is only recommended to omit the file extension if there is only one file with the same name. Optional arguments of the \\includegraphics command are the following: width : the width of the image. example: \\includegraphics[width=0.5\\textwidth]{image.png} scale : the scale of the image with respect to the original size. example: \\includegraphics[scale=0.5]{image.png}","title":"Figures"},{"location":"LaTeX/Latex%20manual/#subfigures","text":"For more images in one float, we can use the subfigure environment from the subcaption package. The subfigure environment is used as follows: \\begin{figure}[h] \\centering \\begin{subfigure}{0.3\\textwidth} \\includegraphics[width=\\textwidth]{image1.png} \\caption{Image 1} \\label{fig:image1} \\end{subfigure} \\begin{subfigure}{0.3\\textwidth} \\includegraphics[width=\\textwidth]{image2.png} \\caption{Image 2} \\label{fig:image2} \\end{subfigure} \\caption{My figure} \\label{fig:my_figure} \\end{figure} If the figures are not of the same height, they will be aligned at the bottom. To align them at the top, we can use the T option for the subfigure environment. Example: \\begin{subfigure}[T]{0.3\\textwidth} ... \\end{subfigure}","title":"Subfigures"},{"location":"LaTeX/Latex%20manual/#tables","text":"The float environment for tables is table . However, the rows and columns are wrapped in another environment. The default inner enviroment is tabular , however, there are many other packages that extends the functionality. In practice, there are currently three inner environments to consider: tabular : the default environment that is sufficient for simple tables tabulary : the environment that allows to create columns with automatic width. If the main or only issue of the table is that it needs to fit a specific width, this is the environment to use. tblr : the tblr environment from the tabulararray package is the most up to date tabular environment that support many features. Also, it splits the table presentation from the table content, which can make generating tables from code easier. The only downside is that it does not support automatic column width .","title":"Tables"},{"location":"LaTeX/Latex%20manual/#column-types","text":"The column types are specified in the argument of the tabular or equivalent environment. The following column types are available by default: l : left aligned c : centered r : right aligned p{width} : paragraph column with specified width Other column types can be provided by the inner environment package or by the user.","title":"Column types"},{"location":"LaTeX/Latex%20manual/#simple-tables-with-tabular-environment","text":"The usual way to create a table in the tabular environment is: \\begin{table}[h] \\centering % center the content of the table environment \\begin{tabular}{|c|c|} ... rows and columns \\end{tabular} \\caption{My table} \\label{tab:my_table} \\end{table}","title":"Simple tables with tabular environment"},{"location":"LaTeX/Latex%20manual/#columns-with-automatic-width-tabulary","text":"By default, laTeX does not support automatic width for columns, i.e., sizing the columns by their content. To enable this feature, we can use the tabulary package, which provides the tabulary environment (which is a replacement for the tabular environment). The columns with automatic width are specified by the L , C , R column types. Note that the new column types can be combined with the standard column types. In that case, the standard columns will have width according to their content, and the rest of the space will be distributed among the new column types.","title":"Columns with automatic width: tabulary"},{"location":"LaTeX/Latex%20manual/#complex-tables-with-tabulararray-package","text":"The tabulararray package provides : full control over the table, most features among all packages separation of the table content and the table presentation simpler code for basic tasks like multirows and multicolumns, wrapping text in cells, etc. Notable features present in the tabulararray package missing in other packages: footnotes in tables (otherwise, it requires a threeparttable environment wrapper) As other tabular packages, there are some incompatibilities related to the tabulararray package. So far, I observed only incompatibilities with the tabulararray talltblr environment, not with the standard tblr environment. The following table summarizes the incompatibilities I found so far: cases : Obviously, the cases environment uses table components internally. When using together with talltblr , the cases environment compilation results with the following error: latex \"\\begin{cases}\" package array empty preamble l' used . The solution is to use the use the new +cases environment provided by the tabulararray package. As a bonus, the +cases environment also fixes some visual glitches. Steps: enable the +cases environment by adding \\UseTblrLibrary{amsmath} to the preamble replace the cases environment with the +cases environment tabular : yes, the talltblr environment is incompatible with the default tabular environment. The solution is simple: replace all tabular environments with tblr environments.","title":"Complex tables with tabulararray package"},{"location":"LaTeX/Latex%20manual/#styling","text":"","title":"Styling"},{"location":"LaTeX/Latex%20manual/#table-grid-visual-lines","text":"Usually, tables contain some horizontal and vertical lines to separate the cells. Unfortunately, the way how to create these lines differs completely between horizontal and vertical lines.","title":"Table grid (visual lines)"},{"location":"LaTeX/Latex%20manual/#horizontal-lines","text":"For horizontal lines, we can: use various commands to create a line in a specific row: \\hline : creates a line in the current row specify the lines outside the data with the tabularray package:","title":"Horizontal lines"},{"location":"LaTeX/Latex%20manual/#specify-the-lines-outside-the-data-with-the-tabularray-package","text":"The format is hline{<list of rows>} = {<line style>} . Example: \\begin{tblr}{ colspec={|c|c|c|}, hline{1,Z} = {1pt, solid}, } ... \\end{tblr} The <list of rows> can be specified as: single row: hline{1} = {1pt, solid} multiple rows: hline{1,3,5} = {1pt, solid} range of rows: hline{1-Z} = {1pt, solid} Rows are numbered from 1. The Z character is used to specify the last row.","title":"Specify the lines outside the data with the tabularray package"},{"location":"LaTeX/Latex%20manual/#styling-the-specific-rows-or-columns","text":"With the tabularray package, we can style the table header differently than the rest of the table. In fact, we can style each specific row or column differently. for that, we use the row and column keys. Example \\begin{tblr}{ colspec={|c|c|c|}, row{1} = {bg=gray9, fg=white}, column{1} = {bg=gray9, fg=white}, } The syntax is the same for both keys: row{<row number>} = {<style>} . For the <row number> specification, see the grid section. The <style> specification is a list of key-value pairs. The keys are the following: font : the font style","title":"Styling the specific rows or columns"},{"location":"LaTeX/Latex%20manual/#configure-the-space-between-columns","text":"In most packages, the space between columns is configured using the \\tabcolsep variable. Example: \\setlength{\\tabcolsep}{10pt} However, in the tblr environment, the space between columns is configured using the leftsep and rightsep keys. Example:, \\begin{tblr} { colspec={llllrr}, leftsep=2pt, rightsep=2pt } By default, the leftsep and rightsep are set to 6pt .","title":"Configure the space between columns"},{"location":"LaTeX/Latex%20manual/#define-table-styles-with-tabularray-package","text":"The tabularray package provides a way to define table styles. First, we define the new environment with the \\NewTblrEnviron command. Later, we use the SetTblrInner and SetTblrOuter commands to define the style. Example: \\NewTblrEnviron{mytblr}{ \\SetTblrInner{rowsep=1pt} \\SetTblrOuter{hspan=minimal} }","title":"Define table styles with tabularray package"},{"location":"LaTeX/Latex%20manual/#rotated-text","text":"To rotate some text, we can use the \\rotatebox command: \\rotatebox{90}{Rotated text}","title":"Rotated text"},{"location":"LaTeX/Latex%20manual/#multirows-and-multicolumns","text":"Depending on the inner environment, the multirows and multicolumns are created using different commands.","title":"Multirows and multicolumns"},{"location":"LaTeX/Latex%20manual/#tabular-environment","text":"In the tabular environment, the multirows and multicolumns are created using the \\multicolumn and \\multirow commands. Example: \\begin{tabular}{cc} \\multicolumn{2}{c}{multi column} \\\\ \\multirow{2}{*}{multi row} & 1 \\\\ & 2 \\\\ \\end{tabular}","title":"tabular environment"},{"location":"LaTeX/Latex%20manual/#tabulararray-environment","text":"In the tabulararray environment, the multirows and multicolumns are created using the \\SetCell command. Example: \\begin{tblr}{cc} \\SetCell[c=2]{c} multi column & \\\\ \\SetCell[r=2]{c} multi row & 1 \\\\ & 2 \\\\ \\end{tblr} Note that for multicolumns, we need to add the column divider ( & ) after the \\SetCell command for each column that is spanned by the multicolumn .","title":"tabulararray environment"},{"location":"LaTeX/Latex%20manual/#export-google-sheets-to-latex-tables","text":"There is ann addon called LatexKit which can be used for that.","title":"Export google sheets to latex tables"},{"location":"LaTeX/Latex%20manual/#footnotes-in-tables","text":"In tables and table captions, the \\footnote command does not work correctly. Also, it is not desirable to have the footnote at the bottom of page, instead, we want the footnote to be at the bottom of the table. To achieve this, we use a special environment: threeparttable : if we are using the tabular or tabulary environment talltblr : if we are using the tblr environment","title":"Footnotes in tables"},{"location":"LaTeX/Latex%20manual/#using-the-threeparttable","text":"The threeparttable environment is used as follows: \\begin{table}[h] \\centering \\caption{My table} \\label{tab:my_table} \\begin{threeparttable} \\begin{tabular}{|c|c|} one$^a$ & two$^b$ \\\\ ... other rows and columns \\end{tabular} \\begin{tablenotes} \\item $^a$footnote 1 \\item $^b$footnote 2 \\end{tablenotes} \\end{threeparttable} \\end{table}","title":"Using the threeparttable"},{"location":"LaTeX/Latex%20manual/#using-the-talltblr","text":"The talltblr environment is used as follows: \\begin{table}[h] \\centering \\caption{My table} \\label{tab:my_table} \\begin{talltblr}[ label = none, note{a} = {footnote 1}, note{b} = {footnote 2} ]{ colspec={|c|c|}, } one\\TblrNote{a} & two\\TblrNote{b} \\\\ ... other rows and columns \\end{talltblr} \\end{table} Notice the label = none option. Without it, the table numbering is raised again, resulting in the table being numbered twice.","title":"Using the talltblr"},{"location":"LaTeX/Latex%20manual/#rotated-text_1","text":"To rotate some text, we can use the \\rotatebox command: \\rotatebox{90}{Rotated text} If we need to limit the width of the rotated text, we can use the \\parbox command. Example: \\rotatebox{90}{\\parbox{2cm}{Rotated text}}","title":"Rotated text"},{"location":"LaTeX/Latex%20manual/#algorithms","text":"wiki There are two types of enviroments for, depending on the purpose: algorithm : for pseudocode lstlisting : for code in a specific language","title":"Algorithms"},{"location":"LaTeX/Latex%20manual/#pseudocode","text":"The floating environment for pseudocode is algorithm . It is an equivalent of the table environment for tables. Same as with tables, there is also an inner environment for the pseudocode. The options for the inner environment are: algorithmic : basic environment for pseudocode algorithmicx : extension of the algorithmic environment, supports custom keywords algorithm2e : algpseudocodex : extension of the algorithmicx package, various improvements pseudo : a package for writing pseudocode directly, not using any special commands Their properties are summarized in the following table: | Environment | Package | Updated | Custom keywords | |--- | --- | --- | --- | | algorithmic | algorithms | 2009 | no | | algorithmicx | algorithmicx | 2005 | yes | | algorithm2e | algorithm2e | 2017 | yes | | algpseudocodex | algpseudocodex | 2023 | yes | | program | program | 2017 | no | | pseudo | pseudo | 2023 | yes | If the algorithm requires a complex description and cannot be expressed using typical pseudocode, we can resort to using human language directly in the algorithm environment. There is such example at SO","title":"Pseudocode"},{"location":"LaTeX/Latex%20manual/#algoritmicx-and-algpseudocodex","text":"Normal code lines are written using the \\State command: \\State $x \\gets 0$","title":"Algoritmicx and Algpseudocodex"},{"location":"LaTeX/Latex%20manual/#conditions","text":"Conditions are written using the \\If , \\ElseIf , and \\Else commands. Example: \\If{$x < 0$} \\State $x \\gets 0$ \\ElsIf{$x > 100$} \\State $x \\gets 100$ \\Else \\State $x \\gets x$ \\EndIf","title":"Conditions"},{"location":"LaTeX/Latex%20manual/#loops","text":"For loop: \\For{$ i = 1, \\dots, n $} ... \\EndFor","title":"Loops"},{"location":"LaTeX/Latex%20manual/#boolean-operators","text":"Boolean operators are not defined by default. Either use plain text, or define them (see the Additional keywords section below).","title":"Boolean operators"},{"location":"LaTeX/Latex%20manual/#additional-keywords","text":"We can define additional keywords using the \\algnonewcommand command. The format is \\algnonewcommand<custom command>{<keyword>} . Example: \\algnewcommand\\Not{\\textbf{not}}","title":"Additional keywords"},{"location":"LaTeX/Latex%20manual/#empty-lines","text":"To add an empty line, use the \\State command without any argument. If the numbering is on and we want to skip the numbering for the empty line, we can use the \\Statex command.","title":"Empty lines"},{"location":"LaTeX/Latex%20manual/#functions-and-procedures","text":"Functions and procedures are defined using the \\Function and \\Procedure commands. Example: \\Function{my_function}{a, b} \\State $x \\gets a + b$ \\State \\Return $x$ \\EndFunction We can call the function or procedure using the \\Call command. Example: \\Call{my_function}{1, 2}","title":"Functions and procedures"},{"location":"LaTeX/Latex%20manual/#line-numbering","text":"To add line numbering, add an optional argument to the algorithmic environment. The argument is the number determining the frequency of the line numbering. Example: \\begin{algorithmic}[3] % number every 3rd line ... \\end{algorithmic}","title":"Line numbering"},{"location":"LaTeX/Latex%20manual/#centering-of-floats-wider-than-text-width","text":"If the float is wider than the text width, it is not centered, but instead it is left-aligned with the text and it overflows on the right side. To fix this, we can wrap the too-wide float content in the \\makebox command. Example: \\begin{figure}[h] \\makebox[\\textwidth]{ \\includegraphics[width=1.2\\textwidth]{my_image.png} } \\caption{My figure} \\label{fig:my_figure} \\end{figure}","title":"Centering of floats wider than text width"},{"location":"LaTeX/Latex%20manual/#boxes_1","text":"wiki Everythign in LaTeX is a box. Each character is a box, stored in a larger box for each word, and analogically for each line, paragraph, etc. Most of the time we just set properties for the boxes, but sometimes we need to create a box manually to format or position the content. The following table presents the most common box commands: Command paragraph witdth \\parbox single fixed \\pbox single flexible \\minipage multiple fixed The first two parameters are shared for these commands: pos : the position of the box, e.g., t for top, b for bottom, c for center. The position refers to the part of the box that is aligned with the surrounding text. height : the height of the box. The parbox and minipage share another two parameters that follows: contentpos : the position of the content inside the box, e.g., t for top, b for bottom, c for center. width : the width of the box.","title":"Boxes"},{"location":"LaTeX/Latex%20manual/#make-a-box-wider-than-the-text-width_1","text":"To make a box wider than the text width, we can use the \\adjustwidth command from the changepage package. Example: \\begin{adjustwidth}{-1cm}{-1cm} ... content \\end{adjustwidth} The box above will be 1cm wider on each side than the text width.","title":"Make a box wider than the text width"},{"location":"LaTeX/Latex%20manual/#math","text":"wiki To use math, we need the amsmath package. The math commands only works in math mode which can be entered in one of the many math environments.","title":"Math"},{"location":"LaTeX/Latex%20manual/#common-math-constructs","text":"The following table lists the most common math constructs: fractions : \\frac{<numerator>}{<denominator>} binomial coefficients : \\binom{n}{k}","title":"Common math constructs"},{"location":"LaTeX/Latex%20manual/#subscript-and-superscript_1","text":"Subscript and superscript are created using the _ and ^ characters. If the subscript or superscript is longer than one character, we need to wrap it in curly braces. There are aslo some special characters that result in superscript: ' : prime * : star However, these characters alone works only in normal math text. If we want to use them in a subscript or superscript, we need to use the ^ and _ characters. Example: x* % correct print a_{x*} % inccorrect print - the star is not in superscript a_{x^*} % correct print","title":"Subscript and superscript"},{"location":"LaTeX/Latex%20manual/#ifelse-variants","text":"For that, we use the cases environment. Example: \\begin{equation} f(x) = \\begin{cases} 0 & \\quad \\text{if } x < 0 \\\\ 1 & \\quad \\text{if } x \\geq 0 \\end{cases} \\end{equation}","title":"If/else variants"},{"location":"LaTeX/Latex%20manual/#equations-and-similar-environments","text":"The following environments are available for equations: equation : for single equations align : for multiple equations aligned at a specific character alignat : for multiple equations aligned at multiple characters aligned : for multiple equations inside another math environment (e.g., equation ) gather : for multiple equations not aligned, each line numbered separately The new line is created using the \\\\ command. Nothe that the \\\\ command is not allowed in the equation environment . To align the equations in the align and alignat environments, we use the & character.","title":"Equations and similar environments"},{"location":"LaTeX/Latex%20manual/#problem-and-similar-environments","text":"wiki The environments for special math text blocks are not included in the amsmath package. We can define them manually using the \\newtheorem command. Example: \\newtheorem{problem}{Problem} \\begin{problem} This is a problem. \\end{problem} Here, the first argument of the \\newtheorem command is the name of the environment, and the second argument is the name of the environment in the output. There can be also an optional third argument, which is the name of the counter that is used for numbering the environment. For example, if we want to use another environment for propositions with the same numbering as the problems, we can use the following command: \\newtheorem{proposition}[problem]{Proposition} Some environments are already defined in the amsthm package, e.g., proof .","title":"Problem and similar environments"},{"location":"LaTeX/Latex%20manual/#common-environments-names-and-their-meaning","text":"theorem : a statement to be proved lemma : a minor theorem, with a limited applicability outside the context of the main theorem corollary : a theorem that follows directly from another theorem proposition : a less important theorem, usually used for something elementary, obvious, so that it does not require a proof premise : a statement that is assumed to be true and it represents a fact that is used in the proof of the theorem. For example: \"We only consider sets where half of the numbers are even...\" assumption : a statement that is assumed to be true and it represents a condition under which the theorem is true. For example: \"Assuming half of the numbers in the set are even...\" More can be fount on proofwiki","title":"Common environments names and their meaning"},{"location":"LaTeX/Latex%20manual/#math-fonts","text":"wiki the default math font is typed as common math italic. To use a different font, we need to use special commands: \\mathrm{} : for normal font in math mode, e.g., for multi-letter subscripts and superscripts \\mathbb{} : for blackboard bold font, e.g., for special sets (R, N, ...). This font requires the amsfonts package.","title":"Math fonts"},{"location":"LaTeX/Latex%20manual/#bold-math-font","text":"To use a bold font, we can use the \\bm command from the bm package: \\usepackage{bm} \\begin{equation} \\bm{a} = \\bm{b} + \\bm{c} \\end{equation}","title":"Bold math font"},{"location":"LaTeX/Latex%20manual/#correct-size-of-braces","text":"To size any type of braces correctly, if the content is larger than the braces, we can use the \\left and \\right commands. Example: \\left( \\frac{a}{b} \\right)","title":"Correct size of braces"},{"location":"LaTeX/Latex%20manual/#links","text":"wiki For links, we need the hyperref package. Typical usage: \\url{https://www.google.com} \\href{https://www.google.com}{Google}","title":"Links"},{"location":"LaTeX/Latex%20manual/#footnotes","text":"The footnote is created using the \\footnote{} command.","title":"Footnotes"},{"location":"LaTeX/Latex%20manual/#override-footnote-numbering","text":"To override the footnote numbering (e.g. to repeat the same number twice), we can use the \\setcounter command. Example: \\setcounter{footnote}{1} # set the footnote counter to 1","title":"Override footnote numbering"},{"location":"LaTeX/Latex%20manual/#disallow-footnote-splitting","text":"To disallow footnote splitting, we can increase the \\interfootnotelinepenalty command. Example: \\interfootnotelinepenalty=10000","title":"Disallow footnote splitting"},{"location":"LaTeX/Latex%20manual/#bibliography","text":"See more on SE . For bibliography management, whole toolchain is usually needed, including: a tool that generates the bibliography file (e.g. Zotero, Mendeley, ...) a latex package that cares about the citations style (e.g. biblatex, natbib, or default style) the real bibliography processer that generates and sorts the bibliography (e.g. bibtex, biber, ...) However, not all combinations of theses tools are possible. For understanding the pipeline and the possible combinations, see the following figure: When choosing what package to use in latex, we have to take care that we: have the bibliography file in the right format ( .bib for all pipelines, but the content differs) have the style in the right format ( .bst for default or natbib, .bbx for biblatex) By default, we should use the biblatex - Biber pipeline. Howevver, there are some circumstances where we need to use bibtex, for example, if we need to use a style that is not available for biblatex (as there is no conversion tool ). The styles available for biblatex are listed on CTAN .","title":"Bibliography"},{"location":"LaTeX/Latex%20manual/#latex-document-configuration","text":"","title":"Latex document configuration"},{"location":"LaTeX/Latex%20manual/#biblatex-styling","text":"Basic setup: \\usepackage[style=numeric]{biblatex} ... \\addbibresource{bibliography.bib} ... \\printbibliography The style parameter is optional. The styles available for biblatex are listed on CTAN .","title":"Biblatex styling"},{"location":"LaTeX/Latex%20manual/#handle-overflowing-urls-in-bibliography","text":"Sometimes, the links overflow the bibliography. To fix this, we can use the following commands: \\setcounter{biburllcpenalty}{100} \\setcounter{biburlucpenalty}{100} \\setcounter{biburlnumpenalty}{100} \\biburlnumskip=0mu plus 1mu\\relax \\biburlucskip=0mu plus 1mu\\relax \\biburllcskip=0mu plus 1mu\\relax","title":"Handle overflowing URLs in bibliography"},{"location":"LaTeX/Latex%20manual/#default-and-natbib-styling","text":"Basic setup: \\bibliographystyle{plain} ... \\bibliography{bibliography} Note that we do not have to use any package to use basic cite commands. Also note, that the \\bibliographystyle command is mandatory . Finally, we do not need to specify the extension of the bibliography file.","title":"Default and natbib styling"},{"location":"LaTeX/Latex%20manual/#natbib","text":"The bibtex bibliography management system is quite old and does not support many features. To overcome this, we can use the natbib package: \\usepackage{natbib}","title":"Natbib"},{"location":"LaTeX/Latex%20manual/#commands-for-citing","text":"There are multiple commands for citing, each resulting in a different output. The two most important variants are in-text citation and parenthetical citation: In-text citation: the citation is part of the text. IEEE: this was proven by Smith et al. [1] APA: this was proven by Smith et al., 2019 Parenthetical citation: the citation is not part of the text. IEEE: this has been proven before [1] APA: this has been proven before (Smith et al., 2019) Unfortunately, the commands for these two variants are not consistent across the bibliography packages. The following table summarizes the commands for the two variants: | Package | In-text citation | Parenthetical citation | Full citation | | --- | --- | --- | --- | | Biblatex | \\textcite{<key>} | \\cite{<key>} ( \\parencite for APA) | \\fullcite{<key>} | | Natbib | \\cite{<key>} | \\citep{<key>} | \\bibentry{<key>} (requires the bibentry package) | There are more citation commands resulting in different styles for each bibliography styling package, and each of these packages can be also configurated for even more customized look. For more information, see the following links: Natbib styles","title":"Commands for citing"},{"location":"LaTeX/Latex%20manual/#adittional-details-for-citation-page-number-chapter","text":"To add additional details to the citation, we can use the optional argument of the citation command: \\cite[page~123]{key}","title":"Adittional details for citation (page number, chapter, ... )"},{"location":"LaTeX/Latex%20manual/#adding-a-reference-to-the-bibliography-without-citing-it","text":"For this, we use the \\nocite command. Example: \\nocite{key}","title":"Adding a reference to the bibliography without citing it"},{"location":"LaTeX/Latex%20manual/#bibliography-entries","text":"There are many types of bibliography entries, each of them with different fields. to make things even more complicated, these entries does not match the entry types in Zotero. To make it easier, many use-cases are covered in the table below: Use-case Biblatex Zotero Book @book Book Book chapter @incollection Book Section Conference paper @inproceedings Conference Paper Journal article @article Journal Article Report @report Report Thesis @thesis Thesis Web page @online Web Page Legal document @legal Unavailable. Use Report instead. Other sources: Biblatex documentation Zotero documentation Zotero legal types","title":"Bibliography entries"},{"location":"LaTeX/Latex%20manual/#custom-commands-and-variables","text":"wiki Basic syntax for defining a new command or variable is: \\newcommand{\\<command name>}[<number of arguments>]{<command definition>} The number of arguments is optional. If it is not specified, the command does not take any arguments. The command is then used as follows: \\<command name>{<argument 1>}{<argument 2>}...{<argument n>}","title":"Custom commands and variables"},{"location":"LaTeX/Latex%20manual/#providing-default-values-for-arguments","text":"Latex support default values for the first argument. The syntax is: \\newcommand{\\<command name>}[<number of arguments>][<default value>]{<command definition>} The command can than be used both with and without the optional argument: \\newcommand{\\mycommand}[2][0]{...} \\mycommand{<argument 2>} \\mycommand[<argument 1>]{<argument 2>} Note that that if we want to supply the optional argument, we use the square brackets. For two optional arguments, we have to use the twoopt package. Example: \\usepackage{twoopt} ... \\newcommandtwoopt{\\mycommand}[2][default1][default2]{...} Here again, both optioanl arguments, if supplied, must be supplied in the square brackets.","title":"Providing default values for arguments"},{"location":"LaTeX/Latex%20manual/#splitting-the-document-into-multiple-files","text":"There are two ways to split the document into multiple files: \\input{file} \\include{file} The \\include is intended for chapters or other large parts of the document. It has the following properties: it starts a new page before and after the included file it does not allow nesting there is a special command \\includeonly{file1,file2,...} which allows to include only the specified files. This is useful for large documents where we want to compile only a part of the document. Without this command we would need to search for the include command and comment it out. The \\input command is intended for smaller parts of the document. Contrary to the \\include command, there is no special behavior involved. Instead, the content of the file is simply pasted at the place of the \\input command.","title":"Splitting the document into multiple files"},{"location":"LaTeX/Latex%20manual/#speedup-techniques","text":"The compilation of large documents can be slow. There are several techniques to speed up the compilation: split the document into multiple files and use \\includeonly to include only the relevant files precompiling the preamble using draft mode","title":"Speedup Techniques"},{"location":"LaTeX/Latex%20manual/#precompiling-the-preamble","text":"The preamble is the part of the document before the \\begin{document} command. It contains the document configuration, packages, etc. Because the included packages are usually large, the compilation of the preamble can be slow. To speed up the compilation, we can precompile the preamble and use the precompiled preamble in the main document. This can be done using the mylatexformat package. The usage is as follows: At the beginning of the preamble, add the following comment: %&<format name> . This will tell the compiler to use the specified format. The <format name> can be arbitrary, but it is recommended to use the same name as the main document. To spare some preamble content from being precompiled (dynamic content), add a command \\endofdump after the content that should not be precompiled. run the following command: PowerShell pdflatex --ini -jobname=\"<format name>\" \"&pdflatex\" mylatexformat.ltx <format name>.tex Afther this, the compilation of the main document should be faster. For more information, see the package documentation or the SO question .","title":"Precompiling the preamble"},{"location":"LaTeX/Latex%20manual/#miscelaneous-tasks","text":"","title":"Miscelaneous tasks"},{"location":"LaTeX/Latex%20manual/#balancing-columns-in-two-column-documents","text":"To balance the columns at the end of the document, we can use the flushend package. Just add \\usepackage{flushend} to the preamble.","title":"Balancing columns in two-column documents"},{"location":"LaTeX/Latex%20manual/#common-problems","text":"","title":"Common problems"},{"location":"LaTeX/Latex%20manual/#ugly-font-in-pdf","text":"This can be cause by the missing vector fonts. If the vector fonts are missing, the bitmap fonts are used instead. To check if this is the cause, zoom in on the pdf. If the text is blurry, the bitmap fonts are used. To fix this, install the vector fonts. On Windows, install the cm-super package through MikTeX.","title":"Ugly font in pdf"},{"location":"Programming/Common/","text":"Keymap \u00b6 Copy : Ctrl + C Cut : Ctrl + X Paste : Ctrl + V Toggle comment : Ctrl + Q Search in file : Ctrl + S Sellect all : Ctrl + A Format selection : Ctrl + F Format File : Ctrl + Shift + F Build : Ctrl + B Refactoring \u00b6 Rename : Ctrl + R Change signature : Ctrl + G Text transform : Ctrl + T + U : to upper case Surround with : Ctrl + W Command Line Interface (CLI) \u00b6 This chapter should guide you on how to design CLI in a user-friendly and predictable way. Mostly, it follows the POSIX standard , with the GNU long option extensions . There are two main types of CLI arguments: Options : (e.g., --help , -h ) are used to change the behavior of the program. They are usually optional and can be in any order. Operands : (e.g., file.txt ) are the input data for the program. They are usually required and their order is important. All operands should be placed after all options. Options \u00b6 Options can be of two types: Short options : (e.g., -h ) are one character long and are prefixed with a single dash. Long options : (e.g., --help ) are multi-characcter and are prefixed with two dashes. Options can have arguments: Short options and their arguments are separated by a space or can be concatenated, e.g., -o file.txt or -ofile.txt . The first variant is strongly recommended. Long options and their arguments are separated by a space or can be concatenated with an equal sign, e.g., --output file.txt or --output=file.txt . Also, short options can be grouped, e.g., -h -v -o file.txt can be written as -hvo file.txt . Multiple values in one option or operand \u00b6 If an option or operand contains multiple values (e.g., a list of files), the values should be separated by a comma, e.g., --files file1.txt,file2.txt,file3.txt . Exceptions \u00b6 Exceptions should be used to handle erroneus situations that are expected to happen. Exceptions should not be used for: Flow control , e.g., parse float from input, catch exception and try integer, then catch exception and try string... Unexpected situations , e.g., a method should always return a positive number, but it returns a negative one. For this we should use assertions, not exceptions. There are many types of exceptions, encapsulating different types of error description data. However, to begin with, it is not important to use some specific exception type. Using some general exception class is always much better than not using exceptions at all. Tests \u00b6 Testing private methods \u00b6 An urgent need to test privete method accompanied with a lack of knowledge of how to do it is a common problem. In almost all programming languages, the testing of private methods is obsturcted by the language itself, i.e., the test frameworks does not have a special access to private methods. In this section we disscuss the usuall solutions to this problem. These implementation is specific to a particular language, but the general ideas are the same. The possible approaches are: Makeing the method public : Only recommended if the method should be exposed, i.e., its functionality is not limited to the class itself. Move the method to a different class : Maybe, the method is correcly marked as private in the current context, but it can also be extracted to its own class, where it will become the main method of the class. This applies to methods that can be used in other contexts, or for methods contained in large classes. Mark the method as internal and make it public : This is a strategy that can be always applied with minimum effort. Various ways how to signalize that the method is intended for internal use are: Naming convention : The method name can start with an underscore, e.g., _my_method . Documentation : The comments can contain a warning that the method is intended for internal use. Namespace : The method can be placed in a namespace that signals that it is intended for internal use, e.g., internal::my_method . Special access : We can use special language-dependant tools that can provide a special access to private methods: in C++ the friend keyword can be used to grant access to a class to another class. In Java, the @VisibleForTesting annotation can be used to mark a method as visible for testing. In Python, the __test__ attribute can be used to mark a method as visible for testing. Finding Duplicates \u00b6 For finding duplicates, there are two possible approaches: Using hash sets : iteratively checking if the current element is in the set of already seen elements and adding it to the set if not. Sorting : sorting the collection and then for each element checking if the current element is the same as the previous one. Comparison: | Approach | Time complexity (worst case asymptothic)| Time complexity (average expected) | Space complexity | allocation complexity | | --- | --- | --- | --- | --- | | Sets | O(log n) (both contains and add) | O(1) (both contains and add) | O(n) | O(1) | | Sorting | O(n log n) (sorting) | O(n log n) (sorting) + O(n) (duplicates check) | 0 or O(n) if we need to left the source collection unsorted | 0 or O(1) in case of new collection | IntelliJ Config \u00b6 Compact tabs \u00b6 Settings -> Appearance & Behavior -> New UI and select Compact mode","title":"Common"},{"location":"Programming/Common/#keymap","text":"Copy : Ctrl + C Cut : Ctrl + X Paste : Ctrl + V Toggle comment : Ctrl + Q Search in file : Ctrl + S Sellect all : Ctrl + A Format selection : Ctrl + F Format File : Ctrl + Shift + F Build : Ctrl + B","title":"Keymap"},{"location":"Programming/Common/#refactoring","text":"Rename : Ctrl + R Change signature : Ctrl + G Text transform : Ctrl + T + U : to upper case Surround with : Ctrl + W","title":"Refactoring"},{"location":"Programming/Common/#command-line-interface-cli","text":"This chapter should guide you on how to design CLI in a user-friendly and predictable way. Mostly, it follows the POSIX standard , with the GNU long option extensions . There are two main types of CLI arguments: Options : (e.g., --help , -h ) are used to change the behavior of the program. They are usually optional and can be in any order. Operands : (e.g., file.txt ) are the input data for the program. They are usually required and their order is important. All operands should be placed after all options.","title":"Command Line Interface (CLI)"},{"location":"Programming/Common/#options","text":"Options can be of two types: Short options : (e.g., -h ) are one character long and are prefixed with a single dash. Long options : (e.g., --help ) are multi-characcter and are prefixed with two dashes. Options can have arguments: Short options and their arguments are separated by a space or can be concatenated, e.g., -o file.txt or -ofile.txt . The first variant is strongly recommended. Long options and their arguments are separated by a space or can be concatenated with an equal sign, e.g., --output file.txt or --output=file.txt . Also, short options can be grouped, e.g., -h -v -o file.txt can be written as -hvo file.txt .","title":"Options"},{"location":"Programming/Common/#multiple-values-in-one-option-or-operand","text":"If an option or operand contains multiple values (e.g., a list of files), the values should be separated by a comma, e.g., --files file1.txt,file2.txt,file3.txt .","title":"Multiple values in one option or operand"},{"location":"Programming/Common/#exceptions","text":"Exceptions should be used to handle erroneus situations that are expected to happen. Exceptions should not be used for: Flow control , e.g., parse float from input, catch exception and try integer, then catch exception and try string... Unexpected situations , e.g., a method should always return a positive number, but it returns a negative one. For this we should use assertions, not exceptions. There are many types of exceptions, encapsulating different types of error description data. However, to begin with, it is not important to use some specific exception type. Using some general exception class is always much better than not using exceptions at all.","title":"Exceptions"},{"location":"Programming/Common/#tests","text":"","title":"Tests"},{"location":"Programming/Common/#testing-private-methods","text":"An urgent need to test privete method accompanied with a lack of knowledge of how to do it is a common problem. In almost all programming languages, the testing of private methods is obsturcted by the language itself, i.e., the test frameworks does not have a special access to private methods. In this section we disscuss the usuall solutions to this problem. These implementation is specific to a particular language, but the general ideas are the same. The possible approaches are: Makeing the method public : Only recommended if the method should be exposed, i.e., its functionality is not limited to the class itself. Move the method to a different class : Maybe, the method is correcly marked as private in the current context, but it can also be extracted to its own class, where it will become the main method of the class. This applies to methods that can be used in other contexts, or for methods contained in large classes. Mark the method as internal and make it public : This is a strategy that can be always applied with minimum effort. Various ways how to signalize that the method is intended for internal use are: Naming convention : The method name can start with an underscore, e.g., _my_method . Documentation : The comments can contain a warning that the method is intended for internal use. Namespace : The method can be placed in a namespace that signals that it is intended for internal use, e.g., internal::my_method . Special access : We can use special language-dependant tools that can provide a special access to private methods: in C++ the friend keyword can be used to grant access to a class to another class. In Java, the @VisibleForTesting annotation can be used to mark a method as visible for testing. In Python, the __test__ attribute can be used to mark a method as visible for testing.","title":"Testing private methods"},{"location":"Programming/Common/#finding-duplicates","text":"For finding duplicates, there are two possible approaches: Using hash sets : iteratively checking if the current element is in the set of already seen elements and adding it to the set if not. Sorting : sorting the collection and then for each element checking if the current element is the same as the previous one. Comparison: | Approach | Time complexity (worst case asymptothic)| Time complexity (average expected) | Space complexity | allocation complexity | | --- | --- | --- | --- | --- | | Sets | O(log n) (both contains and add) | O(1) (both contains and add) | O(n) | O(1) | | Sorting | O(n log n) (sorting) | O(n log n) (sorting) + O(n) (duplicates check) | 0 or O(n) if we need to left the source collection unsorted | 0 or O(1) in case of new collection |","title":"Finding Duplicates"},{"location":"Programming/Common/#intellij-config","text":"","title":"IntelliJ Config"},{"location":"Programming/Common/#compact-tabs","text":"Settings -> Appearance & Behavior -> New UI and select Compact mode","title":"Compact tabs"},{"location":"Programming/Git/","text":"Basics \u00b6 The structure of a single git repository is displayed in the following picture: Explanation of the picture: The working tree or working directory is the directory where the files are stored, typically the root directory of the project where the .git directory is located. The index or staging area is a cache of all changes that are marked ( staged ) for the next commit. To stage a change, we need to call git add command. Only files in the index are committed when we call git commit . The HEAD is a pointer to the last commit in the current branch. The following scheme shows the operations that can be performed between the working tree, index, HEAD and the remote repository (blue arrows represent the typical workflow): Configuration \u00b6 The git configuration is stored in the .gitconfig file in the user's home directory. It can be edited by editing the file directly, or by calling git config command. To display the active configuration in the command line, call: git config --list We can also show whether the configuration results from the system, user or local configuration file: git config --list --show-origin Basic Tasks \u00b6 Rewrite remote with local changes without merging \u00b6 git push -f Go back to branch: \u00b6 git revert --no-commit 0766c053..HEAD Untrack files, but not delete them locally: \u00b6 git rm --cached <FILEPATH> usefull params: -r : recursive - deletes content of directories -n dry run Add remote \u00b6 git remote add origin <URL> Where origin is the name of the remote and <URL> is is the link we use to clone the repository. Reverting \u00b6 When we want to revert something using git there are multiple options depending on the situation. The commands are: git checkout for overwriting local files with version from a specified tree (also for switching branches) and git reset for reverting commits and effectively rewriting the history. git read-tree : similar to checkout, but does not require a cleanup before The following table shows the differences between the commands: | Command | revrerts commits | overwrite local changes | delete files committed after <commit> | | --- | --- | --- | --- | | git checkout <commit> | no | yes | no | | git read-tree -m -u <commit> | no | yes | yes | | git reset --soft <commit> | yes | no | no | | git reset --hard <commit> | yes | yes | yes | To decide between the commands, the first consideration should be whether we want to preserve the history or not: we want to reach a specific state in the history and commit the changes as a new commit -> use git checkout or git read-tree we want to reach a specific state in the history and discard all changes after that point -> use git reset Keep the history \u00b6 If we want to keep the history, there are still two options: we want to revert the whole working tree and also delete all files that were committed after the specified commit -> use git read-tree -m -u <commit> we want to revert the whole working tree, but keep all files that were committed after the specified commit -> use git checkout <commit> we want to revert only some files -> use git checkout <commit> <filepath> Using git checkout \u00b6 To reset an individual file, call: git checkout <commit> <filepath> , to reset all files, call: git checkout <commit> . . If the <commit> parameter is ommited, the local files will be overwritten by the HEAD. Drop the history \u00b6 Dropping the history can be useful in many cases. For example, we may commit some changes to the master, but then we realize that they belong to a branch. A simple solution is to create a branch, and then reset the master to the previous commit. Note that if the wrong history was already pushed to the remote, we need to fix the history on the remote as well. This is done by force pushing: git push -f Wildcards \u00b6 Can be used in gitignore and also in some git commands. All described in the Manual . Usefull wildcards: **/ in any directory /** everything in a directory Removing files from all branches, local and remote \u00b6 Removing files from history can be done using multiple tools: bfg repo cleaner is the simplest tool. It can only select files by filename or size, but that is sufficient in most cases. git filter-repo is a more sophisticated successor to BFG. It can do almost anything possible. Nevertheless, it is less intuitive to operate, and it is harder to analyze its result. The filter-branch command is the original tool for filtering git history. It is slow and problematic, so it should be used only if the above two tools are not available. No matter of the used tool, before you begin: commit and push from all machines, backup the repository Similarly, at the end: It is important not to merge (branch or conflitcs) that originated before the cleanup on other machines. Otherwise, the deleted file can be reintroduced to the history. Pull on other machines. Add the file to gitignore so that it wont be commited again BFG \u00b6 With BFG, only a file with specific filename can be deleted . It is not possible to use exact file path. To remove file by its name: remove the file locally clone the whole repo again with the --mirror option on the mirrored repo, run the cleaner: bfg --delete-files <FILENAME> run the git commands that appears at the end of the bfg output run git push Git filter-repo \u00b6 The git filter-repo can be installed using pip: pip install git-filter-repo . To remove file by its path: run the command: git filter-repo --invert-paths --force --dry-run --path <PATH TO THE FILE TO BE REMOVED> inspect the changes in .git/filter-repo directory: Compare the files in KDiff3 To skip the lines starting with original-oid , go to the file selection dialog click Configure to the line-matching preprocessor command, add: sed 's/original-oid .*//' add remote again: git remote add origin <REPO SSH ADDRESS> force push the comman to the remote git push origin --force --all If the push is rejected by the remote hook, the master branch is probably protected. It has to be unprotected first in the repository config. git filter-repo manual More information on github filter-branch \u00b6 git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch <FILE>' --prune-empty --tag-name-filter cat -- --all Merging \u00b6 Syntax: git merge <params> <commit> Merge can be aborted at any time by calling git merge --abort . This resets the state of the repository. Resolving Conflicts \u00b6 Sometimes, the same file is changed on the same line in both branches we are merging. This is called a conflict. In this case, the conflicting content is marked with conflict markers: <<<<<<< HEAD This is the content of the file in the current branch ======= This is the content of the file in the branch we are merging >>>>>>> branch-a We can resolve the conflict manually by editing the file, but more often, we want to use a merge tool. To do that, we can call: git mergetool The mergetool should be first configured, as the default one (vimdiff) is not very user friendly. To configure the mergetool, we edit the git configuration. The appropriate section is mergetool . It has a lot of options, but for typical mergetool, is enough to set the name, as the mergetool command for that specific merge tool is preconfigured in recent versions of git. Example of the .gitconfig file configured for KDiff3: ... [mergetool \"kdiff3\"] ... More information on git mergetool Debugging mergetool \u00b6 Sometimes, this error can occur after calling git mergetool : git-mergetool--lib \"command not found\" This means that the mergetool is misconfigured. Inspect the mergetool section in the .gitconfig file to find the error. Merging Moved Files \u00b6 Sometimes, it's necessary to tweak the command to help it locate the moved files. What can help: -X rename-threshold=25 : Changing the threshold is important for languages that changes the file content when moving it with automatic refactoring (especially important for Java files, which usually have tons of imports) -X ignore-space-change Revert merge: \u00b6 git revert -m 1 Note that when the merge is reverted, the changes cannot be merged again, because they predates the revert! Update \u00b6 On windows, run: git update-git-for-windows Pull Requests \u00b6 Pull requests are a way to propose changes to the repository. The principle is as follows: Create a branch for the changes Commit the changes to the branch and push it to the remote Create a pull request on the remote, that suggest to merge the branch into the master branch There are two possible scenarios: We have the permission to create a branch in the repository: in this case, we can create the branch directly on the remote We do not have the permission to create a branch in the repository: in this case, we have to: Fork the repository Clone the fork Create the branch locally in the fork Commit and push the changes to the fork Create a pull request from the fork to the original repository Update pull request \u00b6 If there are changes requested by the reviewer or we just forgot to add something, we can update the pull request by pushing the updates to the PR branch. The pull request will be automatically updated. Repository Migration \u00b6 https://github.com/piceaTech/node-gitlab-2-github","title":"Git"},{"location":"Programming/Git/#basics","text":"The structure of a single git repository is displayed in the following picture: Explanation of the picture: The working tree or working directory is the directory where the files are stored, typically the root directory of the project where the .git directory is located. The index or staging area is a cache of all changes that are marked ( staged ) for the next commit. To stage a change, we need to call git add command. Only files in the index are committed when we call git commit . The HEAD is a pointer to the last commit in the current branch. The following scheme shows the operations that can be performed between the working tree, index, HEAD and the remote repository (blue arrows represent the typical workflow):","title":"Basics"},{"location":"Programming/Git/#configuration","text":"The git configuration is stored in the .gitconfig file in the user's home directory. It can be edited by editing the file directly, or by calling git config command. To display the active configuration in the command line, call: git config --list We can also show whether the configuration results from the system, user or local configuration file: git config --list --show-origin","title":"Configuration"},{"location":"Programming/Git/#basic-tasks","text":"","title":"Basic Tasks"},{"location":"Programming/Git/#rewrite-remote-with-local-changes-without-merging","text":"git push -f","title":"Rewrite remote with local changes without merging"},{"location":"Programming/Git/#go-back-to-branch","text":"git revert --no-commit 0766c053..HEAD","title":"Go back to branch:"},{"location":"Programming/Git/#untrack-files-but-not-delete-them-locally","text":"git rm --cached <FILEPATH> usefull params: -r : recursive - deletes content of directories -n dry run","title":"Untrack files, but not delete them locally:"},{"location":"Programming/Git/#add-remote","text":"git remote add origin <URL> Where origin is the name of the remote and <URL> is is the link we use to clone the repository.","title":"Add remote"},{"location":"Programming/Git/#reverting","text":"When we want to revert something using git there are multiple options depending on the situation. The commands are: git checkout for overwriting local files with version from a specified tree (also for switching branches) and git reset for reverting commits and effectively rewriting the history. git read-tree : similar to checkout, but does not require a cleanup before The following table shows the differences between the commands: | Command | revrerts commits | overwrite local changes | delete files committed after <commit> | | --- | --- | --- | --- | | git checkout <commit> | no | yes | no | | git read-tree -m -u <commit> | no | yes | yes | | git reset --soft <commit> | yes | no | no | | git reset --hard <commit> | yes | yes | yes | To decide between the commands, the first consideration should be whether we want to preserve the history or not: we want to reach a specific state in the history and commit the changes as a new commit -> use git checkout or git read-tree we want to reach a specific state in the history and discard all changes after that point -> use git reset","title":"Reverting"},{"location":"Programming/Git/#keep-the-history","text":"If we want to keep the history, there are still two options: we want to revert the whole working tree and also delete all files that were committed after the specified commit -> use git read-tree -m -u <commit> we want to revert the whole working tree, but keep all files that were committed after the specified commit -> use git checkout <commit> we want to revert only some files -> use git checkout <commit> <filepath>","title":"Keep the history"},{"location":"Programming/Git/#using-git-checkout","text":"To reset an individual file, call: git checkout <commit> <filepath> , to reset all files, call: git checkout <commit> . . If the <commit> parameter is ommited, the local files will be overwritten by the HEAD.","title":"Using git checkout"},{"location":"Programming/Git/#drop-the-history","text":"Dropping the history can be useful in many cases. For example, we may commit some changes to the master, but then we realize that they belong to a branch. A simple solution is to create a branch, and then reset the master to the previous commit. Note that if the wrong history was already pushed to the remote, we need to fix the history on the remote as well. This is done by force pushing: git push -f","title":"Drop the history"},{"location":"Programming/Git/#wildcards","text":"Can be used in gitignore and also in some git commands. All described in the Manual . Usefull wildcards: **/ in any directory /** everything in a directory","title":"Wildcards"},{"location":"Programming/Git/#removing-files-from-all-branches-local-and-remote","text":"Removing files from history can be done using multiple tools: bfg repo cleaner is the simplest tool. It can only select files by filename or size, but that is sufficient in most cases. git filter-repo is a more sophisticated successor to BFG. It can do almost anything possible. Nevertheless, it is less intuitive to operate, and it is harder to analyze its result. The filter-branch command is the original tool for filtering git history. It is slow and problematic, so it should be used only if the above two tools are not available. No matter of the used tool, before you begin: commit and push from all machines, backup the repository Similarly, at the end: It is important not to merge (branch or conflitcs) that originated before the cleanup on other machines. Otherwise, the deleted file can be reintroduced to the history. Pull on other machines. Add the file to gitignore so that it wont be commited again","title":"Removing files from all branches, local and remote"},{"location":"Programming/Git/#bfg","text":"With BFG, only a file with specific filename can be deleted . It is not possible to use exact file path. To remove file by its name: remove the file locally clone the whole repo again with the --mirror option on the mirrored repo, run the cleaner: bfg --delete-files <FILENAME> run the git commands that appears at the end of the bfg output run git push","title":"BFG"},{"location":"Programming/Git/#git-filter-repo","text":"The git filter-repo can be installed using pip: pip install git-filter-repo . To remove file by its path: run the command: git filter-repo --invert-paths --force --dry-run --path <PATH TO THE FILE TO BE REMOVED> inspect the changes in .git/filter-repo directory: Compare the files in KDiff3 To skip the lines starting with original-oid , go to the file selection dialog click Configure to the line-matching preprocessor command, add: sed 's/original-oid .*//' add remote again: git remote add origin <REPO SSH ADDRESS> force push the comman to the remote git push origin --force --all If the push is rejected by the remote hook, the master branch is probably protected. It has to be unprotected first in the repository config. git filter-repo manual More information on github","title":"Git filter-repo"},{"location":"Programming/Git/#filter-branch","text":"git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch <FILE>' --prune-empty --tag-name-filter cat -- --all","title":"filter-branch"},{"location":"Programming/Git/#merging","text":"Syntax: git merge <params> <commit> Merge can be aborted at any time by calling git merge --abort . This resets the state of the repository.","title":"Merging"},{"location":"Programming/Git/#resolving-conflicts","text":"Sometimes, the same file is changed on the same line in both branches we are merging. This is called a conflict. In this case, the conflicting content is marked with conflict markers: <<<<<<< HEAD This is the content of the file in the current branch ======= This is the content of the file in the branch we are merging >>>>>>> branch-a We can resolve the conflict manually by editing the file, but more often, we want to use a merge tool. To do that, we can call: git mergetool The mergetool should be first configured, as the default one (vimdiff) is not very user friendly. To configure the mergetool, we edit the git configuration. The appropriate section is mergetool . It has a lot of options, but for typical mergetool, is enough to set the name, as the mergetool command for that specific merge tool is preconfigured in recent versions of git. Example of the .gitconfig file configured for KDiff3: ... [mergetool \"kdiff3\"] ... More information on git mergetool","title":"Resolving Conflicts"},{"location":"Programming/Git/#debugging-mergetool","text":"Sometimes, this error can occur after calling git mergetool : git-mergetool--lib \"command not found\" This means that the mergetool is misconfigured. Inspect the mergetool section in the .gitconfig file to find the error.","title":"Debugging mergetool"},{"location":"Programming/Git/#merging-moved-files","text":"Sometimes, it's necessary to tweak the command to help it locate the moved files. What can help: -X rename-threshold=25 : Changing the threshold is important for languages that changes the file content when moving it with automatic refactoring (especially important for Java files, which usually have tons of imports) -X ignore-space-change","title":"Merging Moved Files"},{"location":"Programming/Git/#revert-merge","text":"git revert -m 1 Note that when the merge is reverted, the changes cannot be merged again, because they predates the revert!","title":"Revert merge:"},{"location":"Programming/Git/#update","text":"On windows, run: git update-git-for-windows","title":"Update"},{"location":"Programming/Git/#pull-requests","text":"Pull requests are a way to propose changes to the repository. The principle is as follows: Create a branch for the changes Commit the changes to the branch and push it to the remote Create a pull request on the remote, that suggest to merge the branch into the master branch There are two possible scenarios: We have the permission to create a branch in the repository: in this case, we can create the branch directly on the remote We do not have the permission to create a branch in the repository: in this case, we have to: Fork the repository Clone the fork Create the branch locally in the fork Commit and push the changes to the fork Create a pull request from the fork to the original repository","title":"Pull Requests"},{"location":"Programming/Git/#update-pull-request","text":"If there are changes requested by the reviewer or we just forgot to add something, we can update the pull request by pushing the updates to the PR branch. The pull request will be automatically updated.","title":"Update pull request"},{"location":"Programming/Git/#repository-migration","text":"https://github.com/piceaTech/node-gitlab-2-github","title":"Repository Migration"},{"location":"Programming/Gurobi/","text":"Running Gurobi CLI \u00b6 There are two gurobi command line tools: gurobi_cl : for solving the optimization problems passed as a file gurobi : for running the interactive shell (ipython-like) Parallel Execution \u00b6 The gurobi solver solves a problem in parallel by default, trying multiple solution methods at the same time (see the official description ). It is also possible to run multiple problems in parallel ( source ), but each problem should be run in its own gurobi environment. Also, each environment should be configured to use only a single thread (e.g., in C++: env.set(GRB_IntParam_Threads, 1); ). The problem with this approach is that the CPU is usually not the bottleneck of the computation, the bottleneck is the memory ( source ). Therefore, solving multiple problems in parallel does not guarantee any speed up, it could be actually slower. The performance could be most likely improved when running the problems in parallel on multiple machines (not multiple cores of the same machine). Some advised to use MPI for that.","title":"Gurobi"},{"location":"Programming/Gurobi/#running-gurobi-cli","text":"There are two gurobi command line tools: gurobi_cl : for solving the optimization problems passed as a file gurobi : for running the interactive shell (ipython-like)","title":"Running Gurobi CLI"},{"location":"Programming/Gurobi/#parallel-execution","text":"The gurobi solver solves a problem in parallel by default, trying multiple solution methods at the same time (see the official description ). It is also possible to run multiple problems in parallel ( source ), but each problem should be run in its own gurobi environment. Also, each environment should be configured to use only a single thread (e.g., in C++: env.set(GRB_IntParam_Threads, 1); ). The problem with this approach is that the CPU is usually not the bottleneck of the computation, the bottleneck is the memory ( source ). Therefore, solving multiple problems in parallel does not guarantee any speed up, it could be actually slower. The performance could be most likely improved when running the problems in parallel on multiple machines (not multiple cores of the same machine). Some advised to use MPI for that.","title":"Parallel Execution"},{"location":"Programming/HDF/","text":"Hierarchical Data Format (HDF) is a binary data format designed to store large amounts of data. It is widely used and it has support for many programming languages. It is a self-contained format: all metadata needed to read the data is embedded in the file. HDF in C++ \u00b6 HDF in Python \u00b6 HDF in Java \u00b6 In Java, there are many options to work with HDF files. We list some of them in the following table: Name Type Description last update HDF5 wrapper The official wrapper for the HDF5 library. It is a jar file provided by the main installer of HDF5 library. (in the lib folder). The documentation for Java is missing. 2023-10-27 sis-jhdf5 wrapper A wrapper for the HDF5 library developed at ETH Zurich. 2022-07-28 netCDF-Java library for multiple file formats A library that provides an interface to read and write netCDF, HDF5, GRIB, BUFR, and other file formats. daily, last release 2022-07-05 HDFQL query language access to HDF5 A library that provides a SQL-like access to HDF5 files. It has wrappers for Java, Python, C#, Fortran, R. Also, it contains a command-line tool to query HDF5 files. 2023-09-04 JHDF pure Java implementation A pure Java implementation of the HDF5 file format. Currently, it only supports reading HDF5 files. Also, the file size is limited to 32-bit integer size (an array of about 4 billion members) weekly, last release 2023-05-10","title":"HDF"},{"location":"Programming/HDF/#hdf-in-c","text":"","title":"HDF in C++"},{"location":"Programming/HDF/#hdf-in-python","text":"","title":"HDF in Python"},{"location":"Programming/HDF/#hdf-in-java","text":"In Java, there are many options to work with HDF files. We list some of them in the following table: Name Type Description last update HDF5 wrapper The official wrapper for the HDF5 library. It is a jar file provided by the main installer of HDF5 library. (in the lib folder). The documentation for Java is missing. 2023-10-27 sis-jhdf5 wrapper A wrapper for the HDF5 library developed at ETH Zurich. 2022-07-28 netCDF-Java library for multiple file formats A library that provides an interface to read and write netCDF, HDF5, GRIB, BUFR, and other file formats. daily, last release 2022-07-05 HDFQL query language access to HDF5 A library that provides a SQL-like access to HDF5 files. It has wrappers for Java, Python, C#, Fortran, R. Also, it contains a command-line tool to query HDF5 files. 2023-09-04 JHDF pure Java implementation A pure Java implementation of the HDF5 file format. Currently, it only supports reading HDF5 files. Also, the file size is limited to 32-bit integer size (an array of about 4 billion members) weekly, last release 2023-05-10","title":"HDF in Java"},{"location":"Programming/JSON/","text":"Jackson \u00b6 Usefull Annotations \u00b6 @JsonIncludeProperties : Ignore all properties except listed @JsonProperty(\"my_name\") : Custom name of the JSON key @JsonIgnore : Ignore the json property below Wrapping the Obejct in Another JSON Object \u00b6 To do that, use these annotations above the class. @JsonTypeName(value = \"action\") @JsonTypeInfo(include=As.WRAPPER_OBJECT, use=Id.NAME) If you do not care about the name, you can skip the @JsonTypeName annotation. Written with StackEdit .","title":"JSON"},{"location":"Programming/JSON/#jackson","text":"","title":"Jackson"},{"location":"Programming/JSON/#usefull-annotations","text":"@JsonIncludeProperties : Ignore all properties except listed @JsonProperty(\"my_name\") : Custom name of the JSON key @JsonIgnore : Ignore the json property below","title":"Usefull Annotations"},{"location":"Programming/JSON/#wrapping-the-obejct-in-another-json-object","text":"To do that, use these annotations above the class. @JsonTypeName(value = \"action\") @JsonTypeInfo(include=As.WRAPPER_OBJECT, use=Id.NAME) If you do not care about the name, you can skip the @JsonTypeName annotation. Written with StackEdit .","title":"Wrapping the Obejct in Another JSON Object"},{"location":"Programming/Native%20Libraries/","text":"Native libraries are an important part of programming in most languages. They allow us to run the code without the need for an interpreter or virtual machine. Native libraries can be written in any language that can be compiled to machine code. This includes C, C++, Rust, and many others. There are two ways how to link a native library to your code: statically and dynamically: Static libraries are linked at compile time. This means that the library code is included in the final executable. This makes the executable larger, but it also means that the executable is self-contained and does not require the library to be installed on the target system. Dynamic libraries are usually linked at runtime. This means that the library code is not included in the final executable. Instead, the executable loads the library code from a separate file when it is run. This makes the executable smaller, but it also means that the library must be installed on the target system. Each operating system has its own way of loading and using native libraries. Also, The following table shows the extension of the library file for each operating system: Operating System Static Library Extension Dynamic (shared) Library Extension Windows .lib .dll Linux .a .so Deciding between static and dynamic libraries \u00b6 When deciding between static and dynamic libraries, there are a few things to consider: Property Static Libraries Dynamic Libraries Memory footprint Each executable includes a copy of all used library code All executables share the same copy of the library code Modularity None We can extend the functionality of the program by adding new shared libraries Licensing Some licenses (e.g. GPL) require that the source code of the library is made available if the library is linked statically The library can be linked dynamically without having to make the source code available Symbol export All symbols are exported by default Only symbols marked for export are exported Linking of transitive dependencies All transitive dependencies must be linked explicitly Transitive dependencies are loaded automatically at runtime Historically, there were other factors that are not relevant anymore: Size : The historical argument was that dynamic libraries can save disk space because only one copy of the library is needed in the system. However, a) disk space is cheap now and b) dynamic libraries are now distributed with the application on many platforms. Maintenance : The historical argument was that dynamic libraries are easier to maintain because they can be updated without recompiling the application and distributing a new version. However, this is now not true because on many platforms the dynamic libraries are distributed with the application. Portability : The historical argument was that static libraries are more portable because they do not depend on the presence of the library on the target system. However, we can distribute dynamic libraries with the application to mitigate this issue of dynamic libraries. So finally, how to decide between static and dynamic libraries? We create a library that will be used by many applications, and it is large -> dynamic libraries. We use a GPL library and we do not want to make the source code available -> dynamic libraries. Users are expected to only use a fraction of the library -> static libraries. Otherwise -> it does not matter.","title":"Native Libraries"},{"location":"Programming/Native%20Libraries/#deciding-between-static-and-dynamic-libraries","text":"When deciding between static and dynamic libraries, there are a few things to consider: Property Static Libraries Dynamic Libraries Memory footprint Each executable includes a copy of all used library code All executables share the same copy of the library code Modularity None We can extend the functionality of the program by adding new shared libraries Licensing Some licenses (e.g. GPL) require that the source code of the library is made available if the library is linked statically The library can be linked dynamically without having to make the source code available Symbol export All symbols are exported by default Only symbols marked for export are exported Linking of transitive dependencies All transitive dependencies must be linked explicitly Transitive dependencies are loaded automatically at runtime Historically, there were other factors that are not relevant anymore: Size : The historical argument was that dynamic libraries can save disk space because only one copy of the library is needed in the system. However, a) disk space is cheap now and b) dynamic libraries are now distributed with the application on many platforms. Maintenance : The historical argument was that dynamic libraries are easier to maintain because they can be updated without recompiling the application and distributing a new version. However, this is now not true because on many platforms the dynamic libraries are distributed with the application. Portability : The historical argument was that static libraries are more portable because they do not depend on the presence of the library on the target system. However, we can distribute dynamic libraries with the application to mitigate this issue of dynamic libraries. So finally, how to decide between static and dynamic libraries? We create a library that will be used by many applications, and it is large -> dynamic libraries. We use a GPL library and we do not want to make the source code available -> dynamic libraries. Users are expected to only use a fraction of the library -> static libraries. Otherwise -> it does not matter.","title":"Deciding between static and dynamic libraries"},{"location":"Programming/PostgreSQL%20Manual/","text":"Data types \u00b6 official documentation Date \u00b6 official documentation date : for dates time for time timestmp for both date and time interval Select a part of date/time/timestamp \u00b6 If we want just a part of a date, time, or timestamp, we can use the extract function. Example: SELECT extract(hour FROM <date column name>) FROM... Other parts can be extracted too. To extract day of week , we can use isodow (assigns 1 to Monday and 7 to Sunday). Auto incrementing columns \u00b6 In PostgreSQL, sequences are used for auto-incrementing columns. When you are creating a new db table or adding a new column, the process of creating a new sequence can be automated by choosing an identity or a serial column type. When updating an aexisting column, a manual intervention is required: change the column to some numerical datatype create the sequence: CREATE SEQUENCE <SEQUENCE NAME> OWNED BY <TABLE NAME>.<COLUMN NAME>; adjust the value of the sequence: SELECT setval(pg_get_serial_sequence('<TABLE NAME>', '<COLUMN NAME>'), max(<COLUMN NAME>)) FROM <TABLE NAME>; set the column to be incremented by the sequence: ALTER TABLE <TABLE NAME> ALTER COLUMN <COLUMN NAME> SET DEFAULT nextval('<SEQUENCE NAME>'); Strings \u00b6 There are many string function available, including the format function that works similarly to the C format function. For all functions, check the documentation . Arrays \u00b6 arrays array functions and operators arrays are declared as <type>[] , e.g., integer[] . The type can be any type, including composite types. To compute array length , use array_length(contracted_vertices, 1) where 1 stands for the first dimension. To cretea an array literal , we use single quatation and curly brackets: '{1, 2, 3}' . To check that some value match at least some member of the array, we use ANY : SELECT ... FROM tab WHERE tab.a = ANY(<array>) Working with the array members individualy \u00b6 For using the members of an array in the SELECT or JOIN , we have to first split the array using the unnest function. This function transforms the result set to a form where there is a separate row for each member of the array (a kind of inverse operation to group by). If we want to also keep the array index, we can use the WITH ORDINALITY expression, as shown in the manual or on SO . hstore \u00b6 A specific feature of PostgreSQL is the hstore column type. It enables to store structured data in a single column. It can be used to dump variables that we do not plan to utilize in the database (i.e., in the SELECT, JOIN statements) frequently. This data type requires the hstore extension to be enabled in the database. To enable it, use the following command: CREATE EXTENSION hstore; When we, exceptionally, want to access a variable from a hstore column, we can use the following syntax: SELECT <COLUMN NAME>-><VARIABLE NAME> AS ... Schemas \u00b6 official documentation Schemas in PostgreSQL are implemented as namespaces according to the SQL standard. Search path \u00b6 official documentation Usually, we does not have to qualify the table name with the schema name, as the schema name is in the search path . By default the search path is set to \"$user\", public , which means that the tables are first searched in the schema with the same name as the user, and then in the public schema. To show the current search path, run: SHOW search_path; To change the search path, run: SET search_path TO <schema name list>; Procedures and functions \u00b6 Calling a procedure \u00b6 to exectue a stored procedure, use\" CALL <procedure name>(<procedure arguments>) Unlike in programing languages, there is no implicit type cast of the program arguments, including literals. Therefore, we need to cast all parameters explicitely, as in the following example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments(target_area_id smallint) ... CALL compute_speeds_for_segments(1::smallint); Creating a procedure \u00b6 documentation The syntax is as follows: CREATE PROCEDURE <name> (<params>) LANGUAGE <language name> <procedure body> , while <procedure body> can be both a delimited string: AS $$ <sql statements> $$ OR an active SQL body (since PostgreSQL 14): BEGIN ATOMIC <sql statements> END There are some differences between those syntaxes (e.g., the second one works only for SQL and is evaluated/checked for validity at the time of creation), bot in most cases, they are interchangable. For more details, check the manual . Variables \u00b6 In PostgreSQL, all variables must be declared before assignmant in the DECLARE block which is before the sql body of the function/procedure/ DO . The syntax is: <variable name> <variable type>[ = <varianle value>]; Example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments() LANGUAGE plpgsql AS $$ DECLARE dataset_quality smallint = 1; BEGIN ... The, the value of a variable can be change using the classical assignment syntax: <variable name> = <varianle value>; Be carful to not use a variable name equal to the name of some of the columns used in the same context, which results in a name clash. Assigning a value to a variable using SQL \u00b6 There are two options how to assign a value to a variable using SQL: using the INTO clause in the SELECT statement using a SELECT statement as rvlaue of the assignment Example with INTO : SELECT name INTO customer_name FROM customers WHERE id = 1 EXAMPLE with SELECT as rvalue: customer_name = (SELECT name FROM customers WHERE id = 1) Functions \u00b6 Functions in PostgreSQL have a similar syntax to procedures. Unlike for procedures, we need to specify a return type for function, either as an OUT / INOUT parameter, or using the RETURNS clause. To return tabular data, we use TABLE return type: RETURNS TABLE(<param 1 name> <param 1 type>, ..., <param n name> <param n type>) To select the result of a function with the return type above, call: SELECT * FROM <function signature> RETURN NEXT and RETURN QUERY \u00b6 Sometimes, we need to do some cleanup after selecting the rows to be returned from function, or we need to build the result in a loop. In classical programming languages, we use variables for this purpose. In PG/plSQL, we can also use the RETURN NEXT and RETURN QUERY constructs. These constructs prepare the result, and does not return from the function . Instead, use an empty RETURN to return from the function. Example: RETURN QUERY SELECT ...; DROP TABLE target_ways; RETURN; Note that for these constructs, the return type needs to be a table or setof type. The RETURN QUERY cannot be used for returning a single value even if the query returns a single value. If we have a single value return type and need to do some postprocessing between selecting the value and returning from the function, we have to use a variable instead. Deciding the language \u00b6 For simple statements, we can use the SQL language. We need the PL/pgSQL if: we need to use variables or control statements specific to PL/pgSQL we need to use temporary tables, as the SQL language fails to recognize them if they are created inside the function/procedure Conditional filters based on the value of a variable or input parameter \u00b6 To add some filter to the WHERE or ON clause of a query based on the value of a variable or input parameter, we can use the following technique: set the variable or input parameter to NULL if we do not want to apply the filter in the filter test for disjunction of NULL or the filter condition Example: SELECT ... FROM ... WHERE param IS NULL OR some_column = param Temporary tables \u00b6 To use a result set efficiently in a function or procedure, we often use temporary tables. Unlike in other relational database systems, in PostgreSQl, the lifetime of a temporary table is bound to a session. Therefoe, if we call a function that creates a tempoary table multiple times in a single session, we encounter an error, because the table already exists. To tackle this problem, we need to delete all temporary tables manually. Luckily, there is a special DISCARD command that can be used to dtop all temporary tables at once: DISCARD TEMPORARY; DO command \u00b6 The DO command can be used to execude an anonymus code block in any of the languages suported by PostgreSQL. It behaves like a function with no parameters and no return value. Syntax: DO [LANGUAGE <lang name>] <code> The default language is plpgsql . Example: DO $$ BEGIN RAISE NOTICE 'Hello world'; END $$ Window functions \u00b6 PostgreSQL supports an extended syntax for window functions . We can use it for example to retrieve the value of a column that has maximum value in another column, as demonstrated in an SO answer . Various specific tasks \u00b6 Selecting rows for deletion based on data from another table \u00b6 If we want to delete rows from a table based on some condition on data from another table, we can use the DELETE statement with a USING clause. Example: DELETE FROM nodes_ways_speeds USING nodes_ways WHERE nodes_ways_speeds.to_node_ways_id = nodes_ways.id AND nodes_ways.area IN (5,6) Handeling duplicates in the INSERT statement \u00b6 To handle duplicates on INSERT , PostgreSQL provides the ON CONFLICT clause (see the INSERT documentation). The options are: DO NOTHING : do nothing DO UPDATE SET <column name> = <value> : update the column to the given value Random oredering \u00b6 To order the result set randomly, we can use the RANDOM() function in the ORDER BY clause: SELECT ... FROM ... ORDER BY RANDOM() Random ordering with a seed (Pseudo-random ordering) \u00b6 To receive a determinisic (repeatable) random ordering, we can use the setseed function: SELECT setseed(0.5); SELECT ... FROM ... ORDER BY RANDOM(); Note that we need two queries, one for setting the seed and one for the actual query. If we does not have an option to call arbitrary queries, we have to use UNION : SELECT col_1, ..., col_n FROM ( SELECT _, null AS col_1, ..., null AS col_n FROM setseed(0.5) UNION ALL SELECT null AS _, col_1, ..., col_n FROM ... OFFSET 1 ) ORDER BY RANDOM() The OFFSET 1 is used to skip the first row, which is the result of the setseed function. The union is the only way to guarantee that the seed will be set before the actual query. Other options, such as WITH or SELECT ... FROM (SELECT setseed(0.5)) do not guarantee the order of execution, and produce a different result for each call. PostGis \u00b6 PolsGIS is an extension for PostgreSQL that adds support for spatial data. To enable the extension, we need to: install the extension e.g., using the bundeled stack builder application enable the extension in the database SQL CREATE EXTENSION postgis; Geometry columns \u00b6 Postgis features can be utilized with geometry and geography column types. To add a new geometry column: ADD COLUMN <COLUUMN NAME> geometry(<GEOMETRY TYPE>, <SRID>) Spatial Indexing \u00b6 Documentation Analogously to standard SQL column indicis, there are spatial indices in PostGIS. The only difference is that we need to add the USING GIST at the end of the CREATE INDEX statement: CREATE INDEX nodes_geom_idx ON nodes USING GIST (geom); Converting between geometry types \u00b6 There are dedicated functions whcich we can use to convert between geometry types: ST_Multi : converts geometries to their multi-variant, e.g., LineString to MultiLineString . Compute area surrounding geometries \u00b6 If we are ok with a convex envelope of the geometries, we can simply use the St_ConvexHull functon. Howeever, if we need the exact shape, We have to use the St_ConcaveHull function which computes the concave hull of a geometry. The St_ConcaveHull function takes an additional parameter param_pctconvex which determines how concave is the result: 0 means strictly concave, while 1 means convex hull. Note that while lowere values leads to more acurate results, the computation is much slower. There is also another parameter param_allow_holes which determines whether holes in the object are permited (default false). Split area to polygons based on points \u00b6 The split of spece into polygons based on a set of points is called Voronoi diagram . In PostGIS, we have a `ST_Voronoi_Polygons for that. To obtain a set of polygons from a set of points, it is necessary to Aggregate the rows ( ST_Collect ) Compute the polygon geometry ( ST_Voronoi_Polygons ) Disaggregate the geometry into individual polygons ( ST_Dump ) Also, there is an important aspect of how far the polygons will reach outside of the points. By default, it enlarge the area determined by the points by about 50%. If we need a larger area, we can use the extend_to parameter. If we need a smaller area, however, we need to compute the intersection with this smaller area afterwards manually. Full example: SELECT st_intersection( (st_dump( st_voronoipolygons(st_collect(<GEOMETRY COLUMN>)) )).geom, (<select clause for area of desired voronoi polygons>) ) AS geom FROM ... If we need to join the polygons to the original points, we need to do it manually (e.g. by JOIN ... ON ST_Within(<POINT COLUMN>, <VORONOI POLYGONS GEO DUMP>) ). Other Useful Functions \u00b6 [ ST_Within ] ST_Within (A, B) if A is completly inside B ST_Intersects ST_Intersects(g1, g2) if g1 and g2 have at least one point in common. ST_Transform : ST_Transform(g, srid) transforms geometry g to a projection defined by the srid and returns the result as a geometry. ST_Buffer : ST_Buffer(g, radius) computes a geometry that is an extension of g by radius to all directions St_Collect aggregates data into single geometry. It is usually apllied to a geometry column in an SQL selection. ST_Union ST_Equals ST_MakeLine : make line between two points ST_SetSRID : sets the SRID of the geometry and returns the result as a new geometry. ST_MakePoint : creates a point geometry from the given coordinates. ST_Area : computes the area of a geometry. The units of the result are the same as the units of the SRID of the geometry (use UTM coordinate system for getting the area in square meters). PgRouting \u00b6 PgRouting is a PostgreSQL extension focused on graph/network manpulation. It contains functions for: finding the strongly connected components: `pgr_strongComponents graph contraction/simplification Finding strongly connected components \u00b6 The function pgr_strongComponents finds the strongly connected components of a graph. The only parameter of the script is a query that should return edge data in the folowing format: id , source , target , cost , reverse_cost . The first three parameters are obvious. The cost parameter does not have any effect. You should provide a negative reverse_cost , othervise, the edge will be considered as bidirectional! PL/pgSQL \u00b6 PL/pgSQL is a procedural language available in PostgreSQL databases. It can be used inside: functions procedures DO command Branching \u00b6 PL/pgSQL has the following branching: IF <condition> THEN ... [ELSEIF ... ] [ELSE ... ] END IF Logging \u00b6 Basic logging can be done using the RAISE command: RAISE NOTICE 'Some message'; We can add parameters by using the % placeholder: RAISE NOTICE 'Some message %', <variable or SQL command>; For more, see the documentation . Query diagnostics \u00b6 Various information about the last query can be obtained using the GET DIAGNOSTIC command. For example, the number of rows affected by the last query can be obtained using the ROW_COUNT parameter: GET DIAGNOSTIC <variable> = ROW_COUNT; The result is stored in the variable <variable> . Note that this constract is not available in the SQL queries but only in a PL/pgSQL block. For other diagnostic fields, see the documentation . psql \u00b6 Documentation psql is a basic command line utitilty for manipulating postgres database. To connect, type: psql -d <db name> Then the SQL commands can be executed. Note that no matter what SQL commands you plan to execute, you have to connect to a specific database . Ommitting the -d parameter will not connect you to the server in some admin mode, but instead, it will connect you to the default database, which is usually the database with the same name as the user name. If there is no such database, the connection will fail. To execute command immediatelly without starting interactive session, use the -c parameter: psql -d <db name> -c \"<command>\" Do not forget to quote the command. Also, note that certain SQL commands, such as CREATE DATABASE requires its own session. To combine them with other SQL commands, you can use multiple -c parameters: psql -d <db name> -c \"CREATE DATABASE <db name>\" -c \"<other command>\" Other useful parameters: -X : do not read the ~/.psqlrc file. This is useful when debuging the psql commands or running scripts, as it disables any customizations. -U <user name> : connect as a different user. If not specified, the current user is used. -p : specify the port number. (default is 5432) meta-commands \u00b6 The psql has its own set of commands, called meta-commands . These commands start with a backslash ( \\ ) and can be used inside SQL queries (either interactively, or in the -c argument). Example: psql -d <db name> -c \"\\l+\" The above command lists all databases, including additional information. Note that the meta-commands cannot be combined with SQL queries passed to the -c parameter . As -c argument, we can use either: a plain SQL query without any meta-commands a single meta-command without any SQL query (like the example above) (In \\copy public.nodes FROM nodes.csv CSV HEADER , the string after \\copy is a list of arguments, not an SQL query) When we want to combine a meta-command with an SQL query, we need to use some of the workarounds: use the psql interactive mode use the psql --file parameter to execute a script from a file pipe the commands to psql using echo : bash echo \"COPY (SELECT * FROM opendata.public.nodes WHERE area = 13) TO STDOUT WITH CSV HEADER \\g 'nodes.csv'\" | psql -d opendata Importing data \u00b6 A simple SQL data (database dump) can be imported using the psql command: psql -d <db name> -f <file name> we can also import from stdin by omitting the -f parameter: psql -d <db name> < <file name> # or <command> | psql -d <db name> Importing compressed SQL dump \u00b6 To import a compressed SQL dump we need to know how the dump was compressed. If it was compressed using the pg_dump with the -Z parameter, we use a pg_restore command: bash pg_restore -d <db name> <file name> If it was compressed using a compression tool, we need to pipe the output to the decompression tool and then to psql : bash <decompression tool> < <file name> | psql -d <db name> Importing data from csv \u00b6 The easiest way to import data from csv is to use the psql \\copy meta-command: psql -d <db name> -c \"\\copy <table name> FROM <file name> CSV HEADER\" The syntax of this command and its parameters is almost identical to the COPY command. Important parameters: CSV : the file is in CSV format HEADER : the first line of the file contains column names Except simplicity, the \\copy command has another advantage over the COPY command: it has the same access rights as the user that is currently logged in. The COPY command, on the other hand, has the access rights of the user that started the database server. The COPY command, however, has an advantage over the \\copy command when we access a remote database. In that case, the \\copy command would first download the whole file to the client and then upload it to the server. The COPY command, on the other hand, can be executed directly on the server, so the data are not downloaded to the client. Importing data from csv with columns than are not in the db table \u00b6 The COPY (and \\copy ) command can be used to copy the input into a database table even if it contains only a subset of database table columns. However, it does not work the other way, i.e, all input columns have to be used. If only a subset of input columns needs to be used, or some of the input columns requires processing, we need to use some workaround. The prefered mathod depends on the characte of the data: data do not match the table, but they are small: load the data with pandas process the data as needed use pandas.to_sql to upload the data data do not match the table and they are large: preprocess the data with batch commands use COPY (or \\copy ) to upload the data data do not match the table and they are large and dirty : use the file_fdw module: create a table for SQL mapping with tolerant column types (e.g., text for problematic columns) select from the mapping to the real table Importing data from a shapefile \u00b6 There are multiple options: shp2psql : simple tool that creates sql from a shapefile easy start, almost no configuration always imports all data from shapefile, cannot be configured to skip columns included in the Postgres instalation ogr2ogr needs to be installed, part of GDAL QGIS The db manger can be used to export data from QGIS data can be viewed before import only suitable for creating new table, not for appending to an existing one Importing data from GeoJSON \u00b6 For a single geometry stored in a GeoJSON file, the function ST_GeomFromGeoJSON can be used. just copy the geometry part of the file change the geometry type to match the type in db don't forget to surround the object with curly brackets For the whole document, the ogr2ogr tool can be used. Exporting data \u00b6 The data can be exported to SQL using the pg_dump command. The simplest usage is: pg_dump -f <output file name> <db name> Important parameters: -s : export only the schema, not the data If we need to further process the data, we can use the stdout as an output simply by omitting the -f parameter: pg_dump <db name> | <command> Exporting the database server configuration \u00b6 To export the database server configuration, we can use the pg_dumpall command: pg_dumpall -f <output file name> Compressing the SQL dump \u00b6 To compress the SQL dump, we have two options: use the pg_dump command with the -Z parameter, e.g., pg_dump <db name> -Z1 -f <output file name> or pipe the output to a compression tool, e.g., pg_dump <db name> | gzip > <output file name> Exporting data to csv \u00b6 When exporting large data sets, it is not wise to export them as an SQL dump. To export data to csv, we can use either: psql \\copy command COPY command in SQL The simplest way is to use the \\copy command. However, it may be slow if we call psql from a client and we want to export the data to a file on the server, because the data are first sent to the client and then back to the server. The COPY command is the fastest way to export data to the server. However, by default, it can be tricky to use, as the sql server needs to have write access to the file. To overcome this problem, we can use the following workaround: choose STDOUT as an output for the COPY command at the end of the command, add \\g <file name> to redirect the output to a file. Example: echo \"COPY (SELECT * FROM opendata.public.nodes WHERE area = 13) TO STDOUT WITH CSV HEADER \\g 'nodes.csv'\" | psql -d opendata Information and statistics \u00b6 To show the table size , run: SELECT pg_size_pretty(pg_total_relation_size('<table name>')); To show the version of the PostgreSQL server, run: SELECT version(); To list all extensions for a database, run: psql -d <db name> -c \"\\dx\" To check if a specific table exists, run: SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = '<schema_name>' AND table_name = '<table_name>'); Databases \u00b6 To list all databases, we can use the -l parameter: psql -l To get more information about the databases, we can use the \\l+ meta-command. To check if a specific database exists, we can use the following query: SELECT EXISTS (SELECT 1 FROM pg_database WHERE datname = '<db name>'); Managing the database clusters \u00b6 As a first step, it is always good to know which clusters are installed and running. To show this information, use the pg_lsclusters command. Starting, stopping, and restarting the cluster \u00b6 Sometimes it is needed to restart the cluster. There are two commands: pg_ctl restart : restarts the cluster pg_ctl reload : reloads the configuration of the cluster Always check which one is needed in each case. For both commands, the path to the data directory of the cluster is needed. We can specify it in two ways: using the -D parameter of the pg_ctl command setting the PGDATA environment variable Monitoring activity \u00b6 To monitor the activity on Linux, we can use the pg_activity : sudo -u postgres pg_activity -U postgres For a detailed monitoring on all platforms, we can use the Cumulative Statistics System . It contains collections of statistics that can be accessed similarly to tables. The difference is that these collections are server-wide and can be accessed from any database or scheme. Important collections: - pg_stat_activity : contains information about the current activity on the server pg_stat_activity \u00b6 The pg_stat_activity collection contains information about the current activity on the server. Some activities belong to background processes it is therefore best to query the collection like: SELECT * FROM pg_stat_activity WHERE state IS NOT NULL; Kill a hanging query \u00b6 To kill the query, run: SELECT pg_cancel_backend(<PID>) The PID can be obtained from the database activity monitoring tool . Creating new user \u00b6 For creating a new user, we can use the createuser command. Important parameters: -P or --pwprompt : prompt for password. If not used, the user will be created without a password. Deleting database \u00b6 To delete a database, we can use the dropdb command: dropdb <db name> Granting privileges \u00b6 Grant privileges for database schema \u00b6 To grant all privileges for a database schema to a user, we can use the following command: GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO <user name>; To grant only the SELECT privilege, use: GRANT SELECT ON ALL TABLES IN SCHEMA public TO <user name>; Upgrading the database cluster \u00b6 With PostgreSQL version 9.3 and later, we can upgrade easily even between major versions using the pg_upgrade command. We can even skip versions, e.g., upgrade from 12 to 16. The process is described in the pg_upgrade manual, however, usually, most of the steps are not necessary as they apply only to very specific cases. On top of that, some aspects important for the upgrade are not mentioned in the manual. The steps for a typical ubuntu/debian installation are: stop the new cluster using systemctl stop postgresql@<version>-main run the pg_upgrade command with --check to check the compatibility stop the old cluster using systemctl stop postgresql run the pg_upgrade command without --check to perform the upgrade apply all actions recommended by the pg_upgrade output change the port in the /etc/postgresql/<version>/main/postgresql.conf file of the new cluster to the original port (usually 5432) start the new cluster using systemctl start postgresql@<version>-main an check if the connection works The standard pg_upgrade command looks like this: sudo -u postgres pg_upgrade --link -j <number of cores> -b <old bin dir> -d <old data dir> -D <new data dir> -o 'config_file=<old conf file>' -O 'config_file=<new conf file>' Description: --link : links the old data directory to the new one instead of copying the data. Fastest migration method. <old bin dir> : The bin directory of the old cluster. usually /usr/lib/postgresql/<old version>/bin <old/new data dir> : The data directory of the cluster. usually /var/lib/postgresql/<old/new version>/main . Can be found using pg_lsclusters <old/new conf file> : The path to the postgresql.conf file. usually /etc/postgresql/<old/new version>/main/postgresql.conf Upgrading extensions \u00b6 Some PostgreSQL extensions uses separate libraries. These are installed fro each version of the PostgreSQL server separately. If a library is not foun on the new cluster, it is detected by the pg_upgrade command automaticly. In that case, you have to install the library according to the instructions of the library provider. Managing access to the database \u00b6 To manage access to the database, we can use the pg_hba.conf file. This file is located in the data directory of the PostgreSQL installation. Unfortunately, the location of the data directory is not standardized, and the variants are many. However, there is a remedy, just execute the following SQL command: SHOW hba_file Documentation Lost Password to the Postgres Server \u00b6 The password for the db superuser is stored in db postgres . In order to log there and change it, the whole authentification has to be turned off, and then we can proceed with changing the password. Steps: find the pg_hba.conf file usually located in C:\\Program Files\\PostgreSQL\\13\\data backup the file and replace all occurances of scram-sha-256 in the file with trust restart the posgreSQL service in the Windows service management, there should be a service for postgresql running change the password for the superuser psql -U postgres ALTER USER postgres WITH password 'yourpassword'; (do not forget the semicolon at the end!) restore the pg_hba.conf file from backup restart the postgreSQL service again test if the new password works Configuration \u00b6 Documentation PostgreSQL server can be configured using parameters . The parameters itself can be set in multiple ways: default values are set in the configuration file stored in the <postgres data dir>/postgresql.conf . the values can be set at runtime using SQL the values can be set at runtime using shell commands Getting and setting parameters at runtime using SQL \u00b6 to get the value of a parameter, we can use the SHOW command or the current_setting function: SHOW <parameter name>; SELECT current_setting('<parameter name>'); To set the value of a parameter, we can use the SET command or the set_config function: SET <parameter name> TO <value>; SELECT set_config('<parameter name>', '<value>', false); If the third parameter of the set_config function is set to true , the value is set for the current transaction only. Logging \u00b6 Documentation The logs are stored in the <postgres data dir>/log directory. By default, only errors are logged. To logg statements, we need to change the log_statement parameter. Valid values are: none : no statements are logged ddl : only DDL statements are logged mod : only statements that modify the database are logged all : all statements are logged Garbage collection and optimization \u00b6 There is a shared command for both garbage collection (vacuum) and optimization (analyze) of the database. To execute it from the command line, use the vacuumdb` command. DataGrip \u00b6 Import Formats \u00b6 DataGrip can handle imports only from separator baset files (csv, tsv). View Geometry \u00b6 In the reuslt window/tab, click on the gear wheel -> Show GeoView . However, the geoviewer has a fixed WGS84 projection, so you have to project the result to this projection first . Create a spatial Index \u00b6 There is currently no GUI tool for that in DataGrip. Just add a normal index and modify the auto generated statement by changing <column> to USING GIST(<column>) at the end of the statement. Filter out store procedures \u00b6 right-click on routines click open table sort by type Creating functions and procedures \u00b6 There is no UI available currently, use Navicat or console Duplicate table \u00b6 Drag the table in the database explorer and drop it to the location you want it to copy to. Known issues and workarounds \u00b6 Cannot delete a database due to DataGrip's own connections \u00b6 Before deleting a database we need to close all DataGrip sessions connected to the database. We can do that in the sessions window. DataGrip displays objects that were deleted \u00b6 Sometimes, DataGrip displays objects that were deleted. Additionally, it it displays errors when trying to refresh the view. Solution: right-click on database connection (root object in the database explorer) click on Diagnostics -> Force refresh Navicat \u00b6 Cannot connect to db \u00b6 Symptoms: cant connect to db server: Could not connect after editing the connection and trying to save it (ok button): connection is being used Try: close navicat open navicat, edit connection click test connection click ok, and start the connection by double click PgAdmin \u00b6 The best way to install the PgAdmin is to use the EDB PostgreSQL installer and uncheck the database installation during the installation configuration. This way, we also install useful tools like psql Diagrams \u00b6 To create diagram from an existing database: right click on the database -> Generate ERD Troubleshooting \u00b6 If the db tools are unresponsive on certain tasks/queries, check if the table needed for those queries is not locke by some problematic query. Select PostgreSQL version \u00b6 SELECT version() Select PostGIS version \u00b6 SELECT PostGIS_version() Tried to send an out-of-range integer as a 2-byte value \u00b6 This error is caused by a too large number of values in the insert statement. The maximum index is a 2-byte number (max value: 32767). The solution is to split the insert statement into smaller bulks.","title":"PostgreSQL Manual"},{"location":"Programming/PostgreSQL%20Manual/#data-types","text":"official documentation","title":"Data types"},{"location":"Programming/PostgreSQL%20Manual/#date","text":"official documentation date : for dates time for time timestmp for both date and time interval","title":"Date"},{"location":"Programming/PostgreSQL%20Manual/#select-a-part-of-datetimetimestamp","text":"If we want just a part of a date, time, or timestamp, we can use the extract function. Example: SELECT extract(hour FROM <date column name>) FROM... Other parts can be extracted too. To extract day of week , we can use isodow (assigns 1 to Monday and 7 to Sunday).","title":"Select a part of date/time/timestamp"},{"location":"Programming/PostgreSQL%20Manual/#auto-incrementing-columns","text":"In PostgreSQL, sequences are used for auto-incrementing columns. When you are creating a new db table or adding a new column, the process of creating a new sequence can be automated by choosing an identity or a serial column type. When updating an aexisting column, a manual intervention is required: change the column to some numerical datatype create the sequence: CREATE SEQUENCE <SEQUENCE NAME> OWNED BY <TABLE NAME>.<COLUMN NAME>; adjust the value of the sequence: SELECT setval(pg_get_serial_sequence('<TABLE NAME>', '<COLUMN NAME>'), max(<COLUMN NAME>)) FROM <TABLE NAME>; set the column to be incremented by the sequence: ALTER TABLE <TABLE NAME> ALTER COLUMN <COLUMN NAME> SET DEFAULT nextval('<SEQUENCE NAME>');","title":"Auto incrementing columns"},{"location":"Programming/PostgreSQL%20Manual/#strings","text":"There are many string function available, including the format function that works similarly to the C format function. For all functions, check the documentation .","title":"Strings"},{"location":"Programming/PostgreSQL%20Manual/#arrays","text":"arrays array functions and operators arrays are declared as <type>[] , e.g., integer[] . The type can be any type, including composite types. To compute array length , use array_length(contracted_vertices, 1) where 1 stands for the first dimension. To cretea an array literal , we use single quatation and curly brackets: '{1, 2, 3}' . To check that some value match at least some member of the array, we use ANY : SELECT ... FROM tab WHERE tab.a = ANY(<array>)","title":"Arrays"},{"location":"Programming/PostgreSQL%20Manual/#working-with-the-array-members-individualy","text":"For using the members of an array in the SELECT or JOIN , we have to first split the array using the unnest function. This function transforms the result set to a form where there is a separate row for each member of the array (a kind of inverse operation to group by). If we want to also keep the array index, we can use the WITH ORDINALITY expression, as shown in the manual or on SO .","title":"Working with the array members individualy"},{"location":"Programming/PostgreSQL%20Manual/#hstore","text":"A specific feature of PostgreSQL is the hstore column type. It enables to store structured data in a single column. It can be used to dump variables that we do not plan to utilize in the database (i.e., in the SELECT, JOIN statements) frequently. This data type requires the hstore extension to be enabled in the database. To enable it, use the following command: CREATE EXTENSION hstore; When we, exceptionally, want to access a variable from a hstore column, we can use the following syntax: SELECT <COLUMN NAME>-><VARIABLE NAME> AS ...","title":"hstore"},{"location":"Programming/PostgreSQL%20Manual/#schemas","text":"official documentation Schemas in PostgreSQL are implemented as namespaces according to the SQL standard.","title":"Schemas"},{"location":"Programming/PostgreSQL%20Manual/#search-path","text":"official documentation Usually, we does not have to qualify the table name with the schema name, as the schema name is in the search path . By default the search path is set to \"$user\", public , which means that the tables are first searched in the schema with the same name as the user, and then in the public schema. To show the current search path, run: SHOW search_path; To change the search path, run: SET search_path TO <schema name list>;","title":"Search path"},{"location":"Programming/PostgreSQL%20Manual/#procedures-and-functions","text":"","title":"Procedures and functions"},{"location":"Programming/PostgreSQL%20Manual/#calling-a-procedure","text":"to exectue a stored procedure, use\" CALL <procedure name>(<procedure arguments>) Unlike in programing languages, there is no implicit type cast of the program arguments, including literals. Therefore, we need to cast all parameters explicitely, as in the following example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments(target_area_id smallint) ... CALL compute_speeds_for_segments(1::smallint);","title":"Calling a procedure"},{"location":"Programming/PostgreSQL%20Manual/#creating-a-procedure","text":"documentation The syntax is as follows: CREATE PROCEDURE <name> (<params>) LANGUAGE <language name> <procedure body> , while <procedure body> can be both a delimited string: AS $$ <sql statements> $$ OR an active SQL body (since PostgreSQL 14): BEGIN ATOMIC <sql statements> END There are some differences between those syntaxes (e.g., the second one works only for SQL and is evaluated/checked for validity at the time of creation), bot in most cases, they are interchangable. For more details, check the manual .","title":"Creating a procedure"},{"location":"Programming/PostgreSQL%20Manual/#variables","text":"In PostgreSQL, all variables must be declared before assignmant in the DECLARE block which is before the sql body of the function/procedure/ DO . The syntax is: <variable name> <variable type>[ = <varianle value>]; Example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments() LANGUAGE plpgsql AS $$ DECLARE dataset_quality smallint = 1; BEGIN ... The, the value of a variable can be change using the classical assignment syntax: <variable name> = <varianle value>; Be carful to not use a variable name equal to the name of some of the columns used in the same context, which results in a name clash.","title":"Variables"},{"location":"Programming/PostgreSQL%20Manual/#assigning-a-value-to-a-variable-using-sql","text":"There are two options how to assign a value to a variable using SQL: using the INTO clause in the SELECT statement using a SELECT statement as rvlaue of the assignment Example with INTO : SELECT name INTO customer_name FROM customers WHERE id = 1 EXAMPLE with SELECT as rvalue: customer_name = (SELECT name FROM customers WHERE id = 1)","title":"Assigning a value to a variable using SQL"},{"location":"Programming/PostgreSQL%20Manual/#functions","text":"Functions in PostgreSQL have a similar syntax to procedures. Unlike for procedures, we need to specify a return type for function, either as an OUT / INOUT parameter, or using the RETURNS clause. To return tabular data, we use TABLE return type: RETURNS TABLE(<param 1 name> <param 1 type>, ..., <param n name> <param n type>) To select the result of a function with the return type above, call: SELECT * FROM <function signature>","title":"Functions"},{"location":"Programming/PostgreSQL%20Manual/#return-next-and-return-query","text":"Sometimes, we need to do some cleanup after selecting the rows to be returned from function, or we need to build the result in a loop. In classical programming languages, we use variables for this purpose. In PG/plSQL, we can also use the RETURN NEXT and RETURN QUERY constructs. These constructs prepare the result, and does not return from the function . Instead, use an empty RETURN to return from the function. Example: RETURN QUERY SELECT ...; DROP TABLE target_ways; RETURN; Note that for these constructs, the return type needs to be a table or setof type. The RETURN QUERY cannot be used for returning a single value even if the query returns a single value. If we have a single value return type and need to do some postprocessing between selecting the value and returning from the function, we have to use a variable instead.","title":"RETURN NEXT and RETURN QUERY"},{"location":"Programming/PostgreSQL%20Manual/#deciding-the-language","text":"For simple statements, we can use the SQL language. We need the PL/pgSQL if: we need to use variables or control statements specific to PL/pgSQL we need to use temporary tables, as the SQL language fails to recognize them if they are created inside the function/procedure","title":"Deciding the language"},{"location":"Programming/PostgreSQL%20Manual/#conditional-filters-based-on-the-value-of-a-variable-or-input-parameter","text":"To add some filter to the WHERE or ON clause of a query based on the value of a variable or input parameter, we can use the following technique: set the variable or input parameter to NULL if we do not want to apply the filter in the filter test for disjunction of NULL or the filter condition Example: SELECT ... FROM ... WHERE param IS NULL OR some_column = param","title":"Conditional filters based on the value of a variable or input parameter"},{"location":"Programming/PostgreSQL%20Manual/#temporary-tables","text":"To use a result set efficiently in a function or procedure, we often use temporary tables. Unlike in other relational database systems, in PostgreSQl, the lifetime of a temporary table is bound to a session. Therefoe, if we call a function that creates a tempoary table multiple times in a single session, we encounter an error, because the table already exists. To tackle this problem, we need to delete all temporary tables manually. Luckily, there is a special DISCARD command that can be used to dtop all temporary tables at once: DISCARD TEMPORARY;","title":"Temporary tables"},{"location":"Programming/PostgreSQL%20Manual/#do-command","text":"The DO command can be used to execude an anonymus code block in any of the languages suported by PostgreSQL. It behaves like a function with no parameters and no return value. Syntax: DO [LANGUAGE <lang name>] <code> The default language is plpgsql . Example: DO $$ BEGIN RAISE NOTICE 'Hello world'; END $$","title":"DO command"},{"location":"Programming/PostgreSQL%20Manual/#window-functions","text":"PostgreSQL supports an extended syntax for window functions . We can use it for example to retrieve the value of a column that has maximum value in another column, as demonstrated in an SO answer .","title":"Window functions"},{"location":"Programming/PostgreSQL%20Manual/#various-specific-tasks","text":"","title":"Various specific tasks"},{"location":"Programming/PostgreSQL%20Manual/#selecting-rows-for-deletion-based-on-data-from-another-table","text":"If we want to delete rows from a table based on some condition on data from another table, we can use the DELETE statement with a USING clause. Example: DELETE FROM nodes_ways_speeds USING nodes_ways WHERE nodes_ways_speeds.to_node_ways_id = nodes_ways.id AND nodes_ways.area IN (5,6)","title":"Selecting rows for deletion based on data from another table"},{"location":"Programming/PostgreSQL%20Manual/#handeling-duplicates-in-the-insert-statement","text":"To handle duplicates on INSERT , PostgreSQL provides the ON CONFLICT clause (see the INSERT documentation). The options are: DO NOTHING : do nothing DO UPDATE SET <column name> = <value> : update the column to the given value","title":"Handeling duplicates in the INSERT statement"},{"location":"Programming/PostgreSQL%20Manual/#random-oredering","text":"To order the result set randomly, we can use the RANDOM() function in the ORDER BY clause: SELECT ... FROM ... ORDER BY RANDOM()","title":"Random oredering"},{"location":"Programming/PostgreSQL%20Manual/#random-ordering-with-a-seed-pseudo-random-ordering","text":"To receive a determinisic (repeatable) random ordering, we can use the setseed function: SELECT setseed(0.5); SELECT ... FROM ... ORDER BY RANDOM(); Note that we need two queries, one for setting the seed and one for the actual query. If we does not have an option to call arbitrary queries, we have to use UNION : SELECT col_1, ..., col_n FROM ( SELECT _, null AS col_1, ..., null AS col_n FROM setseed(0.5) UNION ALL SELECT null AS _, col_1, ..., col_n FROM ... OFFSET 1 ) ORDER BY RANDOM() The OFFSET 1 is used to skip the first row, which is the result of the setseed function. The union is the only way to guarantee that the seed will be set before the actual query. Other options, such as WITH or SELECT ... FROM (SELECT setseed(0.5)) do not guarantee the order of execution, and produce a different result for each call.","title":"Random ordering with a seed (Pseudo-random ordering)"},{"location":"Programming/PostgreSQL%20Manual/#postgis","text":"PolsGIS is an extension for PostgreSQL that adds support for spatial data. To enable the extension, we need to: install the extension e.g., using the bundeled stack builder application enable the extension in the database SQL CREATE EXTENSION postgis;","title":"PostGis"},{"location":"Programming/PostgreSQL%20Manual/#geometry-columns","text":"Postgis features can be utilized with geometry and geography column types. To add a new geometry column: ADD COLUMN <COLUUMN NAME> geometry(<GEOMETRY TYPE>, <SRID>)","title":"Geometry columns"},{"location":"Programming/PostgreSQL%20Manual/#spatial-indexing","text":"Documentation Analogously to standard SQL column indicis, there are spatial indices in PostGIS. The only difference is that we need to add the USING GIST at the end of the CREATE INDEX statement: CREATE INDEX nodes_geom_idx ON nodes USING GIST (geom);","title":"Spatial Indexing"},{"location":"Programming/PostgreSQL%20Manual/#converting-between-geometry-types","text":"There are dedicated functions whcich we can use to convert between geometry types: ST_Multi : converts geometries to their multi-variant, e.g., LineString to MultiLineString .","title":"Converting between geometry types"},{"location":"Programming/PostgreSQL%20Manual/#compute-area-surrounding-geometries","text":"If we are ok with a convex envelope of the geometries, we can simply use the St_ConvexHull functon. Howeever, if we need the exact shape, We have to use the St_ConcaveHull function which computes the concave hull of a geometry. The St_ConcaveHull function takes an additional parameter param_pctconvex which determines how concave is the result: 0 means strictly concave, while 1 means convex hull. Note that while lowere values leads to more acurate results, the computation is much slower. There is also another parameter param_allow_holes which determines whether holes in the object are permited (default false).","title":"Compute area surrounding geometries"},{"location":"Programming/PostgreSQL%20Manual/#split-area-to-polygons-based-on-points","text":"The split of spece into polygons based on a set of points is called Voronoi diagram . In PostGIS, we have a `ST_Voronoi_Polygons for that. To obtain a set of polygons from a set of points, it is necessary to Aggregate the rows ( ST_Collect ) Compute the polygon geometry ( ST_Voronoi_Polygons ) Disaggregate the geometry into individual polygons ( ST_Dump ) Also, there is an important aspect of how far the polygons will reach outside of the points. By default, it enlarge the area determined by the points by about 50%. If we need a larger area, we can use the extend_to parameter. If we need a smaller area, however, we need to compute the intersection with this smaller area afterwards manually. Full example: SELECT st_intersection( (st_dump( st_voronoipolygons(st_collect(<GEOMETRY COLUMN>)) )).geom, (<select clause for area of desired voronoi polygons>) ) AS geom FROM ... If we need to join the polygons to the original points, we need to do it manually (e.g. by JOIN ... ON ST_Within(<POINT COLUMN>, <VORONOI POLYGONS GEO DUMP>) ).","title":"Split area to polygons based on points"},{"location":"Programming/PostgreSQL%20Manual/#other-useful-functions","text":"[ ST_Within ] ST_Within (A, B) if A is completly inside B ST_Intersects ST_Intersects(g1, g2) if g1 and g2 have at least one point in common. ST_Transform : ST_Transform(g, srid) transforms geometry g to a projection defined by the srid and returns the result as a geometry. ST_Buffer : ST_Buffer(g, radius) computes a geometry that is an extension of g by radius to all directions St_Collect aggregates data into single geometry. It is usually apllied to a geometry column in an SQL selection. ST_Union ST_Equals ST_MakeLine : make line between two points ST_SetSRID : sets the SRID of the geometry and returns the result as a new geometry. ST_MakePoint : creates a point geometry from the given coordinates. ST_Area : computes the area of a geometry. The units of the result are the same as the units of the SRID of the geometry (use UTM coordinate system for getting the area in square meters).","title":"Other Useful Functions"},{"location":"Programming/PostgreSQL%20Manual/#pgrouting","text":"PgRouting is a PostgreSQL extension focused on graph/network manpulation. It contains functions for: finding the strongly connected components: `pgr_strongComponents graph contraction/simplification","title":"PgRouting"},{"location":"Programming/PostgreSQL%20Manual/#finding-strongly-connected-components","text":"The function pgr_strongComponents finds the strongly connected components of a graph. The only parameter of the script is a query that should return edge data in the folowing format: id , source , target , cost , reverse_cost . The first three parameters are obvious. The cost parameter does not have any effect. You should provide a negative reverse_cost , othervise, the edge will be considered as bidirectional!","title":"Finding strongly connected components"},{"location":"Programming/PostgreSQL%20Manual/#plpgsql","text":"PL/pgSQL is a procedural language available in PostgreSQL databases. It can be used inside: functions procedures DO command","title":"PL/pgSQL"},{"location":"Programming/PostgreSQL%20Manual/#branching","text":"PL/pgSQL has the following branching: IF <condition> THEN ... [ELSEIF ... ] [ELSE ... ] END IF","title":"Branching"},{"location":"Programming/PostgreSQL%20Manual/#logging","text":"Basic logging can be done using the RAISE command: RAISE NOTICE 'Some message'; We can add parameters by using the % placeholder: RAISE NOTICE 'Some message %', <variable or SQL command>; For more, see the documentation .","title":"Logging"},{"location":"Programming/PostgreSQL%20Manual/#query-diagnostics","text":"Various information about the last query can be obtained using the GET DIAGNOSTIC command. For example, the number of rows affected by the last query can be obtained using the ROW_COUNT parameter: GET DIAGNOSTIC <variable> = ROW_COUNT; The result is stored in the variable <variable> . Note that this constract is not available in the SQL queries but only in a PL/pgSQL block. For other diagnostic fields, see the documentation .","title":"Query diagnostics"},{"location":"Programming/PostgreSQL%20Manual/#psql","text":"Documentation psql is a basic command line utitilty for manipulating postgres database. To connect, type: psql -d <db name> Then the SQL commands can be executed. Note that no matter what SQL commands you plan to execute, you have to connect to a specific database . Ommitting the -d parameter will not connect you to the server in some admin mode, but instead, it will connect you to the default database, which is usually the database with the same name as the user name. If there is no such database, the connection will fail. To execute command immediatelly without starting interactive session, use the -c parameter: psql -d <db name> -c \"<command>\" Do not forget to quote the command. Also, note that certain SQL commands, such as CREATE DATABASE requires its own session. To combine them with other SQL commands, you can use multiple -c parameters: psql -d <db name> -c \"CREATE DATABASE <db name>\" -c \"<other command>\" Other useful parameters: -X : do not read the ~/.psqlrc file. This is useful when debuging the psql commands or running scripts, as it disables any customizations. -U <user name> : connect as a different user. If not specified, the current user is used. -p : specify the port number. (default is 5432)","title":"psql"},{"location":"Programming/PostgreSQL%20Manual/#meta-commands","text":"The psql has its own set of commands, called meta-commands . These commands start with a backslash ( \\ ) and can be used inside SQL queries (either interactively, or in the -c argument). Example: psql -d <db name> -c \"\\l+\" The above command lists all databases, including additional information. Note that the meta-commands cannot be combined with SQL queries passed to the -c parameter . As -c argument, we can use either: a plain SQL query without any meta-commands a single meta-command without any SQL query (like the example above) (In \\copy public.nodes FROM nodes.csv CSV HEADER , the string after \\copy is a list of arguments, not an SQL query) When we want to combine a meta-command with an SQL query, we need to use some of the workarounds: use the psql interactive mode use the psql --file parameter to execute a script from a file pipe the commands to psql using echo : bash echo \"COPY (SELECT * FROM opendata.public.nodes WHERE area = 13) TO STDOUT WITH CSV HEADER \\g 'nodes.csv'\" | psql -d opendata","title":"meta-commands"},{"location":"Programming/PostgreSQL%20Manual/#importing-data","text":"A simple SQL data (database dump) can be imported using the psql command: psql -d <db name> -f <file name> we can also import from stdin by omitting the -f parameter: psql -d <db name> < <file name> # or <command> | psql -d <db name>","title":"Importing data"},{"location":"Programming/PostgreSQL%20Manual/#importing-compressed-sql-dump","text":"To import a compressed SQL dump we need to know how the dump was compressed. If it was compressed using the pg_dump with the -Z parameter, we use a pg_restore command: bash pg_restore -d <db name> <file name> If it was compressed using a compression tool, we need to pipe the output to the decompression tool and then to psql : bash <decompression tool> < <file name> | psql -d <db name>","title":"Importing compressed SQL dump"},{"location":"Programming/PostgreSQL%20Manual/#importing-data-from-csv","text":"The easiest way to import data from csv is to use the psql \\copy meta-command: psql -d <db name> -c \"\\copy <table name> FROM <file name> CSV HEADER\" The syntax of this command and its parameters is almost identical to the COPY command. Important parameters: CSV : the file is in CSV format HEADER : the first line of the file contains column names Except simplicity, the \\copy command has another advantage over the COPY command: it has the same access rights as the user that is currently logged in. The COPY command, on the other hand, has the access rights of the user that started the database server. The COPY command, however, has an advantage over the \\copy command when we access a remote database. In that case, the \\copy command would first download the whole file to the client and then upload it to the server. The COPY command, on the other hand, can be executed directly on the server, so the data are not downloaded to the client.","title":"Importing data from csv"},{"location":"Programming/PostgreSQL%20Manual/#importing-data-from-csv-with-columns-than-are-not-in-the-db-table","text":"The COPY (and \\copy ) command can be used to copy the input into a database table even if it contains only a subset of database table columns. However, it does not work the other way, i.e, all input columns have to be used. If only a subset of input columns needs to be used, or some of the input columns requires processing, we need to use some workaround. The prefered mathod depends on the characte of the data: data do not match the table, but they are small: load the data with pandas process the data as needed use pandas.to_sql to upload the data data do not match the table and they are large: preprocess the data with batch commands use COPY (or \\copy ) to upload the data data do not match the table and they are large and dirty : use the file_fdw module: create a table for SQL mapping with tolerant column types (e.g., text for problematic columns) select from the mapping to the real table","title":"Importing data from csv with columns than are not in the db table"},{"location":"Programming/PostgreSQL%20Manual/#importing-data-from-a-shapefile","text":"There are multiple options: shp2psql : simple tool that creates sql from a shapefile easy start, almost no configuration always imports all data from shapefile, cannot be configured to skip columns included in the Postgres instalation ogr2ogr needs to be installed, part of GDAL QGIS The db manger can be used to export data from QGIS data can be viewed before import only suitable for creating new table, not for appending to an existing one","title":"Importing data from a shapefile"},{"location":"Programming/PostgreSQL%20Manual/#importing-data-from-geojson","text":"For a single geometry stored in a GeoJSON file, the function ST_GeomFromGeoJSON can be used. just copy the geometry part of the file change the geometry type to match the type in db don't forget to surround the object with curly brackets For the whole document, the ogr2ogr tool can be used.","title":"Importing data from GeoJSON"},{"location":"Programming/PostgreSQL%20Manual/#exporting-data","text":"The data can be exported to SQL using the pg_dump command. The simplest usage is: pg_dump -f <output file name> <db name> Important parameters: -s : export only the schema, not the data If we need to further process the data, we can use the stdout as an output simply by omitting the -f parameter: pg_dump <db name> | <command>","title":"Exporting data"},{"location":"Programming/PostgreSQL%20Manual/#exporting-the-database-server-configuration","text":"To export the database server configuration, we can use the pg_dumpall command: pg_dumpall -f <output file name>","title":"Exporting the database server configuration"},{"location":"Programming/PostgreSQL%20Manual/#compressing-the-sql-dump","text":"To compress the SQL dump, we have two options: use the pg_dump command with the -Z parameter, e.g., pg_dump <db name> -Z1 -f <output file name> or pipe the output to a compression tool, e.g., pg_dump <db name> | gzip > <output file name>","title":"Compressing the SQL dump"},{"location":"Programming/PostgreSQL%20Manual/#exporting-data-to-csv","text":"When exporting large data sets, it is not wise to export them as an SQL dump. To export data to csv, we can use either: psql \\copy command COPY command in SQL The simplest way is to use the \\copy command. However, it may be slow if we call psql from a client and we want to export the data to a file on the server, because the data are first sent to the client and then back to the server. The COPY command is the fastest way to export data to the server. However, by default, it can be tricky to use, as the sql server needs to have write access to the file. To overcome this problem, we can use the following workaround: choose STDOUT as an output for the COPY command at the end of the command, add \\g <file name> to redirect the output to a file. Example: echo \"COPY (SELECT * FROM opendata.public.nodes WHERE area = 13) TO STDOUT WITH CSV HEADER \\g 'nodes.csv'\" | psql -d opendata","title":"Exporting data to csv"},{"location":"Programming/PostgreSQL%20Manual/#information-and-statistics","text":"To show the table size , run: SELECT pg_size_pretty(pg_total_relation_size('<table name>')); To show the version of the PostgreSQL server, run: SELECT version(); To list all extensions for a database, run: psql -d <db name> -c \"\\dx\" To check if a specific table exists, run: SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_schema = '<schema_name>' AND table_name = '<table_name>');","title":"Information and statistics"},{"location":"Programming/PostgreSQL%20Manual/#databases","text":"To list all databases, we can use the -l parameter: psql -l To get more information about the databases, we can use the \\l+ meta-command. To check if a specific database exists, we can use the following query: SELECT EXISTS (SELECT 1 FROM pg_database WHERE datname = '<db name>');","title":"Databases"},{"location":"Programming/PostgreSQL%20Manual/#managing-the-database-clusters","text":"As a first step, it is always good to know which clusters are installed and running. To show this information, use the pg_lsclusters command.","title":"Managing the database clusters"},{"location":"Programming/PostgreSQL%20Manual/#starting-stopping-and-restarting-the-cluster","text":"Sometimes it is needed to restart the cluster. There are two commands: pg_ctl restart : restarts the cluster pg_ctl reload : reloads the configuration of the cluster Always check which one is needed in each case. For both commands, the path to the data directory of the cluster is needed. We can specify it in two ways: using the -D parameter of the pg_ctl command setting the PGDATA environment variable","title":"Starting, stopping, and restarting the cluster"},{"location":"Programming/PostgreSQL%20Manual/#monitoring-activity","text":"To monitor the activity on Linux, we can use the pg_activity : sudo -u postgres pg_activity -U postgres For a detailed monitoring on all platforms, we can use the Cumulative Statistics System . It contains collections of statistics that can be accessed similarly to tables. The difference is that these collections are server-wide and can be accessed from any database or scheme. Important collections: - pg_stat_activity : contains information about the current activity on the server","title":"Monitoring activity"},{"location":"Programming/PostgreSQL%20Manual/#pg_stat_activity","text":"The pg_stat_activity collection contains information about the current activity on the server. Some activities belong to background processes it is therefore best to query the collection like: SELECT * FROM pg_stat_activity WHERE state IS NOT NULL;","title":"pg_stat_activity"},{"location":"Programming/PostgreSQL%20Manual/#kill-a-hanging-query","text":"To kill the query, run: SELECT pg_cancel_backend(<PID>) The PID can be obtained from the database activity monitoring tool .","title":"Kill a hanging query"},{"location":"Programming/PostgreSQL%20Manual/#creating-new-user","text":"For creating a new user, we can use the createuser command. Important parameters: -P or --pwprompt : prompt for password. If not used, the user will be created without a password.","title":"Creating new user"},{"location":"Programming/PostgreSQL%20Manual/#deleting-database","text":"To delete a database, we can use the dropdb command: dropdb <db name>","title":"Deleting database"},{"location":"Programming/PostgreSQL%20Manual/#granting-privileges","text":"","title":"Granting privileges"},{"location":"Programming/PostgreSQL%20Manual/#grant-privileges-for-database-schema","text":"To grant all privileges for a database schema to a user, we can use the following command: GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO <user name>; To grant only the SELECT privilege, use: GRANT SELECT ON ALL TABLES IN SCHEMA public TO <user name>;","title":"Grant privileges for database schema"},{"location":"Programming/PostgreSQL%20Manual/#upgrading-the-database-cluster","text":"With PostgreSQL version 9.3 and later, we can upgrade easily even between major versions using the pg_upgrade command. We can even skip versions, e.g., upgrade from 12 to 16. The process is described in the pg_upgrade manual, however, usually, most of the steps are not necessary as they apply only to very specific cases. On top of that, some aspects important for the upgrade are not mentioned in the manual. The steps for a typical ubuntu/debian installation are: stop the new cluster using systemctl stop postgresql@<version>-main run the pg_upgrade command with --check to check the compatibility stop the old cluster using systemctl stop postgresql run the pg_upgrade command without --check to perform the upgrade apply all actions recommended by the pg_upgrade output change the port in the /etc/postgresql/<version>/main/postgresql.conf file of the new cluster to the original port (usually 5432) start the new cluster using systemctl start postgresql@<version>-main an check if the connection works The standard pg_upgrade command looks like this: sudo -u postgres pg_upgrade --link -j <number of cores> -b <old bin dir> -d <old data dir> -D <new data dir> -o 'config_file=<old conf file>' -O 'config_file=<new conf file>' Description: --link : links the old data directory to the new one instead of copying the data. Fastest migration method. <old bin dir> : The bin directory of the old cluster. usually /usr/lib/postgresql/<old version>/bin <old/new data dir> : The data directory of the cluster. usually /var/lib/postgresql/<old/new version>/main . Can be found using pg_lsclusters <old/new conf file> : The path to the postgresql.conf file. usually /etc/postgresql/<old/new version>/main/postgresql.conf","title":"Upgrading the database cluster"},{"location":"Programming/PostgreSQL%20Manual/#upgrading-extensions","text":"Some PostgreSQL extensions uses separate libraries. These are installed fro each version of the PostgreSQL server separately. If a library is not foun on the new cluster, it is detected by the pg_upgrade command automaticly. In that case, you have to install the library according to the instructions of the library provider.","title":"Upgrading extensions"},{"location":"Programming/PostgreSQL%20Manual/#managing-access-to-the-database","text":"To manage access to the database, we can use the pg_hba.conf file. This file is located in the data directory of the PostgreSQL installation. Unfortunately, the location of the data directory is not standardized, and the variants are many. However, there is a remedy, just execute the following SQL command: SHOW hba_file Documentation","title":"Managing access to the database"},{"location":"Programming/PostgreSQL%20Manual/#lost-password-to-the-postgres-server","text":"The password for the db superuser is stored in db postgres . In order to log there and change it, the whole authentification has to be turned off, and then we can proceed with changing the password. Steps: find the pg_hba.conf file usually located in C:\\Program Files\\PostgreSQL\\13\\data backup the file and replace all occurances of scram-sha-256 in the file with trust restart the posgreSQL service in the Windows service management, there should be a service for postgresql running change the password for the superuser psql -U postgres ALTER USER postgres WITH password 'yourpassword'; (do not forget the semicolon at the end!) restore the pg_hba.conf file from backup restart the postgreSQL service again test if the new password works","title":"Lost Password to the Postgres Server"},{"location":"Programming/PostgreSQL%20Manual/#configuration","text":"Documentation PostgreSQL server can be configured using parameters . The parameters itself can be set in multiple ways: default values are set in the configuration file stored in the <postgres data dir>/postgresql.conf . the values can be set at runtime using SQL the values can be set at runtime using shell commands","title":"Configuration"},{"location":"Programming/PostgreSQL%20Manual/#getting-and-setting-parameters-at-runtime-using-sql","text":"to get the value of a parameter, we can use the SHOW command or the current_setting function: SHOW <parameter name>; SELECT current_setting('<parameter name>'); To set the value of a parameter, we can use the SET command or the set_config function: SET <parameter name> TO <value>; SELECT set_config('<parameter name>', '<value>', false); If the third parameter of the set_config function is set to true , the value is set for the current transaction only.","title":"Getting and setting parameters at runtime using SQL"},{"location":"Programming/PostgreSQL%20Manual/#logging_1","text":"Documentation The logs are stored in the <postgres data dir>/log directory. By default, only errors are logged. To logg statements, we need to change the log_statement parameter. Valid values are: none : no statements are logged ddl : only DDL statements are logged mod : only statements that modify the database are logged all : all statements are logged","title":"Logging"},{"location":"Programming/PostgreSQL%20Manual/#garbage-collection-and-optimization","text":"There is a shared command for both garbage collection (vacuum) and optimization (analyze) of the database. To execute it from the command line, use the vacuumdb` command.","title":"Garbage collection and optimization"},{"location":"Programming/PostgreSQL%20Manual/#datagrip","text":"","title":"DataGrip"},{"location":"Programming/PostgreSQL%20Manual/#import-formats","text":"DataGrip can handle imports only from separator baset files (csv, tsv).","title":"Import Formats"},{"location":"Programming/PostgreSQL%20Manual/#view-geometry","text":"In the reuslt window/tab, click on the gear wheel -> Show GeoView . However, the geoviewer has a fixed WGS84 projection, so you have to project the result to this projection first .","title":"View Geometry"},{"location":"Programming/PostgreSQL%20Manual/#create-a-spatial-index","text":"There is currently no GUI tool for that in DataGrip. Just add a normal index and modify the auto generated statement by changing <column> to USING GIST(<column>) at the end of the statement.","title":"Create a spatial Index"},{"location":"Programming/PostgreSQL%20Manual/#filter-out-store-procedures","text":"right-click on routines click open table sort by type","title":"Filter out store procedures"},{"location":"Programming/PostgreSQL%20Manual/#creating-functions-and-procedures","text":"There is no UI available currently, use Navicat or console","title":"Creating functions and procedures"},{"location":"Programming/PostgreSQL%20Manual/#duplicate-table","text":"Drag the table in the database explorer and drop it to the location you want it to copy to.","title":"Duplicate table"},{"location":"Programming/PostgreSQL%20Manual/#known-issues-and-workarounds","text":"","title":"Known issues and workarounds"},{"location":"Programming/PostgreSQL%20Manual/#cannot-delete-a-database-due-to-datagrips-own-connections","text":"Before deleting a database we need to close all DataGrip sessions connected to the database. We can do that in the sessions window.","title":"Cannot delete a database due to DataGrip's own connections"},{"location":"Programming/PostgreSQL%20Manual/#datagrip-displays-objects-that-were-deleted","text":"Sometimes, DataGrip displays objects that were deleted. Additionally, it it displays errors when trying to refresh the view. Solution: right-click on database connection (root object in the database explorer) click on Diagnostics -> Force refresh","title":"DataGrip displays objects that were deleted"},{"location":"Programming/PostgreSQL%20Manual/#navicat","text":"","title":"Navicat"},{"location":"Programming/PostgreSQL%20Manual/#cannot-connect-to-db","text":"Symptoms: cant connect to db server: Could not connect after editing the connection and trying to save it (ok button): connection is being used Try: close navicat open navicat, edit connection click test connection click ok, and start the connection by double click","title":"Cannot connect to db"},{"location":"Programming/PostgreSQL%20Manual/#pgadmin","text":"The best way to install the PgAdmin is to use the EDB PostgreSQL installer and uncheck the database installation during the installation configuration. This way, we also install useful tools like psql","title":"PgAdmin"},{"location":"Programming/PostgreSQL%20Manual/#diagrams","text":"To create diagram from an existing database: right click on the database -> Generate ERD","title":"Diagrams"},{"location":"Programming/PostgreSQL%20Manual/#troubleshooting","text":"If the db tools are unresponsive on certain tasks/queries, check if the table needed for those queries is not locke by some problematic query.","title":"Troubleshooting"},{"location":"Programming/PostgreSQL%20Manual/#select-postgresql-version","text":"SELECT version()","title":"Select PostgreSQL version"},{"location":"Programming/PostgreSQL%20Manual/#select-postgis-version","text":"SELECT PostGIS_version()","title":"Select PostGIS version"},{"location":"Programming/PostgreSQL%20Manual/#tried-to-send-an-out-of-range-integer-as-a-2-byte-value","text":"This error is caused by a too large number of values in the insert statement. The maximum index is a 2-byte number (max value: 32767). The solution is to split the insert statement into smaller bulks.","title":"Tried to send an out-of-range integer as a 2-byte value"},{"location":"Programming/Regex/","text":"Symbol meaning \u00b6 . any character [xyz] one of these characters [c-j] any character between c and j . We can combine this and the previou syntax, e.g.: [az0-4jsd] . Note that the minus sign is interpreted as a range only if it is between two characters. [^c-g] ^ means negation: anything except the following set of characters. Note that the negation( ^ ) sign needs to be the first character in the bracket. \\ : escape character. | means OR. It has the lowest precedence, so it is evaluated last. ? lazy quantifier. It will try to match as few characters as possible (i.e., previous pattern will try to match only till the next patern matches). ?R recursive pattern. Quantifiers \u00b6 * : zero or more + : one or more ? zero or one {<from>, <to>} between from and to times. If to is omitted, it means infinity. If from is omitted, it means zero. If there is only one number, it means exact count. If both are omitted, it means one. Anchors \u00b6 ^x must start with x x$ must end with x Groups and Lookarounds \u00b6 () capture group. We can refer to it later, either in the regex, or in the result of the match, depending on the programming language. The nubering starts from 1, the 0 group is usually the whole match. in the regex we refer to group using \\1 , \\2 , etc. (?:) non-caputing group. It is useful when we want to use the quantifiers on a group, but we don't want to capture it. (?=) positive lookahead. It will try to match the pattern, but it will not consume it. (?!) negative lookahead. It is useful when we want to match a pattern, but we don't want to consume it. (?<=) positive lookbehind. Same as positive lookahead, but it looks behind. (?<!) negative lookbehind. Same as negative lookahead, but it looks behind. Principles \u00b6 Non-capturing groups \u00b6 Non-capturing groups are groups that helps to specify the match but they are not captured. They are useful when we want to use the group content to specify the match, but we don't want to capture/consume the group. Some of them can be replaced, but usually with a more complicated regex. All of the non-capturing groups start with (? and end with ) . The ? is followed by a character that specifies the type of the group. The most common are: ?: non-capturing group ?= positive lookahead ?! negative lookahead ?<= positive lookbehind ?<! negative lookbehind The actual content of the group is specified between the group type specifier (e.g., ?= ) and the closing bracket ( ) ). Example: (?=d)a This regex will match a only if it is followed by d . The d will not be consumed. Note that some regex engines don't support variable length lookbehind . To overcome this, we can use the following tricks: use multiple lookbehinds with fixed length construct a more complicated regex that will match the same thing place a marker with one regex replace and then use the lookbehind to match the marker Examples \u00b6 Any whitespace \u00b6 /[\\x{00a0}\\s]/u Non-breaking space \u00b6 ((?!&nbsp;)[^\\s\\x{00a0}]) Transform Google sheet to latex table \u00b6 naj\u00edt ([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n nahradit \\1 & \\2 & \\3 & \\4 \\\\\\\\\\r\\n CSV to Latex \u00b6 Search: ([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,\\r\\n]*)\\n Replace \\1 & \\2 & \\3 & \\4 & \\5 & \\6 & \\7 \\\\\\\\\\r\\n Name regex \u00b6 ([A\u00c1BC\u010cDEFGHIJKLMNOPQR\u0158S\u0160TU\u00daVWXYZ\u017d]{1}[a\u00e1bc\u010dd\u010fe\u00e9\u011bfghchi\u00edjklmn\u0148o\u00f3pqr\u0159s\u0161t\u0165u\u00fa\u016fvwxy\u00fdz\u017ew]+ *){2,3} Archive \u00b6 vasdomovnik \u00b6 naj\u00edt [0-9]+[ ]*([^\\r\\n]*)[\\r\\n]+ nahradit '\\1' , sloupce na pole php \u00b6 naj\u00edt ([^\\r\\n]*)([\\r\\n])+ nahradit '\\1' => ''\\2, Pogamut - jen logy od jednoho bota \u00b6 \\(TeamCTF[^2][^\\r\\n]*\\r\\n nahradit za pr\u00e1zdno","title":"Regex"},{"location":"Programming/Regex/#symbol-meaning","text":". any character [xyz] one of these characters [c-j] any character between c and j . We can combine this and the previou syntax, e.g.: [az0-4jsd] . Note that the minus sign is interpreted as a range only if it is between two characters. [^c-g] ^ means negation: anything except the following set of characters. Note that the negation( ^ ) sign needs to be the first character in the bracket. \\ : escape character. | means OR. It has the lowest precedence, so it is evaluated last. ? lazy quantifier. It will try to match as few characters as possible (i.e., previous pattern will try to match only till the next patern matches). ?R recursive pattern.","title":"Symbol meaning"},{"location":"Programming/Regex/#quantifiers","text":"* : zero or more + : one or more ? zero or one {<from>, <to>} between from and to times. If to is omitted, it means infinity. If from is omitted, it means zero. If there is only one number, it means exact count. If both are omitted, it means one.","title":"Quantifiers"},{"location":"Programming/Regex/#anchors","text":"^x must start with x x$ must end with x","title":"Anchors"},{"location":"Programming/Regex/#groups-and-lookarounds","text":"() capture group. We can refer to it later, either in the regex, or in the result of the match, depending on the programming language. The nubering starts from 1, the 0 group is usually the whole match. in the regex we refer to group using \\1 , \\2 , etc. (?:) non-caputing group. It is useful when we want to use the quantifiers on a group, but we don't want to capture it. (?=) positive lookahead. It will try to match the pattern, but it will not consume it. (?!) negative lookahead. It is useful when we want to match a pattern, but we don't want to consume it. (?<=) positive lookbehind. Same as positive lookahead, but it looks behind. (?<!) negative lookbehind. Same as negative lookahead, but it looks behind.","title":"Groups and Lookarounds"},{"location":"Programming/Regex/#principles","text":"","title":"Principles"},{"location":"Programming/Regex/#non-capturing-groups","text":"Non-capturing groups are groups that helps to specify the match but they are not captured. They are useful when we want to use the group content to specify the match, but we don't want to capture/consume the group. Some of them can be replaced, but usually with a more complicated regex. All of the non-capturing groups start with (? and end with ) . The ? is followed by a character that specifies the type of the group. The most common are: ?: non-capturing group ?= positive lookahead ?! negative lookahead ?<= positive lookbehind ?<! negative lookbehind The actual content of the group is specified between the group type specifier (e.g., ?= ) and the closing bracket ( ) ). Example: (?=d)a This regex will match a only if it is followed by d . The d will not be consumed. Note that some regex engines don't support variable length lookbehind . To overcome this, we can use the following tricks: use multiple lookbehinds with fixed length construct a more complicated regex that will match the same thing place a marker with one regex replace and then use the lookbehind to match the marker","title":"Non-capturing groups"},{"location":"Programming/Regex/#examples","text":"","title":"Examples"},{"location":"Programming/Regex/#any-whitespace","text":"/[\\x{00a0}\\s]/u","title":"Any whitespace"},{"location":"Programming/Regex/#non-breaking-space","text":"((?!&nbsp;)[^\\s\\x{00a0}])","title":"Non-breaking space"},{"location":"Programming/Regex/#transform-google-sheet-to-latex-table","text":"naj\u00edt ([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n nahradit \\1 & \\2 & \\3 & \\4 \\\\\\\\\\r\\n","title":"Transform Google sheet to latex table"},{"location":"Programming/Regex/#csv-to-latex","text":"Search: ([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,\\r\\n]*)\\n Replace \\1 & \\2 & \\3 & \\4 & \\5 & \\6 & \\7 \\\\\\\\\\r\\n","title":"CSV to Latex"},{"location":"Programming/Regex/#name-regex","text":"([A\u00c1BC\u010cDEFGHIJKLMNOPQR\u0158S\u0160TU\u00daVWXYZ\u017d]{1}[a\u00e1bc\u010dd\u010fe\u00e9\u011bfghchi\u00edjklmn\u0148o\u00f3pqr\u0159s\u0161t\u0165u\u00fa\u016fvwxy\u00fdz\u017ew]+ *){2,3}","title":"Name regex"},{"location":"Programming/Regex/#archive","text":"","title":"Archive"},{"location":"Programming/Regex/#vasdomovnik","text":"naj\u00edt [0-9]+[ ]*([^\\r\\n]*)[\\r\\n]+ nahradit '\\1' ,","title":"vasdomovnik"},{"location":"Programming/Regex/#sloupce-na-pole-php","text":"naj\u00edt ([^\\r\\n]*)([\\r\\n])+ nahradit '\\1' => ''\\2,","title":"sloupce na pole php"},{"location":"Programming/Regex/#pogamut-jen-logy-od-jednoho-bota","text":"\\(TeamCTF[^2][^\\r\\n]*\\r\\n nahradit za pr\u00e1zdno","title":"Pogamut - jen logy od jednoho bota"},{"location":"Programming/Ruby%20Workflow/","text":"Classical workflow is to use: official Ruby distribution Bundler to manage dependencies Installation \u00b6 Windows \u00b6 Download and run Ruby installer use the versin with DevKit at the end, a command prompt will open, confitm the prompt with Enter Project setup \u00b6 The project configuration is stored in the Gemfile file. It contains a list of dependencies, which are installed using the bundle command. Typically, the file contains the following lines: source \"https://rubygems.org\" # the source of the gems gem \"jekyll\" # the gem to install Gems \u00b6 Gems are packages for Ruby. They can be installed using the gem command, but moslty, they are installed as dependencies using the bundle command. The gem specification in the Gemfile contains the following parameters (split by spaces): gem : the name of the gem (required) version : the version of the gem group : the group of the gem. It can be used run a command only for a specific group. For example, bundle install --without development will not install gems from the development group. The group parameter is a new syntax, the old syntax is to use the group command: group :development do gem \"jekyll\" end","title":"Ruby Workflow"},{"location":"Programming/Ruby%20Workflow/#installation","text":"","title":"Installation"},{"location":"Programming/Ruby%20Workflow/#windows","text":"Download and run Ruby installer use the versin with DevKit at the end, a command prompt will open, confitm the prompt with Enter","title":"Windows"},{"location":"Programming/Ruby%20Workflow/#project-setup","text":"The project configuration is stored in the Gemfile file. It contains a list of dependencies, which are installed using the bundle command. Typically, the file contains the following lines: source \"https://rubygems.org\" # the source of the gems gem \"jekyll\" # the gem to install","title":"Project setup"},{"location":"Programming/Ruby%20Workflow/#gems","text":"Gems are packages for Ruby. They can be installed using the gem command, but moslty, they are installed as dependencies using the bundle command. The gem specification in the Gemfile contains the following parameters (split by spaces): gem : the name of the gem (required) version : the version of the gem group : the group of the gem. It can be used run a command only for a specific group. For example, bundle install --without development will not install gems from the development group. The group parameter is a new syntax, the old syntax is to use the group command: group :development do gem \"jekyll\" end","title":"Gems"},{"location":"Programming/SQL%20Manual/","text":"Literals \u00b6 In SQL, there are three basic types of literals: numeric : 1 , 1.2 , 1.2e3 string : 'string' , \"string\" boolean : true , false When we need a constant value for other types, we usually use either a constructor function or a specifically formatted string literal. String literals can also span multiple lines , we do not need any operator to split the line (unlike in Python and despite the JetBrains IDEs adds a string concatenation operator automatically on newline). WITH \u00b6 Statement for defining variables pointing to temporary data, that can be used in the related SELECT statement. Usage: WITH <var> AS (<sql that assigns data to var>) <sql that use the variable> Note that the variable has to appear in the FROM clause ! Multiple variables in the WITH statement ahould be delimited by a comma: WITH <var> AS (<sql that assigns data to var>), <var 2> AS (<sql that assigns data to var 2>) <sql that use the variables> SELECT \u00b6 Most common SQL statement, syntax: SELECT <EXPRESSION> [AS <ALIAS>][, <EXPRESSION 2> [AS <ALIAS 2> ...]] The most common expression is just a column name. Selecting row number \u00b6 We can select row numbers using the function ROW_SELECT() in the SELECT statement: SELECT ... ROW_NUMBER() OVER([<PARTITIONING AND NUMBERING ORDER>]) AS <RESULT COLUMN NAME>, ... Inside the OVER statement, we can specify the order of the row numbering. Note however, that this does not order the result, for that, we use the ORDER BY statement. If we want the rown numbering to correspond with the row order in the result, we can left the OVER statement empty. Select unique rows \u00b6 To select unique rows, we can use the DISTINCT keyword: SELECT DISTINCT <column name> FROM ... Count selecting rows \u00b6 The count() function can be used to count the selection. The standard syntax is: SELECT count(1) FROM ... Count distinct \u00b6 To count distinct values in a selection we can use: SELECT count(DISTINCT <column name>) FROM... Select from another column if the specified column is NULL \u00b6 We can use a replacement column using the coalesce function: SELECT coalesce (<primary column>, <secondary column>) The secondary column value will be used if the primary column value in the row is NULL . UNION \u00b6 UNION and UNION ALL are important keywords that enables to merge query results verticaly, i.e., appending rows of one query to the results set of another. The difference between them is that UNION discards duplicate rows, while UNION ALL keeps them The UNUION statement appends one SELECT statement to another, but some statements that appears to be part of the SELECT needs to stay outside (i.e., be specified just once for the whole union), namely ORDER BY , and LIMIT . In contrast, the GROUP BY and HAVING statement stays inside each individual select. JOIN \u00b6 Classical syntax: JOIN <table name> [[AS] <ALIAS>] ON <CONDITION> The alias is obligatory if there are duplicate names (e.g., we are joining one table twice) Types \u00b6 INNER (default): when there is nothing to join, the row is discarded [LEFT/RIGHT/FULL] OUTER : when there is nothing to join, the missing values are set to null CROSS : creates cartesian product between tables OUTER JOIN \u00b6 The OUTER JOIN has three subtypes LEFT : joins right table to left, fills the missing rows on right with null RIGHT : joins left to right, fills the missing rows on left with null FULL : both LEFT and RIGHT JOIN is performend Properties \u00b6 When there are multiple matching rows, all of them are matched (i.e., it creates duplicit rows) If you want to filter the tables before join, you need to specify the condition inside ON caluse Join Only One Row \u00b6 Joining a specific row \u00b6 Sometimes, we want to join only one row from many fulfilling some condition. One way to do that is to use a subquery: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY column_in_b DESC LIMIT 1 ) This query joins table b to table a , using the row with the highest column_in_b . Note however, that all rows from a will be joined to the same row from b . To use a different row from b depending on a , we need to look outside the subquery to filter out b according to a . The folowing query, which should do exactly that, is invalid : SELECT * FROM a JOIN ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 ) The problem here is that the subquery cannot refer the tables outside in, in the preceeeding FROM clause. Luckily, in the most use db systems, there is a magical keyword LATERAL that enables exacly that: SELECT * FROM a JOIN LATERAL ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 ) Join random row \u00b6 To join a random row, we can use the RANDOM function in ordering, e.g.: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY RANDOM() LIMIT 1 ) However, this subquery is evaluated just once, hence we have the same problem as with the first example in this section: to every row in a , we are joining the same random row from b . To force the db system to execute the subquery for each row in a , we need to refer a in the subquery, even with some useless filter (and of course we need a LATERAL join for that): SELECT * FROM a JOIN LATERAL( SELECT * FROM b WHERE a.column_in_a IS NOT NULL ORDER BY RANDOM() LIMIT 1 ) Joining any row find active nodes \u00b6 Sometimes, we need to join any matching row from B to find a set of active (referenced) rows in A. For example, we need to find a set of custommers with pending orders. IF the average number of matches in B is low, we proceed with normal join and then aggregate the results or use DISTINCT . However, sometimes, the average number of matching rows in B can be very high (e.g., finding all countries with at least one MC Donald). For those cases, the LATERAL join is again the solution: SELECT * FROM zones JOIN LATERAL ( SELECT id AS request_id FROM demand WHERE zones.id = demand.destination LIMIT 1 ) demand ON TRUE For more information about the trade offs of this solution, check the SO answer Getting All Combinations of Rows \u00b6 This can be done using the CROSS JOIN , e.g.: SELECT * FROM table_a CROSS JOIN table_b The proble arises when you want to filter one of the tables before joining, because CROSS JOIN does not support the ON clause (see more in the Oracle docs ). Then you can use the equivalent INNER JOIN : SELECT * FROM table_a INNER JOIN table_b ON true Here you replace the true value with your intended condition. Inverse JOIN \u00b6 Sometimes, it is useful to find all rows in table A that has no match in table B. The usual approach is to use LEFT JOIN and filter out non null rows after the join: SELECT ... FROM tableA LEFT JOIN tableB WHERE tableB.id is NULL Joining a table on multiple options \u00b6 There are situations, where we want to join a single table on multiple possible matches. For example, we want to match all people having birthday or name day the same day as some promotion is happanning. The foollowing query is a straighrorward solution: SELECT * FROM promo JOIN people ON promo.date = people.birthaday OR promo.date = people.name_day However, as we explain in the performance chapter, using OR in SQL is almost never a good solution. The usual way of gatting rid of OR is to use IN : SELECT * FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day) Nevertheless, this query can still lead to problems, despite being more compact. When we use any complex condition while joining tables, we risk poor performance. In general, it is better to use just simple column-to-column matches, even when there are more joins as a result. If you have performance problems, consult the \"Replacing OR \" section in the \"Performance Optimization\" chapter. GROUP BY \u00b6 wiki GROUP BY is used to aggregate data. Usual syntax: GROUP BY <LIST OF COLUMNS> Note that we need to use some aggreagte function for all columns in the SELECT clause that are not present in the GROUP BY list. On the other hand, we can use columns not present in the SELECT in the GROUP BY statement. Using aggregate functions for the whole result set while using GROUP BY \u00b6 If a query contains a GROUP BY statement, all aggregate functions (e.g., count , avg ) are applied to groups, i.e., for each row of the result set. If we need to apply an aggregate function to the whole result set while using GROUP BY , we need to specify it using the OVER statement: SELECT count(1) OVER () as result_count ... This will add a coulumn with a total count to each row of the result. If we do not need the actual groups, but only the distinct count, we can use a LIMIT statement. Get any row from each group \u00b6 Sometimes, we need a value from a non-grouped column, but we do not care which one. The reson can be, for example, that we no that the values are the same for all rows in the group. There is no dedicated aggregation for this case, but we can use some simple ones as MIN or MAX . Window functions \u00b6 Sometimes, we would need an aggregate function that somehow use two different columns (e.g., value of col A for the row where col B is largest). For that, we cannot use the classical aggregation, but we rather have to use a window function . A window function is a function that for each row returns a value computed from one or multiple rows. Syntactically, we recognize a window function by the OVER clause that determines the rows used as an input for the function. Functions with the same name can exist as aggregate and window functions. Window functions are evaluated after the GROUP BY clause and aggregate functions. Specifiing the range \u00b6 () : the whole result set (PARTITION BY <column set>) : the rows with the same values for that set of columns We can also order the result for the selected range using ORDER BY inside the parantheses. In some SQL dialects (e.g., PostgreSQL), there are even more sophisticated ways how to specify the range for the window functions. ORDER BY \u00b6 By default, there is no guarantee that the result will be in any particular order. To sort the result, we need to add an ORDER BY statement at the end of the query: ORDER BY <LIST OF COLUMNS> INSERT \u00b6 Standard syntax for the INSERT statement is INSERT INTO <table> (<col_1>, <col_2>,...) VALUES (<val_1>, <val_2>,...) If we fill all columns and we are confident with the column ordering, we can omit columns: INSERT INTO <table> VALUES (<val_1>, <val_2>,...) Sometimes, we need to handle the eroro cases, e.g., the case when the record already exists. The solutions for these cases are, however, database specific. INSERT SELECT \u00b6 If we want to duplicate the records, we can use: INSERT INTO <table> SELECT * FROM <table> [WHERE <condition>] If we need to fill some columns ourselfs: INSERT INTO <table> (<col_1>, <col_2>,...) SELECT <col_1>, <any expression> FROM <anything suported by select> [WHERE <condition>] UPDATE \u00b6 UPDATE query has the following structure: UPDATE table_name SET column1 = value1, column2 = value2...., columnN = valueN WHERE <condition> Unfortunately, the statement is ready to update N records with one set of values, but not to update N records with N set of values. To do that, we have only an option to select from another table: UPDATE table_name SET column1 = other_table.column1 FROM other_table WHERE other_table.id = table_name.id Don't forget the WHERE clause here, otherwise, you are matching the whole result set returned by the FROM clause to each row of the table. Use the table for updating itself \u00b6 If we are abou to update the table using date stored in int, we need to use aliases: UPDATE nodes_ways new SET way_id = ways.osm_id FROM nodes_ways old JOIN ways ON old.way_id = ways.id AND old.area = ways.area WHERE new.way_id = old.way_id AND new.area = old.area AND new.position = old.position DELETE \u00b6 To delete records from a table, we use the DELETE statement: DELETE FROM <table_name> WHERE <condition> If some data from another table are required for the selection of the records to be deleted, the syntax varies depending on the database engine. EXPLAIN \u00b6 Sources official documentation official cosumentation: usage https://docs.gitlab.com/ee/development/understanding_explain_plans.html Remarks: \u00b6 to show the actual run times, we need to run EXPLAIN ANALYZE Nodes \u00b6 Sources Plan nodes source code PG documentation with nodes described Node example: -> Parallel Seq Scan on records_mon (cost=0.00..4053077.22 rows=2074111 width=6) (actual time=398.243..74465.389 rows=7221902 loops=2) Filter: ((trip_length_0_1_mi = '0'::double precision) AND (trip_length_1_2_mi = '0'::double precision) AND (trip_length_2_3_mi = '0'::double precision) AND (trip_length_3_4_mi = '0'::double precision) AND (trip_length_4_5_mi = '0'::double precision)) Rows Removed by Filter: 8639817 Buffers: shared hit=157989 read=3805864 Description: In first parantheses, there are expected values: cost the estimated cost in arbitrary units. The format is startup cost..total cost , where startup cost is a flat cost of the node, an init cost, while total cost is the estimated cost of the node. Averege per loop. rows : expected number of rows produced by this node. Averege per loop. width the width of each row in bytes In the second parantheses, there are measured results: actual time : The measured execution time in miliseconds. The format is startup time..total time . rows The real number of rows returned. The Rows Removed by the Filter indicates the number of rows that were filtered out. The Buffers statistic shows the number of buffers used. Each buffer consists of 8 KB of data. Keys \u00b6 Keys serves as a way of identifying table rows, they are unique . There are many type of keys, see databastar article for the terminology overview. Primary key \u00b6 Most important keys in ORM are primary keys. Each table should have a single primary key. A primary key has to be non null. When choosing primary key, we can either use a uniqu combination of database colums: a natural key use and extra column: surogate key If we use a natural key and it is composed from multiple columns, we call it a composite key The following table summarize the adventages and disadvantages of each of the solutions: Area | Property | Natural key | Composite key | Surrogate key | |-|-|-|-|-| | usage | SQL joins | easy | hard | easy | || changing natural key columns | hard | hard | easy | | Performance | extra space | none | A lot if there are reference tables, otherwise none | one extra column | || space for indexes | normal | extra | normal || extra insertion time | no | A lot if there are reference tables, otherwise none | yes | || join performance | suboptimal due to sparse values | even more sub-optimal due to sparse values | optimal From the table above, we can see that using natural keys should be considered only if rows can be identified by a single column and we have a strong confidence in that natural id, specifically in its uniquness and timelessnes. Non-primary (UNIQUE) keys \u00b6 Sometimes, we need to enforse a uniqueness of a set of columns that does not compose a primary key (e.g., we use a surogate key). We can use a non primary key for that. One of the differences between primary and non-primary keys is that non-primary keys can be null, and each of the null values is considered unique. Indices \u00b6 Indices are essential for speeding queries containing conditions (including conditional joins). The basic syntax for creating an index is: CREATE INDEX <index name> ON <table name>(<column name>); Show Table Indices \u00b6 MySQL: SHOW INDEX FROM <tablename>; PostgreSQL SELECT * FROM pg_indexes WHERE tablename = '<tablename >'; Show Indices From All Tables \u00b6 MySQL: SELECT DISTINCT TABLE_NAME, INDEX_NAME FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = '<schemaname>'; CREATE TABLE \u00b6 The syntax is: CREATE TABLE <TABLE NAME> ( <COLUNM 1 NAME> <COLUNM 1 TYPE>, ... <COLUNM N NAME> <COLUNM N TYPE> [, PRIMARY KEY (<LIST OF KEY COLUMNS>)] ) The primary key is optional, but usually, any table should have one. Each row has to have a unique primary key. An index is created automatically for the list of primary key columns. ALTER TABLE \u00b6 Add column generated from other columns \u00b6 SQL: ALTER TABLE <table name> ADD <column name> AS (<generator expression>); In MySQL we have to add the data type: ALTER TABLE <tablename> ADD <columnname> <datatype> AS (<generator expression>); In PostgreSQL, the syntax is quite different: ALTER TABLE <tablename> ADD <columnname> <datatype> GENERATED ALWAYS AS (<generator expression>) STORED The advantage of the Postgres approach is that the column is set as generated, therefore it is generated for the new rows automatically. Procedures and functions \u00b6 In SQL there are two constructs that are able to encapsulate SQL statements to increase resusability and readability: functions and procedues. The main difference between these two is that functions are intended to be called inline from SLQ statements, while procedures cannot be used in SQL statements and instead, they are used as a wrapper of a set of SQL statement to be called repeatedly with different arguments. The extensive summary of the different capabilities of functions and procedures is on SO . Calling a procedure \u00b6 The keyword for calling a procedure differs between database system, refer to the documentation for your system for the right keyword. Creating a procedure \u00b6 The syntax for calling a procedure differs between database system, refer to the documentation for your system for the right syntax. Parameters \u00b6 Procedures and functions can have parameters similar to parameters in programming languages. We can use those parameters in the body of a function/procedure equally as a column or constant. Parameters can have default values , suplied after the = sign. We can test whether a default argument was supplied by testing the parameter for the default value. Views \u00b6 Views are basically named SQL queries stored in database. The queries are run on each view invocation, unles the view is materialized. The syntax is: CREATE VIEW <VIEW NAME> AS <QUERY> Modifying the view \u00b6 The view can be modified with CREATE OR REPLACE VIEW , however, existing columns cannot be changed . If you need to change existing columns, drop the view first. Schemas \u00b6 Schema in SQL is a container or namespace for tables, views, and other database objects. This means that we can have multiple objects with the same name in different schemas. The SQL schema should not be confused with the database schema, which is a logical structure of the database. Sadly, the concept of SQL schema is not standardized across different database systems. The following table shows how SQL schema is implemented in different systems: Database System Schema concept default schema MySQL Databases are used as schemas - PostgreSQL Multiple schemas in a single database as per the SQL standard public Oracle Each user has an associated schema (user = schema) user name SQL Server Multiple schemas in a single database as per the SQL standard dbo Performace Optimization \u00b6 When the query is slow, first inspect the following checklist: Do not use OR or IN for a set of columns (see replacing OR below). Check that all column and combination of columns used in conditions ( WHERE ) are indexed. Check that all foreign keys are indexed. Check that all joins are simple joins (column to column, or set of columns to a matching set of columns). If nothing from the above works, try to start with a simple query and add more complex pars to find where the problem is. If decomposing the query also does not bing light into the problem, refer to either one of the subsections below, or to the external sources. Also, note that some IDEs limits the number of returned rows automatically, which can hide serious problems and confuse you. Try to remove the limit when testing the performance this way. Replacing OR \u00b6 We can slow down the query significantly using OR or IN statements if the set of available options is not constant (e.g., IN(1, 2) is okish, while IN(origin, destination) can have drastic performance impact). To get rid of these disjunctioncs, we can use the UNION statement, basically duplicating the query. The resulting query will be double in size, but much faster: SELECT people.id FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day); -- can be revritten as SELECT * FROM promo JOIN people ON promo.date = people.birthaday UNION SELECT * FROM promo JOIN people ON promo.date = people.name_day GROUP BY people.id A specific join makes the query slow \u00b6 If a single join makes the query slow, there is a great chance that the index is not used for the join. Even if the table has an index on the referenced column(s), the join can still not use it if we are joining not to the table itself but to: a subquery, a variable created with a WITH statement, a view, or temborary table created from the indexed table. You can solve the situation by creating a materialized view or temporary table instead, and adding inedices to the table manualy. Specifically, you need to split the query into multiple queries: delete the materialized view/table if exists create the materialized view/table create the required indices perform the actual query that utilizes th view/table Of course we can skip the first three steps if the materialized view is constant for all queries, which is common during the testing phase. Slow DELETE \u00b6 When the delete is slow, the cause can be the missing index on the child table that refers to the table we are deleting from. Other possible causes are listed in the SO answer . Sources \u00b6 Database development mistakes made by application developers","title":"SQL Manual"},{"location":"Programming/SQL%20Manual/#literals","text":"In SQL, there are three basic types of literals: numeric : 1 , 1.2 , 1.2e3 string : 'string' , \"string\" boolean : true , false When we need a constant value for other types, we usually use either a constructor function or a specifically formatted string literal. String literals can also span multiple lines , we do not need any operator to split the line (unlike in Python and despite the JetBrains IDEs adds a string concatenation operator automatically on newline).","title":"Literals"},{"location":"Programming/SQL%20Manual/#with","text":"Statement for defining variables pointing to temporary data, that can be used in the related SELECT statement. Usage: WITH <var> AS (<sql that assigns data to var>) <sql that use the variable> Note that the variable has to appear in the FROM clause ! Multiple variables in the WITH statement ahould be delimited by a comma: WITH <var> AS (<sql that assigns data to var>), <var 2> AS (<sql that assigns data to var 2>) <sql that use the variables>","title":"WITH"},{"location":"Programming/SQL%20Manual/#select","text":"Most common SQL statement, syntax: SELECT <EXPRESSION> [AS <ALIAS>][, <EXPRESSION 2> [AS <ALIAS 2> ...]] The most common expression is just a column name.","title":"SELECT"},{"location":"Programming/SQL%20Manual/#selecting-row-number","text":"We can select row numbers using the function ROW_SELECT() in the SELECT statement: SELECT ... ROW_NUMBER() OVER([<PARTITIONING AND NUMBERING ORDER>]) AS <RESULT COLUMN NAME>, ... Inside the OVER statement, we can specify the order of the row numbering. Note however, that this does not order the result, for that, we use the ORDER BY statement. If we want the rown numbering to correspond with the row order in the result, we can left the OVER statement empty.","title":"Selecting row number"},{"location":"Programming/SQL%20Manual/#select-unique-rows","text":"To select unique rows, we can use the DISTINCT keyword: SELECT DISTINCT <column name> FROM ...","title":"Select unique rows"},{"location":"Programming/SQL%20Manual/#count-selecting-rows","text":"The count() function can be used to count the selection. The standard syntax is: SELECT count(1) FROM ...","title":"Count selecting rows"},{"location":"Programming/SQL%20Manual/#count-distinct","text":"To count distinct values in a selection we can use: SELECT count(DISTINCT <column name>) FROM...","title":"Count distinct"},{"location":"Programming/SQL%20Manual/#select-from-another-column-if-the-specified-column-is-null","text":"We can use a replacement column using the coalesce function: SELECT coalesce (<primary column>, <secondary column>) The secondary column value will be used if the primary column value in the row is NULL .","title":"Select from another column if the specified column is NULL"},{"location":"Programming/SQL%20Manual/#union","text":"UNION and UNION ALL are important keywords that enables to merge query results verticaly, i.e., appending rows of one query to the results set of another. The difference between them is that UNION discards duplicate rows, while UNION ALL keeps them The UNUION statement appends one SELECT statement to another, but some statements that appears to be part of the SELECT needs to stay outside (i.e., be specified just once for the whole union), namely ORDER BY , and LIMIT . In contrast, the GROUP BY and HAVING statement stays inside each individual select.","title":"UNION"},{"location":"Programming/SQL%20Manual/#join","text":"Classical syntax: JOIN <table name> [[AS] <ALIAS>] ON <CONDITION> The alias is obligatory if there are duplicate names (e.g., we are joining one table twice)","title":"JOIN"},{"location":"Programming/SQL%20Manual/#types","text":"INNER (default): when there is nothing to join, the row is discarded [LEFT/RIGHT/FULL] OUTER : when there is nothing to join, the missing values are set to null CROSS : creates cartesian product between tables","title":"Types"},{"location":"Programming/SQL%20Manual/#outer-join","text":"The OUTER JOIN has three subtypes LEFT : joins right table to left, fills the missing rows on right with null RIGHT : joins left to right, fills the missing rows on left with null FULL : both LEFT and RIGHT JOIN is performend","title":"OUTER JOIN"},{"location":"Programming/SQL%20Manual/#properties","text":"When there are multiple matching rows, all of them are matched (i.e., it creates duplicit rows) If you want to filter the tables before join, you need to specify the condition inside ON caluse","title":"Properties"},{"location":"Programming/SQL%20Manual/#join-only-one-row","text":"","title":"Join Only One Row"},{"location":"Programming/SQL%20Manual/#joining-a-specific-row","text":"Sometimes, we want to join only one row from many fulfilling some condition. One way to do that is to use a subquery: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY column_in_b DESC LIMIT 1 ) This query joins table b to table a , using the row with the highest column_in_b . Note however, that all rows from a will be joined to the same row from b . To use a different row from b depending on a , we need to look outside the subquery to filter out b according to a . The folowing query, which should do exactly that, is invalid : SELECT * FROM a JOIN ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 ) The problem here is that the subquery cannot refer the tables outside in, in the preceeeding FROM clause. Luckily, in the most use db systems, there is a magical keyword LATERAL that enables exacly that: SELECT * FROM a JOIN LATERAL ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 )","title":"Joining a specific row"},{"location":"Programming/SQL%20Manual/#join-random-row","text":"To join a random row, we can use the RANDOM function in ordering, e.g.: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY RANDOM() LIMIT 1 ) However, this subquery is evaluated just once, hence we have the same problem as with the first example in this section: to every row in a , we are joining the same random row from b . To force the db system to execute the subquery for each row in a , we need to refer a in the subquery, even with some useless filter (and of course we need a LATERAL join for that): SELECT * FROM a JOIN LATERAL( SELECT * FROM b WHERE a.column_in_a IS NOT NULL ORDER BY RANDOM() LIMIT 1 )","title":"Join random row"},{"location":"Programming/SQL%20Manual/#joining-any-row-find-active-nodes","text":"Sometimes, we need to join any matching row from B to find a set of active (referenced) rows in A. For example, we need to find a set of custommers with pending orders. IF the average number of matches in B is low, we proceed with normal join and then aggregate the results or use DISTINCT . However, sometimes, the average number of matching rows in B can be very high (e.g., finding all countries with at least one MC Donald). For those cases, the LATERAL join is again the solution: SELECT * FROM zones JOIN LATERAL ( SELECT id AS request_id FROM demand WHERE zones.id = demand.destination LIMIT 1 ) demand ON TRUE For more information about the trade offs of this solution, check the SO answer","title":"Joining any row find active nodes"},{"location":"Programming/SQL%20Manual/#getting-all-combinations-of-rows","text":"This can be done using the CROSS JOIN , e.g.: SELECT * FROM table_a CROSS JOIN table_b The proble arises when you want to filter one of the tables before joining, because CROSS JOIN does not support the ON clause (see more in the Oracle docs ). Then you can use the equivalent INNER JOIN : SELECT * FROM table_a INNER JOIN table_b ON true Here you replace the true value with your intended condition.","title":"Getting All Combinations of Rows"},{"location":"Programming/SQL%20Manual/#inverse-join","text":"Sometimes, it is useful to find all rows in table A that has no match in table B. The usual approach is to use LEFT JOIN and filter out non null rows after the join: SELECT ... FROM tableA LEFT JOIN tableB WHERE tableB.id is NULL","title":"Inverse JOIN"},{"location":"Programming/SQL%20Manual/#joining-a-table-on-multiple-options","text":"There are situations, where we want to join a single table on multiple possible matches. For example, we want to match all people having birthday or name day the same day as some promotion is happanning. The foollowing query is a straighrorward solution: SELECT * FROM promo JOIN people ON promo.date = people.birthaday OR promo.date = people.name_day However, as we explain in the performance chapter, using OR in SQL is almost never a good solution. The usual way of gatting rid of OR is to use IN : SELECT * FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day) Nevertheless, this query can still lead to problems, despite being more compact. When we use any complex condition while joining tables, we risk poor performance. In general, it is better to use just simple column-to-column matches, even when there are more joins as a result. If you have performance problems, consult the \"Replacing OR \" section in the \"Performance Optimization\" chapter.","title":"Joining a table on multiple options"},{"location":"Programming/SQL%20Manual/#group-by","text":"wiki GROUP BY is used to aggregate data. Usual syntax: GROUP BY <LIST OF COLUMNS> Note that we need to use some aggreagte function for all columns in the SELECT clause that are not present in the GROUP BY list. On the other hand, we can use columns not present in the SELECT in the GROUP BY statement.","title":"GROUP BY"},{"location":"Programming/SQL%20Manual/#using-aggregate-functions-for-the-whole-result-set-while-using-group-by","text":"If a query contains a GROUP BY statement, all aggregate functions (e.g., count , avg ) are applied to groups, i.e., for each row of the result set. If we need to apply an aggregate function to the whole result set while using GROUP BY , we need to specify it using the OVER statement: SELECT count(1) OVER () as result_count ... This will add a coulumn with a total count to each row of the result. If we do not need the actual groups, but only the distinct count, we can use a LIMIT statement.","title":"Using aggregate functions for the whole result set while using GROUP BY"},{"location":"Programming/SQL%20Manual/#get-any-row-from-each-group","text":"Sometimes, we need a value from a non-grouped column, but we do not care which one. The reson can be, for example, that we no that the values are the same for all rows in the group. There is no dedicated aggregation for this case, but we can use some simple ones as MIN or MAX .","title":"Get any row from each group"},{"location":"Programming/SQL%20Manual/#window-functions","text":"Sometimes, we would need an aggregate function that somehow use two different columns (e.g., value of col A for the row where col B is largest). For that, we cannot use the classical aggregation, but we rather have to use a window function . A window function is a function that for each row returns a value computed from one or multiple rows. Syntactically, we recognize a window function by the OVER clause that determines the rows used as an input for the function. Functions with the same name can exist as aggregate and window functions. Window functions are evaluated after the GROUP BY clause and aggregate functions.","title":"Window functions"},{"location":"Programming/SQL%20Manual/#specifiing-the-range","text":"() : the whole result set (PARTITION BY <column set>) : the rows with the same values for that set of columns We can also order the result for the selected range using ORDER BY inside the parantheses. In some SQL dialects (e.g., PostgreSQL), there are even more sophisticated ways how to specify the range for the window functions.","title":"Specifiing the range"},{"location":"Programming/SQL%20Manual/#order-by","text":"By default, there is no guarantee that the result will be in any particular order. To sort the result, we need to add an ORDER BY statement at the end of the query: ORDER BY <LIST OF COLUMNS>","title":"ORDER BY"},{"location":"Programming/SQL%20Manual/#insert","text":"Standard syntax for the INSERT statement is INSERT INTO <table> (<col_1>, <col_2>,...) VALUES (<val_1>, <val_2>,...) If we fill all columns and we are confident with the column ordering, we can omit columns: INSERT INTO <table> VALUES (<val_1>, <val_2>,...) Sometimes, we need to handle the eroro cases, e.g., the case when the record already exists. The solutions for these cases are, however, database specific.","title":"INSERT"},{"location":"Programming/SQL%20Manual/#insert-select","text":"If we want to duplicate the records, we can use: INSERT INTO <table> SELECT * FROM <table> [WHERE <condition>] If we need to fill some columns ourselfs: INSERT INTO <table> (<col_1>, <col_2>,...) SELECT <col_1>, <any expression> FROM <anything suported by select> [WHERE <condition>]","title":"INSERT SELECT"},{"location":"Programming/SQL%20Manual/#update","text":"UPDATE query has the following structure: UPDATE table_name SET column1 = value1, column2 = value2...., columnN = valueN WHERE <condition> Unfortunately, the statement is ready to update N records with one set of values, but not to update N records with N set of values. To do that, we have only an option to select from another table: UPDATE table_name SET column1 = other_table.column1 FROM other_table WHERE other_table.id = table_name.id Don't forget the WHERE clause here, otherwise, you are matching the whole result set returned by the FROM clause to each row of the table.","title":"UPDATE"},{"location":"Programming/SQL%20Manual/#use-the-table-for-updating-itself","text":"If we are abou to update the table using date stored in int, we need to use aliases: UPDATE nodes_ways new SET way_id = ways.osm_id FROM nodes_ways old JOIN ways ON old.way_id = ways.id AND old.area = ways.area WHERE new.way_id = old.way_id AND new.area = old.area AND new.position = old.position","title":"Use the table for updating itself"},{"location":"Programming/SQL%20Manual/#delete","text":"To delete records from a table, we use the DELETE statement: DELETE FROM <table_name> WHERE <condition> If some data from another table are required for the selection of the records to be deleted, the syntax varies depending on the database engine.","title":"DELETE"},{"location":"Programming/SQL%20Manual/#explain","text":"Sources official documentation official cosumentation: usage https://docs.gitlab.com/ee/development/understanding_explain_plans.html","title":"EXPLAIN"},{"location":"Programming/SQL%20Manual/#remarks","text":"to show the actual run times, we need to run EXPLAIN ANALYZE","title":"Remarks:"},{"location":"Programming/SQL%20Manual/#nodes","text":"Sources Plan nodes source code PG documentation with nodes described Node example: -> Parallel Seq Scan on records_mon (cost=0.00..4053077.22 rows=2074111 width=6) (actual time=398.243..74465.389 rows=7221902 loops=2) Filter: ((trip_length_0_1_mi = '0'::double precision) AND (trip_length_1_2_mi = '0'::double precision) AND (trip_length_2_3_mi = '0'::double precision) AND (trip_length_3_4_mi = '0'::double precision) AND (trip_length_4_5_mi = '0'::double precision)) Rows Removed by Filter: 8639817 Buffers: shared hit=157989 read=3805864 Description: In first parantheses, there are expected values: cost the estimated cost in arbitrary units. The format is startup cost..total cost , where startup cost is a flat cost of the node, an init cost, while total cost is the estimated cost of the node. Averege per loop. rows : expected number of rows produced by this node. Averege per loop. width the width of each row in bytes In the second parantheses, there are measured results: actual time : The measured execution time in miliseconds. The format is startup time..total time . rows The real number of rows returned. The Rows Removed by the Filter indicates the number of rows that were filtered out. The Buffers statistic shows the number of buffers used. Each buffer consists of 8 KB of data.","title":"Nodes"},{"location":"Programming/SQL%20Manual/#keys","text":"Keys serves as a way of identifying table rows, they are unique . There are many type of keys, see databastar article for the terminology overview.","title":"Keys"},{"location":"Programming/SQL%20Manual/#primary-key","text":"Most important keys in ORM are primary keys. Each table should have a single primary key. A primary key has to be non null. When choosing primary key, we can either use a uniqu combination of database colums: a natural key use and extra column: surogate key If we use a natural key and it is composed from multiple columns, we call it a composite key The following table summarize the adventages and disadvantages of each of the solutions: Area | Property | Natural key | Composite key | Surrogate key | |-|-|-|-|-| | usage | SQL joins | easy | hard | easy | || changing natural key columns | hard | hard | easy | | Performance | extra space | none | A lot if there are reference tables, otherwise none | one extra column | || space for indexes | normal | extra | normal || extra insertion time | no | A lot if there are reference tables, otherwise none | yes | || join performance | suboptimal due to sparse values | even more sub-optimal due to sparse values | optimal From the table above, we can see that using natural keys should be considered only if rows can be identified by a single column and we have a strong confidence in that natural id, specifically in its uniquness and timelessnes.","title":"Primary key"},{"location":"Programming/SQL%20Manual/#non-primary-unique-keys","text":"Sometimes, we need to enforse a uniqueness of a set of columns that does not compose a primary key (e.g., we use a surogate key). We can use a non primary key for that. One of the differences between primary and non-primary keys is that non-primary keys can be null, and each of the null values is considered unique.","title":"Non-primary (UNIQUE) keys"},{"location":"Programming/SQL%20Manual/#indices","text":"Indices are essential for speeding queries containing conditions (including conditional joins). The basic syntax for creating an index is: CREATE INDEX <index name> ON <table name>(<column name>);","title":"Indices"},{"location":"Programming/SQL%20Manual/#show-table-indices","text":"MySQL: SHOW INDEX FROM <tablename>; PostgreSQL SELECT * FROM pg_indexes WHERE tablename = '<tablename >';","title":"Show Table Indices"},{"location":"Programming/SQL%20Manual/#show-indices-from-all-tables","text":"MySQL: SELECT DISTINCT TABLE_NAME, INDEX_NAME FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = '<schemaname>';","title":"Show Indices From All Tables"},{"location":"Programming/SQL%20Manual/#create-table","text":"The syntax is: CREATE TABLE <TABLE NAME> ( <COLUNM 1 NAME> <COLUNM 1 TYPE>, ... <COLUNM N NAME> <COLUNM N TYPE> [, PRIMARY KEY (<LIST OF KEY COLUMNS>)] ) The primary key is optional, but usually, any table should have one. Each row has to have a unique primary key. An index is created automatically for the list of primary key columns.","title":"CREATE TABLE"},{"location":"Programming/SQL%20Manual/#alter-table","text":"","title":"ALTER TABLE"},{"location":"Programming/SQL%20Manual/#add-column-generated-from-other-columns","text":"SQL: ALTER TABLE <table name> ADD <column name> AS (<generator expression>); In MySQL we have to add the data type: ALTER TABLE <tablename> ADD <columnname> <datatype> AS (<generator expression>); In PostgreSQL, the syntax is quite different: ALTER TABLE <tablename> ADD <columnname> <datatype> GENERATED ALWAYS AS (<generator expression>) STORED The advantage of the Postgres approach is that the column is set as generated, therefore it is generated for the new rows automatically.","title":"Add column generated from other columns"},{"location":"Programming/SQL%20Manual/#procedures-and-functions","text":"In SQL there are two constructs that are able to encapsulate SQL statements to increase resusability and readability: functions and procedues. The main difference between these two is that functions are intended to be called inline from SLQ statements, while procedures cannot be used in SQL statements and instead, they are used as a wrapper of a set of SQL statement to be called repeatedly with different arguments. The extensive summary of the different capabilities of functions and procedures is on SO .","title":"Procedures and functions"},{"location":"Programming/SQL%20Manual/#calling-a-procedure","text":"The keyword for calling a procedure differs between database system, refer to the documentation for your system for the right keyword.","title":"Calling a procedure"},{"location":"Programming/SQL%20Manual/#creating-a-procedure","text":"The syntax for calling a procedure differs between database system, refer to the documentation for your system for the right syntax.","title":"Creating a procedure"},{"location":"Programming/SQL%20Manual/#parameters","text":"Procedures and functions can have parameters similar to parameters in programming languages. We can use those parameters in the body of a function/procedure equally as a column or constant. Parameters can have default values , suplied after the = sign. We can test whether a default argument was supplied by testing the parameter for the default value.","title":"Parameters"},{"location":"Programming/SQL%20Manual/#views","text":"Views are basically named SQL queries stored in database. The queries are run on each view invocation, unles the view is materialized. The syntax is: CREATE VIEW <VIEW NAME> AS <QUERY>","title":"Views"},{"location":"Programming/SQL%20Manual/#modifying-the-view","text":"The view can be modified with CREATE OR REPLACE VIEW , however, existing columns cannot be changed . If you need to change existing columns, drop the view first.","title":"Modifying the view"},{"location":"Programming/SQL%20Manual/#schemas","text":"Schema in SQL is a container or namespace for tables, views, and other database objects. This means that we can have multiple objects with the same name in different schemas. The SQL schema should not be confused with the database schema, which is a logical structure of the database. Sadly, the concept of SQL schema is not standardized across different database systems. The following table shows how SQL schema is implemented in different systems: Database System Schema concept default schema MySQL Databases are used as schemas - PostgreSQL Multiple schemas in a single database as per the SQL standard public Oracle Each user has an associated schema (user = schema) user name SQL Server Multiple schemas in a single database as per the SQL standard dbo","title":"Schemas"},{"location":"Programming/SQL%20Manual/#performace-optimization","text":"When the query is slow, first inspect the following checklist: Do not use OR or IN for a set of columns (see replacing OR below). Check that all column and combination of columns used in conditions ( WHERE ) are indexed. Check that all foreign keys are indexed. Check that all joins are simple joins (column to column, or set of columns to a matching set of columns). If nothing from the above works, try to start with a simple query and add more complex pars to find where the problem is. If decomposing the query also does not bing light into the problem, refer to either one of the subsections below, or to the external sources. Also, note that some IDEs limits the number of returned rows automatically, which can hide serious problems and confuse you. Try to remove the limit when testing the performance this way.","title":"Performace Optimization"},{"location":"Programming/SQL%20Manual/#replacing-or","text":"We can slow down the query significantly using OR or IN statements if the set of available options is not constant (e.g., IN(1, 2) is okish, while IN(origin, destination) can have drastic performance impact). To get rid of these disjunctioncs, we can use the UNION statement, basically duplicating the query. The resulting query will be double in size, but much faster: SELECT people.id FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day); -- can be revritten as SELECT * FROM promo JOIN people ON promo.date = people.birthaday UNION SELECT * FROM promo JOIN people ON promo.date = people.name_day GROUP BY people.id","title":"Replacing OR"},{"location":"Programming/SQL%20Manual/#a-specific-join-makes-the-query-slow","text":"If a single join makes the query slow, there is a great chance that the index is not used for the join. Even if the table has an index on the referenced column(s), the join can still not use it if we are joining not to the table itself but to: a subquery, a variable created with a WITH statement, a view, or temborary table created from the indexed table. You can solve the situation by creating a materialized view or temporary table instead, and adding inedices to the table manualy. Specifically, you need to split the query into multiple queries: delete the materialized view/table if exists create the materialized view/table create the required indices perform the actual query that utilizes th view/table Of course we can skip the first three steps if the materialized view is constant for all queries, which is common during the testing phase.","title":"A specific join makes the query slow"},{"location":"Programming/SQL%20Manual/#slow-delete","text":"When the delete is slow, the cause can be the missing index on the child table that refers to the table we are deleting from. Other possible causes are listed in the SO answer .","title":"Slow DELETE"},{"location":"Programming/SQL%20Manual/#sources","text":"Database development mistakes made by application developers","title":"Sources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/","text":"Compilation \u00b6 General rules \u00b6 Make the code you want to compile reachable. In C++, only reachable methods/classes are compiled! Solve errors that stop the compilation first. Warnings can stay in logs even if solved until the main error is gone and the build is finished Be aware that the cause of the error can be on a different line than the one in the error log! If the source of the compilation bug cannot be found \u00b6 read the error examples below check that the code follow the my guidelines and C++ Core Guidelines . read the cpp reference for the parts of the problematic code check the const correctness . It is a causes a lot of problems. try a different compiler, the error message can be more informative try to isolate the case in some small example copy the project and remove the stuff until the problem is gone Practical static assertions \u00b6 Test for concept satisfaction: static_assert(My_concept<My_class>); Useful Predicates \u00b6 std::is_same_as for checking the type equality. Determining type at compile time \u00b6 Sometimes, it is practical to know the exact type during compile time. There is no direct way for that, but we can trick the compiler to print the type in an error message: template <typename...> struct Get_type; class Something {}; Get_type<<TYPE TO GET>> my_type{}; This should print an error message similar to: error: variable \u2018Get_type<<TYPE TO GET RESOLVED>> my_type\u2019 has initializer but incomplete type . Errors with a missing copy constructor \u00b6 e.g. error C2280: 'Solution<N>::Solution(const Solution<N> &)': attempting to reference a deleted function These can be split into two groups: We want to copy the object, but the copy constructor is missing \u00b6 We do not want to copy the object, but the copy constructor is still called \u00b6 First, check if the class is move-constructible in the first place: static_assert(std::is_move_constructible<Solution<Cordeau_node>>::value); If the above check fails, check why the move constructor is not present. Possible reasons, why the move constructor is not available: Move constructor is not implicitly declared due to a broken rule of five, i.e., one of the other constructors/assignments/destructors is defined Implicitly declared move constructor is deleted. Possible reasons: the class have a member that cannot be moved from const member If the check passes, and the copy constructor is still being called: Errors with missing move constructor \u00b6 First, we should check whether the object's type T is movable using static_assert(std::is_move_constructible_v<T>) If the static assertion is false: Check whether all base classes has move constructors available the std::is_move_constructible_v<T> cannot be used for that, as the move operations can be protected. Instead, look for the presence of the move constructors in base classes (any base class should have them declared, as the implicit declaration of copy/move/destruction does not work with virtual classes) Check whether all class members are move constructible using the std::is_move_constructible_v<T> concept. Do not forget the const qualifiers! Multiply Defined Symbols \u00b6 e.g. name already used for a template in the current scope. The source of the duplicate should be in the compiler output. Usually, this can be solved by using namespaces. Conversion is inaccessible \u00b6 This mostly happens if we forgot to add the public keyword when inheriting from a base class, resulting in the (default) private inheritance. Cannot convert from 'initializer list' to... \u00b6 This happans when there is no function signature matching the arguments. Sometimes, the specific argument can be found by using constructor instead of initializer list. Otherwise, check the arguments one by one. For each argument: check if the type matches check if the value type matches, i.e. value/ref/pointer check if the const matches check if the problem is not in wrong template argument deduction. The automatic deduction can use value type instead of reference type... Returning address of local variable or temporary \u00b6 This happens when we assign a lambda to std::function and the labda return by reference and does not have an explicit return type. The solution is to add an explicit return type. Cannot resolve symbol \u00b6 or alternatively: 'identifier' was unexpected here; expected 'type specifier' . It simply means that the type cannot be reolved from the code location. Possible reasons: Circular dinclude Linker Errors \u00b6 Undefined \u00b6 Something like unresolved external symbol... . check that all used files are listed in CMakeLists.txt ( add_executable command\u2026) check that all templates are defined in header files check that all functions are defined correctly (even unsigned vs unsigned int can make problems...) check this list: https://stackoverflow.com/questions/12573816/what-is-an-undefined-reference-unresolved-external-symbol-error-and-how-do-i-fix? Multiply Defined \u00b6 Check the recent includes, are all of them correct? Check the multiply defined symbol. If it is a function defined in a header file, it has to be static or inline. there can be a bug even in an external library! https://stackoverflow.com/questions/30180145/general-techniques-for-debugging-the-multiple-definition-of-error Runtime errors \u00b6 First, identify the exception context . To do that, look at the line where the exception is thrown. If the throwing line is not on the call stack, it is possible that the debugger does not break on the particular exception type. to change that go to the Exception Settings and check the exception type there. If the cause of the exception is not clear from the context, it may be usefull to check the exception details . First, look at the exception message. The easiest way is to catch the exception in the code and print the message. In Google Test, there is a catch-all handler, just run the test without the --gtest_break_on_failure flag. If the message is not enough, look at the exception content in the debugger. Unfortunately, it is not possible to inspect unhandled exception object easily. To do so, add the following watch: (<exception type>*) <exception address> where <exception type> is the type of the exception (e.g. std::exception ) and <exception address> is the address from the exception dialog. Finally, if the cause of the exception is still unclear, look at the exception type, and proceed to the respective section below, if there is one. Error codes \u00b6 Error codes are the only thing visible when the exception is not caught and no debugger is attached. Unfortunately, the error codes are platform-specific, and mostly undocumented even for operating system facilities and standard libraries. Windows Error Codes \u00b6 On windows, many error codes can be emitted by the system or standard libraries: 0x0 to 0x3e7f : Win32 error codes : Errors emitted by Windows high-level functionalities 0xC0000000 to 0xCFFFFFFF : NT status codes : Standardized 32-bit error codes used in Windows kernel, drivers, and protocols. Notable examples: 0xC0000005 : Access violation Improve debugger experience with natvis \u00b6 Natvis is a visualization system that enables to enhance the presentation of variables in debugger locals or watch windows. It is a XML file, where we can define visualization rules for any type. Structure: <Type Name=\"My_class\"> ... visualization rules </Type> The name must be fully qualified, i.e., we have to include namespaces. The big advantage is that natvis files can be changed while the debugger is running and the presentation of the type in locals/watch window is changed immediatly after saving the natvis file. Natvis expressions \u00b6 The expression in natvis are sorounded by {} . If we want curly braces in the text, we can double them {{...}} . Unfortunatelly, function calles cannot be used in natvis expressions . Natvis Errors \u00b6 Natvis errors can be displayed in the output window if turned on in settins: Debug -> Options -> Debugging -> Output Window . Existing visualisations \u00b6 Existing natvis files are stored in <VS Installation Folder>\\Common7\\Packages\\Debugger\\Visualizers folder. The STL visualizations are in stl.natvis Debugger manual \u00b6 Be aware that in CLion debugger, the program does not terminate on unhandled exceptions by default. Address breakpoints \u00b6 Address breakpoints can be used to watch a change of a variable or in general, a change of any memory location. To set an address breakpoint, we nned to first find the address of the variable. To do that, we can: use the & operator on the variable in the watch window use the & operator on the variable in the immediate window The address should have a format 0x0000000000000000 . Memory Errors \u00b6 These exception are raised when an unallocated memory is accesed. The following signalize a memory error: Read Access Violation HEAP CORRUPTION DETECTED First, most of the memory errors can be caught by various assertions and guards in the debug mode. If possible, try to run the program in the debugg mode, even if it takes a long time, because this way, you can catch the problem when it happens, before the memory is corrupted. If that does not help, read the following sections. Other reassons are also discussed here Accessing null pointer \u00b6 A frequent cause of memory errors is accessing a null pointer object's data. In this case, the cause of the problem can be quickly determined in the debugger. Just follow the lifetime of the pointer and find the momemnt when it becom null. Read Access Violation Caused by a Demaged vtable \u00b6 In case of some previous memory mismanagement, the heap can be demaged, possibly resulting in a corrupted virtual table for objects on the heap. To check whether the virtual table is corrupted, add the following watch to the debugger: <var name>.__vfptr Where <var name> is the name of the object you want to inspect. To resolve this problem, see debugging memory errors. Using Application Verifier to find causes of memory related errors. \u00b6 A lot of memory errors can be caught just by running the program in the debugger. The STL containers, for example, containst various assertions that break the code on wrong memory access related to these collections. To add even more assert guards (e.g., for dynamic arrays), we can use the Application Verifier which is installed as a part of Windows SDK (which is typically installed together with Visual Studio). To debug the application with the Application verifier enabled: Open AV right click -> add executable and select the executable to test select the appropriete test suite (the basic one is enouh for the memory testing) click save close AV run the executable in the debugger, find the problem, fix it open AV delete the exectable from the list Using Address Sanitizer \u00b6 A linux memory tool called address sanitizer can be used to debug memory related errors. To use it from the Visual Studio: Check that the libasan lib is installed on WSL In Cmake Settings for the debug configuration, check Enable AddressSanitizer build the project run The program should now break on the first problem. The details are displayed in the output window More at Microsoft learn Using Valgrinfd to debug memory errors \u00b6 Valgrind is a toolbox for debugging C/C++ code. The most famous tool is calle Memcheck and is intended for memory error detection. Basic usage: valgrind --leak-check=yes <program> <program arguments> The explanation of the error messages can be found on the Valgrind website The most common errors and tips: Conditional jump or move depends on uninitialised value : triggers on the first usage (not copy) of the uninitialized data note that the uninitialized variable can look normal (e.g. if it is a number), just the value is random. Invalid read of size ... : can happen to both stack and heap memory the content can still be in memory, it just means that the memory has been freed/invalidated. There are some expected messeges not to be worried about: Warning: set address range perms: large range Logical Errors \u00b6 C++ Specific Numerical Errors \u00b6 First possible error is overflow . C++ does not handle or report overflow! The behaviour is undefined. Second potential danger is the unsigned integer overflow . In case the result below zero is stored in unsigned integer, the number is wrapped around, resulting in large positive integer. Another thing is that when signed and unsigned integers are used in one operation, the signed integer is implicitely converted to unsigned integer before the operation! This is called promotion and it also works for other types (see in a table on SO ). In general to prevent the overflow: check each operation for the potential overflow, inluding the unsigned integer overflow with negative numbers if the overflow can happen, cast the types before any arithmetic operation to prevent the overflow also, one have to use the right type in templates like std::max Testing - Google Test \u00b6 Debugging tests \u00b6 To debug a test, you need to run it with the flag some flags: --gtest_break_on_failure . This flag breaks the test on a failed test or failed assertion. --gtest_catch_exceptions=0 . This flag stops google test from catching exceptions, hence the program breaks at the place where the exception occurred, which is what you want --gtest_filter=Some_test_prefix* for choosing just some tests Also, you need to run tests from Visual Studio, otherwise, the program ends when clicking on the retry button. Visual Studio Errors \u00b6 False errors appears in a file / in many files \u00b6 close visual studio delete the out folder open the visual studio again Refactoring options not present \u00b6 If the refactoring options like change signature are not present, try to send the Resharper bug reports :), or create a new project. IntalliSense false errors \u00b6 Sometimes, the errors disappears by deleting the .suo file located in <project dir>/.vs/<project name>/<VS version> Running the debugger \u00b6 Clion \u00b6 Unlike in Visual Studio, the Clion debugger does not break be default. To break on exceptions or breakpoints, we need to use the debug button instead of the run button. To debug multiple targets at once: Open the Run/Debug Configurations dialog Add a new configuration of type Compound Add the configurations you want to run together using the + button Debug the compound configuration Profiling \u00b6 There are multiple profiler options for C++ , but not all the tools are easy to use or maintained. CPU Profiling \u00b6 CLion \u00b6 documentation CLion profiler is based on the perf tool and therefore it is available only on Linux (and WSL). However, in WSL, currently the profiler does not work . VTune \u00b6 VTune can be run ftom the Visual Studio only for VS solution projects. In case of CMake projects, we need to run the VTune GUI and configure the debugging there. Memory Profiling \u00b6 For memory profiling to work, two things needs to be taken care of: if the application allocates a lot memory inside parallel region, disable paralelization for profiling. Otherwise, there can be too many allocation events for the profiler to handle if you use a custom memory allocator, disable it and use a standard allocator for memory profiling Memory Profiling in Visual Studio \u00b6 To profile memory in Visul Studio Set a breakpoint before the region of interest Run the app and wait for the hit In Diagnostic Tools tab -> Summary , click Enable heap profiling for next session Restart the app and wait for the hit. Take memory snapshot Add a breakpoint to the end of the region of interest Wait for the hit, take snapshot and check both snapshots Memory \u00b6","title":"C++ Debugging and Profiling"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#compilation","text":"","title":"Compilation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#general-rules","text":"Make the code you want to compile reachable. In C++, only reachable methods/classes are compiled! Solve errors that stop the compilation first. Warnings can stay in logs even if solved until the main error is gone and the build is finished Be aware that the cause of the error can be on a different line than the one in the error log!","title":"General rules"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#if-the-source-of-the-compilation-bug-cannot-be-found","text":"read the error examples below check that the code follow the my guidelines and C++ Core Guidelines . read the cpp reference for the parts of the problematic code check the const correctness . It is a causes a lot of problems. try a different compiler, the error message can be more informative try to isolate the case in some small example copy the project and remove the stuff until the problem is gone","title":"If the source of the compilation bug cannot be found"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#practical-static-assertions","text":"Test for concept satisfaction: static_assert(My_concept<My_class>);","title":"Practical static assertions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#useful-predicates","text":"std::is_same_as for checking the type equality.","title":"Useful Predicates"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#determining-type-at-compile-time","text":"Sometimes, it is practical to know the exact type during compile time. There is no direct way for that, but we can trick the compiler to print the type in an error message: template <typename...> struct Get_type; class Something {}; Get_type<<TYPE TO GET>> my_type{}; This should print an error message similar to: error: variable \u2018Get_type<<TYPE TO GET RESOLVED>> my_type\u2019 has initializer but incomplete type .","title":"Determining type at compile time"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#errors-with-a-missing-copy-constructor","text":"e.g. error C2280: 'Solution<N>::Solution(const Solution<N> &)': attempting to reference a deleted function These can be split into two groups:","title":"Errors with a missing copy constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#we-want-to-copy-the-object-but-the-copy-constructor-is-missing","text":"","title":"We want to copy the object, but the copy constructor is missing"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#we-do-not-want-to-copy-the-object-but-the-copy-constructor-is-still-called","text":"First, check if the class is move-constructible in the first place: static_assert(std::is_move_constructible<Solution<Cordeau_node>>::value); If the above check fails, check why the move constructor is not present. Possible reasons, why the move constructor is not available: Move constructor is not implicitly declared due to a broken rule of five, i.e., one of the other constructors/assignments/destructors is defined Implicitly declared move constructor is deleted. Possible reasons: the class have a member that cannot be moved from const member If the check passes, and the copy constructor is still being called:","title":"We do not want to copy the object, but the copy constructor is still called"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#errors-with-missing-move-constructor","text":"First, we should check whether the object's type T is movable using static_assert(std::is_move_constructible_v<T>) If the static assertion is false: Check whether all base classes has move constructors available the std::is_move_constructible_v<T> cannot be used for that, as the move operations can be protected. Instead, look for the presence of the move constructors in base classes (any base class should have them declared, as the implicit declaration of copy/move/destruction does not work with virtual classes) Check whether all class members are move constructible using the std::is_move_constructible_v<T> concept. Do not forget the const qualifiers!","title":"Errors with missing move constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#multiply-defined-symbols","text":"e.g. name already used for a template in the current scope. The source of the duplicate should be in the compiler output. Usually, this can be solved by using namespaces.","title":"Multiply Defined Symbols"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#conversion-is-inaccessible","text":"This mostly happens if we forgot to add the public keyword when inheriting from a base class, resulting in the (default) private inheritance.","title":"Conversion is inaccessible"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#cannot-convert-from-initializer-list-to","text":"This happans when there is no function signature matching the arguments. Sometimes, the specific argument can be found by using constructor instead of initializer list. Otherwise, check the arguments one by one. For each argument: check if the type matches check if the value type matches, i.e. value/ref/pointer check if the const matches check if the problem is not in wrong template argument deduction. The automatic deduction can use value type instead of reference type...","title":"Cannot convert from 'initializer list' to..."},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#returning-address-of-local-variable-or-temporary","text":"This happens when we assign a lambda to std::function and the labda return by reference and does not have an explicit return type. The solution is to add an explicit return type.","title":"Returning address of local variable or temporary"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#cannot-resolve-symbol","text":"or alternatively: 'identifier' was unexpected here; expected 'type specifier' . It simply means that the type cannot be reolved from the code location. Possible reasons: Circular dinclude","title":"Cannot resolve symbol"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#linker-errors","text":"","title":"Linker Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#undefined","text":"Something like unresolved external symbol... . check that all used files are listed in CMakeLists.txt ( add_executable command\u2026) check that all templates are defined in header files check that all functions are defined correctly (even unsigned vs unsigned int can make problems...) check this list: https://stackoverflow.com/questions/12573816/what-is-an-undefined-reference-unresolved-external-symbol-error-and-how-do-i-fix?","title":"Undefined"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#multiply-defined","text":"Check the recent includes, are all of them correct? Check the multiply defined symbol. If it is a function defined in a header file, it has to be static or inline. there can be a bug even in an external library! https://stackoverflow.com/questions/30180145/general-techniques-for-debugging-the-multiple-definition-of-error","title":"Multiply Defined"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#runtime-errors","text":"First, identify the exception context . To do that, look at the line where the exception is thrown. If the throwing line is not on the call stack, it is possible that the debugger does not break on the particular exception type. to change that go to the Exception Settings and check the exception type there. If the cause of the exception is not clear from the context, it may be usefull to check the exception details . First, look at the exception message. The easiest way is to catch the exception in the code and print the message. In Google Test, there is a catch-all handler, just run the test without the --gtest_break_on_failure flag. If the message is not enough, look at the exception content in the debugger. Unfortunately, it is not possible to inspect unhandled exception object easily. To do so, add the following watch: (<exception type>*) <exception address> where <exception type> is the type of the exception (e.g. std::exception ) and <exception address> is the address from the exception dialog. Finally, if the cause of the exception is still unclear, look at the exception type, and proceed to the respective section below, if there is one.","title":"Runtime errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#error-codes","text":"Error codes are the only thing visible when the exception is not caught and no debugger is attached. Unfortunately, the error codes are platform-specific, and mostly undocumented even for operating system facilities and standard libraries.","title":"Error codes"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#windows-error-codes","text":"On windows, many error codes can be emitted by the system or standard libraries: 0x0 to 0x3e7f : Win32 error codes : Errors emitted by Windows high-level functionalities 0xC0000000 to 0xCFFFFFFF : NT status codes : Standardized 32-bit error codes used in Windows kernel, drivers, and protocols. Notable examples: 0xC0000005 : Access violation","title":"Windows Error Codes"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#improve-debugger-experience-with-natvis","text":"Natvis is a visualization system that enables to enhance the presentation of variables in debugger locals or watch windows. It is a XML file, where we can define visualization rules for any type. Structure: <Type Name=\"My_class\"> ... visualization rules </Type> The name must be fully qualified, i.e., we have to include namespaces. The big advantage is that natvis files can be changed while the debugger is running and the presentation of the type in locals/watch window is changed immediatly after saving the natvis file.","title":"Improve debugger experience with natvis"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#natvis-expressions","text":"The expression in natvis are sorounded by {} . If we want curly braces in the text, we can double them {{...}} . Unfortunatelly, function calles cannot be used in natvis expressions .","title":"Natvis expressions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#natvis-errors","text":"Natvis errors can be displayed in the output window if turned on in settins: Debug -> Options -> Debugging -> Output Window .","title":"Natvis Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#existing-visualisations","text":"Existing natvis files are stored in <VS Installation Folder>\\Common7\\Packages\\Debugger\\Visualizers folder. The STL visualizations are in stl.natvis","title":"Existing visualisations"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#debugger-manual","text":"Be aware that in CLion debugger, the program does not terminate on unhandled exceptions by default.","title":"Debugger manual"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#address-breakpoints","text":"Address breakpoints can be used to watch a change of a variable or in general, a change of any memory location. To set an address breakpoint, we nned to first find the address of the variable. To do that, we can: use the & operator on the variable in the watch window use the & operator on the variable in the immediate window The address should have a format 0x0000000000000000 .","title":"Address breakpoints"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#memory-errors","text":"These exception are raised when an unallocated memory is accesed. The following signalize a memory error: Read Access Violation HEAP CORRUPTION DETECTED First, most of the memory errors can be caught by various assertions and guards in the debug mode. If possible, try to run the program in the debugg mode, even if it takes a long time, because this way, you can catch the problem when it happens, before the memory is corrupted. If that does not help, read the following sections. Other reassons are also discussed here","title":"Memory Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#accessing-null-pointer","text":"A frequent cause of memory errors is accessing a null pointer object's data. In this case, the cause of the problem can be quickly determined in the debugger. Just follow the lifetime of the pointer and find the momemnt when it becom null.","title":"Accessing null pointer"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#read-access-violation-caused-by-a-demaged-vtable","text":"In case of some previous memory mismanagement, the heap can be demaged, possibly resulting in a corrupted virtual table for objects on the heap. To check whether the virtual table is corrupted, add the following watch to the debugger: <var name>.__vfptr Where <var name> is the name of the object you want to inspect. To resolve this problem, see debugging memory errors.","title":"Read Access Violation Caused by a Demaged vtable"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#using-application-verifier-to-find-causes-of-memory-related-errors","text":"A lot of memory errors can be caught just by running the program in the debugger. The STL containers, for example, containst various assertions that break the code on wrong memory access related to these collections. To add even more assert guards (e.g., for dynamic arrays), we can use the Application Verifier which is installed as a part of Windows SDK (which is typically installed together with Visual Studio). To debug the application with the Application verifier enabled: Open AV right click -> add executable and select the executable to test select the appropriete test suite (the basic one is enouh for the memory testing) click save close AV run the executable in the debugger, find the problem, fix it open AV delete the exectable from the list","title":"Using Application Verifier to find causes of memory related errors."},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#using-address-sanitizer","text":"A linux memory tool called address sanitizer can be used to debug memory related errors. To use it from the Visual Studio: Check that the libasan lib is installed on WSL In Cmake Settings for the debug configuration, check Enable AddressSanitizer build the project run The program should now break on the first problem. The details are displayed in the output window More at Microsoft learn","title":"Using Address Sanitizer"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#using-valgrinfd-to-debug-memory-errors","text":"Valgrind is a toolbox for debugging C/C++ code. The most famous tool is calle Memcheck and is intended for memory error detection. Basic usage: valgrind --leak-check=yes <program> <program arguments> The explanation of the error messages can be found on the Valgrind website The most common errors and tips: Conditional jump or move depends on uninitialised value : triggers on the first usage (not copy) of the uninitialized data note that the uninitialized variable can look normal (e.g. if it is a number), just the value is random. Invalid read of size ... : can happen to both stack and heap memory the content can still be in memory, it just means that the memory has been freed/invalidated. There are some expected messeges not to be worried about: Warning: set address range perms: large range","title":"Using Valgrinfd to debug memory errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#logical-errors","text":"","title":"Logical Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#c-specific-numerical-errors","text":"First possible error is overflow . C++ does not handle or report overflow! The behaviour is undefined. Second potential danger is the unsigned integer overflow . In case the result below zero is stored in unsigned integer, the number is wrapped around, resulting in large positive integer. Another thing is that when signed and unsigned integers are used in one operation, the signed integer is implicitely converted to unsigned integer before the operation! This is called promotion and it also works for other types (see in a table on SO ). In general to prevent the overflow: check each operation for the potential overflow, inluding the unsigned integer overflow with negative numbers if the overflow can happen, cast the types before any arithmetic operation to prevent the overflow also, one have to use the right type in templates like std::max","title":"C++ Specific Numerical Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#testing-google-test","text":"","title":"Testing - Google Test"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#debugging-tests","text":"To debug a test, you need to run it with the flag some flags: --gtest_break_on_failure . This flag breaks the test on a failed test or failed assertion. --gtest_catch_exceptions=0 . This flag stops google test from catching exceptions, hence the program breaks at the place where the exception occurred, which is what you want --gtest_filter=Some_test_prefix* for choosing just some tests Also, you need to run tests from Visual Studio, otherwise, the program ends when clicking on the retry button.","title":"Debugging tests"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#visual-studio-errors","text":"","title":"Visual Studio Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#false-errors-appears-in-a-file-in-many-files","text":"close visual studio delete the out folder open the visual studio again","title":"False errors appears in a file / in many files"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#refactoring-options-not-present","text":"If the refactoring options like change signature are not present, try to send the Resharper bug reports :), or create a new project.","title":"Refactoring options not present"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#intallisense-false-errors","text":"Sometimes, the errors disappears by deleting the .suo file located in <project dir>/.vs/<project name>/<VS version>","title":"IntalliSense false errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#running-the-debugger","text":"","title":"Running the debugger"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#clion","text":"Unlike in Visual Studio, the Clion debugger does not break be default. To break on exceptions or breakpoints, we need to use the debug button instead of the run button. To debug multiple targets at once: Open the Run/Debug Configurations dialog Add a new configuration of type Compound Add the configurations you want to run together using the + button Debug the compound configuration","title":"Clion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#profiling","text":"There are multiple profiler options for C++ , but not all the tools are easy to use or maintained.","title":"Profiling"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#cpu-profiling","text":"","title":"CPU Profiling"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#clion_1","text":"documentation CLion profiler is based on the perf tool and therefore it is available only on Linux (and WSL). However, in WSL, currently the profiler does not work .","title":"CLion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#vtune","text":"VTune can be run ftom the Visual Studio only for VS solution projects. In case of CMake projects, we need to run the VTune GUI and configure the debugging there.","title":"VTune"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#memory-profiling","text":"For memory profiling to work, two things needs to be taken care of: if the application allocates a lot memory inside parallel region, disable paralelization for profiling. Otherwise, there can be too many allocation events for the profiler to handle if you use a custom memory allocator, disable it and use a standard allocator for memory profiling","title":"Memory Profiling"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#memory-profiling-in-visual-studio","text":"To profile memory in Visul Studio Set a breakpoint before the region of interest Run the app and wait for the hit In Diagnostic Tools tab -> Summary , click Enable heap profiling for next session Restart the app and wait for the hit. Take memory snapshot Add a breakpoint to the end of the region of interest Wait for the hit, take snapshot and check both snapshots","title":"Memory Profiling in Visual Studio"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#memory","text":"","title":"Memory"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/","text":"Type System and basic types \u00b6 cppreference Type is a property of each: object reference function expression Arithmetic Types \u00b6 cppreference Integers \u00b6 Integer types varies in the sign and size. Unfortunatelly, the minimum sizes guaranteed by the standard are not usable, because the real size is different and it differs even between platforms . Especially the long type. To use an integer with a specific size, or a specific minimal size, we can use type aliases defined in cstdint Overflow and Underflow \u00b6 The overflow (and underflow) is a common problem in most programming languages. The problem in C++ is that: overflows are not detected overflows can happen in many unexpected situations Dangerous situations \u00b6 In addition to the usual suspects like assigning a value to a variable of a smaller type, there are some less obvious situations that can cause overflows. Some examples: the result of an arithmetic operation is assigned to a variable of large enough type, but the overflow happens before the assignment itself: short a = 32767; short b = 1; int c = a + b; // overflow happens beffore the assignment A solution to this problem is to use a numeric cast of the opperands (even one is enouhg): short a = 32767; short b = 1; int c = static_cast<int>(a) + b; Detecting overflows \u00b6 There are some methods how to detect overflows automatically by suppliying arguments to the compiler. These are summarized here: MSVC : not implemented GCC : only detectes signed and floating point overflows, as the unsigned overflows are not considered as errors (the behaviour is defined in the standard). All undefined behaviour can be detected using the -fsanitize=undefined flag. Documentation Clang : Both signed and unsigned overflow can be detected. The undefined behaviour can be detected using the -fsanitize=undefined flag. Fo all integer overflows, the -fsanitize=integer flag can be used. Documentation The reasoning behind excluding the unsigned overflows from GCC are described here . It is also possible to do an ad-hoc overflow check in the code, the possible solutions are described in this SO question Characters \u00b6 Characters in C++ are represented by the char type, which is an integer type. This type can be signed or unsigned, and it is at least 8 bits long. Useful functions for working with characters are: std::isspace : checks if the character is a whitespace (space, tab, newline, etc.) std::toupper : converts the character to upper case Pointers \u00b6 cppreference Pointers to Functions \u00b6 Function pointers are declared as: <return_type> (*<pointer_name>)(<arg_1_type>, ..., <arg_n_type>) For example a function the_function returning bool and accepting int can be stored to pointer like this: bool (*ptr)(int) = &the_function The above example can be then simply called as bool b = ptr(2) Pointers to Member Objects \u00b6 Pointers to member objects has a cumbersome syntax declaration: <member type> <class type>::*<pointer name> = ... usage: <object name>.*<pointer name> = ... Example: class My_class{ public: int my_member; } int main{ // declaring the pointer int My_class::*ptr = &My_class::my_member; // creating the instance My_class inst; // using the pointer to a member object inst.*ptr = 2; } Pointers to Member Functions \u00b6 Pointers to member functions are even more scary in C++. We need to use the member object and the function adress and combine it in a obscure way: class My_class{ public: bool my_method(int par); } int main{ // creating the instance My_class inst; // assigning method address to a pointer bool (My_class::*ptr)(int) = &My_class::my_method; // using the pointer to a member function bool b = (inst.*ptr)(2) } The first unexpected change is the My_class before the name of the pointer. It's because unlike a pointer to function the_function which is of type (*)(int) , the pointer to my_method is of type (My_class::*)(int) The second difference is the call. We have t use the pointer to member binding operator .* to access the member of the specific instance inst . But this operator has a lower priority then the function call operator, so we must use the extra parantheses. References \u00b6 References serve as an alias to already existing objects. Standard ( Lvalue ) references works the same way as pointers, with two differences: they cannot be NULL they cannot be reassigned The second property is the most important, as the assignment is a common operation, which often happens under do hood. In conslusion, reference types cannot be used in most of the containers and objets that needs to be copied . Rvalue references \u00b6 Rvalue references are used to refer to temporary objects. They eneable to prevent copying local objets by extending lifetime of temporary objects. They are mostly used as function parameters: void f(int& x){ } f(3); // 3 needs to be copied to f, because it is a temporary variable // we can add the rvalue overload void f(int&& x){ } f(3) // rvalue overload called, no copy Forwarding references \u00b6 Forwarding references are references that preserves the value category (i.e. r/l-value reference, const ). They have two forms: function parameter forwarding references auto forwarding references Function parameter forwarding references \u00b6 In a function template, if we use the rvalue reference syntax for a function parameter of whose type is a function template parameter, the reference is actually a forwarding reference. Example: template<class T> void f(T&& arg) // parameter is T& or T&& depending on the supplied argument Important details: it works only for non const references the reference type has to be a function template argument, not a class template argument auto forwarding reference \u00b6 When we assign to `auto&&, it is a forwarding reference, not rvalue reference: auto&& a = f() // both type and value category depends on the return value of f() for(auto&& a: g(){ // same } Arrays \u00b6 cppreference There are two types of arrays: static , i.e., their size is known at compile type, and dynamic , the size of which is computed at runtime We can use the array name to access the first elemnt of the array as it is the pointer to that element. Static arrays \u00b6 Declaration: int a[nrows]; int a[nrwows][ncols]; // 2D int a[x_1]...[x_n]; // ND Initialization: int a[3] = {1, 2, 5} int b[3] = {} // zero initialization int c[3][2] = {{1,5}, {2,9}, {4,4}} // 2D int d[] = {1,5} // we can skip dimensions if their can be derived from data Note that the multi-dimensional syntax is just an abstraction for the programmers. The following code blocks are therefore equivalent: Matrix syntax const int rowns = 5; const int cols = 3; int matrix[rows][cols]; int main(){ for(int n = 0; n < rows; ++n){ for(int m = 0; m < cols; ++m){ Table[n][m] = (n + 1) * (m + 1); } } } Flat syntax const int rowns = 5; const int cols = 3; int matrix[rows * cols]; int main(){ for(int n = 0; n < rows; ++n){ for(int m = 0; m < cols; ++m){ Table[n * cols + m] = (n + 1) * (m + 1); } } } Using the matrix syntax adds the possibility to access the element of the array using multiple dimensions. But the underlying memory is the same. Dynamic arrays \u00b6 Declaration: int* a = new int[size] For multiple dimensions, this syntax does not scale, i.e, only one dimension can be dynamic: int(*a)[4] = new int[rows][4] // static column count int(*b)[cols] = new int[rows][cols] // does not compile unless cols is a constant! Array to pointer implicit conversion \u00b6 When we use the array name in an expression, it can be implicitly converted to a pointer to the first element of the array. This is true for both static and dynamic arrays. Example: int a[3] = {1, 2, 5} int* ptr = a; // ptr points to the first element of a This implicit conversion is called array-to-pointer decay . Mutli-dimensional dynamic arrays \u00b6 To simulate multi-dimensional dynamic arrays, we have two options: use the flat syntax, as demonstrated on static arrays use aray of pointers to arrays Method | Pros | Cons --|--|-- Flat Syntax | Fast: single continuous allocations | different access syntax than static 2D arrays Array of pointers | Slow: one allocation per row, unrelated memory addresses between rows | same access syntax as static 2D arrays Flat array \u00b6 int* a = new int[rows * cols] Then we can access the array as: a[x * cols + y] = 5 Array of pointers to array \u00b6 Declaration and Definition int** a= new int*[rows] for(int i = 0; i < rows; ++i){ a[i] = new int[cols] } Access is than like for static 2D array: a[x][y] = 5 . This works because the pointers can be also accessed using the array index operator ( [] ). In other words, it works \"by coincidence\", but we have not created a real 2D array. Auto dealocation of dynamic arrays \u00b6 We can replace the error-prone usage of new and delete by wraping the array into unique pointer: std:unique_ptr<int[]> a; a = std::make_unique<int[]>(size) References and Pointers to arrays \u00b6 cppreference The pointer to array is declared as <type> (*<pointer_name>)[<size>] : int a[5]; int (*ptr)[5] = &a; Analogously, the reference to array is declared as <type> (&<reference_name>)[<size>] : int a[5]; int (&ref)[5] = a; Function Type \u00b6 A function type consist from the function arguments and the return type. The function type is written as return_type(arg_1_type, ..., arg_n_type) . Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo), int(double, double)>) // TRUE Reference to Function and Pointer to Function Types \u00b6 cppreference A refrence to function has a type return_type(&)(arg_1_type, ..., arg_n_type) . Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo)&, int(&)(double, double)>); // TRUE A pointer to function has a type: return_type(*)(arg_1_type, ..., arg_n_type) Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo)*, int(*)(double, double)>); // TRUE Enumerations \u00b6 cppreference C++ supports simple enumerations, which are a set of named integer constants. The enumeration can be defined as: enum Color {red, green, blue}; // global scope enum class Color {red, green, blue}; // scoped, preferred There is no support for enum members like in Python, but we can use a wrapper class for that: class Color{ public: enum Value {red, green, blue}; Color(Value v): value(v){} // non-explicit constructor for easy initialization Value get_value() const {return value;} std::string to_string() const{ switch(value){ case Value::red: return \"red\"; case Value::green: return \"green\"; case Value::blue: return \"blue\"; } } private: Value value; } Color get_color(){ return Color::red; // this works due to the non-explicit constructor } int main(){ Color c = get_color(); std::cout << c.to_string() << std::endl; switch(c.get_value()){ case Color::red: std::cout << \"red\" << std::endl; case Color::green: std::cout << \"green\" << std::endl; case Color::blue: std::cout << \"blue\" << std::endl; } } For more complex code requireing automatic conversion to string and more, we can consider the magic_enum library . It supports the following features: enum to string conversion string to enum conversion enum iteration sequence of possible values Complete and Incomplete Types \u00b6 In many context, we have to supply a type with a requirement of being a complete type. So what types are incomplete? The void type is always incomplete Any structure without definition (e.g. using struct structure *ps; , without defining structure .) An array without dimensions is an incomplete type: int a[]; is incomplete, while int a[5]; is complete. An array of incomplete elements is incomplete. A type trait that can be used to determine whether a type is complete is described here . Aggregate types \u00b6 Aggregate types are: array types class types that fullfill the following conditions no private or protected members no constructores declared (including inherited constructors) no private or protected base classes no virtual member functions The elements of the aggregate types can and are ment to be constructed using the aggregate initialization (see the local variable initialization section). Type Conversion \u00b6 cppreference: implicit conversion In some context, an implicit type conversion is aplied. This happens if we use a value of one type in a context that expects a different type. The conversion is applied automatically by the compiler, but it can be also applied explicitly using the static_cast operator. In some cases where the conversion is potentially dangerous, the static_cast is the only way to prevent compiler warnings. Numeric Conversion \u00b6 There are two basic types of numeric conversion: standard implicit conversion that can be of many types: this conversion is applied if we use an expression of type T in a context that expects a type U . Example: ```cpp void print_int(int a){ std::cout << a << std::endl; } int main(){ short a = 5; print_int(a); // a is implicitly converted to int } ``` usual arithmetic conversion which is applied when we use two different types in an arithmetic binary operation. Example: cpp int main(){ short a = 5; int b = 2; int c = a + b; // a is converted to int } Implicit Numeric Conversion \u00b6 Integral Promotion \u00b6 Integral promotion is a coversion of an integer type to a larger integer type. The promotion should be safe in a sense that it never changes the value. Important promotions are: bool is promoted to int : false -> 0 , true -> 1 Integral Conversion \u00b6 Unlike integral promotion, integral conversion coverts to a smaller type, so the value can be changed. The conversion is safe only if the value is in the range of the target type. Important conversions are: Usual Arithmetic Conversion \u00b6 cppreference This conversion is applied when we use two different types in an arithmetic binary operation. The purpose of this conversion is convert both operands to the same type before the operation is applied. The result of the conversion is then the type of the operands. The conversion has the following steps steps: lvalue to rvalue conversion of both operands special step for enum types special step for floating point types conversion of both operands to the common type The last step: the conversion of both operands to the common type is performed using the following rules: If both operands have the same type, no conversion is performed. If both operands have signed integer types or both have unsigned integer types, the operand with the type of lesser integer conversion rank (size) is converted to the type of the operand with greater rank. otherwise, we have a mix of signed and unsigned types. The following rules are applied: If the unsigned type has conversion rank greater or equal to the rank of the signed type, then the unsigned type is used. Otherwise, if the signed type can represent all values of the unsigned type, then the signed type is used. Otherwise, both operands are converted to the unsigned type corresponding to the signed type (same rank). Here especially the rule 3.1 leads to many unexpected results and hard to find bugs. Example: int main(){ unsigned int a = 10; int b = -1; auto c = b - a; // c is unsigned and the value is 4294967285 } To avoid this problem, always use the static_cast operator if dealing with mixed signed/unsigned types . Show the Type at Runtime \u00b6 It may be useful to show the type of a variable at runtime: for debugging purposes for logging to compare the types of two variables Note however, that in C++, there is no reflection support. Therefore, we cannot retrieve the name of the type at runtime in a reliable way . Instead, the name retrieved by the methods described below can depend on the compiler and the compiler settings. Resolved complicated types \u00b6 Sometimes, it is useful to print the type, so that we can see the real type of some complicated template code. For that, the following template can be used: #include <string_view> template <typename T> constexpr auto type_name() { std::string_view name, prefix, suffix; #ifdef __clang__ name = __PRETTY_FUNCTION__; prefix = \"auto type_name() [T = \"; suffix = \"]\"; #elif defined(__GNUC__) name = __PRETTY_FUNCTION__; prefix = \"constexpr auto type_name() [with T = \"; suffix = \"]\"; #elif defined(_MSC_VER) name = __FUNCSIG__; prefix = \"auto __cdecl type_name<\"; suffix = \">(void)\"; #endif name.remove_prefix(prefix.size()); name.remove_suffix(suffix.size()); return name; } Usage: std::cout << type_name<std::remove_pointer_t<typename std::vector<std::string>::iterator::value_type>>() << std::endl; // Prints: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > Source on SO Show the user-provided types (std::type_info) \u00b6 If we want to show the type of a variable provided by the user (e.g., by a function accepting std::any ), we can use the typeid operator which returns a std::type_info object. Standard Library Types \u00b6 Smart Pointers \u00b6 For managing resources in dynamic memory, smart pointers (sometimes called handles ) should be used. They manage the memory (alocation, dealocation) automatically, but their usage requires some practice. There are two types of smart pointers: std::unique_ptr for unique ownership std::shared_ptr for shared ownership Creation \u00b6 Usually, we create the pointer together with the target object in one call: std::make_unique<T>(<OBJECT PARAMS>) for unique pointer std::make_shared<T>(<OBJECT PARAMS>) for shared pointer These methods work well for objects, but cannot be used for arbitrary array initialization (only the empty/zero-initialized array can be created using these methods). For arbitrary array initialization, we need to use the smart pointer constructor: std::unique_ptr<int[]> ptr(new int[]{1, 2, 3}); Counter-intuitively, smart pointers created using the empty constructor of the respective pointer type does not default-construct the target object, but initialize the pointer to null instead: std::unique_ptr<My_class> ptr(std::null_ptr); // ptr is null std::unique_ptr<My_class> ptr(); // ptr is also null Shared Pointer \u00b6 Pointer to object with non-trivial ownership (owned by multiple objects). std::reference_wrapper \u00b6 cppreference Reference wrapper is a class template that can be used to store references in containers or aggregated objects. The disintinction from normal references is that the reference wrapper can be copied and assigned, so it does not prevent the copy/move operations on the object it belongs to. Otherwise, it behaves like a normal reference: it has to be assigned to a valid object and it cannot be null. Strings \u00b6 In C++, there are two types of strings: std::string is an owning class for a string. std::string_view is a non-owning class for a string. Also, there is a C-style string ( char* ), but it is not recommended to use it in modern C++. The difference between std::string and std::string_view is best explained by a table below: std::string std::string_view Owning Yes No Null-terminated Yes No Size Dynamic Static Lifetime Managed by the string Managed by the underlying char sequence Can be constexpr No Yes and the following code: std::string_view sv = \"hello\"; // sv is a view of the string literal \"hello\" std::string s = \"hello\"; // s stores a copy of the string literal \"hello\" String Literals \u00b6 [cppreference]](https://en.cppreference.com/w/cpp/language/string_literal) The standard string literal is writen as \"literal\" . However, we need to escape some special characters in such literals, therefore, a raw string literal is sometimes more desirable: R\"(literal)\" . If our literal contains ( or ) , this is stil not enough, however, the delimiter can be extended to any string with a maximum length of 16 characters, for example: R\"lit(literal)lit\" . Raw string literals also useful for multi-line string literals . Formatting strings \u00b6 The usage of modern string formating is either std::format from the <format> header if the compiler supports C++20 string formatting ( compiler support ) or fmt::format from the fmt library if not. Either way, the usage is the same: format(<literal>, <arguments>) where the literal is a string literal with {} placeholders and the arguments are the values to be inserted into the placeholders. The placeholders can be filled width argument identification, if we want to use the same argument multiple times or change the order in the string while keep the order of arguments in the function call or format specification. These two parts are separated by : , both of them are optional. The most common format specifications are: data type: d for decimal integer f for floating point number s for string width and precision, in the format <width>.<precision> . Both values can be dynamic: std::format(\"{:{}.{}f}\", a, b, c) formats a float number a with width b and precision c . The formating reference can be found in the cppreference Spliting the string into tokens \u00b6 Unfortunately, the STL does not provide a simple way to split the string into tokens like Python's split method or PHP's explode function. It is not even planned for the future. If we want to split a string on a character or pattern, the easiest way is to use the split view from the ranges library, which has a std::ranges::subrange as its element type: // get a range of character subranges auto parts = std::ranges::views::split(str, '-'); // iterate over the parts for (auto part : parts) { std::cout << part << std::endl; // prints the part // convert part to string std::string s(part.begin(), part.end()); // convert part to string (C++23) std::string s(std::from_range, part); } The last string constructor is only available in C++23, and moreover, it requires the stl::from_range tag. The std::string_view is equiped with a range constructor which does not require the tag in C++23. However, it is explicit, so its usage is limited: std::string_view s(part) // invalid std::string_view s = std::string_view(part) // valid in C++23 Converting string to int \u00b6 There are simple functions for converting std::string to numbers, named std::stoi , std::stoul , etc. See cppreference for details. For C strings, the situation is more complicated. Substring \u00b6 A substring can be obtained using a member function substr : str.substr(str.size() - 1, 1)) // returns the last character as a string change the case \u00b6 Unfortunatelly, the STL has case changing functions only for characters, so we need to iterate over the string ourselfs. The boost has a solution, however: #include <boost/algorithm/string.hpp> auto upper = boost::to_upper(str); Building strings \u00b6 Unlike other languages, in C++, strings are mutable, so we can build them using the + operator without performance penalty. Alternatively, we can use the std::stringstream class. Testting for whitespace \u00b6 To test if a string contains only whitespace characters, we can use the std::all_of algorithm: std::all_of(str.begin(), str.end(), [](char c){return std::isspace(c);}) Date and time \u00b6 The date and time structure in C++ is std::tm . We can create it from the date and time string using std::get_time function: std::tm tm; std::istringstream ss(\"2011-Feb-18 23:12:34\"); ss >> std::get_time(&tm, \"%Y-%b-%d %H:%M:%S\"); Collections \u00b6 std::array std::vector Sets \u00b6 Normal set collection for C++ is std::unordered_set . By default, the set uses a Hash , KeyEqual and Allocator template params provided by std functions. However, they need to exist, specifically: std::hash<Key> std::equal_to<Key> std::allocator<Key> So either those specializations needs to be provided by the snadard library (check cppreference), or you have to provide it. Providing custom hash function \u00b6 There are two options for providing custom hash function for a type T s: implementing an explicit specialization of the template function std::hash<T> providing the Hash template param when constructing the hash The first method is prefered if we want to provide a default hash function for some type for which there is no hash function specialization in the standard library. The second method is prefered only when we want some special hash function for a type T for which std::hash<T> is already defined. Implementing custom hash function \u00b6 First check whether the hash function is not provide by STL on cppreference . Then, many other hash specializations are implemented by boost, check the reference . If there is no implementation, we can implement the hash function as follows (example for set): template<> struct std::hash<std::unordered_set<const Request*>> { size_t operator()(const std::unordered_set<const Request*>& set) const { std::hash<const Request> hash_f; size_t sum{0}; for (const Request* r : set) { sum += hash_f(*r); } return sum; } }; Important implementation details: the function needs to be implemented inside std or annonymous namespace, not inside a custom namespace do not forget to add template<> above the function, this indicates that it is a template specialization. Maps \u00b6 The maps has similar requiremnts for keys as the requirements for set value types (see previous section). The hash map type is called std::unordered_map . Geeting value by key \u00b6 To access the map element, the array operator ( [] ) can be used. Note however, that this operator does not check the existence of the key, even if we do not provide a value. Example: std::unordered_map<int,std::string> map; map[0] = \"hello\" map[0] = \"world\" // OK, tha value is overwritten a = map[1] // a == map[1] == \"\" unintuitively, the default value is inserted if the key does not exist Therefore, if we just read from the map, it is safer to use the at() member function. Inserting into map \u00b6 There are three options: map[key] = value; or map.insert({key, value}); map.emplace(key, value); There are some considerations with these options: 1 inserts the value into the map even if the key already exists, overwriting the previous value. 2 and 3 do not overwrite the new value, instead, they return the position in the map and the indicator of success ( true if the insertion happend). 1 requires the value to be default constructible and assignable 3 avoids the creation of temporary objects, it sends references to key and value directly to the map. 3 ignores the insertion if the key already exists. Tuples \u00b6 We have two standard class templates for tuples: std::pair for pairs std::tuple for tuples with unlimited size Although named differently, these class templates behaves mostly the same. Creating tuples \u00b6 There are two ways of creating a tuple: constructor ( auto p = std::pair(...) ) initializer ( auto p = {} ) Beware that by default , the deduced types are decayed, i.e., const and references are removed and the tuple stores value types . If you need to store the reference in a tuple, you have to specify the type: auto p = std::pair<int, constr std::string&>(...) Also, beware that the RVO does not apply for tuple members. This means that if we store values types in the tuple, the types are copied/moved, and in conclusion, they have to by copyable/movable! This is the reason why we frequently use smart pointers in tuples even though we would reurn directly by value if we returned a single value. Creating tuples with std::make_pair / std::make_tuple \u00b6 TLDR: from C++17, there is no reason to use make_pair / make_tuple . There are also factory methods make_pair / make_tuple . Before C++17, argument deduction did not work for constructors, so there is a dedicated method for creating tuples. However, now we can just call the constructor and the template arguments are deduced from the constructor arguments. Also, the make_pair / make_tuple functions can only produce tuples containing values, not references (even if we specify the reference type in the make_pair / make_tuple template argument, the returned tuple will be value-typed). Accessing tuple members \u00b6 The standard way to access the tuple/pair mamber is using the std::get function: auto tuple = std::tuple<int, std::string, float>(0, \"hello\", 1.5); auto hello = std::get<1>(tuple); Unpacking tuples into variables \u00b6 There are two scenarios of unpacking tuples into variables: unpacking into new variables : for that, we use structured binding. unpacking into existing variables : for that, we use std::tie function. Structured binding \u00b6 If we don't need the whole tuple objects, but only its members, we can use a structured binding . Example: std::pair<int, int> get_data(); void main(){ const auto& [x, y] = get_data(); } std::tie \u00b6 If we want to unpack the tuple into existing variables, we can use the std::tie function: std::pair<int, int> get_data(); void main(){ int x, y; std::tie(x, y) = get_data(); } Unpacking tuples to constructor params with std::make_from_tuple \u00b6 We cannot use structured binding to unpack tuple directly into function arguments. For normal functions, this is not a problem, as we can first use structured binding into local variables, and then we use those variables to call the function. However, it is a problem for parent/member initializer calls, as we cannot introduce any variables there. Luckily, there is a std::make_from_tuple template function prepared for this purpose. Example: std::tuple<int,float> get_data(){ ... } class Parent{ public: Parent(int a, float b){...} { class Child: public Parent{ public: Child(): Parent(std::make_from_tuple<Parent>(get_data())){} } std::optional \u00b6 cppreference std::optional<T> is a class template that can be used to store a value of type T or nothing. The advantage over other options like null pointers or is that the std::optional is a value type, so it can wrap stack objects as well. The type T must satisfy std::is_move_constructible_v<T> (must be either movable or copyable). The usage is easy as the class has a value constructor from T and a default constructor that creates an empty optional. Also, the type T is convertible to std::optional<T> , and std::nullopt is convertible to an empty optional. Finally, std::optional<T> is convertible to bool , so it can be used in if statements. A typical usage is: class My_class{ public: My_class(int a, int b); } std::optional<My_class> f(){ ... return My_class(a, b); // or return {a, b}; // or, in case of fail return std::nullopt; } std::optional<int> a = f(); if(a){ // a has a value } Unions and Variants \u00b6 The idea of a union is to store multiple types in the same memory location. Compared to the polymorphism, when we work with pointers and to templates, where the actual type is determined at compile time, the union actually has a shared memory for all the types. The union can be therefore used in cases where nor polymorphism neither templates are suitable. One example can be storing different unrelated types (e.g., std::string and int ) in a container. We cannot use templates as that require a single type. Nor we can use polymorphism, as the types are unrelated. The big disadvantage of unions is that they are not type safe. The compiler cannot check if the type we are accessing is the same as the type we stored. Therefore, we have to be very careful when using unions. Therefore, unless some special case, we should use std::variant instead of unions . std::variant \u00b6 The declaration of std::variant is similar to the declaration of std::tuple : std::variant<int, double> v; The std::variant can store any of the types specified in the template parameters. The type of the stored value can be obtained using: std::holds_alternative method that returns a boolean value if the variant stores the type specified in the template parameter or std::variant::index method that returns the index of the stored value. this method can be used also in a switch statement as the index is integral The value can be accessed using: the std::get function, if we know the type stored in the variant or the std::get_if function if we are guesing the type. Both functions return a pointer to the stored value. Example: std::variant<int, double> v = 1; std::cout << v.index() << std::endl; // prints 0 std::cout << *std::get_if<int>(&v) << std::endl; // prints 1 A really usefull feature of std::variant is the std::visit method, which allows us to call a function on the stored value. The function is selected based on the type of the stored value. Example: std::variant<int, double> v = 1; std::visit([](auto&& arg) { std::cout << arg << std::endl; }, v); // prints 1 More on variants: cppreference cppstories Value Categories \u00b6 [cppreferencepreerecege/value_category). In many contexts, the value category of an expression is important in deciding whether the code compiles or not, or which function or template overload is chosen. Therefore, it is usefull to be able to read value categories. expression value types: lvalue , meaning left-value. An expression typically on the left side of compound expression a statement, e.g. variable, member, or function name. Also, lvalues expressions are are: function ratoalls to fuctions returning lvalue assignments ++a , --a and similar pre operators *a indirection string literal cast prvalue , meaning pure rvalue. It is either a result of some operand ( + , / ) or a constructor/initializer result. The foloowing expressions are prvalues: literals with exception of string literals, e.g.: 4 , true , nullptr function or operator calls that return rvalue (non-reference) a++ , a-- and other post operators arithmetic and logical expressions &a address of expression this non-type template parameters, unless they are references lambda expressions requires expressions and concept spetializations xvalue , meaning expiring value. These valaues usually represent lvalues converted to rvalues. Xvalue expressions are: function call to functions returning rvalue reference (e.g., std::move ). member object expression ( a.m ) if a is an rvlaue and m is a non-reference type glvalue = lvalue || xvalue . rvalue = prvlaue || xvalue . Operators \u00b6 cppreferencen C++ supports almost all the standard operators known from other languages like Java, Python, or C#. Additionally, thsese operators can be overloaded. Note that the standard also supports alternative tokens for some operators (e.g., && -> and , || -> or , ! -> not ). However, these are not supported by all compilers. In MSVC, the /permissive- flag needs to be used to enable these tokens. User-defined Operators \u00b6 In C++ there are more operators than in other popular es like Python or Java. Additionally, these operators can be overloaded. See cppreferencen page for detailed description. Comparison Operators \u00b6 Default Comparison Operators \u00b6 cppreference . The != is usually not a problem, because it is implicitely generated as a negation of the == operator. However, the == is not generated by default, even for simple classes . To force the generation of a default member-wise comparison operator, we need to write: bool operator==(const My_class&) const = default; However, to do that, all members and base classes have to ae the operator == defined, otherwise the default operator will be implicitely deleted. The comparability can be checked with a std::equality_comparable<T> concept: staic_assert(std::equality_comparable<My_class>); Control Structures \u00b6 C++ supports the control structures known from other languages like Java, Python, or C#. Here, we focus on the specifics of C++. Switch Statement \u00b6 cppreference In C++, we can switch on integer types or enumeration types. Also, we can use classes that are implicitely convertible to integers or enums. Switch on string is not possible. The switch statement has the following syntax: switch(expression){ case value1: // code break; case value2: // code break; default: // code } However, it is usually a good idea to wrap each case in a block to create a separate scope for each case. Without it, the whole switch is a single block (contrary to if/else statements). The swich statements just jump to a case that matches the value, similarly to a goto statement. This can create problems, as for example variable initialization cannot be jumped over. The safe case statement looks like: switch(expression){ case value1:{ // code break; } case value2:{ // code break; } default:{ // code } } Functions \u00b6 cppreference Function Declaration and Definition \u00b6 In C and C++, functions must have a: - declaration (signature) that specifies the function name, return type, and parameters - definition that specifies the function body The declaration has to be provided before the first use (call) of the function. The definition can be provided later. The declaration is typically provided in a header file, so that the function can be used outside the translation unit. The definition is typically provided in a source file. Merged Declaration and Definition \u00b6 If the function is not used outside the translation unit, the declaration and definition can be merged, i.e., the definition is itself a declaration. However, this is not recommended because after adding a corresponding declaration to one of the included headers (including libraries), the merged declaration/definition will become a definition of that function, which will be manifested as a linker error (multiple definitions of the function). Therefore, to control the visibility of the function, it is better to use other methods, provides in Section Visibility of Functions . Deciding between free function, member function and static member function \u00b6 Basically, you should decide as follows: Function needs access to instance -> member function Function should be called only by class members (i.e., member functions), so we want to limit its visibility, or we need to access static members of the class -> static member function Otherwise -> free function Argument-parameter Conversions \u00b6 Arg/param | value | reference | rvalue -- |--|--|-- value | - | - | std::move reference | implicit copy | - | copy constructor rvalue | - | not possible | - Default Parameters \u00b6 Default function parameters in C++ works similarly to other languages: int add(int a, int b = 10); add(1, 2) // 3 add(1) // 11 However, the default parameters works only if we call the function by name. Therefore, we cannot use them in std::function and similar contexts. Example: std::function<int(int,int)> addf = add; std::function<int(int)> addf = add; // does not compile addf(1) // does not compile Also, the default parameters need to be values, not references or pointers. For references and pointers, we should use function overloading. Default Parameters and Inheritance \u00b6 TLDR: do not use default parameters in virtual functions. The default parameters are resolved at compile time. Therefore, the value does not depend on the actual type of the object, but on the declared type of the variable. This have following consequences: the default parameters are not inherited A* a = new B(); a->foo() will call B::foo() , with the default parameters of A::foo() To prevent confusion with inheritence we should use function overloading instead of default parameters in virtual functions (like in Java). Return values and NRVO \u00b6 For deciding the return value format, refer to the return value decision tree . Especially, note that NRVO is used in modern C++ and therefore, we can return all objects by value with no overhead most of the time. The NRVO works as follows: compiler tries to just tranfer the object to the parent stack frame (i. e. to the caller) without any move or copy if the above is not possible, the move constructor is called. if the above is not possible, the copy constructor is called. From C++17, the RVO is mandatory, therefore, it is unlikely that the compiler use a move/copy constructor. Consequently, most of the times, we can just return the local variable and let the rest to the compiler: unique_ptr<int> f(){ auto p = std::make_unique<int>(0); return p; // works, calls the move constructor automatically in the worst case (pre C++17 compiler) // return move( p ); // also works, but prevents NRVO } The NRVO is described also on cppreference together with initializer copy elision. Function Overlaoding \u00b6 Both normal and member funcions in C++ can be overloaded. The oveload mechanic, however, is quite complicated. There can be three results of overload resolution of some function call: no function fits -> error one function fits the best multiple functions fits the best -> error The whole algorithm of overload resolution can be found on cppreference . First, viable funcions are determined as functions with the same name and: with the same number of parameters with a greater number of parameters if the extra parameters has default arguments If there are no viable functions, the compilation fails. Otherwise, all viable functions are compared to get the best fit. The comparison has multiple levels. The basic principle is that if only one function fits the rules at certain level, it is chosen as a best fit. If there are multiple such functions, the compilation fails. Levels: Better conversion priority (most of the time, the best fit is found here, see conversion priority and ranking bellow) non-template constructor priority Conversion prioritiy and ranking \u00b6 cppreference When the conversion takes priority during the best viable function search, we say it is better . The (incomplete) algorithm of determining better conversion works as follows: standard conversion is better than user defined conversion user defined conversion is better then elipsis ( ... ) conversion comparing two standard conversions: if a conversion sequence S1 is a subsequence of conversion sequence S2, S1 is better then S2 lower rank priority rvalue over lvalue if both applicable ref over const ref if both applicable Conversion sequence ranks \u00b6 exact match promotion conversion : includes class to base conversion Constructor argument type resolution in list initialization \u00b6 When we use a list initailization and it results in a constructor call, it is not immediatelly clear which types will be used for arguments as the initialization list is not an expression. These types are, however, critical for the finding of best viable constructor. The following rules are used to determine the argument types (simplified): auto return type \u00b6 For functions that are defined inside declaration (template functions, lambdas), the return type can be automatically deduced if we use the auto keyword. The decision between value and reference return type is made according to the following rules: return type auto -> return by value return type auto& -> return by reference return type auto* -> return by pointer return type decltyype(auto) -> the return type is decltype(<RETURN EXPRESSION>) See more rules on cppreference Note that the auto return type is not allowed for functions defined outside the declaration (unless using the trailing return type). Function visibility \u00b6 The member function visibility is determined by the access specifier, in the same manner as the member variable visibility. For free functions , the visibility is determined by the linkage specifier . Without the specifier, the function is visible. To make it visible only in the current translation unit, we can use the static specifier. An equivalent way to make a function visible only in the current translation unit is to put it into an anonymous namespace : namespace { void f() {} } This way, the function is visible in the current translation unit, as the namespace is implicitly imported into it, but it is not visible in other translation units, because anonymous namespaces cannot be imported. One of the other approches frequently used in C++ is to put the function declaration into the source file so it cannot be included from the header. This solution is, however, flawed, unsafe, and therefore, not recommended . The problem is that this way, the function is still visible to the linker, and can be mistakenly used from another translation unit if somebody declare a function with the same name. Deleting functions \u00b6 cppreference We can delete functions using the delete keyword. This is mostly used for preventing the usage of copy/move constructors and assignment operators. However, it can be used for any function, as we illustrate in the following example: class My_class{ print_integer(int a){ std::cout << a << std::endl; } // we do not want to print doubles even they can be implicitly converted to int print_integer(double a) = delete; } Classes and structs \u00b6 The only difference between a class and a struct is that in class, all members are private by default. Class Constants \u00b6 Class constants can be defined in two ways: static constexpr member variable if the constant type supports constexpr specifier, or static const member variable In the second case, we have to split the declaration and definition of the variable to avoid multiple definitions: // in the header file class My_class{ static const int a; } // in the cpp file const int My_class::a = 5; Friend declaration \u00b6 cppreference Sometimes, we need to provide an access to privat e members of a class to some other classes. In java, for example, we can put both classes to the same package and set the members as package private (no specifier). In C++, there is an even stronger concept of friend classes. We put a friend declaration to the body of a class whose private members should be accessible from some other class. The declaratiton can look as follows: Class To_be_accesssed { friend Has_access; } Now the Has_access class has access to the To_be_accesssed 's private members. Note that the friend relation is not transitive, nor symetric, and it is not inherited. Template friends \u00b6 If we want a template to be a friend, we can modify the code above: class To_be_accesssed { template<class T> friend class Has_access; } Now every Has_access<T> is a friend of To_be_accesssed . Note thet we need to use keyword class next to friend . We can also use only a template spetialization: class To_be_accesssed { friend class Has_access<int>; } or we can bound the allowed types of two templates togehter if both Has_access and of To_be_accesssed are templates: template<class T> class To_be_accesssed { friend class Has_access<T>; } Initialization and Assignment \u00b6 Loacal variables initialization/assignment \u00b6 Initialization happens in many contexts : in the declaration in new expression function parameter initialization return value initialization The syntax can be: (<expression list>) = expression list {<initializer list>} Finally, there are multiple initialization types, the resulting initialization type depends on both context and syntax: Value initialization: std::string s{}; Direct initialization: std::string s{\"value\"} Copy initialization: std::string s = \"value\" List initialization: std::string s{'v', 'a', 'l', 'u', 'e'} Aggregate initialization: char a[3] = {'a', 'b'} Reference initialization: char& c = a[0] Default initialization: std::string s List initialization \u00b6 List initialization initializes an object from a list. he list initialization has many forms, including: My_class c{arg_1, arg_2} My_class c = {arg_1, arg_2} my_func({arg_1, arg_2}) return {arg_1, arg_2} The list initialization of a type T can result in various initializations/constructios depending on many aspects. Here is the simplified algorithm: 1. T is aggregate -> aggregate initialization 2. The initializer list is empty and T has a default constructor -> value initialization 3. T has an constructor accepting std::initializer_list -> this constructor is called 4. other constructors of T are considered, excluding explicit constructors Value initialization \u00b6 cppreference This initializon is performed when we do not porvide any parameters for the initialization. Depending on the object, it results in either defualt or zero initialization. Aggregate initialization \u00b6 Aggregate initialization is an initialization for aggregate types. It is a form of list initialization. Example: My_class o1{arg_1, arg_2}; My_class o2 = {arg_1, arg_2}; // equivalent The list initialization of type T from an initializer list results in aggregate initialization if these conditions are fullfilled: the initializer list contains more then one element T is an aggregate type Nested initialization \u00b6 It is not possible to create nested initializatio statements like: class My_class{ int a, float b; public: My_class(ina a, float b): a(a), b(b) } std::tuple<int, My_class>{2, {3, 2.5}} // does not compile std::tuple<int, My_class>{2, My_class{3, 2.5}} // correnct version Member Initialization/Assignment \u00b6 There are two ways of member initialization: default member initialization initialization using member initializer list And then, there is an assignment option in constructor body . Reference: default member initialization constructor and initializer list One way or another, all members should be initialized at the constructor body at latest , even if we assign them again during all possible use cases. Reason: some types (numbers, enums, ...) can have arbitrary values when unassigned. This can lead to confusion when debugging the class, i.e., the member can appear as initialized even if it is not. easy support for overloading constructors, we can sometimes skip the call to the constructor with all arguments we can avoid default arguments in the constructor It is important to not use virtual functions in member initialization or constructor body , because the function table is not ready yet, so the calls are hard wired, and the results can be unpredictable, possibly compiler dependent. Default Member Initialization \u00b6 Either a brace initializer : My_class{ int member{1} } or an equals initializer : My_class{ int member = 1 } Member Initializer List \u00b6 Either using direct initialization (calling constructor of member ): My_class{ My_class(): member(1){ } or list initialization : My_class{ My_class(): member{1}{ } Constructor Body \u00b6 My_class{ My_class(){ member = 1 } } Comparison Table \u00b6 Ordered by priority, i.e., each method makes the methods bellow ignored/ovewritten if applied to the same member. Type | In-place | works for const members --| --|-- Constructor body | no | no Member initializer list | yes | yes Default member initializer | yes, if we use direct initialization | yes Constructors and Special Member Functions \u00b6 cppreference Special member functions are member functions that are someetimes defined implicitely by the compiler. The special member functions are: default (no parameter) constructor copy Cconstructor copy sssignment move constructor move assignment destructor Along with the comparison operators, these are the only functions that can be defaulted (see below). Constructor \u00b6 Defualt Variant \u00b6 The default constructor just create an empty object. The default constructor is not implicitly generated if: there is anothe constructor declared, including copy and move constructor there is some member that cannot be defaulty initialized Explicit constructor \u00b6 Sometimes, a normal constructor can lead to unexpected results, especially if it has only a single argument: class My_string { public: String(std::string string); // convert from std::string String(int length); // construct empty string with a preallocated size }; String s = 10; // surprise: empty string of size 10 istead of \"10\" To prevent these surprising conversion, we can mark the constructor explicit . The explicit keyword before the constructor name prevents the assigment using this constructor. The explicit constructor has to be explicitelly called. Call one constructor from another \u00b6 We can call one constructor from another using the delegating constructor . The syntax is: class My_class{ public: My_class(int a, int b): a(a), b(b){} My_class(int a): My_class(a, 0){} // delegating constructor } This way, we can call another constructor of the same class, or of the base class. Copy Constructor \u00b6 cppreference A copy constructor is called if an object is initialized from another object unless the move constructor is called as a better fit or the call is optimized out by copy elision . Some examples: initializing a new object from an existing object: My_class a; My_class b = a; // copy constructor called My_class c(a); // copy constructor called passing an object to a function by value: void f(My_class a){...} My_class a; f(a); // copy constructor called returning an object by value where the type is not movable and the compiler cannot optimize the call out. we call the copy constructor directly Implicit declaration and implicit deletion \u00b6 The copy constructor for type T is implicitely-declared if T has no declared user-defined copy constructors. If some there are some user-defined copy constructors, we can still force the implicit declaration of the copy constructor using the default keyword However, the implicit declaration does not mean that the copy constructor can be used! This is because the copy constructor can be implicitely defined as deleted . This happens if any of the following conditions is true: T has a non-static data member that cannot be copied. This can happen if any of the following is true: it has a deleted copy constructor, the copy constructor is inaccessible ( protected, private ) the copy constructor is ambiguous (e.g., multiple inheritance) T has a base class that cannot be copied, i.e., 1, 2, or 3 applies to at least one base class T has a non-static data member or base class with inaccessible destructor T has a rvlaue data member T has a user-defined move constructor or move assignment operator (this rule does not apply for defaulted copy constructor) The default implementationof copy constructor calls recursively the copy constructor of all base classes and on all members. For a pointer member, the copy object\u2019s member points to the same object as the original object\u2019s member Checking if a class is copy constructible \u00b6 We can check if a class is copy constructible using the std::is_copy_constructible type trait. Copy Assignment \u00b6 Copy Assignment is needed when we use the = operator with the existing class instances, e.g.: Class instanceA {}; Class instanceB; instanceB = instance A Move Constructor \u00b6 cppreference Move constructor semantic is that the new object takes the ownership of the resources of the old object. The state of the old object is unspecified, but it should not be used anymore. Move constructor is typically called when the object is initaialized from xvalue (but not prvalue!) of the same type. Examples: returning xvalue: Type f(){ Type t; return std::move(t); } passing argument as xvalue: f(Type t){ ... } Type t f(std::move(t)); initializing from xvalue: Type t; Type t2 = std::move(t); Note that for prvalues, the move call is eliminated by copy elision . Therefore, some calls that suggest move constructor call are actually optimized out: Type f(){ Type t; return t; // no move constructor call, copy elision } Type t = T(f()) // no move constructor call, copy elision Move constructor is needed: to cheaply move the object out from function if RVO is not possible to store the object in vector without copying it Note that a single class can have multiple move constructors, e.g.: both Type(Type&&) and Type(const Type&&) . Implicit declaration and implicit deletion \u00b6 The move constructor for type T is implicitely-declared if T has no declared copy constructors, copy assignment operators, move assignment operators, or destructors. If some of the above is declared, we can still force the implicit declaration of the move constructor using the default keyword However, that does not mean that the move constructor can be used! This is because the move constructor can be implicitely defined as deleted . This happens if any of the following conditions is true: T has a non-static data member that cannot be moved. A member cannot be moved if any of the following is true: it has a deleted, inaccessible (protected, private), or ambiguous move constructor, it is a reference, it is const -qualified T has a base class that cannot be moved, i.e., 1, 2, or 3 applies to at least one base class T has a non-static data member or base class with inaccessible destructor Checking if a class is move constructible \u00b6 We can check if a class is move constructible using the std::is_move_constructible type trait. However, the std::is_move_constructible does not check if the move constructor is accessible! Instead it checks if the call to the move constructor is valid (can success, compiles). The call can success if the move constructor is accessible, but it can also success if it is not accessible, but the class has a copy constructor, which is used instead. To check if the move constructor is accessible, we have to manually check the conditions, or disable the copy constructor. Move Assignment \u00b6 Trivial special member functions \u00b6 The special member functions are called trivial if they contain no operations other then copying/moving the members and base classes. For a special member function of type T to be trivial, all of the following conditions must be true: it is implicitly-declared or defaulted T has no virtual functions T has no virtual base classes the constructor for all direct base classes is trivial the constructor for all non-static data members is trivial Destructor \u00b6 We need destructor only if the object owns some resources that needs to be manually deallocated Setting special member functions to default \u00b6 Rules \u00b6 if you want to be sure, delete everything you don\u2019t need most likely, either we need no custom constructors, or we need three (move and destructor), or we need all of them. Rules for Typical Object Types \u00b6 Simple Temporary Object \u00b6 the object should live only in some local context we don\u2019t need anything Unique Object \u00b6 usually represents some real object usually, we need constructors for passing the ownership: move constructor move assignment noe sin Default Object \u00b6 copyable object We need copy constructor copy assignment move constructor move assignment Const vs non-const \u00b6 The const keyword makes the object non-mutable. This means that: it cannot be reassigned non-const member functions of the object cannot be called The const keyword is usually used for local variables, function parameters, etc. For members, the const keyword should not be used , as it sometimes breaks the move operations on the object. For example we cannot move f om a const std::unique_ptr<T> object. While this is also true for local variable, in members, it can lead to hard to find compilation errors, as a single const std::unique_ptr<T> member deep in the object hierarchy breaks the move semantic for the whole class and all subclasses. Avoiding duplication between const and non-const version of the same function \u00b6 To solve this problem without threathening the const-correctness, we need to implement the const version of a function and call it from the non-const one with double type cast: one that converts this to const, so we can call the const version of the function another one that removes const from the return value Example: const Content& get_content(unsigned index) const { Content content = ... // complicated code to get the right content return content; } Content& get_content(unsigned index){ return const_cast<Content&>(std::as_const(this*).get_content()); } there are no common supercalss or i## Const/non const overloads and inheritance Normally, the compiler can safely choose the best match between const and non-const overloads. The problem can happen when each version is in a different place in the class hierarchy. Example: class Base { public: const int& get() const { return some; } protected: int some; }; class A : public virtual Base { public: int& get() { return some; } }; class B : public A {}; B test; test.get(); // ambiguous function error The problem is that the overload set is created for each class in the hierarchy separately. So if the overload was resolved prior the virtual function resolution, we would have only one version (non-const), which would be chosen, despite not being the best overload match in both overload sets. To prevent such unexpected result, some compilers (GCC) raise an ambiguous function error in such situations. To resolve that, we can merge the overload sets in class B : class B : public A { using Base:get; using A:get; }; Avoiding duplication between const and non-const version of the same function \u00b6 To solve this problem without threathening the const-correctness, we need to implement the const version of a function and call it from the non-const one with double type cast: one that converts this to const, so we can call the const version of the function another one that removes const from the return value Example: const Content& get_content(unsigned index) const { Content content = ... // complicated code to get the right content return content; } Content& get_content(unsigned index){ return const_cast<Content&>(std::as_const(this*).get_content()); } IO and Filesystem \u00b6 The simple way to print to standard input is: std::cout << \"Hello world\" << std::endl; To return to the begining of the line and overwrite the previous output, we can use the '\\r' character: std::cout << \"Hello world\" << '\\r' << std::flush; File path manipulation \u00b6 Although we can use strings to work with file paths in C++, the standard format which is also easy to use is std::filesystem::path from the filesystem library . Basic operations: To create a path , we jusct call std::filesystem::path(<string path>) . We can easily join two paths by auto full_path = <path 1> / <path 2> ; To get the asolute path , we call std::filesystem::absolute(<path>) to get the path as CWD/<path> std::filesystem::canonical(<path>) to get the dots resolved. Note that this method throws exception if the path does not exists. The path to the current working directory can be obtained by calling std::filesystem::current_path() and set using std::filesystem::current_path(<path>) . To change the file extension (in the C++ representation, not in the filesystem), we can call the replace_extension method. Filesystem manipulation \u00b6 cppreference Copying \u00b6 To copy, we can use std::filesystem::copy(<source path>, <destination path>[, <options>]) function. The options parameter type is std::filesystem::copy_options . This enum is a bitmask type, therefore, multiple options can be combined using the | operator. Example: auto options = std::filesystem::copy_options::recursive | std::filesystem::copy_options::overwrite_existing; std::filesystem::copy(\"C:/temp/data\", \"c:/data/new\", options); Note that unlike the unix cp command, the copy function does not copy the directoy itself , even if the destination directory exists. Suppose we have two direcories: C:/temp/new C:/data/ And we want to copy the new folder, so that the result is: C:/data/new/ . In bash, this will be: cp -r C:/temp/new C:/data/ While in C++, we need to do: std::filesystem::copy(\"C:/temp/new\", \"C:/data/new\", std::filesystem::copy_options::recursive); Creating directories \u00b6 To create a directory, we can use std::filesystem::create_directory(<path>) function. This function fails if the parent directory does not exist. To create the parent directories as well, we can use std::filesystem::create_directories(<path>) function. Removing files and directories \u00b6 To remove a file or an empty directory, we can use std::filesystem::remove(<path>) function. To remove a content of a directory we can use std::filesystem::remove_all(<path>) function listed on the same page of cppreference. Other useful functions \u00b6 std::filesystem::exists(<path>) std::filesystem::is_directory(<path>) std::filesystem::is_regular_file(<path>) std::filesystem::is_empty(<path>) Simple line by line IO \u00b6 Input \u00b6 For input, we can use std::ifstream : std::ifstream file; file.open(<path>); std::string line; while (std::getline(file, line)) { // do something with the line } file.close(); The important thing is that we need to check whether the open call was successful. The open function never throws an exception, even if the file does not exist , which is a common case. Instead, it only sets the failbit of the stream. Without some check, the failure is hidden as an ifstream in a fail state behaves as if it was empty. Output \u00b6 For line by line output, we use std::ofstream : std::ofstream file; file.open(<path>); batch_file << \"first line\" << std::endl; batch_file << \"second line\" << std::endl; ... batch_file.close(); Load whole file into string \u00b6 Again, we use the std::ifstream , but this time, we also use the std::istreambuf_iterator to read the whole file into a string: std::ifstream file(<path>); std::string content(std::istreambuf_iterator<char>{file}, {}); Here, the std::istreambuf_iterator<char> is created using initialization instead of the constructor so that the local variable is not confused with function declaration. The {} is used to create an empty string, which is the end of the range for the iterator. csv \u00b6 Input \u00b6 Output \u00b6 For csv output, we can usually use the general line-by-line approach. YAML \u00b6 For YAML, we can use the yaml-cpp library. To test whether a YAML::Node contains a certain key , we may use the [] operator, as it does not create a new node (unlike the stl containers): YAML::Node node; if (node[\"key\"]) { // do something } The iteration over the keys is done using YAML::const_iterator : for (YAML::const_iterator it = node.begin(); it != node.end(); ++it) { std::string key = it->first.as<std::string>(); YAML::Node value = it->second; } Inheritance \u00b6 Inheritance in C++ is similar to other languages, here are the important points: To enable overiding, a member function needs to be declared as virtual . Otherwise, it will be just hidden in a child with a function with the same name, and the override specifier cannot be used (see Shadowing). Multiple inheritance is possible. No interfaces. Instead, you can use abstract class with no data members. Virtual functions without implementation needs = 0 at the end of the declaration (e.g.: virtual void print() = 0; ) a type is polymorphic if it has at least one virtual function. I.e., the inheritance itself does not make the type polymorphic. Polymorphism \u00b6 Polymorphism is a concept for abstraction using which we can provide a single interface for multiple types that share the same parent. In C++, to use the polymorphism, we need to work with pointers or references . Imagine that we have these two class and a method that can process the base class: class Base { }; class Derived: public Base { }; void process_base(Base* base) { } Now we can use it lake this: Derived* derived = new Derived(); Base* base = derived; // we easilly can convert derived to base process_base(base); process_base(derived); // we can call the function that accepts a base pointer with a derived pointer We can do the same with smart pointers: void process_base_sh(std::shared_ptr<Base> base) { } std::shared_ptr<Derived> derived_sh = std::make_shared<Derived>(); std::shared_ptr<Base> base_sh = derived_sh; process_base_sh(base_sh); process_base_sh(derived_sh); Shadowing/Hiding: why is a function from parent not available in child? \u00b6 Members in child with a same name as another members in parent shadows those members (except the case when the parent member is virtual). When a member is shadowed/hiden, it is not available in the child class and it cannot be called using the child class instance. This can be counter-intuitive for functions as the shadowing considers only the name, not the signature . Example: class Base { public: void print() { printf(\"Base\\n\"); } }; class Child: public Base { public: void print(std::string type) { printf(\"Child \" + type + \"\\n\"); } }; int main() { Child child; child.print(); // does not compile, as the print() is hidden by print(std::string) return 0; } How to call a hidden function? \u00b6 There are two ways how to call a hideen function: we can use the using declaration in the child to introduce the hidden function: c++ class Child: public Base { public: using Base::print; // now the print() is available in Child void print(std::string type) { printf(\"Child \" + type + \"\\n\"); } }; Usiang a fully qualified name of the method: c++ int main() { Child child; child.Base::print(); return 0; } Constructors \u00b6 Parent constructor is always called from a child. By default, an empty constructor is called. Alternatively, we can call another constructor in the initializer. When we do not call the parent constructor in the child's initializer and the parent has no empty constructor, a compilation error is raised. Enablinging Parent Constructors in Child \u00b6 Implicitly, all methods from parent classes are visible in child, with exception of constructors. Constructors can be inherited manually with a using declaration, but only all at once. To enable only some constructors, we need to repeat them manually as child constructors and call parent construcors from them. Inheritance and Constructors/Destructors \u00b6 To prevent the future bugs with polymorphic destruction calls, it's a good habit to declare a public virtual destructor in each base class : class Base{ public: virtual ~Base() = default; } Otherwise, the following code will not call the child destructor: Child* child = new Child(); Base* base = (Base) child; delete base; But when defining destructor, constructor and move operations are not impliciotely generated. Moreover, the copy operations are generated enabling a polymorphic copy, which results in slicing. Therefore, the best approach for the base class is to: declare the virtual destrucor as default declare the default constructor . We need a default constructor, unless we use a diferent constructor and we want to disable the default one. declare the copy and move operations as protected . This way, the polymorpic copy is not possible, but proper copy/move operations are generated for every child class. Initializing base class members \u00b6 The base class members cannot be initialized in the child constructor initializer. Instead, we need to create a constructor in the base class and call it from the child constructor initializer. Slicing \u00b6 Polymorphism does not go well with value types. When a value type is copied, the only part that remains is the part writen in the code. That means that copying base_2 = base_1 result in a new Base object in base_2 , even if base_1 is an instance of child. Abstract classes therefore cannot be used as function value arguments at all . To pass a polymorphic type as a value to a library function, we need a copyable wrapper that forwards all calls to the undelying polymorphic type. Checking the Type \u00b6 There is no equivalent of Java's instanceof in C++. To check the type. it is possible to use dynamic cast: Child& child = dynamic_cast<Child&>(parent) In case of failure, std::bad_cast is thrown. To prevent exceptions (i.e., we need the type check for branching), we can use pointers: Child* child = dynamic_cast<Child*>(&parent) In this case, if the cast fails, then child == nullptr . Note that to use the dynamic_cast on a type, the type, the type needs to have at least one virtual method . However, this should not be an issue as the type should have at least a virtual destructor. Covariant Return Type \u00b6 Covariant return type is a concept of returning a narower type id derived class than the return type specified in base. Example: class Base { public: virtual Base& get() = 0; }; class Derived: public Base{ public: Derived& get() override { return *this; } }; It works with template classes too: template<class T> class Derived_template: public Base { public: Derived_template<T>& get() override { return &this; } }; Use Method from Parent to Override a Method from Other Parent \u00b6 Unlike in java, a parent method cannot be used to implement an interface of a child . Example: class Interface { public: virtual void print() = 0; }; class Base { public: virtual void print() { printf(\"Base\\n\"); } }; class Child: public Base, public Interface { public: }; int main() { Child child; // does not compile, as Child is an abstract class child.print(); return 0; } The above code does not compile as in C++, the parent print() method is not used as an impementation of print() from the interface (like it works e.g. in Java). There simplest solution to this problem is to override the method in Child and call the parent method staticaly: class Child: public Base, public Interface { public: void print() override { Base::print(); } }; Multiple inheritance and virtual base classes \u00b6 wiki cppreference Multiple inheritance is possible in C++. However, it can lead to some problems. Consider the following example: class A { public: int a; }; class B: public A {}; class C: public A {}; class D: public B, public C {}; It may not be obvious, but the class D has two instances of A in it. This is because the B and C both have their own instance of A . This is certainly not what we want as this way, we have two copies of A::a in D , which are only accessible using qualified names ( D::B::a and D::C::a ) and which can have different values. Virtual Inheritance \u00b6 To mitigate this problem, we can use the virtual inheritance . The virtual inheritance is used when we want to have only one instance of a base class in a child class, even if the base class is inherited multiple times. To use the virtual inheritance, we need to declare the base class as virtual in all child classes: class A { public: int a; }; class B: public virtual A {}; class C: public virtual A {}; class D: public B, public C {}; Multiple copy/move calls with virtual inheritance \u00b6 However, this solves only the problem of having multiple instances of the same base class. But there are also problems with the copy and move operations. In the above example, if the class D is copied or moved, it calls the copy/move operations of B and C , which in turn call the copy/move operations of A . This means that the A is copied/moved twice , which is not what we want. To solve this we need to manually define the copy/move operations of classes in the hierarchy so that the copy/move operations of the base class are called only once. However this can be a complex task. Also, it can backfire later when we extend the hierarchy. Other sources \u00b6 SO answer SO answer 2 Templates \u00b6 The templates are a powerful tool for: generic programming, zero-overhead interfaces, and metaprogramming. Although they have similar syntax as generics in Java, they are principialy different both in the way they are implemented and in the way they are used. There are two types of templates: function templates class templates Syntax \u00b6 Template Declaration \u00b6 Both for classes and functions, the template declaration has the following form: template<<template parameters>> The template parameters can be: type parameters: class T value parameters: int N concept parameters: std::integral T Template definition \u00b6 The definition of template functions or functions fo the template class requires the template declaration to be present. The definition has the following form: template<<template parameters>> <standard function definition> Here, the template parameters are the function template parameters if we define a template function, or the class template parameters if we define a function of a template class. If the template function is a member of a template class, we have to specify both the template parameters of the function and the template parameters of the class: template<<class template parameters>> template<<function template parameters>> <standard class function definition> Note that the template definition has to be in the header file , either directly or included from another header file. This includes the member function definitions of a template class, even if they are not templated themselves and does not use the template parameters of the class. Organization rules \u00b6 *.h : declarations *.tpp template definitions *.cpp non-template definitions. For simplicity, we include the tpp files at the end of corresponding header files. If we need to speed up the compilation, we can include the tpp files only in the source files that needs the implementations , as described on SE To speed up the build it is also desireble to move any non-template code to source files , even through inheritance, if needed. Providing Template Arguments \u00b6 A template can be instantiated only if all the template arguments are provided. Arguments can be: provided explicitly: std::vector<int> v; or sum<int>(1,2) deduced from the initialization (classes): std::vector v = {1,2,3}; from the context (functions): sum(1,2); defaulted: template<class T = int> class A {}; template<class T = int> int sum<T>(T a, T b = 0) { return a + b; } auto s = sum(1, 2); A a(); If we want the template arguments to be deduced or defaulted, we usually use the <> : template<class T = int> class A {}; A<> a(); // default argument is used std::vector<A<>> v; // default argument is used In some cases, the <> can be ommited, e.g., when declaring a variable: A a; // default argument is used // but std::vector<A> v; // error, the A is considered a template here, not the instantiation The rules for omitting the <> are quite complex. Therefore, it is better to always use the <> when we want to use the default arguments. Rules for omitting the <> \u00b6 We can ommite the <> in the following cases: when declaring a variable: A a; when using the type in a function call: f(A()); when instantiating a template class: class B: public A {}; We cannot ommite the <> in the following cases: When we use the template as a nested type: std::vector<A<>> v; , not std::vector<A> v; in the return type of a function: A<> f() , not A f() When declaring an alias: using B = A<> not using B = A for template template parameters. Default Template Arguments \u00b6 Default template arguments can be used to provide a default value for any template parameter except parameter packs. For template classes, there is a restriction that after a default argument is used, all the following parameters must have a default argument as well, except the last one wchich can be parameter pack. Template Argument Deduction \u00b6 Details on cppreference . Template argument deduction should work for: constructors function and operator calls storing the function pointer Class Template Argument Deduction (CTAD) \u00b6 Details on cppreference . The main difference from the function templete argument deduction is that in CTAD, all the template arguments needs to be specified, or all must not be specified and must be deducible. Apart from that, there are more subtle differences arising of a complex procedure that is behind CTAD. We explain CTAD principle using a new concept (not a C++ concept :) ) called deduction guides . Deduction Guides \u00b6 The CTAD use so called deductione guides to deduce the template parameters. Deduction guides can be either implicit or explicit. To demonstrate the principle, let's first start with user-defined deduction guides. User defined deduction guides \u00b6 Let's have an iterator wrapper class below: template<class E, Iterator<E> I> class Iter_wrapper{ public: explicit Iter_wrapper(I iterator){ ... } ... }; Here, the argument E cannot be deduced from argument I , despite the use of the Iterator concept may indicate otherwise. We can still enable the deduction by adding the following deduction guide: template<class I> Iter_wrapper(I iterator) -> Iter_wrapper<decltype(*iterator),I>; Here, the part left from -> represents the constructor call that should be guided, and the part right from -> defines the argument types we want to deduce. Some more details about user defined deduction guides are also on the Microsoft Blog . Implicit deduction guides \u00b6 The vast majority of deduction guidedes used in CTAD are implicit. The most important implicit deduction guides are: constructor deduction guides copy deduction guides The copy deduction guide has the following form: template<<class template parameters>> <class>(<class><class template parameters> obj) -> <class><class template parameters>; For a simple wrapper class: template<class C> class Wrapper{ ... } template<class C> Wrapper(Wrapper<C> obj) -> Wrapper<C>; // implicitelly defined copy deduction guide The constructor deduction guides has the following form: template<<class template parameters>> <class>(<constructor arguments>) -> <class><class template parameters>; For a simple wrapper class: template<class C> class Wrapper{ Wrapper(T&& ref); } template<class C> Wrapper(C&&) -> Wrapper<C>; // implicitelly defined constructor deduction guide Deduction guides resolution \u00b6 Note that CTAD is a process independent of the constructor overload! . First an appropriate deduction guide is used to deduce the class template argumnets, this process can fail if there is no guide. Only then, the overload resolution begins. Most of the time, it is not so important and we can just look at the constructor that is chosen by the constructor overload resolution process and see the used deduction guids and consequently, the resulting template arguments. Sometimes, however, this simplified understanding can lead to confusing results: template<class C> class Wrapper{ Wrapper(T&& ref); Wrapper(double&& ref); // special overload for double } auto w1 = Wrapper(1.5) // the double overload is called In the above example, it may be surprising that the second constructor can be called, as it does not have the class argument present, so the implicit deduction guide cannot work: template<class C> Wrapper(double&&) -> Wrapper<C>; // C unknown! However, it compiles and works, because the deduction guide from the first constructor is used for CTAD, and then, the second constructor is chosen by the constructor overload. Template Specialization \u00b6 Template specialization is a way to provide a different implementation of a template for a specific type. For example, we can provide a different implementation of a template for a std::string type. Imagine following class: // declaration template<class T> class Object{ public: void print(T value) }; // definition template<class T> void Object<T>::print(T value){ std::cout << value << std::endl; } Now, we can provide a different implementation for std::string : // declaration template<> class Object{ public: void print(std::string value) }; template<> void Object<std::string>::print(std::string value){ std::cout << value << std::endl; } There are two types of template specialization: full specialization : exact specification for all template arguments partial specialization : exact specification for a subset of template arguments and/or non-type template arguments To demonstrate the difference, let's have a look at the following example: // declaration template<class T, class C> class Object{}; // primary template // full specialization template<> class Object<int, std::string>{}; // full specialization // partial specializations template<class C> class Object<int, C>{}; // not a full specialization, as C is not specified template<std::integral T, My_concept C> class Object<T, C>{}; // not a full specialization, types are not exactly specified While behaving similarly, there are some important differences between the two types: Full specialization is a new type. Therefore, it must be defined in the source file ( .cpp ), just like any other class or function and it must have a separate declaration. On the other hand, partial specialization is still just a template, so it must be defined in the header file ( .h or .tpp ). For functions, we cannot provide a partial specialization . For member functions we can solve this by specializing the whole class. The solution for any function is to alloow all types in the function and use if constexpr to select the correct implementation: template<class T, class C> class Object{ public: bool process(T value, C config){ if constexpr (std::is_same_v<T, std::string>){ return process_string(value, config); } else { return process_value(value, config); } } }; Note that here, the if constexpr requires the corresponding else branch. Otherwise, the code cannot be discarded during the compilation. Example: template<class T, class C> class Object{ public: bool process(T value, C config){ if constexpr (std::is_same_v<T, std::string>){ return process_string(value, config); } return process_value(value, config); // this compiles even if T is std::string } }; Templates and Namespaces \u00b6 If the templated code resides in a namespace, it can be tempting to save few lines of code by sorrounding both .h and .tpp files using one namespace expression: // structs.h hile namespace my_namespace { // declarations... #include 'structs.tpp' } // structs.tpp // definitions However, this can confuse some IDEs (e.g., false positive errors in IntelliSense), so it is better to introduce the namespace in both files: // structs.h hile namespace my_namespace { // declarations... } #include 'structs.tpp' // structs.tpp namespace my_namespace { // definitions } Don't forget to close the file and reopen it after the change to clear the errors. Using Complicated Types as Template Arguments \u00b6 Sometimes, it can be very tricky to determine the template argument we need in order to use the template. The correct argument can be for example a return value of some function, templete function, or even member function of a template instanciation which has other templates as argument... To make it easier, we can, istead of suplying the correct arguments, evaluate an expression that returns the correct type and then use the decltype specifier. For more info, see the Determining Type from Expressions section. Type Traits \u00b6 The purpose of type traits is to create predicates involving teplate parameters. Using type traits, we can ask questios about template parameters. With the answer to these questions, we can even implement conditional compilation, i.e., select a correct template based on parameter type. Most of the STL type traits are defined in header type_traits . A type trate is a template with a constant that holds the result of the predicate, i.e., the answer to the question. More about type traits Usefull Type Traits \u00b6 std::is_same std::is_base_of std::is_convertible std::conditional : enables if-else type selection Replacement for old type traits \u00b6 Some of the old type traits are no longer needed as they can be replaced by new language features, which are more readable and less error prone. Some examples: std::enable_if can be replaced by concepts: // old: enable_if template<class T> void f(T x, typename std::enable_if_t<std::is_integral_v<T>, void> = 0) { std::cout << x << '\\n'; } // new: concepts template<std::integral T> void f(T x) { std::cout << x << '\\n'; } Concepts \u00b6 cppreference Concepts are named sets of requiremnets. They can be used instead of class / typename keywords to restrict the template types. The syntax is: template<class T, ....> concept concept-name = constraint-expression The concept can have multiple template parameters. The first one in the declaration stands for the concept itself, so it can be refered in the constraint expression. More template parameters can be optionally added and their purpose is to make the concept generic. Constraints \u00b6 Constraints can be composed using && and || operatos. For atomic constaints declaration, we can use: Type traits: template<class T> concept Integral = std::is_integral<T>::value; Concepts: template<class T> concept UnsignedIntegral = Integral<T> && !SignedIntegral<T>; Requires expression: template<typename T> concept Addable = requires (T x) { x + x; }; Either form we chose, the atomic constraint have to always evaluate to bool. Requires Expression \u00b6 Requires expressions ar ethe most powerfull conctraints. The syntax is: requires(parameter list){requirements} There are four types of requirements that can appear in the requires expression: simple requiremnet : a requirement that can contain any expression. Evaluates to true if the expression is valid. cpp requires (T x) { x + x; }; type requirement : a requiremnt checking the validity of a type: cpp requires { typename T::inner; // required nested member name typename S<T>; // required class template specialization typename Ref<T>; // required alias template substitution }; compound requirement : Checks the arguments and the return type of some call. It has the form: {expression} -> return-type-requirement; cpp requires(T x) { {*x} -> std::convertible_to<typename T::inner>; } other useful type traits can be used instead of std::convertible_to . Nested requirement : a require expression inside another requires expression: cpp requires(T a, size_t n) { requires Same<T*, decltype(&a)>; // nested } Auto filling the first template argument \u00b6 Concepts have a special feature that their first argument can be autoffiled from outer context. Consequentlly, you then fill only the remaining arguments. Examples: //When using the concept template<class T, class U> concept Derived = std::is_base_of<U, T>::value; template<Derived<Base> T> void f(T); // T is constrained by Derived<T, Base> // When defining the concept template<typename S> concept Stock = requires(S stock) { // return value is constrained by std::same_as<decltype(stock), double> {stock.get_value()} -> std::same_as<double>; } STL Concepts \u00b6 iterator concepts Usefull Patterns \u00b6 Constrain a Template Argument \u00b6 Imagine that you have a template function load and an abstract class Loadable_interface that works as an interface: class Loadable_interface{ virtual void load() = 0; }; template<class T> void load(T to_load){ ... to load.load() ... }; Typically you want to constraint the template argument T to the Loadable_interface type, so that other developer clearly see the interface requirement, and receives a clear error message if the requirement is not met. In Java, we have an extend keyword for this purpose that can constraint the template argument. In C++, this can be solved with concepts. First we have to define a concept that requires the interface: template<typename L> concept Loadable = std::is_base_of_v<Loadable_interface, L>; Than we can use the concept like this: template<Loadable T> void load(T to_load){ ... to load.load() ... }; Constraint a Concept Argument \u00b6 Imagine that you have a concept Loadable that requires a method load to return a type T restricted by a concept Loadable_type . One would expect to write the loadable concept like this: template<typename L, Loadable_type LT> concept Loadable = requires(L loadable) { {loadable.load()} -> LT; }; However, this is not possible, as there is a rule that concept cannot not have associated constraints . The solution is to use an unrestricted template argument and constrain it inside the concept definition: template<typename L, typename LT> concept Loadable = Loadable_type<LT> && requires(L loadable) { {loadable.load()} -> LT; }; Sources \u00b6 https://en.cppreference.com/w/cpp/language/constraints Requires expression explained Interfaces \u00b6 In programming, an interface is usualy a set of requirements that restricts the function or template parameters, so that all types fulfiling the requiremnet can be used as arguments. Therte are two ways how to create an interface in C++: using the polymorphism using templates argument restriction While the polymorphism is easier to implement, the templating is more powerful and it has zero overhead. The most important thing is probably that despite these concepts can be used together in one application, not all \"combinations\" are allowed especialy when using tamplates and polymorphism in the same type. Note that in C++, polymorphism option work only for function argument restriction, but we cannot directly use it to constrain template arguments (unlike in Java). To demonstrate all possible options, imagine an interface that constraints a type that it must have the following two functions: int get_value(); void set_value(int date); The following sections we will demonstrate how to achieve this using multiple techniques. Interface using polymorfism \u00b6 Unlike in java, there are no interface types in C++. However, we can implement polymorfic interface using abstract class. The following class can be used as an interface: class Value_interface{ virtual int get_value() = 0; virtual void set_value(int date) = 0; } To use this interface as a fuction argument or return value, follow this example: std::unique_ptr<Value_interface> increment(std::unique_ptr<Value_interface> orig_value){ return orig_value->set_value(orig_value->get_value() + 1); } This system works in C++ because it supports multiple inheritance. Do not forget to use the virtual keyword, otherwise, the method cannot be overriden. Note that unlike in other languages, in C++, the polymorphism cannot be directly use as a template (generic) interface. Therefore, we cannot use the polymorfism alone to restrict a type. Using template argument restriction as an interface \u00b6 To use template argument restriction as an interface, we can use concepts. The following concept impose the same requirements as the interface from the polymorphism section: To use template argument restriction as an interface, we can use concepts. The following concept impose the same requirements as the interface from the polymorphism section: template<class V> concept Value_interface = requires(V value_interface){{value_interface.get_value()} -> std::same_as<int>; } && requires(V value_interface, int value){{value_interface.set_value(value)} -> std::same_as<void>; } Remember that the return type of the function has to defined by a concept , the type cannot be used directly. Therefore, the following require statement is invalid: requires{(V value_interface){value_interface.get_value()} -> int; } To use this interface as an template argument in class use: template<Value_interface V> class ... And in function arguments and return types: template<Value_interface V> V increment(V orig_value){ return orig_value.set_value(orig_value.get_value() + 1); Restricting the member function to be const \u00b6 To restrict the member function to be const, we neet to make the type value const in the requires expression: template<class V> concept Value_interface = requires{(const V value_interface) {valvalue_interfaceue.get_value() -> std::same_as<int>;}; }; Using concepts and polymorphism together to restrict template parameters with abstract class \u00b6 We cannot restrict template parameters by polymorphic interface directly, however, we can combine it with concept. The folowing concept can be used together with the interface from the polymorphic interface section: template<class V> concept Value_interface_concept = requires std::is_base_of<Value_interface,V> Neverthless, as much as this combination can seem to be clear and elegent, it brings some problems. . We can use concepts to imposed many interfaces on a single type, but with this solution, it can lead to a polymorphic hell. While there is no problem with two concepts that directly requires the same method to be present with abstract classes, this can be problematic. Moreover, we will lose the zero overhead advantage of the concepts, as the polymorphism will be used to implement the interface. The Conflict Between Templates and Polymorphism \u00b6 As described above, messing with polymorphism and templates together can be tricky. Some examples: No Virtual Member Function with Template Parameters \u00b6 An example: a virtual (abstract) function cannot be a template function ( member template function cannot be virtual), so it cannot use template parameters outside of those defined by the class template. Polymorphism cannot be used inside template params \u00b6 If the functin accepts MyContainer<Animal> we cannot call it with MyContainer<Cat> , even if Cat is an instance of Animal. Possible solutions for conflicts \u00b6 do not use templates -> more complicated polymorphism ( type erasure for members/containers) do not use polymorphism -> use templates for interfaces an adapter can be used Polymorphic members and containers \u00b6 When we need to store various object in the same member or container, we can use both templates and polymorphism. However, both techniques has its limits, summarized in the table below: | | Polymorphism | Templates | | -- | -- | -- | | The concrete type has to be known at compile time | No | Yes | For multiple member initializations, the member can contain any element. | No , the elements have to share base class. | Yes | | For a single initialization, the containar can contain multiple types of objects | Yes , if they have the same base class | No | We can work with value members | No | Yes | When using the interface, we need to use downcasting and upcasting | Yes | No Deciding between template and polymorphism \u00b6 Frequently, we need some entity(class, function) to accept multiple objects through some interface. We have to decide, whether we use templates, or polymorphism for that interface. Some decision points: We need to return the same type we enter to the class/function -> use templates We have to access the interface (from outside) without knowing the exact type -> use polymorphism We need to restrict the member/parametr type in the child -> use templates for the template parameter if you need to fix the relation between method parameters/members or template arguments of thouse, you need to use templates If there are space considerations, be aware that every parent class adds an 8 byte pointer to the atribute table In general, the polymorphic interface have the following adventages: easy to implement easy to undestand similar to what people know from other languages On the other hand, the interface using concepts has the following adventages: no need for type cast all types check on compile time -> no runtime errors zero overhead no object slicing -> you don't have to use pointers when working with this kind of interface we can save memory because we don't need the vtable pointers Iterators, STL algorithms, and ranges \u00b6 If we want to iterate over elements in some programming language, we need to fulfill some interface. In Java, this interface is called Iterable . Also, there is usually some interface that formalize the underlying work, in Java, for example, it is called Iterator . In C++, however, the interface for iteration is not handled by polymorphism. Instead, it is handled using type traits and concepts. On top of that, there are multiple interfaces for iteration: legacy iteration, e.g., for (auto it = v.begin(); it != v.end(); ++it) STL algorithms, e.g., std::find(v.begin(), v.end(), 42) STL range algorithms, e.g., std::ranges::find(v, 42) STL range views, e.g., std::ranges::views::filter(v, [](int x){return x > 0;}) The following table summarizes the differences between the interfaces: |---| Plain iteration | STL algorithms | STL range algorithms | STL range views | |---|---|---|---|---| | Interface | type traits | type traits | concepts | concepts | | Iteration | eager | eager | eager | lazy | | Modify the underlying range | no | yes | yes | no | | Can work on temporaries * | yes | yes | yes | no | *If the operation modifies the data, i.e., sorting, shuffling, transforming, etc. The examples below demonstrate the differences between the interfaces on the following task: create a vector of 10 elements with values 0,1,2,...,9, i.e., the same as Python range(10) . // plain iteration std::vector<int> vec(10); int i = 0; for (auto it = vec.begin(); it != vec.end(); ++it) { *it = i; ++i; } // legacy algorithm std::vector<int> vec(10); std::iota(vec.begin(), vec.end(), 0); // C++11 way, legacy interface using type traits // range algorithm std::vector<int> vec(10); std::ranges::iota(vec.begin(), vec.end(), 0); // basically the same, but the constructor arguments are constrained with concepts // same using adaptor auto range = std::views::iota(0, 10); std::vector vec{range.begin(), range.end()}; // in-place vector construction Terminology \u00b6 range : the object we iterate over (Iterable in Java) iterator : the object which does the real work (Iterator in Java) Usually, a range is composed of two iterators: begin : points to the beginning of the range, returned by <range_object>.begin() end : points to the end of the object, returned by <range_object>.end() Each iterator implements the dereference ( * ) operator that acces the element of the range the iterator is pointing to. Depending on the iterator type, the iterator also supports other operations: ++ , -- to iterate along the range, array index operator ( [] ) for random access, etc. Most of the STL collections (vector, set,...) are also ranges. How to choose the correct interface? \u00b6 when deciding which interface to use, we can use the following rules: If the number of tasks and the complexity of the tasks is high, use the legacy iteration . It is hard to write a 20 line for loop with various function calls as algorithm or adaptor and the result would be hard to read. Otherwise, if you need to preserve the original range as it is or you need to compose multiple operations, use the STL range adaptors . Otherwise, use the STL range algorithms . Note that the in this guide, we do not consider the legacy STL algorithms. With the availability of the STL range algorithms, there is no reason to use the legacy algorithms, except for the backward compatibility or for the algorithms that are not yet implemented in the STL. Also note that some STL algorithms are principially non-modifying, e.g., std::ranges::find or std::ranges::count . These algorithms logically do not have the adaptor equivalent. STL ranges and views \u00b6 https://en.cppreference.com/w/cpp/ranges In C++ 20 there is a new range library that provides functional operations for iterators. It is similar to functional addon in Java 8. As explained in the beginning of this chapter, there are two ways how to use the STL ranges: using the range algorithms ( ranges::<alg name> ) that are invoked eagerly. using the range views ( ranges::views::<view name> ) that are invoked lazily. Note that the range algorithms and adaptors cannot produce result without an input, i.e., we always need a range or collection on which we want to apply our algorithm/view. STL range views \u00b6 The difference of range view to range algorithms is that the views are lazy, i.e., they do not produce any result until they are iterated over. This is similar to the Python generators. The advantage is that we can chain multiple views together and the result is computed only when we iterate over the final view. Note that due to the lazy nature of the views, the underlying range has to be alive during the whole iteration . Therefore, we cannot use the views on temporaries, e.g., we cannot useviews directly in the constructor of a vector, or we cannot use the views on a temporary range returned by a function. A custom view can be created so that it can be chained with STL views. However, it has to satisfy the view concept , and more importantly, it should satisfy the view semantic, i.e., it should be cheap to copy and move (without copying the underlying data). Usefull views \u00b6 std::views::iota : generates a sequence of numbers std::views::filter : filters the elements of the range Projections \u00b6 Unlike in Java, we cannot refer to member functions when lambda functions are required. However, we can use these member functions when the algorithm or adaptor has a projection parameter. Example: struct Data { int a; std::string b; ... }; std::vector<Data> data = get_data(); std::sort(data, {}, &Data::b); The trick here is that we can only provide the member that we want to use for sorting, but the sorting logic ( first < second ...) is handeled by a standard comparator (the second argument initialized with {} ). We can have even more complicated projections that are not just references to member functions: std::vector<My_class> objects = get_objects(); std::vector<unsigned> indexes = get_indexes(objects); auto proj = [&objects](std::size_t i) -> const std::string& { return objects[i].get_name(); }; std::ranges::sort(indexes, {}, proj) // sort indexes using the property of objects Useful range algorithms \u00b6 Note that the most frequently used algorithms have a separate section in the Iterators chapter. std::shuffle : shuffles the elements in the range (formerly std::random_shuffle ). std::adjacent_find : finds the first two adjacent elements that are equal. Can be used to find duplicates if the range is sorted. std::ranges::unique : moves the duplicates to the end of the range and returns the iterator to the first duplicate. Only consecutive duplicates are found. std::ranges::min : finds the smallest element in the range. We can use either natural sorting, or a comparator, or a projection. If the range is empty, the behavior is undefined. std::ranges::min_element : finds the smallest element in the range. Unlike std::ranges::min , this function returns an iterator to the smallest element. std::ranges::empty : checks whether the range is empty. Other Resources \u00b6 https://www.modernescpp.com/index.php/c-20-the-ranges-library Boost ranges \u00b6 In addition to the STL range algorithms and adaptors, boost has it's own range library with other more complex algorithms and adaptors. Boost range requirements \u00b6 Sometimes, it is hard to say why a type does not satisfy some of the requirements for boos ranges. Fortunatelly, the boost provides concepts for checking whether a type satisfy each specific range model. Example: BOOST_CONCEPT_ASSERT(( boost::SinglePassRangeConcept<std::vector<int>> )); // true Also, it is necessary to check whether the value of the iterator can be accessed: BOOST_CONCEPT_ASSERT(( boost_concepts::ReadableIteratorConcept< typename boost::range_iterator<std::vector<int>>::type > )); // true Most likely, the compiler will complain that boost::range_iterator<R>::type does not exist for your range R . The boost range library generate this type by a macro from the R::iterator type. Therefore, make sure that your range has an iterator type defined, either as: a type alias to an existing iterator an iterator nested class Note that <RANGE CLASS>::iterator and <RANGE CLASS>::const_iterator has to be accessible (public). Sequences \u00b6 The iota algortihm/adapter is used to create a sequence: auto range = std::views::iota(0, 10); auto vec = std::vector(range.begin(), range.end()); Note that we cannot pass the view directly to the vector, as the vector does not have a range constructor. Zip \u00b6 The classical Python like zip iteration is available using the zip adapator , which is not yet supported in MSVC. However, boost provides a similar functionality boost::combine . boost::combine \u00b6 boost::combine example: std::vector<int> va{1, 2, 3}; std::vectro<float> vb{0.5, 1, 1.5}; for(const auto& [a, b]: boost::combine(va, vb)){ ... } Each argument of combine must satisfy boost::SinglePassRange Enumerating \u00b6 There is no function in standard library equivalent to the python enumerate. We can use a similar boost solution: #include <boost/range/adaptor/indexed.hpp> for(auto const& el: <range> | boost::adaptors::indexed(0)){ std::cout << el.index() << \": \" << el.value() << std::endl; } However, inside the loop, we have to call the index and value functions, so it is probably easier to stick to the good old extra variable: size_t i = 0; for(auto const& el: <range>) { std::cout << i << \": \" << el << std::endl; ++i; } Sorting \u00b6 There is no sorted view or something simmiler, so in order to sort a range, we need to: really sort the object in the range create an adaptor/view from the range, and then sort the view There are two functions for sorting in the STL algorithm library: std::sort : old supports parallelization directly by the policy param std::ranges::sort : new supports comparison using projections There are three types of sorting: natural sorting using the < operator of T : std::sort(<RANGE<T>>) sorting using a comparator: std::sort(<RANGE>, <COMPARATOR>) , where comparator is a fuction with parameters and return value analogous to the natural sorting operator. sorting using projection (only availeble in std::ranges::sort ): std::ranges::sort(<RANGE>, <STANDARD GENERIC COMPARATOR>, <PROJECTION> Sorting using projection \u00b6 When we want to sort the objects by a single property different then natural sorting, the easiest way is to use projection. Example: struct Data { int a; std::string b; ... }; std::vector<Data> data = get_data(); std::sort(data, {}, &Data::b); The trick here is that we can only provide the member that we want to use for sorting, but the sorting logic ( first < second ...) is handeled by a standard comparator (the second argument initialized with {} ). We can have even more complicated projections: std::vector<My_class> objects = get_objects(); std::vector<unsigned> indexes = get_indexes(objects); auto proj = [&objects](std::size_t i) -> const std::string& { return objects[i].get_name(); }; std::ranges::sort(indexes, {}, proj) // sort indexes using the property of objects Transformation \u00b6 Transformation alg/views transforms an input range according to a callable. As with other operation, there are thre options: classical algorithm: std::transform with a direct paralellization using the policy parameter range algorithm: std::ranges::transform with a support for projections range view: std::ranges::views::transform - a lazy variant The algorithms (but not the view) also supports binary transformations , i.e., create an output range using two input ranges. Transform view example: std::vector<int> in(3, 0); // [0, 0, 0] auto ad = std::ranges::transform_view(in, [](const auto in){return in + 1;}); std::vector<int> out(ad.begin(), ad.end()); The transform view can be only constructed from an object satisfying ranges::input_range . If we want to use a general range (e.g., vector), we need to call the addapter, which has a same signature like the view constructor itself. The important thing here is that the adapter return type is not a std::ranges::views::transform<<RANGE>> but std::ranges::views::transform<std::ranges::ref_view<RANGE>>> ( std::ranges::ref_view ). Supporting various collections is therefore possible only with teplates, but not with inheritance. Note that unlike in Java, it is not possible to use a member reference as a transformation function (e.g.: &MyClass::to_sting() ). We have to always use lambda functions, std::bind or similar to create the callable. Aggregating (sum, product, etc.) \u00b6 These operations can be done using the std::accumulate algorithm. This algorithm is about to be replaced by the std::ranges::fold algorithm, but it is not yet implemented in Clang. Examples: // default accumulation -> sum std::vector<int> vec{1, 2, 3, 4, 5}; int sum = std::accumulate(vec.begin(), vec.end(), 0); // product int product = std::accumulate(vec.begin(), vec.end(), 1, std::multiplies<int>()); Implementing a custom range \u00b6 There are different requirements for different types of ranges. Moreover, there are different requirements for the range-based for loop (for each) , or the legacy STL algorithms. Here we focus on requirements for ranges. Not however, that the range requirements are more strict than the requirements for the range-based for loop or the legacy STL algorithms. Therefore, the described approach should work for all three cases. Usually, we proceed as follows: Choose the right range (Iterable) concept for your range from the STL range concepts . The most common is the std::ranges::input_range concept. Implement the range concept for the range. Either, we can do it by using the interface of the undelying range we usein our class (i.e, we just forward the calls to the methods of std::vector or std::unordered_map ) or implement the interface from scratch. For that, we also need to implement the iterator class that fulfills the corresponding iterator concept (e.g., std::input_iterator for the std::ranges::input_range ). Implementing an input range \u00b6 The input range is the most common range type. The only requirement for the input range is that it has to have the begin and end methods that return the input iterator. Example: class My_range { private: std::vector<int> data; public: My_range(std::vector<int> data): data(data) {} auto begin() {return data.begin();} auto end() {return data.end();} // usually, we also want a const version of the range auto begin() const {return data.begin();} auto end() const {return data.end();} }; Boost Iterator Templates \u00b6 The boost.iterator library provides some templates to implement iteratores easily, typically using some existing iterators and modifying just a small part of it: for pointer to type (dereference) iterator, you can use boost indirect iterator zip iterator for Python like iteration over multiple collections [transform iteratorather useful iterators are also included in the boost.iterator library for using another iterator and just modify the access ( * ) opindex.html). including: zip iterator. counting_iterator to create number sequence like Python range gentransform iterator There are also two general (most powerfull) classes: iterator adapter iterator facade Resources \u00b6 How to write a legacy iterator iter_value_t Lambda Functions \u00b6 In c++ lambda functions are defined as: [<capture>](<params>) -> <return_type> { <code> } The rerurn type is optional, but sometimes required (see below). Since C++23, the parantheses are optional if there are no functon parameters. Captures \u00b6 Anything that we want to use from outside has to appear in capture. To prevent copying, we should capture by reference, using & before the name of the variable. [&var_1] // capture by reference [var_1] // capture by value [&] // default capture by reference For the detailed explanation of the captures, see cppreference . Return type \u00b6 The return type of lambda functions can be set only using the trailing return type syntax ( -> <RETURN TYPE> after the function params). The return type can be omited. Note however, that the default return type is auto , so in case we want to return by reference, we need to add at least -> auto , or even a more specific return type. Specifiers \u00b6 Lambda functions can have special specifiers: mutable : lambda can modify function parameters capture by copy Exceptions \u00b6 In C++, exceptions works simillarly as in other languages. Standard runtime error can be thrown using the std::runtime_error class: throw std::runtime_error(\"message\"); Always catch exception by reference! Note that unlike in Java or Python, there is no default exception handler in C++. Therefore, if an exception is not caught and, in conclusion, the program is terminated, there is no useful information about the exception in the standard output. Instead, we only receive the exit code. For this reason, it is a good practice to catch all exceptions in the main function and print the error message. Example: int main() { try { <the code of the whole program here> } catch(...) { const std::exception_ptr& eptr = std::current_exception() if (!eptr) { throw std::bad_exception(); } /*char* message;*/ std::string message; try { std::rethrow_exception(eptr); } catch (const std::exception& e) { message = e.what(); } catch (const std::string& e) { message = e; } catch (const char* e) { message = e; } catch(const GRBException& ex) { message = fmt::format(\"{}: {}\", ex.getErrorCode(), ex.getMessage()); } catch (...) { message = \"Unknown error\"; } spdlog::error(message); return message; } } Rethrowing Exceptions \u00b6 We can rethrow an exception like this: catch(const std::exception& ex){ // do ssomething ... throw; } Note that in parallel regions, the exception have to be caught before the end of the parallel region , otherwise the thread is killed. How to Catch Any Exception \u00b6 In C++, we can catch any exception with: catch (...) { } However, this way, we cannot access the exception object. As there is no base class for exceptions in C++, there is no way to catch all kind of exception objects in C++. noexcept specification \u00b6 A lot of templates in C++ requires functions to be noexcept which is usually checked by a type trait std::is_nothrow_invocable . We can easily modify our function to satisfy this by adding a noexcept to the function declariaton. There are no requirements for a noexcept function. It can call functions without noexcept or even throw exceptions itself. The only difference it that uncought exceptions from a noexcept function are not passed to the caller. Instead the program is terminated by calling std::terminate , which otherwise happens only if the main function throws. By default, only constructors, destructors, and copy/move operations are noexcept. Stack traces \u00b6 Unlike most other languages, C++ does not print stack trace on program termination. The only way to get a stack trace for all exceptions is to set up a custom terminate handler an inside it, print the stack trace. However, as of 2023, all the stack trace printing/generating libraries requires platform dependent configuration and fails to work in some platforms or configurations. Example: void terminate_handler_with_stacktrace() { try { <stack trace generation here>; } catch (...) {} std::abort(); } std::set_terminate(&terminate_handler_with_stacktrace); To create the stacktrace, we can use one of the stacktrace libraries: stacktrace header from the standard library if the compiler supports it (C++ 23) as of 2024-04, only MSVC supports this functionality cpptrace boost stacktrace Logging \u00b6 There is no build in logging in C++. However, there are some libraries that can be used for logging. In this section we will present logging using the spdlog library. We can log using the spdlog::<LEVEL> functions: spdlog::info(\"Hello, {}!\", \"World\"); By default, the log is written to console. In order to write also to a file, we need to create loggers manually and set the list of sinks as a default logger: const auto console_sink = std::make_shared<spdlog::sinks::stdout_sink_st>(); console_sink->set_level(spdlog::level::info); // log level for console sink auto file_sink = std::make_shared<spdlog::sinks::basic_file_sink_st>(<log filepath>, true); std::initializer_list<spdlog::sink_ptr> sink_list{console_sink, file_sink}; const auto logger = std::make_shared<spdlog::logger>(<LOGGER NAME>, sink_list); logger->set_level(spdlog::level::debug); //log level for the whole logger spdlog::set_default_logger(logger); To save performance in case of an intensive logging, we can set an extended flushing period: spdlog::flush_every(std::chrono::seconds(5)); Colors \u00b6 By default, the logger uses colors for different log levels. However, this capability is lost when: using custom sinks or using custom formatters To keep the colors, we need to a) use the color sink and b) explicitly set the usage of the color in the formatter: auto console_sink = std::make_shared<spdlog::sinks::stdout_color_sink_mt>(); auto logger = std::make_shared<spdlog::logger>(\"console\", console_sink); logger->set_pattern(\"[%^%l%$] %v\"); Here %^ and %$ are the color start and end markers. Type Aliases \u00b6 Type aliases are short names bound to some other types. We can introduce it either with typedef or with using keyword. Examples (equvalent): typedef int number; using number = int; typedef void func(int,int); using func = void(int, int) The using new syntax is more readable, as the alias is at the begining of the expression. But why to use type aliases? Two strong motivations can be: iImprove the readebility : When we work with a type with a very long declaration, it is wise to use an alias. We can partialy solve this issue by using auto, but that is not a complete solution Make the refactoring easier : When w work with aliases, it is easy to change the type we work with, just by redefining the alias. Note that type aliases cannot have the same name as variables in the same scope . So it is usually safer to name type aliases with this in mind, i.e., using id_type = .. insted of using id = .. Template Aliasis \u00b6 We can also create template aliases as follows: template<class A, typename B> class some_template{ ... }; template<class T> using my_template_alias = some_template<T, int>; Aliases inside classes \u00b6 The type alias can also be placed inside a class. From outside the class, it can be accessed as <CLASS NAME>::<ALIAS NAME> : class My_class{ public: using number = unsigned long long number n = 0; } My_class::number number = 5; Constant Expressions \u00b6 A constant expression is an expression that can be evaluated at compile time. The result of constant expression can be used in static context, i.e., it can be: assigned to a constexpr variable, tested for true using static_assert Unfortunatelly, there is no universal way how to determine if an expression is a constant expression . More on cppreference . Regular expressions \u00b6 The regex patern is stored in a std::regex object: const std::regex regex{R\"regex(Plan (\\d+))regex\"}; Note that we use the raw string so we do not have to escape the pattern. Also, note that std::regex cannot be constexpr Matching the result \u00b6 We use the std::regex_search to search for the occurence of the pattern in a string. The result is stored in a std::smatch object which contains the whole match on the 0th index and then the macthed groups on subsequent indices. A typical operation: std::smatch matches; const auto found = std::regex_search(string, matches, regex); if(found){ auto plan_id = matches[1].str(); // finds the first group } Note that matches[0] is not the first matched group, but the whole match. Namespaces \u00b6 cppreference Namespace provides duplicit-name protection, it is a similar concept to Java packages. Contrary to java packages and modules, the C++ namespaces are unrelated to the directory structure. namespace my_namespace { ... } The namespaces are used in both declaration and definition (both in header and source files). The inner namespace has access to outer namespaces. For using some namespace inside our namespace without full qualification, we can write: using namespace <NAMESPACE NAME> Anonymous namespaces \u00b6 Anonymous namespaces are declared as: namespace { ... } Each anonnymous namespaces has a different and unknown ID. Therefore, the content of the annonymous namespace cannot be accessed from outside the namespace, with exception of the file where the namespace is declared which has an implicit access to it. Namespace aliases \u00b6 We can create a namespace alias using the namespace keyword to short the nested namespace names. Typicall example: namespace fs = std::filesystem; decltype : Determining Type from Expressions \u00b6 Sometimes, it is usefull to declare a type from expression, instead of do it manualy. Using decltype specifier, we can get the resulting type of an expression as if it was evaluated. Examples: struct A { double x; }; const A* a; decltype(a->x) // evaluates to double decltype(std::accumulate(a, [](double sum, double val){return sum + val;})) // evalutes to double We can use the decltype in any context where type is required. Examples: int i = 1 decltype(i) j = 3 std::vector<decltype(j)> numbers; The Value Category of decltype \u00b6 The value category of decltype is resolved depending on the value category of an expression inside it: deltype(<XVALUE>) -> T&& deltype(<LVALUE>) -> T& deltype(<RVALUE>) -> T The rvalue conversion can lead to unexpected results, in context, where the value type matters: static_assert(std::is_same_v<decltype(0), decltype(std::identity()(0))>); // error The above expressions fails because: decltype(0) , 0 is an rvalue -> the decltype result is int decltype(std::identity()(0)) result of std::identity() is an xvalue -> the decltype result is int&& . Determining Type from Expressions Sometimes, it is usefull to declare a type from expression, instead of do it manualy. Using decltype specifier, we can get the resulting type of an expression as if it was evaluated. Examples: struct A { double x; }; const A* a; decltype(a->x) // evaluates to double decltype(std::accumulate(a, [](double sum, double val){return sum + val;})) // evalutes to double We can use the decltype in any context where type is required. Examples: int i = 1 decltype(i) j = 3 std::vector<decltype(j)> numbers; Determining the Return Value Type of a Function \u00b6 As we can see above, we can use decltype to determine the return value type. But also, there is a type trait for that: std::invoke_result_t (formerly std::result_of ). The std::invoke_result_t should vbe equal to decltype when aplied to return type, with the following limitations: - we cannot use abstract classes as arguments of std::invoke_result_t , while we can use them inside decltype (using std::declval , see below). \u00b6 Construct object inside decltype with std::declval \u00b6 std::declval is a usefull function designed to be used only in static contexts, inside decltype . It enables using member functions inside decltype without using constructors. Without std::declval , some type expressions are hard or even impossible to costruct. Example: class Complex_class{ Complex_class(int a, bool b, ...) ... int compute() } // without declval decltype<Complex_class(1, false, ...).compute()> // using declval decltype(std::declval<Complex_class>().compute()) decltype and Overloading \u00b6 in static context, there is no overloading, the vtable is not available. Therefore, we have to hint the compiler which specific overloaded function we want to evaluate. This also applies to const vs non const overloading. The following example shows how to get the const iterator type of a vector: std::vector<anything> vec // non const iter decltype(vec.begin()) // const iter decltype<std::declval<const decltype(vec)>().begin()> Another example shows how to use the const overload inside std::bind : decltype(std::bind(static_cast<const ActionData<N>&(std::vector<ActionData<N>>::*)(size_t) const>(&std::vector<ActionData<N>>::operator[]), action_data)), Above, we used static cast for choosing the const version of the vector array operator. Instead, we can use explicit template argument for std::bind : decltype(std::bind<const ActionData<N>& (std::vector<ActionData<N>>::*)(size_t) const>(&std::vector<ActionData<N>>::operator[], action_data)), Parallelization \u00b6 While there wa no support of parallelization i earlier versions of C++ , now there are many tools. Standard Threads \u00b6 For-each with Parallel Execution Policy \u00b6 The function std::for_each can be run with a parallel execution policy to process the loop in parallel. Async tasks \u00b6 Tasks for asznchronous execution, like file downloads, db queries, etc. The main function is std::async . Open-MP \u00b6 In MSVC, the Open MP library is automatically included and linked. In GCC, we need to find the libs in CmakeLists.txt : find_package(OpenMP REQUIRED) Standard Templates for Callables \u00b6 Using std::invoke to call the member function \u00b6 using std::invoke , the cal syntax bool b = (inst.*ptr)() can be replaced with longer but more straighforward call: bool b = std::invoke(ptr, inst, 2) Using std::mem_fn to Store a Pointer to Member Function in a Callable \u00b6 With std::mem_fn , we can store the pointer to a member function in a callable object. Later, we can call the object without the pointer to the member function. Example: auto mem_ptr = std::mem_fn(&My_class::my_method) bool b = mem_ptr(inst, 2) Using a Pointer to Member Function as a Functor \u00b6 A normal function can be usually send instead of functor, as it can be invoked in the same way. However, in case of member function, we usually need to somehow bind the function pointer to the instance. We can use the std::bind function exactly for that: auto functor = std::bind(&My_class::my_method, inst); bool b = functor(2) Advanteges: we do not need an access to instance in the context from which we call the member function we do not have to remember the complex syntax of a pointer to a member function declaration we receive a callable object, which usage is even simpler than using std::invoke Note that in case we want to bind only some parameters, we need to supply placeholders for the remaining parameters ( std::placeholders ). Using Lambdas Instead of std::bind \u00b6 For more readable code and better compile error messages, it is usefull to replace std::bind callls with labda functions. The above example can be rewritten as: auto functor = [inst](int num){return inst.my_method(num);); bool b = functor(2) Store the Result of std::bind \u00b6 Sometimes, we need to know the return type of the std::bind . In many context, we need to provide the type instead of using auto . But luckily, there is a type exactly for that: std::function . Example: std::function<bool(int)> functor = std::bind(&My_class::my_method, inst); bool b = functor(2) A lambda can also be stored to std::function . But be carefull to add an explicit return type to it, if it returns by a reference. Example: My_class{ public: int my_member } My_class inst; std::function f = [inst](){return &inst.my_member; } // wrong, reference to a temporary due to return type deduction std::function f = [inst]() -> const int& {return &inst.my_member; } // correct More detailed information about pointers to member functions std::mem_fn and Data Members \u00b6 Data member pointers can be aslo stored as std::mem_fn . A call to this object with an instance as the only argument then return the data member value. The plain syntax is <type> <class name>.*<pointer name> = <class name>.<member name> , and the pointer is then accessed as <instance>.*<pointer name> . Example: int Car::*pSpeed = &Car::speed; c1.*pSpeed = 2; Usefull STL functions std::for_each : iterates over iterable objects and call a callable for each iteration std::bind : Binds a function call to a variable that can be called some parameters of the function can be fixed in the variable, while others can be provided for each call each reference parameter has to be wrapped as a reference_wrapper std:mem_fn : Creates a variable that represents a callable that calls member function std::function \u00b6 The std::function template can hold any callable. It can be initialized from: function pointer/reference, member function pointer/reference, lambda function functor It can be easily passed to functions, used as template parameter, etc. The template parameters for std::function has the form of std::function<<RETURN TYPE>(<ARGUMENTS>)> . Example: auto lambda = [](std::size_t i) -> My_class { return My_class(i); }; std::function<My_class(std::size_t)> f{lambda} std::function and overloading \u00b6 one of the traps when using std::function is the ambiguity when using an overloade function: int add(int, int); double add(double, double); std::function<int(int, int)> func = add; // fails due to ambiguity. The solution is to cant the function to its type first and then assign it to the template: std::function<int(int, int)> func = static_cast<int(*)(int, int)>add; Preprocessor Directives \u00b6 The C language has a preprocessor that uses a specific syntax to modify the code before the compilation. This preprocessor is also used in C++. The most used tasks are: - including files ( #include ): equivalent to Java or Python import statement - conditional compilation based on OS, compiler, or other conditions Also, preprocessor had some other purposes, now replaced by other tools: - defining constants ( #define ): replaced by const and constexpr - metaprogramming: replaced by templates A simple variable can be defined as: #define PI 3.14159 . The variable can be used in the code as PI . Control structures are defined as: #ifdef <MACRO> ... #elif <MACRO> ... #else ... #endif Testing with Google Test \u00b6 Private method testing \u00b6 The testing of private method is not easy with Google Test, but that is common also for other tets frameworks or even computer languages (see the common manual). Some solutions are described in this SO question . Usually, the easiest solution is to aplly some naming/namespace convention and make the function accessible. For free functions: namespace internal { void private_function(){ ... } } For member functions: class MyClass{ public: void _private_function(); specific tasks \u00b6 Conditional Function Execution \u00b6 W know it from other languages: if the function can be run in two (or more) modes, there is a function parameter that controls the execution. Usually, most of the function is the same (otherwise, we eould create multiple fuctions), and the switch controls just a small part. Unlike in other langueges. C++ has not one, but three options how to implement this. They are described below in atable together with theai properties. function parameter template parameter compiler directive good readability yes no no compiler optimization no yes yes conditional code compilation no no yes Function Parameter \u00b6 void(bool switch = true){ if(switch){ ... } else{ ... } } Template Parameter \u00b6 template<bool S = true> void(){ if(S){ ... } else{ ... } } Compiler Directive \u00b6 void(){ #ifdef SWITCH ... #else ... #endif } Ignoring warnings for specific line of code \u00b6 Sometimes, we want to suppress some warnings, mostly in libraries we are including. The syntax is, unfortunatelly, different for each compiler. Example: #if defined(_MSC_VER) #pragma warning(push) #pragma warning(disable: <WARNING CODE>) #elif defined(__GNUC__) #pragma GCC diagnostic push #pragma GCC diagnostic ignored \"<WARNING TYPE GCC>\" #elif defined(__clang__) #pragma clang diagnostic push #pragma clang diagnostic ignored \"<WARNING TYPE CLANG>\" #endif .. affected code... #if defined(_MSC_VER) #pragma warning(pop) #elif defined(__GNUC__) #pragma GCC diagnostic pop #elif defined(__clang__) #pragma clang diagnostic pop #endif Note that warnings related to the preprocessor macros cannot be suppressed this way in GCC due to a bug (fixed in GCC 13). The same is true for conditions: #if 0 #pragma sdhdhs // unknown pragma raises warning, despite unreachcable #endif Measuring used resource \u00b6 Memory \u00b6 MSVC \u00b6 In MSVC, we can measure the peak used memory using the following code: #include <psapi.h> PROCESS_MEMORY_COUNTERS pmc; K32GetProcessMemoryInfo(GetCurrentProcess(), &pmc, sizeof(pmc)); auto max_mem = pmc.PeakWorkingSetSize Working with tabular data \u00b6 Potential libs similar to Python Pandas: Arrow Dataframe Executing external commands \u00b6 The support for executing external commands in C++ is unsatisfactory. The most common solution is to use the system function. However, the system calls are not portable, e.g., the quotes around the command are not supported in Windows Another option is to use the Boost Process library. Command Line Interface \u00b6 For CLI, please follow the CLI manual . Here we focus on setting up the TCLAP library. TCLAP use Jinja-like Templating \u00b6 For working with Jinja-like templates, we can use the Inja template engine. Exceptions \u00b6 There are the following exceptions types: ParserError thrown on parse_template method RenderError thrown on write method Render Errors \u00b6 empty expression : this signalize that some expression is empty. Unfortunatelly, the line number is incorrect (it is always 1). Look for empty conditions, loops, etc. (e.g., {% if %} , {% for %} , {% else if %} ).","title":"C++ Manual"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#type-system-and-basic-types","text":"cppreference Type is a property of each: object reference function expression","title":"Type System and basic types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#arithmetic-types","text":"cppreference","title":"Arithmetic Types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#integers","text":"Integer types varies in the sign and size. Unfortunatelly, the minimum sizes guaranteed by the standard are not usable, because the real size is different and it differs even between platforms . Especially the long type. To use an integer with a specific size, or a specific minimal size, we can use type aliases defined in cstdint","title":"Integers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#overflow-and-underflow","text":"The overflow (and underflow) is a common problem in most programming languages. The problem in C++ is that: overflows are not detected overflows can happen in many unexpected situations","title":"Overflow and Underflow"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#dangerous-situations","text":"In addition to the usual suspects like assigning a value to a variable of a smaller type, there are some less obvious situations that can cause overflows. Some examples: the result of an arithmetic operation is assigned to a variable of large enough type, but the overflow happens before the assignment itself: short a = 32767; short b = 1; int c = a + b; // overflow happens beffore the assignment A solution to this problem is to use a numeric cast of the opperands (even one is enouhg): short a = 32767; short b = 1; int c = static_cast<int>(a) + b;","title":"Dangerous situations"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#detecting-overflows","text":"There are some methods how to detect overflows automatically by suppliying arguments to the compiler. These are summarized here: MSVC : not implemented GCC : only detectes signed and floating point overflows, as the unsigned overflows are not considered as errors (the behaviour is defined in the standard). All undefined behaviour can be detected using the -fsanitize=undefined flag. Documentation Clang : Both signed and unsigned overflow can be detected. The undefined behaviour can be detected using the -fsanitize=undefined flag. Fo all integer overflows, the -fsanitize=integer flag can be used. Documentation The reasoning behind excluding the unsigned overflows from GCC are described here . It is also possible to do an ad-hoc overflow check in the code, the possible solutions are described in this SO question","title":"Detecting overflows"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#characters","text":"Characters in C++ are represented by the char type, which is an integer type. This type can be signed or unsigned, and it is at least 8 bits long. Useful functions for working with characters are: std::isspace : checks if the character is a whitespace (space, tab, newline, etc.) std::toupper : converts the character to upper case","title":"Characters"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#pointers","text":"cppreference","title":"Pointers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#pointers-to-functions","text":"Function pointers are declared as: <return_type> (*<pointer_name>)(<arg_1_type>, ..., <arg_n_type>) For example a function the_function returning bool and accepting int can be stored to pointer like this: bool (*ptr)(int) = &the_function The above example can be then simply called as bool b = ptr(2)","title":"Pointers to Functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#pointers-to-member-objects","text":"Pointers to member objects has a cumbersome syntax declaration: <member type> <class type>::*<pointer name> = ... usage: <object name>.*<pointer name> = ... Example: class My_class{ public: int my_member; } int main{ // declaring the pointer int My_class::*ptr = &My_class::my_member; // creating the instance My_class inst; // using the pointer to a member object inst.*ptr = 2; }","title":"Pointers to Member Objects"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#pointers-to-member-functions","text":"Pointers to member functions are even more scary in C++. We need to use the member object and the function adress and combine it in a obscure way: class My_class{ public: bool my_method(int par); } int main{ // creating the instance My_class inst; // assigning method address to a pointer bool (My_class::*ptr)(int) = &My_class::my_method; // using the pointer to a member function bool b = (inst.*ptr)(2) } The first unexpected change is the My_class before the name of the pointer. It's because unlike a pointer to function the_function which is of type (*)(int) , the pointer to my_method is of type (My_class::*)(int) The second difference is the call. We have t use the pointer to member binding operator .* to access the member of the specific instance inst . But this operator has a lower priority then the function call operator, so we must use the extra parantheses.","title":"Pointers to Member Functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#references","text":"References serve as an alias to already existing objects. Standard ( Lvalue ) references works the same way as pointers, with two differences: they cannot be NULL they cannot be reassigned The second property is the most important, as the assignment is a common operation, which often happens under do hood. In conslusion, reference types cannot be used in most of the containers and objets that needs to be copied .","title":"References"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#rvalue-references","text":"Rvalue references are used to refer to temporary objects. They eneable to prevent copying local objets by extending lifetime of temporary objects. They are mostly used as function parameters: void f(int& x){ } f(3); // 3 needs to be copied to f, because it is a temporary variable // we can add the rvalue overload void f(int&& x){ } f(3) // rvalue overload called, no copy","title":"Rvalue references"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#forwarding-references","text":"Forwarding references are references that preserves the value category (i.e. r/l-value reference, const ). They have two forms: function parameter forwarding references auto forwarding references","title":"Forwarding references"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-parameter-forwarding-references","text":"In a function template, if we use the rvalue reference syntax for a function parameter of whose type is a function template parameter, the reference is actually a forwarding reference. Example: template<class T> void f(T&& arg) // parameter is T& or T&& depending on the supplied argument Important details: it works only for non const references the reference type has to be a function template argument, not a class template argument","title":"Function parameter forwarding references"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#auto-forwarding-reference","text":"When we assign to `auto&&, it is a forwarding reference, not rvalue reference: auto&& a = f() // both type and value category depends on the return value of f() for(auto&& a: g(){ // same }","title":"auto forwarding reference"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#arrays","text":"cppreference There are two types of arrays: static , i.e., their size is known at compile type, and dynamic , the size of which is computed at runtime We can use the array name to access the first elemnt of the array as it is the pointer to that element.","title":"Arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#static-arrays","text":"Declaration: int a[nrows]; int a[nrwows][ncols]; // 2D int a[x_1]...[x_n]; // ND Initialization: int a[3] = {1, 2, 5} int b[3] = {} // zero initialization int c[3][2] = {{1,5}, {2,9}, {4,4}} // 2D int d[] = {1,5} // we can skip dimensions if their can be derived from data Note that the multi-dimensional syntax is just an abstraction for the programmers. The following code blocks are therefore equivalent: Matrix syntax const int rowns = 5; const int cols = 3; int matrix[rows][cols]; int main(){ for(int n = 0; n < rows; ++n){ for(int m = 0; m < cols; ++m){ Table[n][m] = (n + 1) * (m + 1); } } } Flat syntax const int rowns = 5; const int cols = 3; int matrix[rows * cols]; int main(){ for(int n = 0; n < rows; ++n){ for(int m = 0; m < cols; ++m){ Table[n * cols + m] = (n + 1) * (m + 1); } } } Using the matrix syntax adds the possibility to access the element of the array using multiple dimensions. But the underlying memory is the same.","title":"Static arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#dynamic-arrays","text":"Declaration: int* a = new int[size] For multiple dimensions, this syntax does not scale, i.e, only one dimension can be dynamic: int(*a)[4] = new int[rows][4] // static column count int(*b)[cols] = new int[rows][cols] // does not compile unless cols is a constant!","title":"Dynamic arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#array-to-pointer-implicit-conversion","text":"When we use the array name in an expression, it can be implicitly converted to a pointer to the first element of the array. This is true for both static and dynamic arrays. Example: int a[3] = {1, 2, 5} int* ptr = a; // ptr points to the first element of a This implicit conversion is called array-to-pointer decay .","title":"Array to pointer implicit conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#mutli-dimensional-dynamic-arrays","text":"To simulate multi-dimensional dynamic arrays, we have two options: use the flat syntax, as demonstrated on static arrays use aray of pointers to arrays Method | Pros | Cons --|--|-- Flat Syntax | Fast: single continuous allocations | different access syntax than static 2D arrays Array of pointers | Slow: one allocation per row, unrelated memory addresses between rows | same access syntax as static 2D arrays","title":"Mutli-dimensional dynamic arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#flat-array","text":"int* a = new int[rows * cols] Then we can access the array as: a[x * cols + y] = 5","title":"Flat array"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#array-of-pointers-to-array","text":"Declaration and Definition int** a= new int*[rows] for(int i = 0; i < rows; ++i){ a[i] = new int[cols] } Access is than like for static 2D array: a[x][y] = 5 . This works because the pointers can be also accessed using the array index operator ( [] ). In other words, it works \"by coincidence\", but we have not created a real 2D array.","title":"Array of pointers to array"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#auto-dealocation-of-dynamic-arrays","text":"We can replace the error-prone usage of new and delete by wraping the array into unique pointer: std:unique_ptr<int[]> a; a = std::make_unique<int[]>(size)","title":"Auto dealocation of dynamic arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#references-and-pointers-to-arrays","text":"cppreference The pointer to array is declared as <type> (*<pointer_name>)[<size>] : int a[5]; int (*ptr)[5] = &a; Analogously, the reference to array is declared as <type> (&<reference_name>)[<size>] : int a[5]; int (&ref)[5] = a;","title":"References and Pointers to arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-type","text":"A function type consist from the function arguments and the return type. The function type is written as return_type(arg_1_type, ..., arg_n_type) . Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo), int(double, double)>) // TRUE","title":"Function Type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#reference-to-function-and-pointer-to-function-types","text":"cppreference A refrence to function has a type return_type(&)(arg_1_type, ..., arg_n_type) . Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo)&, int(&)(double, double)>); // TRUE A pointer to function has a type: return_type(*)(arg_1_type, ..., arg_n_type) Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo)*, int(*)(double, double)>); // TRUE","title":"Reference to Function and Pointer to Function Types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#enumerations","text":"cppreference C++ supports simple enumerations, which are a set of named integer constants. The enumeration can be defined as: enum Color {red, green, blue}; // global scope enum class Color {red, green, blue}; // scoped, preferred There is no support for enum members like in Python, but we can use a wrapper class for that: class Color{ public: enum Value {red, green, blue}; Color(Value v): value(v){} // non-explicit constructor for easy initialization Value get_value() const {return value;} std::string to_string() const{ switch(value){ case Value::red: return \"red\"; case Value::green: return \"green\"; case Value::blue: return \"blue\"; } } private: Value value; } Color get_color(){ return Color::red; // this works due to the non-explicit constructor } int main(){ Color c = get_color(); std::cout << c.to_string() << std::endl; switch(c.get_value()){ case Color::red: std::cout << \"red\" << std::endl; case Color::green: std::cout << \"green\" << std::endl; case Color::blue: std::cout << \"blue\" << std::endl; } } For more complex code requireing automatic conversion to string and more, we can consider the magic_enum library . It supports the following features: enum to string conversion string to enum conversion enum iteration sequence of possible values","title":"Enumerations"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#complete-and-incomplete-types","text":"In many context, we have to supply a type with a requirement of being a complete type. So what types are incomplete? The void type is always incomplete Any structure without definition (e.g. using struct structure *ps; , without defining structure .) An array without dimensions is an incomplete type: int a[]; is incomplete, while int a[5]; is complete. An array of incomplete elements is incomplete. A type trait that can be used to determine whether a type is complete is described here .","title":"Complete and Incomplete Types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#aggregate-types","text":"Aggregate types are: array types class types that fullfill the following conditions no private or protected members no constructores declared (including inherited constructors) no private or protected base classes no virtual member functions The elements of the aggregate types can and are ment to be constructed using the aggregate initialization (see the local variable initialization section).","title":"Aggregate types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#type-conversion","text":"cppreference: implicit conversion In some context, an implicit type conversion is aplied. This happens if we use a value of one type in a context that expects a different type. The conversion is applied automatically by the compiler, but it can be also applied explicitly using the static_cast operator. In some cases where the conversion is potentially dangerous, the static_cast is the only way to prevent compiler warnings.","title":"Type Conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#numeric-conversion","text":"There are two basic types of numeric conversion: standard implicit conversion that can be of many types: this conversion is applied if we use an expression of type T in a context that expects a type U . Example: ```cpp void print_int(int a){ std::cout << a << std::endl; } int main(){ short a = 5; print_int(a); // a is implicitly converted to int } ``` usual arithmetic conversion which is applied when we use two different types in an arithmetic binary operation. Example: cpp int main(){ short a = 5; int b = 2; int c = a + b; // a is converted to int }","title":"Numeric Conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implicit-numeric-conversion","text":"","title":"Implicit Numeric Conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#integral-promotion","text":"Integral promotion is a coversion of an integer type to a larger integer type. The promotion should be safe in a sense that it never changes the value. Important promotions are: bool is promoted to int : false -> 0 , true -> 1","title":"Integral Promotion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#integral-conversion","text":"Unlike integral promotion, integral conversion coverts to a smaller type, so the value can be changed. The conversion is safe only if the value is in the range of the target type. Important conversions are:","title":"Integral Conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#usual-arithmetic-conversion","text":"cppreference This conversion is applied when we use two different types in an arithmetic binary operation. The purpose of this conversion is convert both operands to the same type before the operation is applied. The result of the conversion is then the type of the operands. The conversion has the following steps steps: lvalue to rvalue conversion of both operands special step for enum types special step for floating point types conversion of both operands to the common type The last step: the conversion of both operands to the common type is performed using the following rules: If both operands have the same type, no conversion is performed. If both operands have signed integer types or both have unsigned integer types, the operand with the type of lesser integer conversion rank (size) is converted to the type of the operand with greater rank. otherwise, we have a mix of signed and unsigned types. The following rules are applied: If the unsigned type has conversion rank greater or equal to the rank of the signed type, then the unsigned type is used. Otherwise, if the signed type can represent all values of the unsigned type, then the signed type is used. Otherwise, both operands are converted to the unsigned type corresponding to the signed type (same rank). Here especially the rule 3.1 leads to many unexpected results and hard to find bugs. Example: int main(){ unsigned int a = 10; int b = -1; auto c = b - a; // c is unsigned and the value is 4294967285 } To avoid this problem, always use the static_cast operator if dealing with mixed signed/unsigned types .","title":"Usual Arithmetic Conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#show-the-type-at-runtime","text":"It may be useful to show the type of a variable at runtime: for debugging purposes for logging to compare the types of two variables Note however, that in C++, there is no reflection support. Therefore, we cannot retrieve the name of the type at runtime in a reliable way . Instead, the name retrieved by the methods described below can depend on the compiler and the compiler settings.","title":"Show the Type at Runtime"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#resolved-complicated-types","text":"Sometimes, it is useful to print the type, so that we can see the real type of some complicated template code. For that, the following template can be used: #include <string_view> template <typename T> constexpr auto type_name() { std::string_view name, prefix, suffix; #ifdef __clang__ name = __PRETTY_FUNCTION__; prefix = \"auto type_name() [T = \"; suffix = \"]\"; #elif defined(__GNUC__) name = __PRETTY_FUNCTION__; prefix = \"constexpr auto type_name() [with T = \"; suffix = \"]\"; #elif defined(_MSC_VER) name = __FUNCSIG__; prefix = \"auto __cdecl type_name<\"; suffix = \">(void)\"; #endif name.remove_prefix(prefix.size()); name.remove_suffix(suffix.size()); return name; } Usage: std::cout << type_name<std::remove_pointer_t<typename std::vector<std::string>::iterator::value_type>>() << std::endl; // Prints: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > Source on SO","title":"Resolved complicated types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#show-the-user-provided-types-stdtype_info","text":"If we want to show the type of a variable provided by the user (e.g., by a function accepting std::any ), we can use the typeid operator which returns a std::type_info object.","title":"Show the user-provided types (std::type_info)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#standard-library-types","text":"","title":"Standard Library Types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#smart-pointers","text":"For managing resources in dynamic memory, smart pointers (sometimes called handles ) should be used. They manage the memory (alocation, dealocation) automatically, but their usage requires some practice. There are two types of smart pointers: std::unique_ptr for unique ownership std::shared_ptr for shared ownership","title":"Smart Pointers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#creation","text":"Usually, we create the pointer together with the target object in one call: std::make_unique<T>(<OBJECT PARAMS>) for unique pointer std::make_shared<T>(<OBJECT PARAMS>) for shared pointer These methods work well for objects, but cannot be used for arbitrary array initialization (only the empty/zero-initialized array can be created using these methods). For arbitrary array initialization, we need to use the smart pointer constructor: std::unique_ptr<int[]> ptr(new int[]{1, 2, 3}); Counter-intuitively, smart pointers created using the empty constructor of the respective pointer type does not default-construct the target object, but initialize the pointer to null instead: std::unique_ptr<My_class> ptr(std::null_ptr); // ptr is null std::unique_ptr<My_class> ptr(); // ptr is also null","title":"Creation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#shared-pointer","text":"Pointer to object with non-trivial ownership (owned by multiple objects).","title":"Shared Pointer"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdreference_wrapper","text":"cppreference Reference wrapper is a class template that can be used to store references in containers or aggregated objects. The disintinction from normal references is that the reference wrapper can be copied and assigned, so it does not prevent the copy/move operations on the object it belongs to. Otherwise, it behaves like a normal reference: it has to be assigned to a valid object and it cannot be null.","title":"std::reference_wrapper"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#strings","text":"In C++, there are two types of strings: std::string is an owning class for a string. std::string_view is a non-owning class for a string. Also, there is a C-style string ( char* ), but it is not recommended to use it in modern C++. The difference between std::string and std::string_view is best explained by a table below: std::string std::string_view Owning Yes No Null-terminated Yes No Size Dynamic Static Lifetime Managed by the string Managed by the underlying char sequence Can be constexpr No Yes and the following code: std::string_view sv = \"hello\"; // sv is a view of the string literal \"hello\" std::string s = \"hello\"; // s stores a copy of the string literal \"hello\"","title":"Strings"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#string-literals","text":"[cppreference]](https://en.cppreference.com/w/cpp/language/string_literal) The standard string literal is writen as \"literal\" . However, we need to escape some special characters in such literals, therefore, a raw string literal is sometimes more desirable: R\"(literal)\" . If our literal contains ( or ) , this is stil not enough, however, the delimiter can be extended to any string with a maximum length of 16 characters, for example: R\"lit(literal)lit\" . Raw string literals also useful for multi-line string literals .","title":"String Literals"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#formatting-strings","text":"The usage of modern string formating is either std::format from the <format> header if the compiler supports C++20 string formatting ( compiler support ) or fmt::format from the fmt library if not. Either way, the usage is the same: format(<literal>, <arguments>) where the literal is a string literal with {} placeholders and the arguments are the values to be inserted into the placeholders. The placeholders can be filled width argument identification, if we want to use the same argument multiple times or change the order in the string while keep the order of arguments in the function call or format specification. These two parts are separated by : , both of them are optional. The most common format specifications are: data type: d for decimal integer f for floating point number s for string width and precision, in the format <width>.<precision> . Both values can be dynamic: std::format(\"{:{}.{}f}\", a, b, c) formats a float number a with width b and precision c . The formating reference can be found in the cppreference","title":"Formatting strings"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#spliting-the-string-into-tokens","text":"Unfortunately, the STL does not provide a simple way to split the string into tokens like Python's split method or PHP's explode function. It is not even planned for the future. If we want to split a string on a character or pattern, the easiest way is to use the split view from the ranges library, which has a std::ranges::subrange as its element type: // get a range of character subranges auto parts = std::ranges::views::split(str, '-'); // iterate over the parts for (auto part : parts) { std::cout << part << std::endl; // prints the part // convert part to string std::string s(part.begin(), part.end()); // convert part to string (C++23) std::string s(std::from_range, part); } The last string constructor is only available in C++23, and moreover, it requires the stl::from_range tag. The std::string_view is equiped with a range constructor which does not require the tag in C++23. However, it is explicit, so its usage is limited: std::string_view s(part) // invalid std::string_view s = std::string_view(part) // valid in C++23","title":"Spliting the string into tokens"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#converting-string-to-int","text":"There are simple functions for converting std::string to numbers, named std::stoi , std::stoul , etc. See cppreference for details. For C strings, the situation is more complicated.","title":"Converting string to int"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#substring","text":"A substring can be obtained using a member function substr : str.substr(str.size() - 1, 1)) // returns the last character as a string","title":"Substring"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#change-the-case","text":"Unfortunatelly, the STL has case changing functions only for characters, so we need to iterate over the string ourselfs. The boost has a solution, however: #include <boost/algorithm/string.hpp> auto upper = boost::to_upper(str);","title":"change the case"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#building-strings","text":"Unlike other languages, in C++, strings are mutable, so we can build them using the + operator without performance penalty. Alternatively, we can use the std::stringstream class.","title":"Building strings"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#testting-for-whitespace","text":"To test if a string contains only whitespace characters, we can use the std::all_of algorithm: std::all_of(str.begin(), str.end(), [](char c){return std::isspace(c);})","title":"Testting for whitespace"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#date-and-time","text":"The date and time structure in C++ is std::tm . We can create it from the date and time string using std::get_time function: std::tm tm; std::istringstream ss(\"2011-Feb-18 23:12:34\"); ss >> std::get_time(&tm, \"%Y-%b-%d %H:%M:%S\");","title":"Date and time"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#collections","text":"std::array std::vector","title":"Collections"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#sets","text":"Normal set collection for C++ is std::unordered_set . By default, the set uses a Hash , KeyEqual and Allocator template params provided by std functions. However, they need to exist, specifically: std::hash<Key> std::equal_to<Key> std::allocator<Key> So either those specializations needs to be provided by the snadard library (check cppreference), or you have to provide it.","title":"Sets"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#providing-custom-hash-function","text":"There are two options for providing custom hash function for a type T s: implementing an explicit specialization of the template function std::hash<T> providing the Hash template param when constructing the hash The first method is prefered if we want to provide a default hash function for some type for which there is no hash function specialization in the standard library. The second method is prefered only when we want some special hash function for a type T for which std::hash<T> is already defined.","title":"Providing custom hash function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implementing-custom-hash-function","text":"First check whether the hash function is not provide by STL on cppreference . Then, many other hash specializations are implemented by boost, check the reference . If there is no implementation, we can implement the hash function as follows (example for set): template<> struct std::hash<std::unordered_set<const Request*>> { size_t operator()(const std::unordered_set<const Request*>& set) const { std::hash<const Request> hash_f; size_t sum{0}; for (const Request* r : set) { sum += hash_f(*r); } return sum; } }; Important implementation details: the function needs to be implemented inside std or annonymous namespace, not inside a custom namespace do not forget to add template<> above the function, this indicates that it is a template specialization.","title":"Implementing custom hash function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#maps","text":"The maps has similar requiremnts for keys as the requirements for set value types (see previous section). The hash map type is called std::unordered_map .","title":"Maps"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#geeting-value-by-key","text":"To access the map element, the array operator ( [] ) can be used. Note however, that this operator does not check the existence of the key, even if we do not provide a value. Example: std::unordered_map<int,std::string> map; map[0] = \"hello\" map[0] = \"world\" // OK, tha value is overwritten a = map[1] // a == map[1] == \"\" unintuitively, the default value is inserted if the key does not exist Therefore, if we just read from the map, it is safer to use the at() member function.","title":"Geeting value by key"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#inserting-into-map","text":"There are three options: map[key] = value; or map.insert({key, value}); map.emplace(key, value); There are some considerations with these options: 1 inserts the value into the map even if the key already exists, overwriting the previous value. 2 and 3 do not overwrite the new value, instead, they return the position in the map and the indicator of success ( true if the insertion happend). 1 requires the value to be default constructible and assignable 3 avoids the creation of temporary objects, it sends references to key and value directly to the map. 3 ignores the insertion if the key already exists.","title":"Inserting into map"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#tuples","text":"We have two standard class templates for tuples: std::pair for pairs std::tuple for tuples with unlimited size Although named differently, these class templates behaves mostly the same.","title":"Tuples"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#creating-tuples","text":"There are two ways of creating a tuple: constructor ( auto p = std::pair(...) ) initializer ( auto p = {} ) Beware that by default , the deduced types are decayed, i.e., const and references are removed and the tuple stores value types . If you need to store the reference in a tuple, you have to specify the type: auto p = std::pair<int, constr std::string&>(...) Also, beware that the RVO does not apply for tuple members. This means that if we store values types in the tuple, the types are copied/moved, and in conclusion, they have to by copyable/movable! This is the reason why we frequently use smart pointers in tuples even though we would reurn directly by value if we returned a single value.","title":"Creating tuples"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#creating-tuples-with-stdmake_pairstdmake_tuple","text":"TLDR: from C++17, there is no reason to use make_pair / make_tuple . There are also factory methods make_pair / make_tuple . Before C++17, argument deduction did not work for constructors, so there is a dedicated method for creating tuples. However, now we can just call the constructor and the template arguments are deduced from the constructor arguments. Also, the make_pair / make_tuple functions can only produce tuples containing values, not references (even if we specify the reference type in the make_pair / make_tuple template argument, the returned tuple will be value-typed).","title":"Creating tuples with std::make_pair/std::make_tuple"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#accessing-tuple-members","text":"The standard way to access the tuple/pair mamber is using the std::get function: auto tuple = std::tuple<int, std::string, float>(0, \"hello\", 1.5); auto hello = std::get<1>(tuple);","title":"Accessing tuple members"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#unpacking-tuples-into-variables","text":"There are two scenarios of unpacking tuples into variables: unpacking into new variables : for that, we use structured binding. unpacking into existing variables : for that, we use std::tie function.","title":"Unpacking tuples into variables"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#structured-binding","text":"If we don't need the whole tuple objects, but only its members, we can use a structured binding . Example: std::pair<int, int> get_data(); void main(){ const auto& [x, y] = get_data(); }","title":"Structured binding"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdtie","text":"If we want to unpack the tuple into existing variables, we can use the std::tie function: std::pair<int, int> get_data(); void main(){ int x, y; std::tie(x, y) = get_data(); }","title":"std::tie"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#unpacking-tuples-to-constructor-params-with-stdmake_from_tuple","text":"We cannot use structured binding to unpack tuple directly into function arguments. For normal functions, this is not a problem, as we can first use structured binding into local variables, and then we use those variables to call the function. However, it is a problem for parent/member initializer calls, as we cannot introduce any variables there. Luckily, there is a std::make_from_tuple template function prepared for this purpose. Example: std::tuple<int,float> get_data(){ ... } class Parent{ public: Parent(int a, float b){...} { class Child: public Parent{ public: Child(): Parent(std::make_from_tuple<Parent>(get_data())){} }","title":"Unpacking tuples to constructor params with std::make_from_tuple"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdoptional","text":"cppreference std::optional<T> is a class template that can be used to store a value of type T or nothing. The advantage over other options like null pointers or is that the std::optional is a value type, so it can wrap stack objects as well. The type T must satisfy std::is_move_constructible_v<T> (must be either movable or copyable). The usage is easy as the class has a value constructor from T and a default constructor that creates an empty optional. Also, the type T is convertible to std::optional<T> , and std::nullopt is convertible to an empty optional. Finally, std::optional<T> is convertible to bool , so it can be used in if statements. A typical usage is: class My_class{ public: My_class(int a, int b); } std::optional<My_class> f(){ ... return My_class(a, b); // or return {a, b}; // or, in case of fail return std::nullopt; } std::optional<int> a = f(); if(a){ // a has a value }","title":"std::optional"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#unions-and-variants","text":"The idea of a union is to store multiple types in the same memory location. Compared to the polymorphism, when we work with pointers and to templates, where the actual type is determined at compile time, the union actually has a shared memory for all the types. The union can be therefore used in cases where nor polymorphism neither templates are suitable. One example can be storing different unrelated types (e.g., std::string and int ) in a container. We cannot use templates as that require a single type. Nor we can use polymorphism, as the types are unrelated. The big disadvantage of unions is that they are not type safe. The compiler cannot check if the type we are accessing is the same as the type we stored. Therefore, we have to be very careful when using unions. Therefore, unless some special case, we should use std::variant instead of unions .","title":"Unions and Variants"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdvariant","text":"The declaration of std::variant is similar to the declaration of std::tuple : std::variant<int, double> v; The std::variant can store any of the types specified in the template parameters. The type of the stored value can be obtained using: std::holds_alternative method that returns a boolean value if the variant stores the type specified in the template parameter or std::variant::index method that returns the index of the stored value. this method can be used also in a switch statement as the index is integral The value can be accessed using: the std::get function, if we know the type stored in the variant or the std::get_if function if we are guesing the type. Both functions return a pointer to the stored value. Example: std::variant<int, double> v = 1; std::cout << v.index() << std::endl; // prints 0 std::cout << *std::get_if<int>(&v) << std::endl; // prints 1 A really usefull feature of std::variant is the std::visit method, which allows us to call a function on the stored value. The function is selected based on the type of the stored value. Example: std::variant<int, double> v = 1; std::visit([](auto&& arg) { std::cout << arg << std::endl; }, v); // prints 1 More on variants: cppreference cppstories","title":"std::variant"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#value-categories","text":"[cppreferencepreerecege/value_category). In many contexts, the value category of an expression is important in deciding whether the code compiles or not, or which function or template overload is chosen. Therefore, it is usefull to be able to read value categories. expression value types: lvalue , meaning left-value. An expression typically on the left side of compound expression a statement, e.g. variable, member, or function name. Also, lvalues expressions are are: function ratoalls to fuctions returning lvalue assignments ++a , --a and similar pre operators *a indirection string literal cast prvalue , meaning pure rvalue. It is either a result of some operand ( + , / ) or a constructor/initializer result. The foloowing expressions are prvalues: literals with exception of string literals, e.g.: 4 , true , nullptr function or operator calls that return rvalue (non-reference) a++ , a-- and other post operators arithmetic and logical expressions &a address of expression this non-type template parameters, unless they are references lambda expressions requires expressions and concept spetializations xvalue , meaning expiring value. These valaues usually represent lvalues converted to rvalues. Xvalue expressions are: function call to functions returning rvalue reference (e.g., std::move ). member object expression ( a.m ) if a is an rvlaue and m is a non-reference type glvalue = lvalue || xvalue . rvalue = prvlaue || xvalue .","title":"Value Categories"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#operators","text":"cppreferencen C++ supports almost all the standard operators known from other languages like Java, Python, or C#. Additionally, thsese operators can be overloaded. Note that the standard also supports alternative tokens for some operators (e.g., && -> and , || -> or , ! -> not ). However, these are not supported by all compilers. In MSVC, the /permissive- flag needs to be used to enable these tokens.","title":"Operators"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#user-defined-operators","text":"In C++ there are more operators than in other popular es like Python or Java. Additionally, these operators can be overloaded. See cppreferencen page for detailed description.","title":"User-defined Operators"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#comparison-operators","text":"","title":"Comparison Operators"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-comparison-operators","text":"cppreference . The != is usually not a problem, because it is implicitely generated as a negation of the == operator. However, the == is not generated by default, even for simple classes . To force the generation of a default member-wise comparison operator, we need to write: bool operator==(const My_class&) const = default; However, to do that, all members and base classes have to ae the operator == defined, otherwise the default operator will be implicitely deleted. The comparability can be checked with a std::equality_comparable<T> concept: staic_assert(std::equality_comparable<My_class>);","title":"Default Comparison Operators"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#control-structures","text":"C++ supports the control structures known from other languages like Java, Python, or C#. Here, we focus on the specifics of C++.","title":"Control Structures"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#switch-statement","text":"cppreference In C++, we can switch on integer types or enumeration types. Also, we can use classes that are implicitely convertible to integers or enums. Switch on string is not possible. The switch statement has the following syntax: switch(expression){ case value1: // code break; case value2: // code break; default: // code } However, it is usually a good idea to wrap each case in a block to create a separate scope for each case. Without it, the whole switch is a single block (contrary to if/else statements). The swich statements just jump to a case that matches the value, similarly to a goto statement. This can create problems, as for example variable initialization cannot be jumped over. The safe case statement looks like: switch(expression){ case value1:{ // code break; } case value2:{ // code break; } default:{ // code } }","title":"Switch Statement"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#functions","text":"cppreference","title":"Functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-declaration-and-definition","text":"In C and C++, functions must have a: - declaration (signature) that specifies the function name, return type, and parameters - definition that specifies the function body The declaration has to be provided before the first use (call) of the function. The definition can be provided later. The declaration is typically provided in a header file, so that the function can be used outside the translation unit. The definition is typically provided in a source file.","title":"Function Declaration and Definition"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#merged-declaration-and-definition","text":"If the function is not used outside the translation unit, the declaration and definition can be merged, i.e., the definition is itself a declaration. However, this is not recommended because after adding a corresponding declaration to one of the included headers (including libraries), the merged declaration/definition will become a definition of that function, which will be manifested as a linker error (multiple definitions of the function). Therefore, to control the visibility of the function, it is better to use other methods, provides in Section Visibility of Functions .","title":"Merged Declaration and Definition"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deciding-between-free-function-member-function-and-static-member-function","text":"Basically, you should decide as follows: Function needs access to instance -> member function Function should be called only by class members (i.e., member functions), so we want to limit its visibility, or we need to access static members of the class -> static member function Otherwise -> free function","title":"Deciding between free function, member function and static member function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#argument-parameter-conversions","text":"Arg/param | value | reference | rvalue -- |--|--|-- value | - | - | std::move reference | implicit copy | - | copy constructor rvalue | - | not possible | -","title":"Argument-parameter Conversions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-parameters","text":"Default function parameters in C++ works similarly to other languages: int add(int a, int b = 10); add(1, 2) // 3 add(1) // 11 However, the default parameters works only if we call the function by name. Therefore, we cannot use them in std::function and similar contexts. Example: std::function<int(int,int)> addf = add; std::function<int(int)> addf = add; // does not compile addf(1) // does not compile Also, the default parameters need to be values, not references or pointers. For references and pointers, we should use function overloading.","title":"Default Parameters"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-parameters-and-inheritance","text":"TLDR: do not use default parameters in virtual functions. The default parameters are resolved at compile time. Therefore, the value does not depend on the actual type of the object, but on the declared type of the variable. This have following consequences: the default parameters are not inherited A* a = new B(); a->foo() will call B::foo() , with the default parameters of A::foo() To prevent confusion with inheritence we should use function overloading instead of default parameters in virtual functions (like in Java).","title":"Default Parameters and Inheritance"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#return-values-and-nrvo","text":"For deciding the return value format, refer to the return value decision tree . Especially, note that NRVO is used in modern C++ and therefore, we can return all objects by value with no overhead most of the time. The NRVO works as follows: compiler tries to just tranfer the object to the parent stack frame (i. e. to the caller) without any move or copy if the above is not possible, the move constructor is called. if the above is not possible, the copy constructor is called. From C++17, the RVO is mandatory, therefore, it is unlikely that the compiler use a move/copy constructor. Consequently, most of the times, we can just return the local variable and let the rest to the compiler: unique_ptr<int> f(){ auto p = std::make_unique<int>(0); return p; // works, calls the move constructor automatically in the worst case (pre C++17 compiler) // return move( p ); // also works, but prevents NRVO } The NRVO is described also on cppreference together with initializer copy elision.","title":"Return values and NRVO"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-overlaoding","text":"Both normal and member funcions in C++ can be overloaded. The oveload mechanic, however, is quite complicated. There can be three results of overload resolution of some function call: no function fits -> error one function fits the best multiple functions fits the best -> error The whole algorithm of overload resolution can be found on cppreference . First, viable funcions are determined as functions with the same name and: with the same number of parameters with a greater number of parameters if the extra parameters has default arguments If there are no viable functions, the compilation fails. Otherwise, all viable functions are compared to get the best fit. The comparison has multiple levels. The basic principle is that if only one function fits the rules at certain level, it is chosen as a best fit. If there are multiple such functions, the compilation fails. Levels: Better conversion priority (most of the time, the best fit is found here, see conversion priority and ranking bellow) non-template constructor priority","title":"Function Overlaoding"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#conversion-prioritiy-and-ranking","text":"cppreference When the conversion takes priority during the best viable function search, we say it is better . The (incomplete) algorithm of determining better conversion works as follows: standard conversion is better than user defined conversion user defined conversion is better then elipsis ( ... ) conversion comparing two standard conversions: if a conversion sequence S1 is a subsequence of conversion sequence S2, S1 is better then S2 lower rank priority rvalue over lvalue if both applicable ref over const ref if both applicable","title":"Conversion prioritiy and ranking"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#conversion-sequence-ranks","text":"exact match promotion conversion : includes class to base conversion","title":"Conversion sequence ranks"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constructor-argument-type-resolution-in-list-initialization","text":"When we use a list initailization and it results in a constructor call, it is not immediatelly clear which types will be used for arguments as the initialization list is not an expression. These types are, however, critical for the finding of best viable constructor. The following rules are used to determine the argument types (simplified):","title":"Constructor argument type resolution in list initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#auto-return-type","text":"For functions that are defined inside declaration (template functions, lambdas), the return type can be automatically deduced if we use the auto keyword. The decision between value and reference return type is made according to the following rules: return type auto -> return by value return type auto& -> return by reference return type auto* -> return by pointer return type decltyype(auto) -> the return type is decltype(<RETURN EXPRESSION>) See more rules on cppreference Note that the auto return type is not allowed for functions defined outside the declaration (unless using the trailing return type).","title":"auto return type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-visibility","text":"The member function visibility is determined by the access specifier, in the same manner as the member variable visibility. For free functions , the visibility is determined by the linkage specifier . Without the specifier, the function is visible. To make it visible only in the current translation unit, we can use the static specifier. An equivalent way to make a function visible only in the current translation unit is to put it into an anonymous namespace : namespace { void f() {} } This way, the function is visible in the current translation unit, as the namespace is implicitly imported into it, but it is not visible in other translation units, because anonymous namespaces cannot be imported. One of the other approches frequently used in C++ is to put the function declaration into the source file so it cannot be included from the header. This solution is, however, flawed, unsafe, and therefore, not recommended . The problem is that this way, the function is still visible to the linker, and can be mistakenly used from another translation unit if somebody declare a function with the same name.","title":"Function visibility"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deleting-functions","text":"cppreference We can delete functions using the delete keyword. This is mostly used for preventing the usage of copy/move constructors and assignment operators. However, it can be used for any function, as we illustrate in the following example: class My_class{ print_integer(int a){ std::cout << a << std::endl; } // we do not want to print doubles even they can be implicitly converted to int print_integer(double a) = delete; }","title":"Deleting functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#classes-and-structs","text":"The only difference between a class and a struct is that in class, all members are private by default.","title":"Classes and structs"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#class-constants","text":"Class constants can be defined in two ways: static constexpr member variable if the constant type supports constexpr specifier, or static const member variable In the second case, we have to split the declaration and definition of the variable to avoid multiple definitions: // in the header file class My_class{ static const int a; } // in the cpp file const int My_class::a = 5;","title":"Class Constants"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#friend-declaration","text":"cppreference Sometimes, we need to provide an access to privat e members of a class to some other classes. In java, for example, we can put both classes to the same package and set the members as package private (no specifier). In C++, there is an even stronger concept of friend classes. We put a friend declaration to the body of a class whose private members should be accessible from some other class. The declaratiton can look as follows: Class To_be_accesssed { friend Has_access; } Now the Has_access class has access to the To_be_accesssed 's private members. Note that the friend relation is not transitive, nor symetric, and it is not inherited.","title":"Friend declaration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-friends","text":"If we want a template to be a friend, we can modify the code above: class To_be_accesssed { template<class T> friend class Has_access; } Now every Has_access<T> is a friend of To_be_accesssed . Note thet we need to use keyword class next to friend . We can also use only a template spetialization: class To_be_accesssed { friend class Has_access<int>; } or we can bound the allowed types of two templates togehter if both Has_access and of To_be_accesssed are templates: template<class T> class To_be_accesssed { friend class Has_access<T>; }","title":"Template friends"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#initialization-and-assignment","text":"","title":"Initialization and Assignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#loacal-variables-initializationassignment","text":"Initialization happens in many contexts : in the declaration in new expression function parameter initialization return value initialization The syntax can be: (<expression list>) = expression list {<initializer list>} Finally, there are multiple initialization types, the resulting initialization type depends on both context and syntax: Value initialization: std::string s{}; Direct initialization: std::string s{\"value\"} Copy initialization: std::string s = \"value\" List initialization: std::string s{'v', 'a', 'l', 'u', 'e'} Aggregate initialization: char a[3] = {'a', 'b'} Reference initialization: char& c = a[0] Default initialization: std::string s","title":"Loacal variables initialization/assignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#list-initialization","text":"List initialization initializes an object from a list. he list initialization has many forms, including: My_class c{arg_1, arg_2} My_class c = {arg_1, arg_2} my_func({arg_1, arg_2}) return {arg_1, arg_2} The list initialization of a type T can result in various initializations/constructios depending on many aspects. Here is the simplified algorithm: 1. T is aggregate -> aggregate initialization 2. The initializer list is empty and T has a default constructor -> value initialization 3. T has an constructor accepting std::initializer_list -> this constructor is called 4. other constructors of T are considered, excluding explicit constructors","title":"List initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#value-initialization","text":"cppreference This initializon is performed when we do not porvide any parameters for the initialization. Depending on the object, it results in either defualt or zero initialization.","title":"Value initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#aggregate-initialization","text":"Aggregate initialization is an initialization for aggregate types. It is a form of list initialization. Example: My_class o1{arg_1, arg_2}; My_class o2 = {arg_1, arg_2}; // equivalent The list initialization of type T from an initializer list results in aggregate initialization if these conditions are fullfilled: the initializer list contains more then one element T is an aggregate type","title":"Aggregate initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#nested-initialization","text":"It is not possible to create nested initializatio statements like: class My_class{ int a, float b; public: My_class(ina a, float b): a(a), b(b) } std::tuple<int, My_class>{2, {3, 2.5}} // does not compile std::tuple<int, My_class>{2, My_class{3, 2.5}} // correnct version","title":"Nested initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#member-initializationassignment","text":"There are two ways of member initialization: default member initialization initialization using member initializer list And then, there is an assignment option in constructor body . Reference: default member initialization constructor and initializer list One way or another, all members should be initialized at the constructor body at latest , even if we assign them again during all possible use cases. Reason: some types (numbers, enums, ...) can have arbitrary values when unassigned. This can lead to confusion when debugging the class, i.e., the member can appear as initialized even if it is not. easy support for overloading constructors, we can sometimes skip the call to the constructor with all arguments we can avoid default arguments in the constructor It is important to not use virtual functions in member initialization or constructor body , because the function table is not ready yet, so the calls are hard wired, and the results can be unpredictable, possibly compiler dependent.","title":"Member Initialization/Assignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-member-initialization","text":"Either a brace initializer : My_class{ int member{1} } or an equals initializer : My_class{ int member = 1 }","title":"Default Member Initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#member-initializer-list","text":"Either using direct initialization (calling constructor of member ): My_class{ My_class(): member(1){ } or list initialization : My_class{ My_class(): member{1}{ }","title":"Member Initializer List"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constructor-body","text":"My_class{ My_class(){ member = 1 } }","title":"Constructor Body"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#comparison-table","text":"Ordered by priority, i.e., each method makes the methods bellow ignored/ovewritten if applied to the same member. Type | In-place | works for const members --| --|-- Constructor body | no | no Member initializer list | yes | yes Default member initializer | yes, if we use direct initialization | yes","title":"Comparison Table"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constructors-and-special-member-functions","text":"cppreference Special member functions are member functions that are someetimes defined implicitely by the compiler. The special member functions are: default (no parameter) constructor copy Cconstructor copy sssignment move constructor move assignment destructor Along with the comparison operators, these are the only functions that can be defaulted (see below).","title":"Constructors and Special Member Functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constructor","text":"","title":"Constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#defualt-variant","text":"The default constructor just create an empty object. The default constructor is not implicitly generated if: there is anothe constructor declared, including copy and move constructor there is some member that cannot be defaulty initialized","title":"Defualt Variant"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#explicit-constructor","text":"Sometimes, a normal constructor can lead to unexpected results, especially if it has only a single argument: class My_string { public: String(std::string string); // convert from std::string String(int length); // construct empty string with a preallocated size }; String s = 10; // surprise: empty string of size 10 istead of \"10\" To prevent these surprising conversion, we can mark the constructor explicit . The explicit keyword before the constructor name prevents the assigment using this constructor. The explicit constructor has to be explicitelly called.","title":"Explicit constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#call-one-constructor-from-another","text":"We can call one constructor from another using the delegating constructor . The syntax is: class My_class{ public: My_class(int a, int b): a(a), b(b){} My_class(int a): My_class(a, 0){} // delegating constructor } This way, we can call another constructor of the same class, or of the base class.","title":"Call one constructor from another"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#copy-constructor","text":"cppreference A copy constructor is called if an object is initialized from another object unless the move constructor is called as a better fit or the call is optimized out by copy elision . Some examples: initializing a new object from an existing object: My_class a; My_class b = a; // copy constructor called My_class c(a); // copy constructor called passing an object to a function by value: void f(My_class a){...} My_class a; f(a); // copy constructor called returning an object by value where the type is not movable and the compiler cannot optimize the call out. we call the copy constructor directly","title":"Copy Constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implicit-declaration-and-implicit-deletion","text":"The copy constructor for type T is implicitely-declared if T has no declared user-defined copy constructors. If some there are some user-defined copy constructors, we can still force the implicit declaration of the copy constructor using the default keyword However, the implicit declaration does not mean that the copy constructor can be used! This is because the copy constructor can be implicitely defined as deleted . This happens if any of the following conditions is true: T has a non-static data member that cannot be copied. This can happen if any of the following is true: it has a deleted copy constructor, the copy constructor is inaccessible ( protected, private ) the copy constructor is ambiguous (e.g., multiple inheritance) T has a base class that cannot be copied, i.e., 1, 2, or 3 applies to at least one base class T has a non-static data member or base class with inaccessible destructor T has a rvlaue data member T has a user-defined move constructor or move assignment operator (this rule does not apply for defaulted copy constructor) The default implementationof copy constructor calls recursively the copy constructor of all base classes and on all members. For a pointer member, the copy object\u2019s member points to the same object as the original object\u2019s member","title":"Implicit declaration and implicit deletion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#checking-if-a-class-is-copy-constructible","text":"We can check if a class is copy constructible using the std::is_copy_constructible type trait.","title":"Checking if a class is copy constructible"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#copy-assignment","text":"Copy Assignment is needed when we use the = operator with the existing class instances, e.g.: Class instanceA {}; Class instanceB; instanceB = instance A","title":"Copy Assignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#move-constructor","text":"cppreference Move constructor semantic is that the new object takes the ownership of the resources of the old object. The state of the old object is unspecified, but it should not be used anymore. Move constructor is typically called when the object is initaialized from xvalue (but not prvalue!) of the same type. Examples: returning xvalue: Type f(){ Type t; return std::move(t); } passing argument as xvalue: f(Type t){ ... } Type t f(std::move(t)); initializing from xvalue: Type t; Type t2 = std::move(t); Note that for prvalues, the move call is eliminated by copy elision . Therefore, some calls that suggest move constructor call are actually optimized out: Type f(){ Type t; return t; // no move constructor call, copy elision } Type t = T(f()) // no move constructor call, copy elision Move constructor is needed: to cheaply move the object out from function if RVO is not possible to store the object in vector without copying it Note that a single class can have multiple move constructors, e.g.: both Type(Type&&) and Type(const Type&&) .","title":"Move Constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implicit-declaration-and-implicit-deletion_1","text":"The move constructor for type T is implicitely-declared if T has no declared copy constructors, copy assignment operators, move assignment operators, or destructors. If some of the above is declared, we can still force the implicit declaration of the move constructor using the default keyword However, that does not mean that the move constructor can be used! This is because the move constructor can be implicitely defined as deleted . This happens if any of the following conditions is true: T has a non-static data member that cannot be moved. A member cannot be moved if any of the following is true: it has a deleted, inaccessible (protected, private), or ambiguous move constructor, it is a reference, it is const -qualified T has a base class that cannot be moved, i.e., 1, 2, or 3 applies to at least one base class T has a non-static data member or base class with inaccessible destructor","title":"Implicit declaration and implicit deletion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#checking-if-a-class-is-move-constructible","text":"We can check if a class is move constructible using the std::is_move_constructible type trait. However, the std::is_move_constructible does not check if the move constructor is accessible! Instead it checks if the call to the move constructor is valid (can success, compiles). The call can success if the move constructor is accessible, but it can also success if it is not accessible, but the class has a copy constructor, which is used instead. To check if the move constructor is accessible, we have to manually check the conditions, or disable the copy constructor.","title":"Checking if a class is move constructible"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#move-assignment","text":"","title":"Move Assignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#trivial-special-member-functions","text":"The special member functions are called trivial if they contain no operations other then copying/moving the members and base classes. For a special member function of type T to be trivial, all of the following conditions must be true: it is implicitly-declared or defaulted T has no virtual functions T has no virtual base classes the constructor for all direct base classes is trivial the constructor for all non-static data members is trivial","title":"Trivial special member functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#destructor","text":"We need destructor only if the object owns some resources that needs to be manually deallocated","title":"Destructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#setting-special-member-functions-to-default","text":"","title":"Setting special member functions to default"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#rules","text":"if you want to be sure, delete everything you don\u2019t need most likely, either we need no custom constructors, or we need three (move and destructor), or we need all of them.","title":"Rules"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#rules-for-typical-object-types","text":"","title":"Rules for Typical Object Types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#simple-temporary-object","text":"the object should live only in some local context we don\u2019t need anything","title":"Simple Temporary Object"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#unique-object","text":"usually represents some real object usually, we need constructors for passing the ownership: move constructor move assignment noe sin","title":"Unique Object"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-object","text":"copyable object We need copy constructor copy assignment move constructor move assignment","title":"Default Object"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#const-vs-non-const","text":"The const keyword makes the object non-mutable. This means that: it cannot be reassigned non-const member functions of the object cannot be called The const keyword is usually used for local variables, function parameters, etc. For members, the const keyword should not be used , as it sometimes breaks the move operations on the object. For example we cannot move f om a const std::unique_ptr<T> object. While this is also true for local variable, in members, it can lead to hard to find compilation errors, as a single const std::unique_ptr<T> member deep in the object hierarchy breaks the move semantic for the whole class and all subclasses.","title":"Const vs non-const"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#avoiding-duplication-between-const-and-non-const-version-of-the-same-function","text":"To solve this problem without threathening the const-correctness, we need to implement the const version of a function and call it from the non-const one with double type cast: one that converts this to const, so we can call the const version of the function another one that removes const from the return value Example: const Content& get_content(unsigned index) const { Content content = ... // complicated code to get the right content return content; } Content& get_content(unsigned index){ return const_cast<Content&>(std::as_const(this*).get_content()); } there are no common supercalss or i## Const/non const overloads and inheritance Normally, the compiler can safely choose the best match between const and non-const overloads. The problem can happen when each version is in a different place in the class hierarchy. Example: class Base { public: const int& get() const { return some; } protected: int some; }; class A : public virtual Base { public: int& get() { return some; } }; class B : public A {}; B test; test.get(); // ambiguous function error The problem is that the overload set is created for each class in the hierarchy separately. So if the overload was resolved prior the virtual function resolution, we would have only one version (non-const), which would be chosen, despite not being the best overload match in both overload sets. To prevent such unexpected result, some compilers (GCC) raise an ambiguous function error in such situations. To resolve that, we can merge the overload sets in class B : class B : public A { using Base:get; using A:get; };","title":"Avoiding duplication between const and non-const version of the same function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#avoiding-duplication-between-const-and-non-const-version-of-the-same-function_1","text":"To solve this problem without threathening the const-correctness, we need to implement the const version of a function and call it from the non-const one with double type cast: one that converts this to const, so we can call the const version of the function another one that removes const from the return value Example: const Content& get_content(unsigned index) const { Content content = ... // complicated code to get the right content return content; } Content& get_content(unsigned index){ return const_cast<Content&>(std::as_const(this*).get_content()); }","title":"Avoiding duplication between const and non-const version of the same function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#io-and-filesystem","text":"The simple way to print to standard input is: std::cout << \"Hello world\" << std::endl; To return to the begining of the line and overwrite the previous output, we can use the '\\r' character: std::cout << \"Hello world\" << '\\r' << std::flush;","title":"IO and Filesystem"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#file-path-manipulation","text":"Although we can use strings to work with file paths in C++, the standard format which is also easy to use is std::filesystem::path from the filesystem library . Basic operations: To create a path , we jusct call std::filesystem::path(<string path>) . We can easily join two paths by auto full_path = <path 1> / <path 2> ; To get the asolute path , we call std::filesystem::absolute(<path>) to get the path as CWD/<path> std::filesystem::canonical(<path>) to get the dots resolved. Note that this method throws exception if the path does not exists. The path to the current working directory can be obtained by calling std::filesystem::current_path() and set using std::filesystem::current_path(<path>) . To change the file extension (in the C++ representation, not in the filesystem), we can call the replace_extension method.","title":"File path manipulation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#filesystem-manipulation","text":"cppreference","title":"Filesystem manipulation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#copying","text":"To copy, we can use std::filesystem::copy(<source path>, <destination path>[, <options>]) function. The options parameter type is std::filesystem::copy_options . This enum is a bitmask type, therefore, multiple options can be combined using the | operator. Example: auto options = std::filesystem::copy_options::recursive | std::filesystem::copy_options::overwrite_existing; std::filesystem::copy(\"C:/temp/data\", \"c:/data/new\", options); Note that unlike the unix cp command, the copy function does not copy the directoy itself , even if the destination directory exists. Suppose we have two direcories: C:/temp/new C:/data/ And we want to copy the new folder, so that the result is: C:/data/new/ . In bash, this will be: cp -r C:/temp/new C:/data/ While in C++, we need to do: std::filesystem::copy(\"C:/temp/new\", \"C:/data/new\", std::filesystem::copy_options::recursive);","title":"Copying"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#creating-directories","text":"To create a directory, we can use std::filesystem::create_directory(<path>) function. This function fails if the parent directory does not exist. To create the parent directories as well, we can use std::filesystem::create_directories(<path>) function.","title":"Creating directories"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#removing-files-and-directories","text":"To remove a file or an empty directory, we can use std::filesystem::remove(<path>) function. To remove a content of a directory we can use std::filesystem::remove_all(<path>) function listed on the same page of cppreference.","title":"Removing files and directories"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#other-useful-functions","text":"std::filesystem::exists(<path>) std::filesystem::is_directory(<path>) std::filesystem::is_regular_file(<path>) std::filesystem::is_empty(<path>)","title":"Other useful functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#simple-line-by-line-io","text":"","title":"Simple line by line IO"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#input","text":"For input, we can use std::ifstream : std::ifstream file; file.open(<path>); std::string line; while (std::getline(file, line)) { // do something with the line } file.close(); The important thing is that we need to check whether the open call was successful. The open function never throws an exception, even if the file does not exist , which is a common case. Instead, it only sets the failbit of the stream. Without some check, the failure is hidden as an ifstream in a fail state behaves as if it was empty.","title":"Input"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#output","text":"For line by line output, we use std::ofstream : std::ofstream file; file.open(<path>); batch_file << \"first line\" << std::endl; batch_file << \"second line\" << std::endl; ... batch_file.close();","title":"Output"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#load-whole-file-into-string","text":"Again, we use the std::ifstream , but this time, we also use the std::istreambuf_iterator to read the whole file into a string: std::ifstream file(<path>); std::string content(std::istreambuf_iterator<char>{file}, {}); Here, the std::istreambuf_iterator<char> is created using initialization instead of the constructor so that the local variable is not confused with function declaration. The {} is used to create an empty string, which is the end of the range for the iterator.","title":"Load whole file into string"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#csv","text":"","title":"csv"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#input_1","text":"","title":"Input"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#output_1","text":"For csv output, we can usually use the general line-by-line approach.","title":"Output"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#yaml","text":"For YAML, we can use the yaml-cpp library. To test whether a YAML::Node contains a certain key , we may use the [] operator, as it does not create a new node (unlike the stl containers): YAML::Node node; if (node[\"key\"]) { // do something } The iteration over the keys is done using YAML::const_iterator : for (YAML::const_iterator it = node.begin(); it != node.end(); ++it) { std::string key = it->first.as<std::string>(); YAML::Node value = it->second; }","title":"YAML"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#inheritance","text":"Inheritance in C++ is similar to other languages, here are the important points: To enable overiding, a member function needs to be declared as virtual . Otherwise, it will be just hidden in a child with a function with the same name, and the override specifier cannot be used (see Shadowing). Multiple inheritance is possible. No interfaces. Instead, you can use abstract class with no data members. Virtual functions without implementation needs = 0 at the end of the declaration (e.g.: virtual void print() = 0; ) a type is polymorphic if it has at least one virtual function. I.e., the inheritance itself does not make the type polymorphic.","title":"Inheritance"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#polymorphism","text":"Polymorphism is a concept for abstraction using which we can provide a single interface for multiple types that share the same parent. In C++, to use the polymorphism, we need to work with pointers or references . Imagine that we have these two class and a method that can process the base class: class Base { }; class Derived: public Base { }; void process_base(Base* base) { } Now we can use it lake this: Derived* derived = new Derived(); Base* base = derived; // we easilly can convert derived to base process_base(base); process_base(derived); // we can call the function that accepts a base pointer with a derived pointer We can do the same with smart pointers: void process_base_sh(std::shared_ptr<Base> base) { } std::shared_ptr<Derived> derived_sh = std::make_shared<Derived>(); std::shared_ptr<Base> base_sh = derived_sh; process_base_sh(base_sh); process_base_sh(derived_sh);","title":"Polymorphism"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#shadowinghiding-why-is-a-function-from-parent-not-available-in-child","text":"Members in child with a same name as another members in parent shadows those members (except the case when the parent member is virtual). When a member is shadowed/hiden, it is not available in the child class and it cannot be called using the child class instance. This can be counter-intuitive for functions as the shadowing considers only the name, not the signature . Example: class Base { public: void print() { printf(\"Base\\n\"); } }; class Child: public Base { public: void print(std::string type) { printf(\"Child \" + type + \"\\n\"); } }; int main() { Child child; child.print(); // does not compile, as the print() is hidden by print(std::string) return 0; }","title":"Shadowing/Hiding: why is a function from parent not available in child?"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#how-to-call-a-hidden-function","text":"There are two ways how to call a hideen function: we can use the using declaration in the child to introduce the hidden function: c++ class Child: public Base { public: using Base::print; // now the print() is available in Child void print(std::string type) { printf(\"Child \" + type + \"\\n\"); } }; Usiang a fully qualified name of the method: c++ int main() { Child child; child.Base::print(); return 0; }","title":"How to call a hidden function?"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constructors","text":"Parent constructor is always called from a child. By default, an empty constructor is called. Alternatively, we can call another constructor in the initializer. When we do not call the parent constructor in the child's initializer and the parent has no empty constructor, a compilation error is raised.","title":"Constructors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#enablinging-parent-constructors-in-child","text":"Implicitly, all methods from parent classes are visible in child, with exception of constructors. Constructors can be inherited manually with a using declaration, but only all at once. To enable only some constructors, we need to repeat them manually as child constructors and call parent construcors from them.","title":"Enablinging Parent Constructors in Child"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#inheritance-and-constructorsdestructors","text":"To prevent the future bugs with polymorphic destruction calls, it's a good habit to declare a public virtual destructor in each base class : class Base{ public: virtual ~Base() = default; } Otherwise, the following code will not call the child destructor: Child* child = new Child(); Base* base = (Base) child; delete base; But when defining destructor, constructor and move operations are not impliciotely generated. Moreover, the copy operations are generated enabling a polymorphic copy, which results in slicing. Therefore, the best approach for the base class is to: declare the virtual destrucor as default declare the default constructor . We need a default constructor, unless we use a diferent constructor and we want to disable the default one. declare the copy and move operations as protected . This way, the polymorpic copy is not possible, but proper copy/move operations are generated for every child class.","title":"Inheritance and Constructors/Destructors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#initializing-base-class-members","text":"The base class members cannot be initialized in the child constructor initializer. Instead, we need to create a constructor in the base class and call it from the child constructor initializer.","title":"Initializing base class members"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#slicing","text":"Polymorphism does not go well with value types. When a value type is copied, the only part that remains is the part writen in the code. That means that copying base_2 = base_1 result in a new Base object in base_2 , even if base_1 is an instance of child. Abstract classes therefore cannot be used as function value arguments at all . To pass a polymorphic type as a value to a library function, we need a copyable wrapper that forwards all calls to the undelying polymorphic type.","title":"Slicing"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#checking-the-type","text":"There is no equivalent of Java's instanceof in C++. To check the type. it is possible to use dynamic cast: Child& child = dynamic_cast<Child&>(parent) In case of failure, std::bad_cast is thrown. To prevent exceptions (i.e., we need the type check for branching), we can use pointers: Child* child = dynamic_cast<Child*>(&parent) In this case, if the cast fails, then child == nullptr . Note that to use the dynamic_cast on a type, the type, the type needs to have at least one virtual method . However, this should not be an issue as the type should have at least a virtual destructor.","title":"Checking the Type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#covariant-return-type","text":"Covariant return type is a concept of returning a narower type id derived class than the return type specified in base. Example: class Base { public: virtual Base& get() = 0; }; class Derived: public Base{ public: Derived& get() override { return *this; } }; It works with template classes too: template<class T> class Derived_template: public Base { public: Derived_template<T>& get() override { return &this; } };","title":"Covariant Return Type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#use-method-from-parent-to-override-a-method-from-other-parent","text":"Unlike in java, a parent method cannot be used to implement an interface of a child . Example: class Interface { public: virtual void print() = 0; }; class Base { public: virtual void print() { printf(\"Base\\n\"); } }; class Child: public Base, public Interface { public: }; int main() { Child child; // does not compile, as Child is an abstract class child.print(); return 0; } The above code does not compile as in C++, the parent print() method is not used as an impementation of print() from the interface (like it works e.g. in Java). There simplest solution to this problem is to override the method in Child and call the parent method staticaly: class Child: public Base, public Interface { public: void print() override { Base::print(); } };","title":"Use Method from Parent to Override a Method from Other Parent"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#multiple-inheritance-and-virtual-base-classes","text":"wiki cppreference Multiple inheritance is possible in C++. However, it can lead to some problems. Consider the following example: class A { public: int a; }; class B: public A {}; class C: public A {}; class D: public B, public C {}; It may not be obvious, but the class D has two instances of A in it. This is because the B and C both have their own instance of A . This is certainly not what we want as this way, we have two copies of A::a in D , which are only accessible using qualified names ( D::B::a and D::C::a ) and which can have different values.","title":"Multiple inheritance and virtual base classes"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#virtual-inheritance","text":"To mitigate this problem, we can use the virtual inheritance . The virtual inheritance is used when we want to have only one instance of a base class in a child class, even if the base class is inherited multiple times. To use the virtual inheritance, we need to declare the base class as virtual in all child classes: class A { public: int a; }; class B: public virtual A {}; class C: public virtual A {}; class D: public B, public C {};","title":"Virtual Inheritance"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#multiple-copymove-calls-with-virtual-inheritance","text":"However, this solves only the problem of having multiple instances of the same base class. But there are also problems with the copy and move operations. In the above example, if the class D is copied or moved, it calls the copy/move operations of B and C , which in turn call the copy/move operations of A . This means that the A is copied/moved twice , which is not what we want. To solve this we need to manually define the copy/move operations of classes in the hierarchy so that the copy/move operations of the base class are called only once. However this can be a complex task. Also, it can backfire later when we extend the hierarchy.","title":"Multiple copy/move calls with virtual inheritance"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#other-sources","text":"SO answer SO answer 2","title":"Other sources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#templates","text":"The templates are a powerful tool for: generic programming, zero-overhead interfaces, and metaprogramming. Although they have similar syntax as generics in Java, they are principialy different both in the way they are implemented and in the way they are used. There are two types of templates: function templates class templates","title":"Templates"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#syntax","text":"","title":"Syntax"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-declaration","text":"Both for classes and functions, the template declaration has the following form: template<<template parameters>> The template parameters can be: type parameters: class T value parameters: int N concept parameters: std::integral T","title":"Template Declaration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-definition","text":"The definition of template functions or functions fo the template class requires the template declaration to be present. The definition has the following form: template<<template parameters>> <standard function definition> Here, the template parameters are the function template parameters if we define a template function, or the class template parameters if we define a function of a template class. If the template function is a member of a template class, we have to specify both the template parameters of the function and the template parameters of the class: template<<class template parameters>> template<<function template parameters>> <standard class function definition> Note that the template definition has to be in the header file , either directly or included from another header file. This includes the member function definitions of a template class, even if they are not templated themselves and does not use the template parameters of the class.","title":"Template definition"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#organization-rules","text":"*.h : declarations *.tpp template definitions *.cpp non-template definitions. For simplicity, we include the tpp files at the end of corresponding header files. If we need to speed up the compilation, we can include the tpp files only in the source files that needs the implementations , as described on SE To speed up the build it is also desireble to move any non-template code to source files , even through inheritance, if needed.","title":"Organization rules"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#providing-template-arguments","text":"A template can be instantiated only if all the template arguments are provided. Arguments can be: provided explicitly: std::vector<int> v; or sum<int>(1,2) deduced from the initialization (classes): std::vector v = {1,2,3}; from the context (functions): sum(1,2); defaulted: template<class T = int> class A {}; template<class T = int> int sum<T>(T a, T b = 0) { return a + b; } auto s = sum(1, 2); A a(); If we want the template arguments to be deduced or defaulted, we usually use the <> : template<class T = int> class A {}; A<> a(); // default argument is used std::vector<A<>> v; // default argument is used In some cases, the <> can be ommited, e.g., when declaring a variable: A a; // default argument is used // but std::vector<A> v; // error, the A is considered a template here, not the instantiation The rules for omitting the <> are quite complex. Therefore, it is better to always use the <> when we want to use the default arguments.","title":"Providing Template Arguments"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#rules-for-omitting-the","text":"We can ommite the <> in the following cases: when declaring a variable: A a; when using the type in a function call: f(A()); when instantiating a template class: class B: public A {}; We cannot ommite the <> in the following cases: When we use the template as a nested type: std::vector<A<>> v; , not std::vector<A> v; in the return type of a function: A<> f() , not A f() When declaring an alias: using B = A<> not using B = A for template template parameters.","title":"Rules for omitting the &lt;&gt;"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-template-arguments","text":"Default template arguments can be used to provide a default value for any template parameter except parameter packs. For template classes, there is a restriction that after a default argument is used, all the following parameters must have a default argument as well, except the last one wchich can be parameter pack.","title":"Default Template Arguments"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-argument-deduction","text":"Details on cppreference . Template argument deduction should work for: constructors function and operator calls storing the function pointer","title":"Template Argument Deduction"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#class-template-argument-deduction-ctad","text":"Details on cppreference . The main difference from the function templete argument deduction is that in CTAD, all the template arguments needs to be specified, or all must not be specified and must be deducible. Apart from that, there are more subtle differences arising of a complex procedure that is behind CTAD. We explain CTAD principle using a new concept (not a C++ concept :) ) called deduction guides .","title":"Class Template Argument Deduction (CTAD)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deduction-guides","text":"The CTAD use so called deductione guides to deduce the template parameters. Deduction guides can be either implicit or explicit. To demonstrate the principle, let's first start with user-defined deduction guides.","title":"Deduction Guides"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#user-defined-deduction-guides","text":"Let's have an iterator wrapper class below: template<class E, Iterator<E> I> class Iter_wrapper{ public: explicit Iter_wrapper(I iterator){ ... } ... }; Here, the argument E cannot be deduced from argument I , despite the use of the Iterator concept may indicate otherwise. We can still enable the deduction by adding the following deduction guide: template<class I> Iter_wrapper(I iterator) -> Iter_wrapper<decltype(*iterator),I>; Here, the part left from -> represents the constructor call that should be guided, and the part right from -> defines the argument types we want to deduce. Some more details about user defined deduction guides are also on the Microsoft Blog .","title":"User defined deduction guides"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implicit-deduction-guides","text":"The vast majority of deduction guidedes used in CTAD are implicit. The most important implicit deduction guides are: constructor deduction guides copy deduction guides The copy deduction guide has the following form: template<<class template parameters>> <class>(<class><class template parameters> obj) -> <class><class template parameters>; For a simple wrapper class: template<class C> class Wrapper{ ... } template<class C> Wrapper(Wrapper<C> obj) -> Wrapper<C>; // implicitelly defined copy deduction guide The constructor deduction guides has the following form: template<<class template parameters>> <class>(<constructor arguments>) -> <class><class template parameters>; For a simple wrapper class: template<class C> class Wrapper{ Wrapper(T&& ref); } template<class C> Wrapper(C&&) -> Wrapper<C>; // implicitelly defined constructor deduction guide","title":"Implicit deduction guides"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deduction-guides-resolution","text":"Note that CTAD is a process independent of the constructor overload! . First an appropriate deduction guide is used to deduce the class template argumnets, this process can fail if there is no guide. Only then, the overload resolution begins. Most of the time, it is not so important and we can just look at the constructor that is chosen by the constructor overload resolution process and see the used deduction guids and consequently, the resulting template arguments. Sometimes, however, this simplified understanding can lead to confusing results: template<class C> class Wrapper{ Wrapper(T&& ref); Wrapper(double&& ref); // special overload for double } auto w1 = Wrapper(1.5) // the double overload is called In the above example, it may be surprising that the second constructor can be called, as it does not have the class argument present, so the implicit deduction guide cannot work: template<class C> Wrapper(double&&) -> Wrapper<C>; // C unknown! However, it compiles and works, because the deduction guide from the first constructor is used for CTAD, and then, the second constructor is chosen by the constructor overload.","title":"Deduction guides resolution"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-specialization","text":"Template specialization is a way to provide a different implementation of a template for a specific type. For example, we can provide a different implementation of a template for a std::string type. Imagine following class: // declaration template<class T> class Object{ public: void print(T value) }; // definition template<class T> void Object<T>::print(T value){ std::cout << value << std::endl; } Now, we can provide a different implementation for std::string : // declaration template<> class Object{ public: void print(std::string value) }; template<> void Object<std::string>::print(std::string value){ std::cout << value << std::endl; } There are two types of template specialization: full specialization : exact specification for all template arguments partial specialization : exact specification for a subset of template arguments and/or non-type template arguments To demonstrate the difference, let's have a look at the following example: // declaration template<class T, class C> class Object{}; // primary template // full specialization template<> class Object<int, std::string>{}; // full specialization // partial specializations template<class C> class Object<int, C>{}; // not a full specialization, as C is not specified template<std::integral T, My_concept C> class Object<T, C>{}; // not a full specialization, types are not exactly specified While behaving similarly, there are some important differences between the two types: Full specialization is a new type. Therefore, it must be defined in the source file ( .cpp ), just like any other class or function and it must have a separate declaration. On the other hand, partial specialization is still just a template, so it must be defined in the header file ( .h or .tpp ). For functions, we cannot provide a partial specialization . For member functions we can solve this by specializing the whole class. The solution for any function is to alloow all types in the function and use if constexpr to select the correct implementation: template<class T, class C> class Object{ public: bool process(T value, C config){ if constexpr (std::is_same_v<T, std::string>){ return process_string(value, config); } else { return process_value(value, config); } } }; Note that here, the if constexpr requires the corresponding else branch. Otherwise, the code cannot be discarded during the compilation. Example: template<class T, class C> class Object{ public: bool process(T value, C config){ if constexpr (std::is_same_v<T, std::string>){ return process_string(value, config); } return process_value(value, config); // this compiles even if T is std::string } };","title":"Template Specialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#templates-and-namespaces","text":"If the templated code resides in a namespace, it can be tempting to save few lines of code by sorrounding both .h and .tpp files using one namespace expression: // structs.h hile namespace my_namespace { // declarations... #include 'structs.tpp' } // structs.tpp // definitions However, this can confuse some IDEs (e.g., false positive errors in IntelliSense), so it is better to introduce the namespace in both files: // structs.h hile namespace my_namespace { // declarations... } #include 'structs.tpp' // structs.tpp namespace my_namespace { // definitions } Don't forget to close the file and reopen it after the change to clear the errors.","title":"Templates and Namespaces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-complicated-types-as-template-arguments","text":"Sometimes, it can be very tricky to determine the template argument we need in order to use the template. The correct argument can be for example a return value of some function, templete function, or even member function of a template instanciation which has other templates as argument... To make it easier, we can, istead of suplying the correct arguments, evaluate an expression that returns the correct type and then use the decltype specifier. For more info, see the Determining Type from Expressions section.","title":"Using Complicated Types as Template Arguments"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#type-traits","text":"The purpose of type traits is to create predicates involving teplate parameters. Using type traits, we can ask questios about template parameters. With the answer to these questions, we can even implement conditional compilation, i.e., select a correct template based on parameter type. Most of the STL type traits are defined in header type_traits . A type trate is a template with a constant that holds the result of the predicate, i.e., the answer to the question. More about type traits","title":"Type Traits"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#usefull-type-traits","text":"std::is_same std::is_base_of std::is_convertible std::conditional : enables if-else type selection","title":"Usefull Type Traits"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#replacement-for-old-type-traits","text":"Some of the old type traits are no longer needed as they can be replaced by new language features, which are more readable and less error prone. Some examples: std::enable_if can be replaced by concepts: // old: enable_if template<class T> void f(T x, typename std::enable_if_t<std::is_integral_v<T>, void> = 0) { std::cout << x << '\\n'; } // new: concepts template<std::integral T> void f(T x) { std::cout << x << '\\n'; }","title":"Replacement for old type traits"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#concepts","text":"cppreference Concepts are named sets of requiremnets. They can be used instead of class / typename keywords to restrict the template types. The syntax is: template<class T, ....> concept concept-name = constraint-expression The concept can have multiple template parameters. The first one in the declaration stands for the concept itself, so it can be refered in the constraint expression. More template parameters can be optionally added and their purpose is to make the concept generic.","title":"Concepts"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constraints","text":"Constraints can be composed using && and || operatos. For atomic constaints declaration, we can use: Type traits: template<class T> concept Integral = std::is_integral<T>::value; Concepts: template<class T> concept UnsignedIntegral = Integral<T> && !SignedIntegral<T>; Requires expression: template<typename T> concept Addable = requires (T x) { x + x; }; Either form we chose, the atomic constraint have to always evaluate to bool.","title":"Constraints"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#requires-expression","text":"Requires expressions ar ethe most powerfull conctraints. The syntax is: requires(parameter list){requirements} There are four types of requirements that can appear in the requires expression: simple requiremnet : a requirement that can contain any expression. Evaluates to true if the expression is valid. cpp requires (T x) { x + x; }; type requirement : a requiremnt checking the validity of a type: cpp requires { typename T::inner; // required nested member name typename S<T>; // required class template specialization typename Ref<T>; // required alias template substitution }; compound requirement : Checks the arguments and the return type of some call. It has the form: {expression} -> return-type-requirement; cpp requires(T x) { {*x} -> std::convertible_to<typename T::inner>; } other useful type traits can be used instead of std::convertible_to . Nested requirement : a require expression inside another requires expression: cpp requires(T a, size_t n) { requires Same<T*, decltype(&a)>; // nested }","title":"Requires Expression"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#auto-filling-the-first-template-argument","text":"Concepts have a special feature that their first argument can be autoffiled from outer context. Consequentlly, you then fill only the remaining arguments. Examples: //When using the concept template<class T, class U> concept Derived = std::is_base_of<U, T>::value; template<Derived<Base> T> void f(T); // T is constrained by Derived<T, Base> // When defining the concept template<typename S> concept Stock = requires(S stock) { // return value is constrained by std::same_as<decltype(stock), double> {stock.get_value()} -> std::same_as<double>; }","title":"Auto filling the first template argument"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stl-concepts","text":"iterator concepts","title":"STL Concepts"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#usefull-patterns","text":"","title":"Usefull Patterns"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constrain-a-template-argument","text":"Imagine that you have a template function load and an abstract class Loadable_interface that works as an interface: class Loadable_interface{ virtual void load() = 0; }; template<class T> void load(T to_load){ ... to load.load() ... }; Typically you want to constraint the template argument T to the Loadable_interface type, so that other developer clearly see the interface requirement, and receives a clear error message if the requirement is not met. In Java, we have an extend keyword for this purpose that can constraint the template argument. In C++, this can be solved with concepts. First we have to define a concept that requires the interface: template<typename L> concept Loadable = std::is_base_of_v<Loadable_interface, L>; Than we can use the concept like this: template<Loadable T> void load(T to_load){ ... to load.load() ... };","title":"Constrain a Template Argument"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constraint-a-concept-argument","text":"Imagine that you have a concept Loadable that requires a method load to return a type T restricted by a concept Loadable_type . One would expect to write the loadable concept like this: template<typename L, Loadable_type LT> concept Loadable = requires(L loadable) { {loadable.load()} -> LT; }; However, this is not possible, as there is a rule that concept cannot not have associated constraints . The solution is to use an unrestricted template argument and constrain it inside the concept definition: template<typename L, typename LT> concept Loadable = Loadable_type<LT> && requires(L loadable) { {loadable.load()} -> LT; };","title":"Constraint a Concept Argument"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#sources","text":"https://en.cppreference.com/w/cpp/language/constraints Requires expression explained","title":"Sources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#interfaces","text":"In programming, an interface is usualy a set of requirements that restricts the function or template parameters, so that all types fulfiling the requiremnet can be used as arguments. Therte are two ways how to create an interface in C++: using the polymorphism using templates argument restriction While the polymorphism is easier to implement, the templating is more powerful and it has zero overhead. The most important thing is probably that despite these concepts can be used together in one application, not all \"combinations\" are allowed especialy when using tamplates and polymorphism in the same type. Note that in C++, polymorphism option work only for function argument restriction, but we cannot directly use it to constrain template arguments (unlike in Java). To demonstrate all possible options, imagine an interface that constraints a type that it must have the following two functions: int get_value(); void set_value(int date); The following sections we will demonstrate how to achieve this using multiple techniques.","title":"Interfaces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#interface-using-polymorfism","text":"Unlike in java, there are no interface types in C++. However, we can implement polymorfic interface using abstract class. The following class can be used as an interface: class Value_interface{ virtual int get_value() = 0; virtual void set_value(int date) = 0; } To use this interface as a fuction argument or return value, follow this example: std::unique_ptr<Value_interface> increment(std::unique_ptr<Value_interface> orig_value){ return orig_value->set_value(orig_value->get_value() + 1); } This system works in C++ because it supports multiple inheritance. Do not forget to use the virtual keyword, otherwise, the method cannot be overriden. Note that unlike in other languages, in C++, the polymorphism cannot be directly use as a template (generic) interface. Therefore, we cannot use the polymorfism alone to restrict a type.","title":"Interface using polymorfism"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-template-argument-restriction-as-an-interface","text":"To use template argument restriction as an interface, we can use concepts. The following concept impose the same requirements as the interface from the polymorphism section: To use template argument restriction as an interface, we can use concepts. The following concept impose the same requirements as the interface from the polymorphism section: template<class V> concept Value_interface = requires(V value_interface){{value_interface.get_value()} -> std::same_as<int>; } && requires(V value_interface, int value){{value_interface.set_value(value)} -> std::same_as<void>; } Remember that the return type of the function has to defined by a concept , the type cannot be used directly. Therefore, the following require statement is invalid: requires{(V value_interface){value_interface.get_value()} -> int; } To use this interface as an template argument in class use: template<Value_interface V> class ... And in function arguments and return types: template<Value_interface V> V increment(V orig_value){ return orig_value.set_value(orig_value.get_value() + 1);","title":"Using template argument restriction as an interface"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#restricting-the-member-function-to-be-const","text":"To restrict the member function to be const, we neet to make the type value const in the requires expression: template<class V> concept Value_interface = requires{(const V value_interface) {valvalue_interfaceue.get_value() -> std::same_as<int>;}; };","title":"Restricting the member function to be const"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-concepts-and-polymorphism-together-to-restrict-template-parameters-with-abstract-class","text":"We cannot restrict template parameters by polymorphic interface directly, however, we can combine it with concept. The folowing concept can be used together with the interface from the polymorphic interface section: template<class V> concept Value_interface_concept = requires std::is_base_of<Value_interface,V> Neverthless, as much as this combination can seem to be clear and elegent, it brings some problems. . We can use concepts to imposed many interfaces on a single type, but with this solution, it can lead to a polymorphic hell. While there is no problem with two concepts that directly requires the same method to be present with abstract classes, this can be problematic. Moreover, we will lose the zero overhead advantage of the concepts, as the polymorphism will be used to implement the interface.","title":"Using concepts and polymorphism together to restrict template parameters with abstract class"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#the-conflict-between-templates-and-polymorphism","text":"As described above, messing with polymorphism and templates together can be tricky. Some examples:","title":"The Conflict Between Templates and Polymorphism"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#no-virtual-member-function-with-template-parameters","text":"An example: a virtual (abstract) function cannot be a template function ( member template function cannot be virtual), so it cannot use template parameters outside of those defined by the class template.","title":"No Virtual Member Function with Template Parameters"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#polymorphism-cannot-be-used-inside-template-params","text":"If the functin accepts MyContainer<Animal> we cannot call it with MyContainer<Cat> , even if Cat is an instance of Animal.","title":"Polymorphism cannot be used inside template params"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#possible-solutions-for-conflicts","text":"do not use templates -> more complicated polymorphism ( type erasure for members/containers) do not use polymorphism -> use templates for interfaces an adapter can be used","title":"Possible solutions for conflicts"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#polymorphic-members-and-containers","text":"When we need to store various object in the same member or container, we can use both templates and polymorphism. However, both techniques has its limits, summarized in the table below: | | Polymorphism | Templates | | -- | -- | -- | | The concrete type has to be known at compile time | No | Yes | For multiple member initializations, the member can contain any element. | No , the elements have to share base class. | Yes | | For a single initialization, the containar can contain multiple types of objects | Yes , if they have the same base class | No | We can work with value members | No | Yes | When using the interface, we need to use downcasting and upcasting | Yes | No","title":"Polymorphic members and containers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deciding-between-template-and-polymorphism","text":"Frequently, we need some entity(class, function) to accept multiple objects through some interface. We have to decide, whether we use templates, or polymorphism for that interface. Some decision points: We need to return the same type we enter to the class/function -> use templates We have to access the interface (from outside) without knowing the exact type -> use polymorphism We need to restrict the member/parametr type in the child -> use templates for the template parameter if you need to fix the relation between method parameters/members or template arguments of thouse, you need to use templates If there are space considerations, be aware that every parent class adds an 8 byte pointer to the atribute table In general, the polymorphic interface have the following adventages: easy to implement easy to undestand similar to what people know from other languages On the other hand, the interface using concepts has the following adventages: no need for type cast all types check on compile time -> no runtime errors zero overhead no object slicing -> you don't have to use pointers when working with this kind of interface we can save memory because we don't need the vtable pointers","title":"Deciding between template and polymorphism"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#iterators-stl-algorithms-and-ranges","text":"If we want to iterate over elements in some programming language, we need to fulfill some interface. In Java, this interface is called Iterable . Also, there is usually some interface that formalize the underlying work, in Java, for example, it is called Iterator . In C++, however, the interface for iteration is not handled by polymorphism. Instead, it is handled using type traits and concepts. On top of that, there are multiple interfaces for iteration: legacy iteration, e.g., for (auto it = v.begin(); it != v.end(); ++it) STL algorithms, e.g., std::find(v.begin(), v.end(), 42) STL range algorithms, e.g., std::ranges::find(v, 42) STL range views, e.g., std::ranges::views::filter(v, [](int x){return x > 0;}) The following table summarizes the differences between the interfaces: |---| Plain iteration | STL algorithms | STL range algorithms | STL range views | |---|---|---|---|---| | Interface | type traits | type traits | concepts | concepts | | Iteration | eager | eager | eager | lazy | | Modify the underlying range | no | yes | yes | no | | Can work on temporaries * | yes | yes | yes | no | *If the operation modifies the data, i.e., sorting, shuffling, transforming, etc. The examples below demonstrate the differences between the interfaces on the following task: create a vector of 10 elements with values 0,1,2,...,9, i.e., the same as Python range(10) . // plain iteration std::vector<int> vec(10); int i = 0; for (auto it = vec.begin(); it != vec.end(); ++it) { *it = i; ++i; } // legacy algorithm std::vector<int> vec(10); std::iota(vec.begin(), vec.end(), 0); // C++11 way, legacy interface using type traits // range algorithm std::vector<int> vec(10); std::ranges::iota(vec.begin(), vec.end(), 0); // basically the same, but the constructor arguments are constrained with concepts // same using adaptor auto range = std::views::iota(0, 10); std::vector vec{range.begin(), range.end()}; // in-place vector construction","title":"Iterators, STL algorithms, and ranges"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#terminology","text":"range : the object we iterate over (Iterable in Java) iterator : the object which does the real work (Iterator in Java) Usually, a range is composed of two iterators: begin : points to the beginning of the range, returned by <range_object>.begin() end : points to the end of the object, returned by <range_object>.end() Each iterator implements the dereference ( * ) operator that acces the element of the range the iterator is pointing to. Depending on the iterator type, the iterator also supports other operations: ++ , -- to iterate along the range, array index operator ( [] ) for random access, etc. Most of the STL collections (vector, set,...) are also ranges.","title":"Terminology"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#how-to-choose-the-correct-interface","text":"when deciding which interface to use, we can use the following rules: If the number of tasks and the complexity of the tasks is high, use the legacy iteration . It is hard to write a 20 line for loop with various function calls as algorithm or adaptor and the result would be hard to read. Otherwise, if you need to preserve the original range as it is or you need to compose multiple operations, use the STL range adaptors . Otherwise, use the STL range algorithms . Note that the in this guide, we do not consider the legacy STL algorithms. With the availability of the STL range algorithms, there is no reason to use the legacy algorithms, except for the backward compatibility or for the algorithms that are not yet implemented in the STL. Also note that some STL algorithms are principially non-modifying, e.g., std::ranges::find or std::ranges::count . These algorithms logically do not have the adaptor equivalent.","title":"How to choose the correct interface?"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stl-ranges-and-views","text":"https://en.cppreference.com/w/cpp/ranges In C++ 20 there is a new range library that provides functional operations for iterators. It is similar to functional addon in Java 8. As explained in the beginning of this chapter, there are two ways how to use the STL ranges: using the range algorithms ( ranges::<alg name> ) that are invoked eagerly. using the range views ( ranges::views::<view name> ) that are invoked lazily. Note that the range algorithms and adaptors cannot produce result without an input, i.e., we always need a range or collection on which we want to apply our algorithm/view.","title":"STL ranges and views"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stl-range-views","text":"The difference of range view to range algorithms is that the views are lazy, i.e., they do not produce any result until they are iterated over. This is similar to the Python generators. The advantage is that we can chain multiple views together and the result is computed only when we iterate over the final view. Note that due to the lazy nature of the views, the underlying range has to be alive during the whole iteration . Therefore, we cannot use the views on temporaries, e.g., we cannot useviews directly in the constructor of a vector, or we cannot use the views on a temporary range returned by a function. A custom view can be created so that it can be chained with STL views. However, it has to satisfy the view concept , and more importantly, it should satisfy the view semantic, i.e., it should be cheap to copy and move (without copying the underlying data).","title":"STL range views"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#usefull-views","text":"std::views::iota : generates a sequence of numbers std::views::filter : filters the elements of the range","title":"Usefull views"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#projections","text":"Unlike in Java, we cannot refer to member functions when lambda functions are required. However, we can use these member functions when the algorithm or adaptor has a projection parameter. Example: struct Data { int a; std::string b; ... }; std::vector<Data> data = get_data(); std::sort(data, {}, &Data::b); The trick here is that we can only provide the member that we want to use for sorting, but the sorting logic ( first < second ...) is handeled by a standard comparator (the second argument initialized with {} ). We can have even more complicated projections that are not just references to member functions: std::vector<My_class> objects = get_objects(); std::vector<unsigned> indexes = get_indexes(objects); auto proj = [&objects](std::size_t i) -> const std::string& { return objects[i].get_name(); }; std::ranges::sort(indexes, {}, proj) // sort indexes using the property of objects","title":"Projections"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#useful-range-algorithms","text":"Note that the most frequently used algorithms have a separate section in the Iterators chapter. std::shuffle : shuffles the elements in the range (formerly std::random_shuffle ). std::adjacent_find : finds the first two adjacent elements that are equal. Can be used to find duplicates if the range is sorted. std::ranges::unique : moves the duplicates to the end of the range and returns the iterator to the first duplicate. Only consecutive duplicates are found. std::ranges::min : finds the smallest element in the range. We can use either natural sorting, or a comparator, or a projection. If the range is empty, the behavior is undefined. std::ranges::min_element : finds the smallest element in the range. Unlike std::ranges::min , this function returns an iterator to the smallest element. std::ranges::empty : checks whether the range is empty.","title":"Useful range algorithms"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#other-resources","text":"https://www.modernescpp.com/index.php/c-20-the-ranges-library","title":"Other Resources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#boost-ranges","text":"In addition to the STL range algorithms and adaptors, boost has it's own range library with other more complex algorithms and adaptors.","title":"Boost ranges"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#boost-range-requirements","text":"Sometimes, it is hard to say why a type does not satisfy some of the requirements for boos ranges. Fortunatelly, the boost provides concepts for checking whether a type satisfy each specific range model. Example: BOOST_CONCEPT_ASSERT(( boost::SinglePassRangeConcept<std::vector<int>> )); // true Also, it is necessary to check whether the value of the iterator can be accessed: BOOST_CONCEPT_ASSERT(( boost_concepts::ReadableIteratorConcept< typename boost::range_iterator<std::vector<int>>::type > )); // true Most likely, the compiler will complain that boost::range_iterator<R>::type does not exist for your range R . The boost range library generate this type by a macro from the R::iterator type. Therefore, make sure that your range has an iterator type defined, either as: a type alias to an existing iterator an iterator nested class Note that <RANGE CLASS>::iterator and <RANGE CLASS>::const_iterator has to be accessible (public).","title":"Boost range requirements"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#sequences","text":"The iota algortihm/adapter is used to create a sequence: auto range = std::views::iota(0, 10); auto vec = std::vector(range.begin(), range.end()); Note that we cannot pass the view directly to the vector, as the vector does not have a range constructor.","title":"Sequences"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#zip","text":"The classical Python like zip iteration is available using the zip adapator , which is not yet supported in MSVC. However, boost provides a similar functionality boost::combine .","title":"Zip"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#boostcombine","text":"boost::combine example: std::vector<int> va{1, 2, 3}; std::vectro<float> vb{0.5, 1, 1.5}; for(const auto& [a, b]: boost::combine(va, vb)){ ... } Each argument of combine must satisfy boost::SinglePassRange","title":"boost::combine"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#enumerating","text":"There is no function in standard library equivalent to the python enumerate. We can use a similar boost solution: #include <boost/range/adaptor/indexed.hpp> for(auto const& el: <range> | boost::adaptors::indexed(0)){ std::cout << el.index() << \": \" << el.value() << std::endl; } However, inside the loop, we have to call the index and value functions, so it is probably easier to stick to the good old extra variable: size_t i = 0; for(auto const& el: <range>) { std::cout << i << \": \" << el << std::endl; ++i; }","title":"Enumerating"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#sorting","text":"There is no sorted view or something simmiler, so in order to sort a range, we need to: really sort the object in the range create an adaptor/view from the range, and then sort the view There are two functions for sorting in the STL algorithm library: std::sort : old supports parallelization directly by the policy param std::ranges::sort : new supports comparison using projections There are three types of sorting: natural sorting using the < operator of T : std::sort(<RANGE<T>>) sorting using a comparator: std::sort(<RANGE>, <COMPARATOR>) , where comparator is a fuction with parameters and return value analogous to the natural sorting operator. sorting using projection (only availeble in std::ranges::sort ): std::ranges::sort(<RANGE>, <STANDARD GENERIC COMPARATOR>, <PROJECTION>","title":"Sorting"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#sorting-using-projection","text":"When we want to sort the objects by a single property different then natural sorting, the easiest way is to use projection. Example: struct Data { int a; std::string b; ... }; std::vector<Data> data = get_data(); std::sort(data, {}, &Data::b); The trick here is that we can only provide the member that we want to use for sorting, but the sorting logic ( first < second ...) is handeled by a standard comparator (the second argument initialized with {} ). We can have even more complicated projections: std::vector<My_class> objects = get_objects(); std::vector<unsigned> indexes = get_indexes(objects); auto proj = [&objects](std::size_t i) -> const std::string& { return objects[i].get_name(); }; std::ranges::sort(indexes, {}, proj) // sort indexes using the property of objects","title":"Sorting using projection"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#transformation","text":"Transformation alg/views transforms an input range according to a callable. As with other operation, there are thre options: classical algorithm: std::transform with a direct paralellization using the policy parameter range algorithm: std::ranges::transform with a support for projections range view: std::ranges::views::transform - a lazy variant The algorithms (but not the view) also supports binary transformations , i.e., create an output range using two input ranges. Transform view example: std::vector<int> in(3, 0); // [0, 0, 0] auto ad = std::ranges::transform_view(in, [](const auto in){return in + 1;}); std::vector<int> out(ad.begin(), ad.end()); The transform view can be only constructed from an object satisfying ranges::input_range . If we want to use a general range (e.g., vector), we need to call the addapter, which has a same signature like the view constructor itself. The important thing here is that the adapter return type is not a std::ranges::views::transform<<RANGE>> but std::ranges::views::transform<std::ranges::ref_view<RANGE>>> ( std::ranges::ref_view ). Supporting various collections is therefore possible only with teplates, but not with inheritance. Note that unlike in Java, it is not possible to use a member reference as a transformation function (e.g.: &MyClass::to_sting() ). We have to always use lambda functions, std::bind or similar to create the callable.","title":"Transformation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#aggregating-sum-product-etc","text":"These operations can be done using the std::accumulate algorithm. This algorithm is about to be replaced by the std::ranges::fold algorithm, but it is not yet implemented in Clang. Examples: // default accumulation -> sum std::vector<int> vec{1, 2, 3, 4, 5}; int sum = std::accumulate(vec.begin(), vec.end(), 0); // product int product = std::accumulate(vec.begin(), vec.end(), 1, std::multiplies<int>());","title":"Aggregating (sum, product, etc.)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implementing-a-custom-range","text":"There are different requirements for different types of ranges. Moreover, there are different requirements for the range-based for loop (for each) , or the legacy STL algorithms. Here we focus on requirements for ranges. Not however, that the range requirements are more strict than the requirements for the range-based for loop or the legacy STL algorithms. Therefore, the described approach should work for all three cases. Usually, we proceed as follows: Choose the right range (Iterable) concept for your range from the STL range concepts . The most common is the std::ranges::input_range concept. Implement the range concept for the range. Either, we can do it by using the interface of the undelying range we usein our class (i.e, we just forward the calls to the methods of std::vector or std::unordered_map ) or implement the interface from scratch. For that, we also need to implement the iterator class that fulfills the corresponding iterator concept (e.g., std::input_iterator for the std::ranges::input_range ).","title":"Implementing a custom range"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implementing-an-input-range","text":"The input range is the most common range type. The only requirement for the input range is that it has to have the begin and end methods that return the input iterator. Example: class My_range { private: std::vector<int> data; public: My_range(std::vector<int> data): data(data) {} auto begin() {return data.begin();} auto end() {return data.end();} // usually, we also want a const version of the range auto begin() const {return data.begin();} auto end() const {return data.end();} };","title":"Implementing an input range"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#boost-iterator-templates","text":"The boost.iterator library provides some templates to implement iteratores easily, typically using some existing iterators and modifying just a small part of it: for pointer to type (dereference) iterator, you can use boost indirect iterator zip iterator for Python like iteration over multiple collections [transform iteratorather useful iterators are also included in the boost.iterator library for using another iterator and just modify the access ( * ) opindex.html). including: zip iterator. counting_iterator to create number sequence like Python range gentransform iterator There are also two general (most powerfull) classes: iterator adapter iterator facade","title":"Boost Iterator Templates"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#resources","text":"How to write a legacy iterator iter_value_t","title":"Resources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#lambda-functions","text":"In c++ lambda functions are defined as: [<capture>](<params>) -> <return_type> { <code> } The rerurn type is optional, but sometimes required (see below). Since C++23, the parantheses are optional if there are no functon parameters.","title":"Lambda Functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#captures","text":"Anything that we want to use from outside has to appear in capture. To prevent copying, we should capture by reference, using & before the name of the variable. [&var_1] // capture by reference [var_1] // capture by value [&] // default capture by reference For the detailed explanation of the captures, see cppreference .","title":"Captures"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#return-type","text":"The return type of lambda functions can be set only using the trailing return type syntax ( -> <RETURN TYPE> after the function params). The return type can be omited. Note however, that the default return type is auto , so in case we want to return by reference, we need to add at least -> auto , or even a more specific return type.","title":"Return type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#specifiers","text":"Lambda functions can have special specifiers: mutable : lambda can modify function parameters capture by copy","title":"Specifiers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#exceptions","text":"In C++, exceptions works simillarly as in other languages. Standard runtime error can be thrown using the std::runtime_error class: throw std::runtime_error(\"message\"); Always catch exception by reference! Note that unlike in Java or Python, there is no default exception handler in C++. Therefore, if an exception is not caught and, in conclusion, the program is terminated, there is no useful information about the exception in the standard output. Instead, we only receive the exit code. For this reason, it is a good practice to catch all exceptions in the main function and print the error message. Example: int main() { try { <the code of the whole program here> } catch(...) { const std::exception_ptr& eptr = std::current_exception() if (!eptr) { throw std::bad_exception(); } /*char* message;*/ std::string message; try { std::rethrow_exception(eptr); } catch (const std::exception& e) { message = e.what(); } catch (const std::string& e) { message = e; } catch (const char* e) { message = e; } catch(const GRBException& ex) { message = fmt::format(\"{}: {}\", ex.getErrorCode(), ex.getMessage()); } catch (...) { message = \"Unknown error\"; } spdlog::error(message); return message; } }","title":"Exceptions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#rethrowing-exceptions","text":"We can rethrow an exception like this: catch(const std::exception& ex){ // do ssomething ... throw; } Note that in parallel regions, the exception have to be caught before the end of the parallel region , otherwise the thread is killed.","title":"Rethrowing Exceptions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#how-to-catch-any-exception","text":"In C++, we can catch any exception with: catch (...) { } However, this way, we cannot access the exception object. As there is no base class for exceptions in C++, there is no way to catch all kind of exception objects in C++.","title":"How to Catch Any Exception"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#noexcept-specification","text":"A lot of templates in C++ requires functions to be noexcept which is usually checked by a type trait std::is_nothrow_invocable . We can easily modify our function to satisfy this by adding a noexcept to the function declariaton. There are no requirements for a noexcept function. It can call functions without noexcept or even throw exceptions itself. The only difference it that uncought exceptions from a noexcept function are not passed to the caller. Instead the program is terminated by calling std::terminate , which otherwise happens only if the main function throws. By default, only constructors, destructors, and copy/move operations are noexcept.","title":"noexcept specification"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stack-traces","text":"Unlike most other languages, C++ does not print stack trace on program termination. The only way to get a stack trace for all exceptions is to set up a custom terminate handler an inside it, print the stack trace. However, as of 2023, all the stack trace printing/generating libraries requires platform dependent configuration and fails to work in some platforms or configurations. Example: void terminate_handler_with_stacktrace() { try { <stack trace generation here>; } catch (...) {} std::abort(); } std::set_terminate(&terminate_handler_with_stacktrace); To create the stacktrace, we can use one of the stacktrace libraries: stacktrace header from the standard library if the compiler supports it (C++ 23) as of 2024-04, only MSVC supports this functionality cpptrace boost stacktrace","title":"Stack traces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#logging","text":"There is no build in logging in C++. However, there are some libraries that can be used for logging. In this section we will present logging using the spdlog library. We can log using the spdlog::<LEVEL> functions: spdlog::info(\"Hello, {}!\", \"World\"); By default, the log is written to console. In order to write also to a file, we need to create loggers manually and set the list of sinks as a default logger: const auto console_sink = std::make_shared<spdlog::sinks::stdout_sink_st>(); console_sink->set_level(spdlog::level::info); // log level for console sink auto file_sink = std::make_shared<spdlog::sinks::basic_file_sink_st>(<log filepath>, true); std::initializer_list<spdlog::sink_ptr> sink_list{console_sink, file_sink}; const auto logger = std::make_shared<spdlog::logger>(<LOGGER NAME>, sink_list); logger->set_level(spdlog::level::debug); //log level for the whole logger spdlog::set_default_logger(logger); To save performance in case of an intensive logging, we can set an extended flushing period: spdlog::flush_every(std::chrono::seconds(5));","title":"Logging"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#colors","text":"By default, the logger uses colors for different log levels. However, this capability is lost when: using custom sinks or using custom formatters To keep the colors, we need to a) use the color sink and b) explicitly set the usage of the color in the formatter: auto console_sink = std::make_shared<spdlog::sinks::stdout_color_sink_mt>(); auto logger = std::make_shared<spdlog::logger>(\"console\", console_sink); logger->set_pattern(\"[%^%l%$] %v\"); Here %^ and %$ are the color start and end markers.","title":"Colors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#type-aliases","text":"Type aliases are short names bound to some other types. We can introduce it either with typedef or with using keyword. Examples (equvalent): typedef int number; using number = int; typedef void func(int,int); using func = void(int, int) The using new syntax is more readable, as the alias is at the begining of the expression. But why to use type aliases? Two strong motivations can be: iImprove the readebility : When we work with a type with a very long declaration, it is wise to use an alias. We can partialy solve this issue by using auto, but that is not a complete solution Make the refactoring easier : When w work with aliases, it is easy to change the type we work with, just by redefining the alias. Note that type aliases cannot have the same name as variables in the same scope . So it is usually safer to name type aliases with this in mind, i.e., using id_type = .. insted of using id = ..","title":"Type Aliases"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-aliasis","text":"We can also create template aliases as follows: template<class A, typename B> class some_template{ ... }; template<class T> using my_template_alias = some_template<T, int>;","title":"Template Aliasis"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#aliases-inside-classes","text":"The type alias can also be placed inside a class. From outside the class, it can be accessed as <CLASS NAME>::<ALIAS NAME> : class My_class{ public: using number = unsigned long long number n = 0; } My_class::number number = 5;","title":"Aliases inside classes"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constant-expressions","text":"A constant expression is an expression that can be evaluated at compile time. The result of constant expression can be used in static context, i.e., it can be: assigned to a constexpr variable, tested for true using static_assert Unfortunatelly, there is no universal way how to determine if an expression is a constant expression . More on cppreference .","title":"Constant Expressions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#regular-expressions","text":"The regex patern is stored in a std::regex object: const std::regex regex{R\"regex(Plan (\\d+))regex\"}; Note that we use the raw string so we do not have to escape the pattern. Also, note that std::regex cannot be constexpr","title":"Regular expressions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#matching-the-result","text":"We use the std::regex_search to search for the occurence of the pattern in a string. The result is stored in a std::smatch object which contains the whole match on the 0th index and then the macthed groups on subsequent indices. A typical operation: std::smatch matches; const auto found = std::regex_search(string, matches, regex); if(found){ auto plan_id = matches[1].str(); // finds the first group } Note that matches[0] is not the first matched group, but the whole match.","title":"Matching the result"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#namespaces","text":"cppreference Namespace provides duplicit-name protection, it is a similar concept to Java packages. Contrary to java packages and modules, the C++ namespaces are unrelated to the directory structure. namespace my_namespace { ... } The namespaces are used in both declaration and definition (both in header and source files). The inner namespace has access to outer namespaces. For using some namespace inside our namespace without full qualification, we can write: using namespace <NAMESPACE NAME>","title":"Namespaces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#anonymous-namespaces","text":"Anonymous namespaces are declared as: namespace { ... } Each anonnymous namespaces has a different and unknown ID. Therefore, the content of the annonymous namespace cannot be accessed from outside the namespace, with exception of the file where the namespace is declared which has an implicit access to it.","title":"Anonymous namespaces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#namespace-aliases","text":"We can create a namespace alias using the namespace keyword to short the nested namespace names. Typicall example: namespace fs = std::filesystem;","title":"Namespace aliases"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#decltype-determining-type-from-expressions","text":"Sometimes, it is usefull to declare a type from expression, instead of do it manualy. Using decltype specifier, we can get the resulting type of an expression as if it was evaluated. Examples: struct A { double x; }; const A* a; decltype(a->x) // evaluates to double decltype(std::accumulate(a, [](double sum, double val){return sum + val;})) // evalutes to double We can use the decltype in any context where type is required. Examples: int i = 1 decltype(i) j = 3 std::vector<decltype(j)> numbers;","title":"decltype: Determining Type from Expressions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#the-value-category-of-decltype","text":"The value category of decltype is resolved depending on the value category of an expression inside it: deltype(<XVALUE>) -> T&& deltype(<LVALUE>) -> T& deltype(<RVALUE>) -> T The rvalue conversion can lead to unexpected results, in context, where the value type matters: static_assert(std::is_same_v<decltype(0), decltype(std::identity()(0))>); // error The above expressions fails because: decltype(0) , 0 is an rvalue -> the decltype result is int decltype(std::identity()(0)) result of std::identity() is an xvalue -> the decltype result is int&& . Determining Type from Expressions Sometimes, it is usefull to declare a type from expression, instead of do it manualy. Using decltype specifier, we can get the resulting type of an expression as if it was evaluated. Examples: struct A { double x; }; const A* a; decltype(a->x) // evaluates to double decltype(std::accumulate(a, [](double sum, double val){return sum + val;})) // evalutes to double We can use the decltype in any context where type is required. Examples: int i = 1 decltype(i) j = 3 std::vector<decltype(j)> numbers;","title":"The Value Category of decltype"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#determining-the-return-value-type-of-a-function","text":"As we can see above, we can use decltype to determine the return value type. But also, there is a type trait for that: std::invoke_result_t (formerly std::result_of ). The std::invoke_result_t should vbe equal to decltype when aplied to return type, with the following limitations:","title":"Determining the Return Value Type of a Function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#-we-cannot-use-abstract-classes-as-arguments-of-stdinvoke_result_t-while-we-can-use-them-inside-decltype-using-stddeclval-see-below","text":"","title":"- we cannot use abstract classes as arguments of std::invoke_result_t, while we can use them inside decltype (using std::declval, see below)."},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#construct-object-inside-decltype-with-stddeclval","text":"std::declval is a usefull function designed to be used only in static contexts, inside decltype . It enables using member functions inside decltype without using constructors. Without std::declval , some type expressions are hard or even impossible to costruct. Example: class Complex_class{ Complex_class(int a, bool b, ...) ... int compute() } // without declval decltype<Complex_class(1, false, ...).compute()> // using declval decltype(std::declval<Complex_class>().compute())","title":"Construct object inside decltype with std::declval"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#decltype-and-overloading","text":"in static context, there is no overloading, the vtable is not available. Therefore, we have to hint the compiler which specific overloaded function we want to evaluate. This also applies to const vs non const overloading. The following example shows how to get the const iterator type of a vector: std::vector<anything> vec // non const iter decltype(vec.begin()) // const iter decltype<std::declval<const decltype(vec)>().begin()> Another example shows how to use the const overload inside std::bind : decltype(std::bind(static_cast<const ActionData<N>&(std::vector<ActionData<N>>::*)(size_t) const>(&std::vector<ActionData<N>>::operator[]), action_data)), Above, we used static cast for choosing the const version of the vector array operator. Instead, we can use explicit template argument for std::bind : decltype(std::bind<const ActionData<N>& (std::vector<ActionData<N>>::*)(size_t) const>(&std::vector<ActionData<N>>::operator[], action_data)),","title":"decltype and Overloading"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#parallelization","text":"While there wa no support of parallelization i earlier versions of C++ , now there are many tools.","title":"Parallelization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#standard-threads","text":"","title":"Standard Threads"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#for-each-with-parallel-execution-policy","text":"The function std::for_each can be run with a parallel execution policy to process the loop in parallel.","title":"For-each with Parallel Execution Policy"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#async-tasks","text":"Tasks for asznchronous execution, like file downloads, db queries, etc. The main function is std::async .","title":"Async tasks"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#open-mp","text":"In MSVC, the Open MP library is automatically included and linked. In GCC, we need to find the libs in CmakeLists.txt : find_package(OpenMP REQUIRED)","title":"Open-MP"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#standard-templates-for-callables","text":"","title":"Standard Templates for Callables"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-stdinvoke-to-call-the-member-function","text":"using std::invoke , the cal syntax bool b = (inst.*ptr)() can be replaced with longer but more straighforward call: bool b = std::invoke(ptr, inst, 2)","title":"Using std::invoke to call the member function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-stdmem_fn-to-store-a-pointer-to-member-function-in-a-callable","text":"With std::mem_fn , we can store the pointer to a member function in a callable object. Later, we can call the object without the pointer to the member function. Example: auto mem_ptr = std::mem_fn(&My_class::my_method) bool b = mem_ptr(inst, 2)","title":"Using std::mem_fn to Store a Pointer to Member Function in a Callable"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-a-pointer-to-member-function-as-a-functor","text":"A normal function can be usually send instead of functor, as it can be invoked in the same way. However, in case of member function, we usually need to somehow bind the function pointer to the instance. We can use the std::bind function exactly for that: auto functor = std::bind(&My_class::my_method, inst); bool b = functor(2) Advanteges: we do not need an access to instance in the context from which we call the member function we do not have to remember the complex syntax of a pointer to a member function declaration we receive a callable object, which usage is even simpler than using std::invoke Note that in case we want to bind only some parameters, we need to supply placeholders for the remaining parameters ( std::placeholders ).","title":"Using a Pointer to Member Function as a Functor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-lambdas-instead-of-stdbind","text":"For more readable code and better compile error messages, it is usefull to replace std::bind callls with labda functions. The above example can be rewritten as: auto functor = [inst](int num){return inst.my_method(num);); bool b = functor(2)","title":"Using Lambdas Instead of std::bind"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#store-the-result-of-stdbind","text":"Sometimes, we need to know the return type of the std::bind . In many context, we need to provide the type instead of using auto . But luckily, there is a type exactly for that: std::function . Example: std::function<bool(int)> functor = std::bind(&My_class::my_method, inst); bool b = functor(2) A lambda can also be stored to std::function . But be carefull to add an explicit return type to it, if it returns by a reference. Example: My_class{ public: int my_member } My_class inst; std::function f = [inst](){return &inst.my_member; } // wrong, reference to a temporary due to return type deduction std::function f = [inst]() -> const int& {return &inst.my_member; } // correct More detailed information about pointers to member functions","title":"Store the Result of std::bind"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdmem_fn-and-data-members","text":"Data member pointers can be aslo stored as std::mem_fn . A call to this object with an instance as the only argument then return the data member value. The plain syntax is <type> <class name>.*<pointer name> = <class name>.<member name> , and the pointer is then accessed as <instance>.*<pointer name> . Example: int Car::*pSpeed = &Car::speed; c1.*pSpeed = 2; Usefull STL functions std::for_each : iterates over iterable objects and call a callable for each iteration std::bind : Binds a function call to a variable that can be called some parameters of the function can be fixed in the variable, while others can be provided for each call each reference parameter has to be wrapped as a reference_wrapper std:mem_fn : Creates a variable that represents a callable that calls member function","title":"std::mem_fn and Data Members"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdfunction","text":"The std::function template can hold any callable. It can be initialized from: function pointer/reference, member function pointer/reference, lambda function functor It can be easily passed to functions, used as template parameter, etc. The template parameters for std::function has the form of std::function<<RETURN TYPE>(<ARGUMENTS>)> . Example: auto lambda = [](std::size_t i) -> My_class { return My_class(i); }; std::function<My_class(std::size_t)> f{lambda}","title":"std::function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdfunction-and-overloading","text":"one of the traps when using std::function is the ambiguity when using an overloade function: int add(int, int); double add(double, double); std::function<int(int, int)> func = add; // fails due to ambiguity. The solution is to cant the function to its type first and then assign it to the template: std::function<int(int, int)> func = static_cast<int(*)(int, int)>add;","title":"std::function and overloading"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#preprocessor-directives","text":"The C language has a preprocessor that uses a specific syntax to modify the code before the compilation. This preprocessor is also used in C++. The most used tasks are: - including files ( #include ): equivalent to Java or Python import statement - conditional compilation based on OS, compiler, or other conditions Also, preprocessor had some other purposes, now replaced by other tools: - defining constants ( #define ): replaced by const and constexpr - metaprogramming: replaced by templates A simple variable can be defined as: #define PI 3.14159 . The variable can be used in the code as PI . Control structures are defined as: #ifdef <MACRO> ... #elif <MACRO> ... #else ... #endif","title":"Preprocessor Directives"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#testing-with-google-test","text":"","title":"Testing with Google Test"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#private-method-testing","text":"The testing of private method is not easy with Google Test, but that is common also for other tets frameworks or even computer languages (see the common manual). Some solutions are described in this SO question . Usually, the easiest solution is to aplly some naming/namespace convention and make the function accessible. For free functions: namespace internal { void private_function(){ ... } } For member functions: class MyClass{ public: void _private_function();","title":"Private method testing"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#specific-tasks","text":"","title":"specific tasks"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#conditional-function-execution","text":"W know it from other languages: if the function can be run in two (or more) modes, there is a function parameter that controls the execution. Usually, most of the function is the same (otherwise, we eould create multiple fuctions), and the switch controls just a small part. Unlike in other langueges. C++ has not one, but three options how to implement this. They are described below in atable together with theai properties. function parameter template parameter compiler directive good readability yes no no compiler optimization no yes yes conditional code compilation no no yes","title":"Conditional Function Execution"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-parameter","text":"void(bool switch = true){ if(switch){ ... } else{ ... } }","title":"Function Parameter"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-parameter","text":"template<bool S = true> void(){ if(S){ ... } else{ ... } }","title":"Template Parameter"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#compiler-directive","text":"void(){ #ifdef SWITCH ... #else ... #endif }","title":"Compiler Directive"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#ignoring-warnings-for-specific-line-of-code","text":"Sometimes, we want to suppress some warnings, mostly in libraries we are including. The syntax is, unfortunatelly, different for each compiler. Example: #if defined(_MSC_VER) #pragma warning(push) #pragma warning(disable: <WARNING CODE>) #elif defined(__GNUC__) #pragma GCC diagnostic push #pragma GCC diagnostic ignored \"<WARNING TYPE GCC>\" #elif defined(__clang__) #pragma clang diagnostic push #pragma clang diagnostic ignored \"<WARNING TYPE CLANG>\" #endif .. affected code... #if defined(_MSC_VER) #pragma warning(pop) #elif defined(__GNUC__) #pragma GCC diagnostic pop #elif defined(__clang__) #pragma clang diagnostic pop #endif Note that warnings related to the preprocessor macros cannot be suppressed this way in GCC due to a bug (fixed in GCC 13). The same is true for conditions: #if 0 #pragma sdhdhs // unknown pragma raises warning, despite unreachcable #endif","title":"Ignoring warnings for specific line of code"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#measuring-used-resource","text":"","title":"Measuring used resource"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#memory","text":"","title":"Memory"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#msvc","text":"In MSVC, we can measure the peak used memory using the following code: #include <psapi.h> PROCESS_MEMORY_COUNTERS pmc; K32GetProcessMemoryInfo(GetCurrentProcess(), &pmc, sizeof(pmc)); auto max_mem = pmc.PeakWorkingSetSize","title":"MSVC"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#working-with-tabular-data","text":"Potential libs similar to Python Pandas: Arrow Dataframe","title":"Working with tabular data"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#executing-external-commands","text":"The support for executing external commands in C++ is unsatisfactory. The most common solution is to use the system function. However, the system calls are not portable, e.g., the quotes around the command are not supported in Windows Another option is to use the Boost Process library.","title":"Executing external commands"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#command-line-interface","text":"For CLI, please follow the CLI manual . Here we focus on setting up the TCLAP library. TCLAP use","title":"Command Line Interface"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#jinja-like-templating","text":"For working with Jinja-like templates, we can use the Inja template engine.","title":"Jinja-like Templating"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#exceptions_1","text":"There are the following exceptions types: ParserError thrown on parse_template method RenderError thrown on write method","title":"Exceptions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#render-errors","text":"empty expression : this signalize that some expression is empty. Unfortunatelly, the line number is incorrect (it is always 1). Look for empty conditions, loops, etc. (e.g., {% if %} , {% for %} , {% else if %} ).","title":"Render Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/","text":"Introduction \u00b6 In C++, the workflow and especially the build pipeline is much more complicated than in other languages. Therefore, we start with brief overview of the C++ build pipelines. The following scheme shows the possible build pipelines for C++, starting from Integrated developemnt tools (IDE) and ending with the linker. This guide presents mostly the following workflows: Clion or Visual Studio IDE CMake any sequencing tool (these are discribed only briefly as they are configured automaticvally by CMake) MSVC and GCC compiler toolchains Appart from the build pipeline, we also cover the dependency management. For this, we focus on the dependency manager vcpkg. Compiler Toolchains \u00b6 There are various toolchains available on Windows and Linux, but we limit this guide for only some of them, specifically those which are frequently updated and works great with Clion. MSYS2 (Windows) \u00b6 download follow the installation guide on the homepage install MinGW64 using: pacman -S mingw-w64-x86_64-gcc MSVC (Windows) \u00b6 install Visual Studio 2019 Comunity Edition Choosing the runtime library \u00b6 official documentation When compiling with MSVC, it is crucial to choose the correct runtime library. Typically, both the compiled target and all its dependencies have to use the same runtime library. The following options are available: Option Copiler flag MSBuild name Description Linged library Multi-threaded Dynamic /MD MultiThreadedDLL The default option. The application uses the dynamic version of the runtime library. msvcrt.lib Multi-threaded Dynamic Debug /MDd MultiThreadedDebugDLL The debug version of the dynamic runtime library. msvcrtd.lib Multi-threaded Static /MT MultiThreaded The application uses the static version of the runtime library. libcmt.lib Multi-threaded Static Debug /MTd MultiThreadedDebug The debug version of the static runtime library. libcmtd.lib DLL /LD The application is compiled as a DLL. - DLL Debug /LDd The debug version of the DLL. - By default, the /MD / /MDd flags are used depending on the build type. When the mismatch occurs, we usually get the following error message: LNK2038 mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in .. To determine if the type of the runtime library used by the target, we can explore the build script for the target, For MSBuild, the runtime library is set by the RuntimeLibrary property. To determine the runtime library used by a library, we can use the dumpbin tool. This tool is part of the Visual Studio installation and can be run from the Developer Command Prompt or Developer PowerShell. The following command will display the runtime library used by the library: dumpbin /directives <path to the library> Resources \u00b6 Standard library description Common Compiler Flags \u00b6 /nologo : do not print the copyright banner and information messages /EH : exception handeling flags GCC (Linux/WSL) \u00b6 Installation \u00b6 If on Windows, Install the WSL first (WSL 2) check if GCC is installed by typing gcc --version If GCC is not installed: sudo apt-get update sudo apt-get upgrade sudo apt-get install gcc-<version> g++ \u00b6 g++ is the main executable for the GCC compiler. It is botha compiler and a wrapper for the linker. Typical usage is: g++ -o <output file> <cpp files and library files> : compilation and linking g++ -c <cpp files> : compilation only g++ -o <output file> <object files and library files> : linking only Other frequently used flags are: -g : include debug information -l<library> : link the library named lib<library> . Unlike direct library specification, here the linkers searches for the library in standard directories. Build Sequencing Tools \u00b6 Make \u00b6 Most of the time, make scripts should not be written manually, but generated by build script generators like CMake. Therefore, here, we describe the structure of make scripts as they are generated by CMake. The structure is: Makefile : the main file that contains the rules for building the project CMakeFiles : a directory containing the files referenced in the Makefile <target name>.dir directory for each target link.txt : the file containing the linking command MSBuild \u00b6 The MSBuild is a XML-based build sequencing tool. The scripts are typically generated by CMake or directly by Visual Studio. The main file for each target is the <target name>.vcxproj file. All properties are stored in the root element <Project> . The structure inside the project element is as follows (only the most important elements are listed): <ItemDefinitionGroup Condition=\"<configuration>\"> : contains the properties for the given configuration. <ItemGroup><multiple <ClCompile> elements><ItemGroup> : contains the source files that are compiled <ItemGroup><multiple <ProjectReference> elements><ItemGroup> : contains references to other targets (dependencies). Theses targets are built before the current target. The propertis for each configuration are structured as follows: <ClCompile> : contains the properties for the compilation of the source files (e.g., language standard, runtime library, etc.) <ResourceCompile> : <PreprocessorDefinitions> : the preprocessor definitions used for the target. These are introduced either by the target itself or by the dependencies. <Link> : contains the properties for the linking of the target (e.g., the libraries to link, the output file, etc.) Cmake \u00b6 Windows: Install CMake from https://cmake.org/download/ if your CMake is too old (e.g. error: \u201cCMake 3.15 or higher is required\u201d), update CMake (same as new install) Linux: If cmake is installed already, uninstall it! Do not use the cmake from linux repositories!! Download CMake sh installer from https://cmake.org/download/ install: sudo chmod +x <INSTALLER> sudo <INSTALLER> sudo rm <INSTALLER> add cmake executable to path Other details about CMake can be found in the CMake Manual. vcpkg \u00b6 Vcpkg is a package manager for C++ libraries it serves as a developement package manager rather than a system package manager. VCpkg can work in two modes: Classic mode : vcpkg is installed centrally. This mode is useful for development and testing. Manifest mode : vcpkg is installed in the project directory. This mode is useful for deployment. To install vcpkg: clone the repo to the desired location (project directory for manifest mode, any directory for classic mode) run the bootstrap script ( bootstrap-vcpkg.bat on Windows, bootstrap-vcpkg.sh on Linux) for classic mode, add the vcpkg directory to PATH , so the program can be run from anywhere Beware that to run it with sudo on linux, it is not that easy . To install a package, run vcpkg install package . Changing the default triplet \u00b6 To change the default triplet, add a new system variable VCPKG_DEFAULT_TRIPLET , so your default library version installed with vcpkg will be x64 (like our builds), set it to: x64-linux for Linux Compilers x64-windows for MSVC x64-MinGW for MinGW CMake Integration \u00b6 documentation By default, CMake does not see the vcpkg. To set up the appropriate enviroment variables, paths, etc., we need to run cmake commands with path to cmake toolchain file: vcpkg/scripts/buildsystems/vcpkg.cmake . See the IDE and command line section for the detailed instructions how to execute cmake with the path to the vcpkg toolchain file. The toolchain file is executed early on, so it is safe to assume that the environment will be correctly set up before the commands in yor cmake script. Update \u00b6 git pull bootstrap vcpkg again Windows: bootstrap-vcpkg.bat Linux: bootstrap-vcpkg.sh Update package \u00b6 Update vcpkg vcpkg update to get a list of available updates vcpkg upgrade --no-dry-run to actually perform the upgrade All packages are upgraded by default. To upgrade just one package, supply the name of the package (e.g., zlib:x64-windows) as an argument to upgrade command. Upgrade packages matching a pattern \u00b6 For libraries that are divided into many interdependent packages (like boost), it is useful to upgrade all packages that match a pattern at once. Unfortunately, the upgrade command does not support the pattern matching. The following command can be used to upgrade all packages that match a pattern in PowerShell: vcpkg update | sls -pattern \"boost-\\w+\" | foreach-object { vcpkg upgrade $_.Matches.Value --no-dry-run } Package Features \u00b6 Some libraries have optional features, which are not installed by default, but we can install them explicitely. For example llvm . After vcpkg install llvm and typing vcpkg search llvm : llvm 10.0.0#6 The LLVM Compiler Infrastructure llvm[clang] Build C Language Family Front-end. llvm[clang-tools-extra] Build Clang tools. ... llvm[target-all] Build with all backends. llvm[target-amdgpu] Build with AMDGPU backend. llvm[target-arm] Build with ARM backend. Above, we can see that there are a lot of optional targets. To install the the arm target, for example, we can use vcpkg install llvm[target-arm] . Sometimes, a new build of the main package is required, in that case, we need to type vcpkg install llvm[target-arm] --recurse . Package Versions \u00b6 In classic mode, there is no way how to control the version of a package. For that, we have to use the manifest mode Integrate your library to vcpkg \u00b6 For complete integration of your library to vcpkg, the following steps are needed: Configure and test the CMake installation Crate the port and test it locally ( vcpkg installation ) Submit the port to the vcpkg repository ( publishing ) Resources: decovar tutorial Create the Port \u00b6 The official guide for packageing Maintainer guide missing details from other guides contains the list of deprecated functions Vcpkg works with ports which are special directories containing all files describing a C++ package. The usuall process is: The usual port contain these files: portfile.cmake : the main file containing the calls to cmake functions that install the package vcpkg.json : metadata file containing the package name, version, dependencies, etc. usage : a file containing the usage instructions for the package. These instructions are displayed at the end of the installation process. example A simple portfile.cmake can look like this: # download the source code vcpkg_from_github( OUT_SOURCE_PATH SOURCE_PATH REPO <reo owner>/<repo name> REF <tag name> SHA512 <hash of the files> HEAD_REF <branch name> ) # configure the source code vcpkg_cmake_configure( SOURCE_PATH <path to source dir> ) # build the source code and install it vcpkg_cmake_install() # fix the cmake generaed files for vcpkg vcpkg_cmake_config_fixup(PACKAGE_NAME <package name>) # install the license vcpkg_install_copyright(FILE_LIST \"${SOURCE_PATH}/LICENSE.txt\") # install the usage file file(INSTALL \"${CMAKE_CURRENT_LIST_DIR}/usage\" DESTINATION \"${CURRENT_PACKAGES_DIR}/share/${PORT}\") Explanation: vcpkg_from_github : downloads the source code from the github repository the <path to source dir> is the directory where the CMakeLists.txt file is located. It is usually the directory where the source code is downloaded, so we can set it to ${SOURCE_PATH} the <hash of the files> can be easily obtained by: setting the ` to 0 running the vcpkg install <port name> copying the hash from the error message vcpkg_cmake_configure : configures the source code using cmake (wraps the cmake command) vcpkg_cmake_install : builds and installs the source code (wraps the cmake --build . --target install command) the majority of code is in the subroutine vcpkg_cmake_build if we need some libraries installed with vcpkg at runtime during the build of the package, we need to use the ADD_BIN_TO_PATH option in the vcpkg_cmake_install function . This is needed as the automatic dll copy to the output dir ( VCPKG_APPLOCAL_DEPS ) is disabelled by the vcpkg_cmake_build function. This option solve the problem by prepending the PATH environment variable with the path to the vcpkg installed libraries ( <vcpkg root>/installed/<triplet>/bin for release and <vcpkg root>/installed/<triplet>/debug/bin for debug). vcpkg_cmake_config_fixup : fixes the cmake generated files for vcpkg. This is needed because the cmake generated files are not compatible with vcpkg. The function fixes the CMakeConfig.cmake and CMakeConfigVersion.cmake files. the <package name> is the name of the package, usually the same as the port name vcpkg_install_copyright installs the license files listed in the FILE_LIST argument to share/<port name>/copyright file. The copyright file is obligatory for the package to be accepted to the vcpkg repository. The vcpkg.json file can look like this: { { \"name\": \"fconfig\", \"version-string\": \"0.1.0\", \"description\": \"C++ implementation of the fconfig configuration system\", \"homepage\": \"https://github.com/F-I-D-O/Future-Config\", \"license\": \"MIT\", \"dependencies\": [ { \"name\" : \"vcpkg-cmake\", \"host\" : true }, \"yaml-cpp\", \"spdlog\", \"inja\" ] } } Here: the license key is obligatory and has to match the license file of the package The dependencies with the host key set to true are the dependencies that are required for the build, but not for the runtime. Variables and Functions available in the portfile.cmake \u00b6 The variables and functions available in the portfile.cmake are described in the create command documentation . The most important variables are: CURRENT_PACKAGES_DIR : the directory where the package is installed: <vcpkg root>/installed/<triplet>/<port name> Installation \u00b6 To install the port locally, run: vcpkg install <port name> For this command to work, the port has to be located in <vcpkg root>/ports/<port name> . If we want to install the port from an alternative location, we can use the --overlay-ports option. For example, if we have the port stored in the C:/custom_ports/our_new_port directory, we can install it by: vcpkg install our_new_port --overlay-ports=C:/custom_ports If the port installation is failing and the reason is not clear from stdout, check the logs located in <vcpkg root>/buildtrees/<port name>/ Reinistallation after changes \u00b6 During testing, we can reach a scenario where a) we successfully installed the port, b) we need to make some changes. In this case, we need to reinstall the port. However, it is not completely straightforward due to binary caching . The following steps are needed to reinstall the port: uninstall the port: vcpkg remove <port name> disable the binary cache by setting the VCPKG_BINARY_SOURCES environment variable to clear in PowerShell: $env:VCPKG_BINARY_SOURCES = \"clear\" in bash: export VCPKG_BINARY_SOURCES=clear if setting the environment variable does not work (WSL), we can specify the --binarysource=clear option in the next step install the port again: vcpkg install <port name> Executable installation \u00b6 In general vcpgk does not allow to install executables, as it is a dependency manager rather than a package manager for OS. However, it is possible to install executables that are intedned to be used as tools (to the installed/<triplet>/tools directory) used in the build process. To do so, you have to add the vcpgk_copy_tools call to the portfile.cmake file: vcpkg_copy_tools( TOOL_NAMES <tool target name> AUTO_CLEAN ) The AUTO_CLEAN option ensures that the tools are deleted from the bin directory. Without it the tools will be kept in the bin directory, resulting in warnings and non-complicance with the vcpkg rules. The vcpgk_copy_tools function also automatically copies the runtime dependencies of the tools to the tools directory. Executing installed tools from cmake \u00b6 The installed tools can be executed from cmake using cmake comands specified in the CMake manual . To specify the path to the tools directory, use the VCPKG_INSTALLED_DIR and VCPKG_TARGET_TRIPLET variables: execute_process( COMMAND ${VCPKG_INSTALLED_DIR}/${VCPKG_TARGET_TRIPLET}/tools/${PROJECT_NAME}/<tool name> ) Publishing \u00b6 official guide Before publishing the port, we should check for the following: all dependencies in CMakelists.txt are required ( find_package(<package name> REQUIRED) ) and listed in the vcpkg.json file in the dependencies array the port follows the maintainer guide , especially: the port name does not clash with existing packages (check at repology ) the port should work for both Windows and Linux and on both platforms, the port should support both static and dynamic linking. the PR checklist is followed Then, the submission process is as follows: copy the port to the vcpkg repository remove the manual SOURCE_PATH overrides and uncomment the vcpkg_from_github call test the port locally withouth the --overlay-ports option format the vcpkg.json file using the vcpkg format-manifest <path to the vcpkg.json file> command create a new branch and commit the changes to that branch push the branch to the forked vcpkg repository open the forked repository in the browser and create a new pull request to the main vcpkg repository Directory Structure \u00b6 vcpkg has the following directory structure: buildtrees : contains the build directories for each installed package. Each build directory contains the build logs. installed : contains the installed packages. It has subdirectories for each triplet. Each triplet directory is than divided into folloeing subdirectories: bin : contains the shared libraries debug : contains the debug version of everything in a similar structure as the triplet directory examples : contains example binaries include : contains the header files lib : contains the static libraries share : contains the cmake scripts and other files needed for the integration of the package into a cmake project tools : contains the executables installed with vcpkg packages ports : Contains the package information for each package from the official vcpkg list. There is no special way how to update just the port dir, so update the whole vcpkg by git pull in case you need to update the list of available packages. Modules \u00b6 Vcpkg has it s own find_package macro in the toolchain file. It executes the script: vcpkg/installed/<tripplet>/share/<package name>/vcpkg-cmake-wrapper.cmake , if exists. Then, it executes the cmake scripts in that directory using the standard find_package , like a cmake config package. IDE \u00b6 Clion \u00b6 Configuration \u00b6 Set default layout \u00b6 Window -> Layouts -> Save changes in current layout Set up new surround with template \u00b6 In Clion, there are two types of surround with templates: surrond with and surround with live template . The first type use simple predefined templates and cannot be modified. However, the second type can be modified and new templates can be added. Toolchain configuration \u00b6 Go to settings -> Build, Execution, Deployment -> toolchain , add new toolchain and set: Name to whatever you want The environment should point to your toolchain: MSVC: C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community MSYS: C:\\MSYS2 WSL: From the drop-down list, choose the environment you configured for using with CLion in the previous steps Credentials (WSL) click to the setting button next to the credentials and fill host: localhost port: 2222 user and password according to your WSL system credentials Architecture (non WSL): amd64 CMake: C:\\Program Files\\CMake\\bin\\cmake.exe , for WSL, leave it as it is other fields should be filled automatically Project configuration \u00b6 Most project settings resides (hereinafter Project settings ) in settings -> Build, Execution, Deployment -> CMake . For each build configuration, add a new template and set: Name to whatever you want Build type to debug To Cmake options , add: path to vcpkg toolchain file: Linux: -DCMAKE_TOOLCHAIN_FILE=/opt/vcpkg/scripts/buildsystems/vcpkg.cmake Windows: -DCMAKE_TOOLCHAIN_FILE=C:/vcpkg/scripts/buildsystems/vcpkg.cmake Set the correct vcpkg triplet MSVC: -DVCPKG_TARGET_TRIPLET=x64-windows MinGW: -DVCPKG_TARGET_TRIPLET=x64-MinGW Linux: -DVCPKG_TARGET_TRIPLET=x64-linux WSL extra configuration \u00b6 The CLion does not see the WSL's environment variables (as of 2023-03, see here ). To fix it, go to Project settings and set add the necessary environment variables to Environment field. WSL configuration - Deprecated \u00b6 Clion connects to WSL through SSH. Therefore, you need to configure SSH in WSL. To do it, run the following script: wget https://raw.githubusercontent.com/JetBrains/clion-wsl/master/ubuntu_setup_env.sh && bash ubuntu_setup_env.sh Next, It\u2019s necessary to modify the WSL/create the WSL initialization script to fix a CMake issue when connecting from CLion. Download the wsl.conf file, and put it in /etc/. The restart the WSL (wsl.exe -t Ubuntu-20.04) Configuring only some CMake profiles \u00b6 When we click on the CMake reconiguration button, all profiles are reconfigured. Unfortunately, there is no way how to configure only some profiles. To work around this, we can deactivate the profiles we do not want to configure. To do so: go to settings -> Build, Execution, Deployment -> CMake select the profile you want to deactivate uncheck the Enable profile checkbox located at the top of the profile settings Visual Studio \u00b6 Installation \u00b6 Install Visual Studio Open/Create a CMake project Install ReSharper C++ Setting Synchronization \u00b6 Sign-in in Visual Studio using a Mictosoft account. A lot of settings should be synchronized automatically . Apply the layout: Window -> Apply Window Layout -> <Layout Name> Sync ReSharper settings: you can share the file: %APPDATA%\\JetBrains\\Shared\\vAny\\ ( ~\\AppData\\Roaming\\JetBrains\\Shared\\vAny\\ ). This does not work good though as the files are changed on both sides constantly. unfortunately, as of 01/2023, there is no good way how to share resharper settings Install roamed plugins Basic Configuration \u00b6 Add 120 char guideline install the extension add the guideline in command window: Edit.AddGuideline 120 if there is an error extension ... did not load properly , you need to install the developer analytic tools package to the Visual Studio: Visual Studio Installer -> modify Go to the Individual Components tab search for the extension and select it proceed with the Visual Studio Modification If you need to use the system CMake, configure it now (described below) If you use *.tpp file, configure a support for them (described below). installation Enable template implementation files ( .*tpp ) syntax highlighting: \u00b6 Go to Tools -> Options -> Text Editor -> File Extension select Microsoft Visual C++ write tpp to the field and click add (reopen the file to see changes) To Change the Build Verbosity \u00b6 Go to Tools -> Options -> Projects and Solutions -> Build and Run Change the value of the MSBuild project build output verbosity. Project Setting \u00b6 Configure Visual Studio to use system CMake: \u00b6 Go to Project -> CMake Settings it should open the CMakeSettings.json file Scroll to the bottom and click on show advanced settings Set the CMake executable to point to the cmake.exe file of your system CMake Build Setting and Enviromental Variables \u00b6 The build configuration is in the file CMakePresets.json , located in the root of the project. The file can be also opened by right clicking on CMakeLists.txt ad selecting Edit CMake presets . Set the CMake Toolchain File \u00b6 To set the vcpkg toolchain file add the following value to the base configuration cacheVariables dictionary: \"CMAKE_TOOLCHAIN_FILE\": { \"value\": \"C:/vcpkg/scripts/buildsystems/vcpkg.cmake\", \"type\": \"FILEPATH\" } Set the Compiler \u00b6 The MSVC toolchain has two compiler executables, default one, and clang. The default compiler configuration looks like this: \"cacheVariables\": { ... \"CMAKE_C_COMPILER\": \"cl.exe\", \"CMAKE_CXX_COMPILER\": \"cl.exe\" ... }, To change the compiler to clang, replace cl.exe by clang-cl.exe in both rows. Old Method Using CMakeSettings.json \u00b6 We can open the build setting by right click on CMakeList.txt -> Cmake Settings To configure configure vcpkg toolchain file: Under General , fill to the Cmake toolchain file the following: C:/vcpkg/scripts/buildsystems/vcpkg.cmake To configure the enviromental variable, edit the CmakeSettings.json file directly. The global variables can be set in the environments array, the per configuration ones in <config object>/environments ( exmaple ). Launch Setting \u00b6 The launch settings determins the launch configuration, most importantly, the run arguments. To modify the run arguments: 1. open the launch.vs.json file: - use the context menu: - Right-click on CMakeLists.txt -> Add Debug Configuration - select default - or open the file directly, it is stored in <PROJECT DIR>/.vs/ 2. in launch.vs.json configure: - type : default for MSVC or cppgdb for WSL - projectTarget : the name of the target (executable) - name : the display name in Visual Studio - args : json array with arguments as strings - arguments with spaces have to be quoted with escaped quotes 3. Select the launch configuration in the drop-down menu next to the play button If the configuration is not visible in the drop-down menu, double-check the launch.vs.json file. The file is not validated, so it is easy to make a typo. If there is any problem, insted of an error, the launch configuration is not available. The following problems are common: syntax error in the json (should be marked by red squiggly line) typo in the target name Other launch.vs.json options \u00b6 cwd : the working directory Microsoft reference for launch.vs.json WSL Configuration \u00b6 For using GCC 10: go to CmakeSettings.json -> CMake variables and cache select show advanced variables checkbox set CMAKE_CXX_COMPILER variable to /usr/bin/g++-10 Other Configuration \u00b6 show white spaces: Edit -> Advanced -> View White Space . configure indentation: described here Determine Visual Studio version \u00b6 At total, there are 5 different versionigs related to Visual Studio . The version which the compiler support table refers to is the version of the compiler ( cl.exe ). we can find it be examining the compiler executable stored in C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.34.31933\\bin\\Hostx64\\x64 . Problems & solutions \u00b6 Cannot regenerate Cmak cache \u00b6 go to ./vs and look for file named CmakeWorkspaceSettings . It most likelz contains a line with disable = true . Just delete the file, or the specific line. Installing Library Dependencies \u00b6 Vcpkg Libraries \u00b6 type vcpkg list , if the library you need is not listed, continue to the next steps type vcpkg search <library simple name> and inspect the result to determine the exact name of the package you need if the library is not listed, check the presence in vcpkg repo if the library is in repo, but search does not find it, update vcpkg type vcpkg install <exact name> to install the package at the end of the installation log, there will be a cmake command needed to integrate the library, put it to the appropriate place to your CMakeList.txt file To display the cmake commands for the installed libraries, just run vcpkg install <exact name> again. Boost \u00b6 With boost, we should install only the necessary components. Then to include boost, we need: find_package(Boost REQUIRED) with all compiled components listed target_include_directories(<YOUR TARGET NAME> PUBLIC ${Boost_INCLUDE_DIRS}) Sometimes, it may be usefull to find out which boost components require linking. The list in the boost documentation, both for Unix and Windows . JNI \u00b6 for JNI, a JAVA_HOME system property needs to be set to the absolute path to the JDK, e.g., C:\\Program Files\\Java\\jdk-15.0.1 Gurobi \u00b6 If you don\u2019t have Gurobi installed, do it now, and check that the installation is working Windows: just install as usual Linux: download the archive to /opt sudo tar xvfz <gurobi archive> add the the file that introduce environment variables needed for gurobi to etc/profile.d Linux only: it is necessary to build the C++ library for your version of the compiler. Steps: 1. cd <GUROBI DIR>/linux64/src/build/ 2. make 3. mv libgurobi_c++.a ../../lib/libgurobi_c++_<some id for you, like version>.a 4. cd ../../lib/ 5. ln -sf ./libgurobi_c++<some id for you, like version>.a libgurobi_c++.a Follow this guide , specifically: 1. put the attached our custom FindGUROBI script to: - Windows: C:\\Program Files\\CMake\\share\\cmake-<your cmake version>\\Modules/ - Linux: /opt/<CMAKNAME>/share/cmake-<VERSION>/Modules 2. to your CMakeLists.txt , add: - find_package(GUROBI REQUIRED) - target_include_directories(<your executable> PRIVATE ${GUROBI_INCLUDE_DIRS}) - target_link_libraries(<your executable> PRIVATE ${GUROBI_LIBRARY}) - target_link_libraries(<your executable> PRIVATE optimized ${GUROBI_CXX_LIBRARY} debug ${GUROBI_CXX_DEBUG_LIBRARY}) 3. try to load the cmake projects (i.e., generate the build scripts using cmake). 4. if the C++ library is not found ( Gurobi c++ library not found ), check whether the correct C++ library is in the gurobi home, the file <library name>.lib has to be in the lib directory of the gurobi installation. If the file is not there, it is possible that your gurobi version is too old Update Gurobi \u00b6 Updating is done by installing the new version and generating and using new licence key . after update, you need to delete your build dir in order to prevent using of cached path to old Gurobi install Also, you need to update the library name on line 10 of the FindGUROBI.cmake script. Other Libraries Not Available in vcpkg \u00b6 Test Library linking/inclusion \u00b6 For testing purposes, we can follow this simple pattern: build the library include the library: target_include_directories(<target name> PUBLIC <path to include dir>) , where include dir is the directory with the main header file of the library. if the library is not the header only library, we need to: 3.1 link the library: target_link_libraries(<target name> PUBLIC <path to lib file>) , where path to lib file is the path to the dynamic library file used for linking ( .so on Linux, .lib on Windows). 3.2. add the dynamic library to some path visible for the executable - here the library file is .so on Linux and .dll on Windows - there are plenty options for the visible path, the most common being the system PATH variable, or the directory with the executable. Dependencies with WSL and CLion \u00b6 In WSL, when combined with CLion, some find scripts does not work, because they depend on system variables, that are not correctly passed from CLIon SSH connection to CMake. Therefore, it is necessary to add hints with absolute path to these scripts. Some of them can be downloaded here . Package that require these hints: JNI Gurobi Refactoring \u00b6 The refactoring of C++ code is a complex process, so the number of supported refactoring operations is limited. In Visual Studio, the only supported refactoring operation is renaming. In IntelliJ tools (CLion, ReSharper C++), there are more tools available, but still, the refactoring is not as powerful nor reliable as in Java or Python. Other alternative is to implement the refactoring manually, with a help of some compiler tools like clang Refactoring Engine ( example project ). Changing Method Signature \u00b6 As of 2023-10, there is no reliable way how to change the method signature in C++. The most efficient tool is the method signature refactorin in either CLion or ReSharper C++. However, it does not work in all cases, so it is necessary to check and fix the code manually. Exporting symbols for shared libraries \u00b6 When creating a shared library, we have to specify which symbols are exported. These are the only symbols that can be directly used from the client code. This is done using special keywords. Because the keywords are different for different compilers, usually, some macros are used instead. Typically, these macros: use the correct keyword for the compiler support disabling the keyword for building static libraries or executables The macros are typically defined in a dedicated header file called export header . This file is then included in every header file that defines a symbol that should be exported. For the whole export machinery to work, we need to: create the export header file and include it in every header file that defines an exported symbol mark the symbols that should be exported with the export macro use CMake to supply the correct compiler flags used in the export header Creating the Export Header \u00b6 We can generate the export header file using the GenerateExportHeader module. We can get it by the following code: add_library(<target name> SHARED <files>) generate_export_header(<target name>) This will generate the export header file in the build directory. However, the export file is different for different compilers. Therefore, it is best to copy the file for each compiler and then merge the macros to create a universal export header file. Alternatively, we can use some proven export header file. Finally, we can store the export header file in the source directory and include it in every header file that defines an exported symbol. Marking the Symbols \u00b6 Usually, we mark the following symbols for export: classes: class <export macro> MyClass{...} functions: <export macro> <return type> my_function(...) Other symbols does not have to be exported as they are automatically exported by the compiler: enums and enum classes constants For shared libraries, we have to export symbols. Becasue of the differences between the compilers, and to support using the same headers for both shared and static libraries it is better to use macros The macro from the GenerateExportHeader is named <target name>_EXPORT , we can check the exact name in the export header file. Only the symbols needed by external code should be exported. The exeption is when the the interface use templates. In this case, all symbols used by the template should be exported. CMake Configuration \u00b6 Now the shared library build should work correctly. However, the static library build will be full of warnings, because the export macro is not intended for static libraries. The same is true for executables. Therefore, we need to set a special property for each target that uses the export header and is not a shared library: target_compile_definitions(<static/executable target name> PUBLIC <static condition macro >) The static condition macro from the GenerateExportHeader is named <target name>_STATIC_DEFINE . If not clear, we can find the exact macros' names in the export header file. Resouces \u00b6 tutorial on decovar Ensuring the same runtime library (MSVC) usage \u00b6 Using the same runtime library is crucial when using the MSVC compiler. At lover levels, the runtime library is set by compiler flags (see the MSVC section ). These flags are automatically passed to the compiler by the build system based on the build configuration files (e.g., MSBuild files). If we generate thes files by the IDE (Visual Studio), we have to set the runtime library in the IDE. If they are generated by CMake, there are three possible situations: we use the dynamic runtime library (default): nothing has to be done the build is handled by vcpkg (libraries installed with vcpgk install ): the runtime library is set by the VCPKG_CRT_LINKAGE variable in the triplet file. Nothing has to be done. we use the static runtime library: we have to set the CMAKE_MSVC_RUNTIME_LIBRARY variable in the CMakeLists.txt file. if we use vcpgk, we should set it based on the triplet used: cmake if (VCPKG_TARGET_TRIPLET MATCHES \"-static\") set(CMAKE_MSVC_RUNTIME_LIBRARY \"MultiThreaded$<$<CONFIG:Debug>:Debug>\") endif() # Compilation for a specific CPU ## MSVC MSVC cannot compile for a specific CPU or CPU series. It can, however, use new instructions sets more efficiently if it compiles the code without the support for CPUs thad does not support these instruction sets. The command for the compiler is: `/arch: (see MSVC documentation for details). ## GCC In GCC, the march option enables compilation for a specific hardware. ml) option enables compilation for a specific hardware. pects that you use vcpkg in a per-project configuration. To make it work, add: -DCMAKE_TOOLCHAIN_FILE=<vcpkg location>/scripts/buildsystems/vcpkg.cmake - To change build options ( option in CMakeLists.txt ), run cmake with -D <option name>=<option value> <build dir> . Example: cmake -D BUILD_TESTING=OFF . Building \u00b6 For building, use: cmake --build <build dir> where build dir is the directory containing the build scripts ( CmakeFiles folder). To list the build options: cmake -L Specify the build type (Debug, Release) \u00b6 To build in release mode, or any other build mode except for the default, we need to specify the parameters for CMake. Unfortunately, these parameters depends on the build system: Single-configuration systems (Unix, MinGW) Multi-configuration systems (Visual Studio) Single-configuration systems \u00b6 Single configuration systems have the build type hardcoded in the build scripts. Therefore, we need to specify the build type for CMake when we generate the build scripts: cmake ../ -DCMAKE_BUILD_TYPE=Release By default, the build type is Release . Multi-configuration systems \u00b6 In multi-configuration systems, the -DCMAKE_BUILD_TYPE parameter is ignored, because the build configuration is supposed to be determined when building the code (i.e., same build scripts for debug and for release). Therefore, we omit it, and instead specify the --config parameter when building the code: cmake --build . --config Release Specify the target \u00b6 We can use the --target parameter for that: cmake --build . --target <TARGET NAME> Clean the source files \u00b6 Run: cmake --build . --target clean Handling Case Insensitivity \u00b6 Windows builds are, in line with the OS, case insensitive. Moreover, the Visual Studio does some magic with names internally, so the build is case insensitive even on VS WSL builds. The case insensitivity can bring inconsistencies that later breake Unix builds. Therefore, it is desirable to have the build case sensitive even on Windows. Fortunatelly, we can toggle the case sensitivity at the OS level using this PowerShell command: Get-ChildItem <PROJECT ROOT PATH> -Recurse -Directory | ForEach-Object { fsutil.exe file setCaseSensitiveInfo $_.FullName enable } Note that this can break the git commits, so it is necessary to also configure git in your case-sensitive repo: git config core.ignorecase false Installation and Publishing \u00b6 Here, we describe how to make some library or executable available in the system ( installation ) and how to distribute it to other users ( publishing ). Vcpkg \u00b6","title":"C++ Workflow"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#introduction","text":"In C++, the workflow and especially the build pipeline is much more complicated than in other languages. Therefore, we start with brief overview of the C++ build pipelines. The following scheme shows the possible build pipelines for C++, starting from Integrated developemnt tools (IDE) and ending with the linker. This guide presents mostly the following workflows: Clion or Visual Studio IDE CMake any sequencing tool (these are discribed only briefly as they are configured automaticvally by CMake) MSVC and GCC compiler toolchains Appart from the build pipeline, we also cover the dependency management. For this, we focus on the dependency manager vcpkg.","title":"Introduction"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#compiler-toolchains","text":"There are various toolchains available on Windows and Linux, but we limit this guide for only some of them, specifically those which are frequently updated and works great with Clion.","title":"Compiler Toolchains"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#msys2-windows","text":"download follow the installation guide on the homepage install MinGW64 using: pacman -S mingw-w64-x86_64-gcc","title":"MSYS2 (Windows)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#msvc-windows","text":"install Visual Studio 2019 Comunity Edition","title":"MSVC (Windows)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#choosing-the-runtime-library","text":"official documentation When compiling with MSVC, it is crucial to choose the correct runtime library. Typically, both the compiled target and all its dependencies have to use the same runtime library. The following options are available: Option Copiler flag MSBuild name Description Linged library Multi-threaded Dynamic /MD MultiThreadedDLL The default option. The application uses the dynamic version of the runtime library. msvcrt.lib Multi-threaded Dynamic Debug /MDd MultiThreadedDebugDLL The debug version of the dynamic runtime library. msvcrtd.lib Multi-threaded Static /MT MultiThreaded The application uses the static version of the runtime library. libcmt.lib Multi-threaded Static Debug /MTd MultiThreadedDebug The debug version of the static runtime library. libcmtd.lib DLL /LD The application is compiled as a DLL. - DLL Debug /LDd The debug version of the DLL. - By default, the /MD / /MDd flags are used depending on the build type. When the mismatch occurs, we usually get the following error message: LNK2038 mismatch detected for 'RuntimeLibrary': value 'MDd_DynamicDebug' doesn't match value 'MTd_StaticDebug' in .. To determine if the type of the runtime library used by the target, we can explore the build script for the target, For MSBuild, the runtime library is set by the RuntimeLibrary property. To determine the runtime library used by a library, we can use the dumpbin tool. This tool is part of the Visual Studio installation and can be run from the Developer Command Prompt or Developer PowerShell. The following command will display the runtime library used by the library: dumpbin /directives <path to the library>","title":"Choosing the runtime library"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#resources","text":"Standard library description","title":"Resources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#common-compiler-flags","text":"/nologo : do not print the copyright banner and information messages /EH : exception handeling flags","title":"Common Compiler Flags"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#gcc-linuxwsl","text":"","title":"GCC (Linux/WSL)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#installation","text":"If on Windows, Install the WSL first (WSL 2) check if GCC is installed by typing gcc --version If GCC is not installed: sudo apt-get update sudo apt-get upgrade sudo apt-get install gcc-<version>","title":"Installation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#g","text":"g++ is the main executable for the GCC compiler. It is botha compiler and a wrapper for the linker. Typical usage is: g++ -o <output file> <cpp files and library files> : compilation and linking g++ -c <cpp files> : compilation only g++ -o <output file> <object files and library files> : linking only Other frequently used flags are: -g : include debug information -l<library> : link the library named lib<library> . Unlike direct library specification, here the linkers searches for the library in standard directories.","title":"g++"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#build-sequencing-tools","text":"","title":"Build Sequencing Tools"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#make","text":"Most of the time, make scripts should not be written manually, but generated by build script generators like CMake. Therefore, here, we describe the structure of make scripts as they are generated by CMake. The structure is: Makefile : the main file that contains the rules for building the project CMakeFiles : a directory containing the files referenced in the Makefile <target name>.dir directory for each target link.txt : the file containing the linking command","title":"Make"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#msbuild","text":"The MSBuild is a XML-based build sequencing tool. The scripts are typically generated by CMake or directly by Visual Studio. The main file for each target is the <target name>.vcxproj file. All properties are stored in the root element <Project> . The structure inside the project element is as follows (only the most important elements are listed): <ItemDefinitionGroup Condition=\"<configuration>\"> : contains the properties for the given configuration. <ItemGroup><multiple <ClCompile> elements><ItemGroup> : contains the source files that are compiled <ItemGroup><multiple <ProjectReference> elements><ItemGroup> : contains references to other targets (dependencies). Theses targets are built before the current target. The propertis for each configuration are structured as follows: <ClCompile> : contains the properties for the compilation of the source files (e.g., language standard, runtime library, etc.) <ResourceCompile> : <PreprocessorDefinitions> : the preprocessor definitions used for the target. These are introduced either by the target itself or by the dependencies. <Link> : contains the properties for the linking of the target (e.g., the libraries to link, the output file, etc.)","title":"MSBuild"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#cmake","text":"Windows: Install CMake from https://cmake.org/download/ if your CMake is too old (e.g. error: \u201cCMake 3.15 or higher is required\u201d), update CMake (same as new install) Linux: If cmake is installed already, uninstall it! Do not use the cmake from linux repositories!! Download CMake sh installer from https://cmake.org/download/ install: sudo chmod +x <INSTALLER> sudo <INSTALLER> sudo rm <INSTALLER> add cmake executable to path Other details about CMake can be found in the CMake Manual.","title":"Cmake"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#vcpkg","text":"Vcpkg is a package manager for C++ libraries it serves as a developement package manager rather than a system package manager. VCpkg can work in two modes: Classic mode : vcpkg is installed centrally. This mode is useful for development and testing. Manifest mode : vcpkg is installed in the project directory. This mode is useful for deployment. To install vcpkg: clone the repo to the desired location (project directory for manifest mode, any directory for classic mode) run the bootstrap script ( bootstrap-vcpkg.bat on Windows, bootstrap-vcpkg.sh on Linux) for classic mode, add the vcpkg directory to PATH , so the program can be run from anywhere Beware that to run it with sudo on linux, it is not that easy . To install a package, run vcpkg install package .","title":"vcpkg"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#changing-the-default-triplet","text":"To change the default triplet, add a new system variable VCPKG_DEFAULT_TRIPLET , so your default library version installed with vcpkg will be x64 (like our builds), set it to: x64-linux for Linux Compilers x64-windows for MSVC x64-MinGW for MinGW","title":"Changing the default triplet"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#cmake-integration","text":"documentation By default, CMake does not see the vcpkg. To set up the appropriate enviroment variables, paths, etc., we need to run cmake commands with path to cmake toolchain file: vcpkg/scripts/buildsystems/vcpkg.cmake . See the IDE and command line section for the detailed instructions how to execute cmake with the path to the vcpkg toolchain file. The toolchain file is executed early on, so it is safe to assume that the environment will be correctly set up before the commands in yor cmake script.","title":"CMake Integration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#update","text":"git pull bootstrap vcpkg again Windows: bootstrap-vcpkg.bat Linux: bootstrap-vcpkg.sh","title":"Update"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#update-package","text":"Update vcpkg vcpkg update to get a list of available updates vcpkg upgrade --no-dry-run to actually perform the upgrade All packages are upgraded by default. To upgrade just one package, supply the name of the package (e.g., zlib:x64-windows) as an argument to upgrade command.","title":"Update package"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#upgrade-packages-matching-a-pattern","text":"For libraries that are divided into many interdependent packages (like boost), it is useful to upgrade all packages that match a pattern at once. Unfortunately, the upgrade command does not support the pattern matching. The following command can be used to upgrade all packages that match a pattern in PowerShell: vcpkg update | sls -pattern \"boost-\\w+\" | foreach-object { vcpkg upgrade $_.Matches.Value --no-dry-run }","title":"Upgrade packages matching a pattern"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#package-features","text":"Some libraries have optional features, which are not installed by default, but we can install them explicitely. For example llvm . After vcpkg install llvm and typing vcpkg search llvm : llvm 10.0.0#6 The LLVM Compiler Infrastructure llvm[clang] Build C Language Family Front-end. llvm[clang-tools-extra] Build Clang tools. ... llvm[target-all] Build with all backends. llvm[target-amdgpu] Build with AMDGPU backend. llvm[target-arm] Build with ARM backend. Above, we can see that there are a lot of optional targets. To install the the arm target, for example, we can use vcpkg install llvm[target-arm] . Sometimes, a new build of the main package is required, in that case, we need to type vcpkg install llvm[target-arm] --recurse .","title":"Package Features"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#package-versions","text":"In classic mode, there is no way how to control the version of a package. For that, we have to use the manifest mode","title":"Package Versions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#integrate-your-library-to-vcpkg","text":"For complete integration of your library to vcpkg, the following steps are needed: Configure and test the CMake installation Crate the port and test it locally ( vcpkg installation ) Submit the port to the vcpkg repository ( publishing ) Resources: decovar tutorial","title":"Integrate your library to vcpkg"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#create-the-port","text":"The official guide for packageing Maintainer guide missing details from other guides contains the list of deprecated functions Vcpkg works with ports which are special directories containing all files describing a C++ package. The usuall process is: The usual port contain these files: portfile.cmake : the main file containing the calls to cmake functions that install the package vcpkg.json : metadata file containing the package name, version, dependencies, etc. usage : a file containing the usage instructions for the package. These instructions are displayed at the end of the installation process. example A simple portfile.cmake can look like this: # download the source code vcpkg_from_github( OUT_SOURCE_PATH SOURCE_PATH REPO <reo owner>/<repo name> REF <tag name> SHA512 <hash of the files> HEAD_REF <branch name> ) # configure the source code vcpkg_cmake_configure( SOURCE_PATH <path to source dir> ) # build the source code and install it vcpkg_cmake_install() # fix the cmake generaed files for vcpkg vcpkg_cmake_config_fixup(PACKAGE_NAME <package name>) # install the license vcpkg_install_copyright(FILE_LIST \"${SOURCE_PATH}/LICENSE.txt\") # install the usage file file(INSTALL \"${CMAKE_CURRENT_LIST_DIR}/usage\" DESTINATION \"${CURRENT_PACKAGES_DIR}/share/${PORT}\") Explanation: vcpkg_from_github : downloads the source code from the github repository the <path to source dir> is the directory where the CMakeLists.txt file is located. It is usually the directory where the source code is downloaded, so we can set it to ${SOURCE_PATH} the <hash of the files> can be easily obtained by: setting the ` to 0 running the vcpkg install <port name> copying the hash from the error message vcpkg_cmake_configure : configures the source code using cmake (wraps the cmake command) vcpkg_cmake_install : builds and installs the source code (wraps the cmake --build . --target install command) the majority of code is in the subroutine vcpkg_cmake_build if we need some libraries installed with vcpkg at runtime during the build of the package, we need to use the ADD_BIN_TO_PATH option in the vcpkg_cmake_install function . This is needed as the automatic dll copy to the output dir ( VCPKG_APPLOCAL_DEPS ) is disabelled by the vcpkg_cmake_build function. This option solve the problem by prepending the PATH environment variable with the path to the vcpkg installed libraries ( <vcpkg root>/installed/<triplet>/bin for release and <vcpkg root>/installed/<triplet>/debug/bin for debug). vcpkg_cmake_config_fixup : fixes the cmake generated files for vcpkg. This is needed because the cmake generated files are not compatible with vcpkg. The function fixes the CMakeConfig.cmake and CMakeConfigVersion.cmake files. the <package name> is the name of the package, usually the same as the port name vcpkg_install_copyright installs the license files listed in the FILE_LIST argument to share/<port name>/copyright file. The copyright file is obligatory for the package to be accepted to the vcpkg repository. The vcpkg.json file can look like this: { { \"name\": \"fconfig\", \"version-string\": \"0.1.0\", \"description\": \"C++ implementation of the fconfig configuration system\", \"homepage\": \"https://github.com/F-I-D-O/Future-Config\", \"license\": \"MIT\", \"dependencies\": [ { \"name\" : \"vcpkg-cmake\", \"host\" : true }, \"yaml-cpp\", \"spdlog\", \"inja\" ] } } Here: the license key is obligatory and has to match the license file of the package The dependencies with the host key set to true are the dependencies that are required for the build, but not for the runtime.","title":"Create the Port"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#variables-and-functions-available-in-the-portfilecmake","text":"The variables and functions available in the portfile.cmake are described in the create command documentation . The most important variables are: CURRENT_PACKAGES_DIR : the directory where the package is installed: <vcpkg root>/installed/<triplet>/<port name>","title":"Variables and Functions available in the portfile.cmake"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#installation_1","text":"To install the port locally, run: vcpkg install <port name> For this command to work, the port has to be located in <vcpkg root>/ports/<port name> . If we want to install the port from an alternative location, we can use the --overlay-ports option. For example, if we have the port stored in the C:/custom_ports/our_new_port directory, we can install it by: vcpkg install our_new_port --overlay-ports=C:/custom_ports If the port installation is failing and the reason is not clear from stdout, check the logs located in <vcpkg root>/buildtrees/<port name>/","title":"Installation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#reinistallation-after-changes","text":"During testing, we can reach a scenario where a) we successfully installed the port, b) we need to make some changes. In this case, we need to reinstall the port. However, it is not completely straightforward due to binary caching . The following steps are needed to reinstall the port: uninstall the port: vcpkg remove <port name> disable the binary cache by setting the VCPKG_BINARY_SOURCES environment variable to clear in PowerShell: $env:VCPKG_BINARY_SOURCES = \"clear\" in bash: export VCPKG_BINARY_SOURCES=clear if setting the environment variable does not work (WSL), we can specify the --binarysource=clear option in the next step install the port again: vcpkg install <port name>","title":"Reinistallation after changes"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#executable-installation","text":"In general vcpgk does not allow to install executables, as it is a dependency manager rather than a package manager for OS. However, it is possible to install executables that are intedned to be used as tools (to the installed/<triplet>/tools directory) used in the build process. To do so, you have to add the vcpgk_copy_tools call to the portfile.cmake file: vcpkg_copy_tools( TOOL_NAMES <tool target name> AUTO_CLEAN ) The AUTO_CLEAN option ensures that the tools are deleted from the bin directory. Without it the tools will be kept in the bin directory, resulting in warnings and non-complicance with the vcpkg rules. The vcpgk_copy_tools function also automatically copies the runtime dependencies of the tools to the tools directory.","title":"Executable installation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#executing-installed-tools-from-cmake","text":"The installed tools can be executed from cmake using cmake comands specified in the CMake manual . To specify the path to the tools directory, use the VCPKG_INSTALLED_DIR and VCPKG_TARGET_TRIPLET variables: execute_process( COMMAND ${VCPKG_INSTALLED_DIR}/${VCPKG_TARGET_TRIPLET}/tools/${PROJECT_NAME}/<tool name> )","title":"Executing installed tools from cmake"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#publishing","text":"official guide Before publishing the port, we should check for the following: all dependencies in CMakelists.txt are required ( find_package(<package name> REQUIRED) ) and listed in the vcpkg.json file in the dependencies array the port follows the maintainer guide , especially: the port name does not clash with existing packages (check at repology ) the port should work for both Windows and Linux and on both platforms, the port should support both static and dynamic linking. the PR checklist is followed Then, the submission process is as follows: copy the port to the vcpkg repository remove the manual SOURCE_PATH overrides and uncomment the vcpkg_from_github call test the port locally withouth the --overlay-ports option format the vcpkg.json file using the vcpkg format-manifest <path to the vcpkg.json file> command create a new branch and commit the changes to that branch push the branch to the forked vcpkg repository open the forked repository in the browser and create a new pull request to the main vcpkg repository","title":"Publishing"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#directory-structure","text":"vcpkg has the following directory structure: buildtrees : contains the build directories for each installed package. Each build directory contains the build logs. installed : contains the installed packages. It has subdirectories for each triplet. Each triplet directory is than divided into folloeing subdirectories: bin : contains the shared libraries debug : contains the debug version of everything in a similar structure as the triplet directory examples : contains example binaries include : contains the header files lib : contains the static libraries share : contains the cmake scripts and other files needed for the integration of the package into a cmake project tools : contains the executables installed with vcpkg packages ports : Contains the package information for each package from the official vcpkg list. There is no special way how to update just the port dir, so update the whole vcpkg by git pull in case you need to update the list of available packages.","title":"Directory Structure"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#modules","text":"Vcpkg has it s own find_package macro in the toolchain file. It executes the script: vcpkg/installed/<tripplet>/share/<package name>/vcpkg-cmake-wrapper.cmake , if exists. Then, it executes the cmake scripts in that directory using the standard find_package , like a cmake config package.","title":"Modules"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#ide","text":"","title":"IDE"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#clion","text":"","title":"Clion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#configuration","text":"","title":"Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#set-default-layout","text":"Window -> Layouts -> Save changes in current layout","title":"Set default layout"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#set-up-new-surround-with-template","text":"In Clion, there are two types of surround with templates: surrond with and surround with live template . The first type use simple predefined templates and cannot be modified. However, the second type can be modified and new templates can be added.","title":"Set up new surround with template"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#toolchain-configuration","text":"Go to settings -> Build, Execution, Deployment -> toolchain , add new toolchain and set: Name to whatever you want The environment should point to your toolchain: MSVC: C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community MSYS: C:\\MSYS2 WSL: From the drop-down list, choose the environment you configured for using with CLion in the previous steps Credentials (WSL) click to the setting button next to the credentials and fill host: localhost port: 2222 user and password according to your WSL system credentials Architecture (non WSL): amd64 CMake: C:\\Program Files\\CMake\\bin\\cmake.exe , for WSL, leave it as it is other fields should be filled automatically","title":"Toolchain configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#project-configuration","text":"Most project settings resides (hereinafter Project settings ) in settings -> Build, Execution, Deployment -> CMake . For each build configuration, add a new template and set: Name to whatever you want Build type to debug To Cmake options , add: path to vcpkg toolchain file: Linux: -DCMAKE_TOOLCHAIN_FILE=/opt/vcpkg/scripts/buildsystems/vcpkg.cmake Windows: -DCMAKE_TOOLCHAIN_FILE=C:/vcpkg/scripts/buildsystems/vcpkg.cmake Set the correct vcpkg triplet MSVC: -DVCPKG_TARGET_TRIPLET=x64-windows MinGW: -DVCPKG_TARGET_TRIPLET=x64-MinGW Linux: -DVCPKG_TARGET_TRIPLET=x64-linux","title":"Project configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#wsl-extra-configuration","text":"The CLion does not see the WSL's environment variables (as of 2023-03, see here ). To fix it, go to Project settings and set add the necessary environment variables to Environment field.","title":"WSL extra configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#wsl-configuration-deprecated","text":"Clion connects to WSL through SSH. Therefore, you need to configure SSH in WSL. To do it, run the following script: wget https://raw.githubusercontent.com/JetBrains/clion-wsl/master/ubuntu_setup_env.sh && bash ubuntu_setup_env.sh Next, It\u2019s necessary to modify the WSL/create the WSL initialization script to fix a CMake issue when connecting from CLion. Download the wsl.conf file, and put it in /etc/. The restart the WSL (wsl.exe -t Ubuntu-20.04)","title":"WSL configuration - Deprecated"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#configuring-only-some-cmake-profiles","text":"When we click on the CMake reconiguration button, all profiles are reconfigured. Unfortunately, there is no way how to configure only some profiles. To work around this, we can deactivate the profiles we do not want to configure. To do so: go to settings -> Build, Execution, Deployment -> CMake select the profile you want to deactivate uncheck the Enable profile checkbox located at the top of the profile settings","title":"Configuring only some CMake profiles"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#visual-studio","text":"","title":"Visual Studio"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#installation_2","text":"Install Visual Studio Open/Create a CMake project Install ReSharper C++","title":"Installation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#setting-synchronization","text":"Sign-in in Visual Studio using a Mictosoft account. A lot of settings should be synchronized automatically . Apply the layout: Window -> Apply Window Layout -> <Layout Name> Sync ReSharper settings: you can share the file: %APPDATA%\\JetBrains\\Shared\\vAny\\ ( ~\\AppData\\Roaming\\JetBrains\\Shared\\vAny\\ ). This does not work good though as the files are changed on both sides constantly. unfortunately, as of 01/2023, there is no good way how to share resharper settings Install roamed plugins","title":"Setting Synchronization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#basic-configuration","text":"Add 120 char guideline install the extension add the guideline in command window: Edit.AddGuideline 120 if there is an error extension ... did not load properly , you need to install the developer analytic tools package to the Visual Studio: Visual Studio Installer -> modify Go to the Individual Components tab search for the extension and select it proceed with the Visual Studio Modification If you need to use the system CMake, configure it now (described below) If you use *.tpp file, configure a support for them (described below). installation","title":"Basic Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#enable-template-implementation-files-tpp-syntax-highlighting","text":"Go to Tools -> Options -> Text Editor -> File Extension select Microsoft Visual C++ write tpp to the field and click add (reopen the file to see changes)","title":"Enable template implementation files (.*tpp) syntax highlighting:"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#to-change-the-build-verbosity","text":"Go to Tools -> Options -> Projects and Solutions -> Build and Run Change the value of the MSBuild project build output verbosity.","title":"To Change the Build Verbosity"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#project-setting","text":"","title":"Project Setting"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#configure-visual-studio-to-use-system-cmake","text":"Go to Project -> CMake Settings it should open the CMakeSettings.json file Scroll to the bottom and click on show advanced settings Set the CMake executable to point to the cmake.exe file of your system CMake","title":"Configure Visual Studio to use system CMake:"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#build-setting-and-enviromental-variables","text":"The build configuration is in the file CMakePresets.json , located in the root of the project. The file can be also opened by right clicking on CMakeLists.txt ad selecting Edit CMake presets .","title":"Build Setting and Enviromental Variables"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#set-the-cmake-toolchain-file","text":"To set the vcpkg toolchain file add the following value to the base configuration cacheVariables dictionary: \"CMAKE_TOOLCHAIN_FILE\": { \"value\": \"C:/vcpkg/scripts/buildsystems/vcpkg.cmake\", \"type\": \"FILEPATH\" }","title":"Set the CMake Toolchain File"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#set-the-compiler","text":"The MSVC toolchain has two compiler executables, default one, and clang. The default compiler configuration looks like this: \"cacheVariables\": { ... \"CMAKE_C_COMPILER\": \"cl.exe\", \"CMAKE_CXX_COMPILER\": \"cl.exe\" ... }, To change the compiler to clang, replace cl.exe by clang-cl.exe in both rows.","title":"Set the Compiler"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#old-method-using-cmakesettingsjson","text":"We can open the build setting by right click on CMakeList.txt -> Cmake Settings To configure configure vcpkg toolchain file: Under General , fill to the Cmake toolchain file the following: C:/vcpkg/scripts/buildsystems/vcpkg.cmake To configure the enviromental variable, edit the CmakeSettings.json file directly. The global variables can be set in the environments array, the per configuration ones in <config object>/environments ( exmaple ).","title":"Old Method Using CMakeSettings.json"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#launch-setting","text":"The launch settings determins the launch configuration, most importantly, the run arguments. To modify the run arguments: 1. open the launch.vs.json file: - use the context menu: - Right-click on CMakeLists.txt -> Add Debug Configuration - select default - or open the file directly, it is stored in <PROJECT DIR>/.vs/ 2. in launch.vs.json configure: - type : default for MSVC or cppgdb for WSL - projectTarget : the name of the target (executable) - name : the display name in Visual Studio - args : json array with arguments as strings - arguments with spaces have to be quoted with escaped quotes 3. Select the launch configuration in the drop-down menu next to the play button If the configuration is not visible in the drop-down menu, double-check the launch.vs.json file. The file is not validated, so it is easy to make a typo. If there is any problem, insted of an error, the launch configuration is not available. The following problems are common: syntax error in the json (should be marked by red squiggly line) typo in the target name","title":"Launch Setting"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#other-launchvsjson-options","text":"cwd : the working directory Microsoft reference for launch.vs.json","title":"Other launch.vs.json options"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#wsl-configuration","text":"For using GCC 10: go to CmakeSettings.json -> CMake variables and cache select show advanced variables checkbox set CMAKE_CXX_COMPILER variable to /usr/bin/g++-10","title":"WSL Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#other-configuration","text":"show white spaces: Edit -> Advanced -> View White Space . configure indentation: described here","title":"Other Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#determine-visual-studio-version","text":"At total, there are 5 different versionigs related to Visual Studio . The version which the compiler support table refers to is the version of the compiler ( cl.exe ). we can find it be examining the compiler executable stored in C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.34.31933\\bin\\Hostx64\\x64 .","title":"Determine Visual Studio version"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#problems-solutions","text":"","title":"Problems &amp; solutions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#cannot-regenerate-cmak-cache","text":"go to ./vs and look for file named CmakeWorkspaceSettings . It most likelz contains a line with disable = true . Just delete the file, or the specific line.","title":"Cannot regenerate Cmak cache"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#installing-library-dependencies","text":"","title":"Installing Library Dependencies"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#vcpkg-libraries","text":"type vcpkg list , if the library you need is not listed, continue to the next steps type vcpkg search <library simple name> and inspect the result to determine the exact name of the package you need if the library is not listed, check the presence in vcpkg repo if the library is in repo, but search does not find it, update vcpkg type vcpkg install <exact name> to install the package at the end of the installation log, there will be a cmake command needed to integrate the library, put it to the appropriate place to your CMakeList.txt file To display the cmake commands for the installed libraries, just run vcpkg install <exact name> again.","title":"Vcpkg Libraries"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#boost","text":"With boost, we should install only the necessary components. Then to include boost, we need: find_package(Boost REQUIRED) with all compiled components listed target_include_directories(<YOUR TARGET NAME> PUBLIC ${Boost_INCLUDE_DIRS}) Sometimes, it may be usefull to find out which boost components require linking. The list in the boost documentation, both for Unix and Windows .","title":"Boost"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#jni","text":"for JNI, a JAVA_HOME system property needs to be set to the absolute path to the JDK, e.g., C:\\Program Files\\Java\\jdk-15.0.1","title":"JNI"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#gurobi","text":"If you don\u2019t have Gurobi installed, do it now, and check that the installation is working Windows: just install as usual Linux: download the archive to /opt sudo tar xvfz <gurobi archive> add the the file that introduce environment variables needed for gurobi to etc/profile.d Linux only: it is necessary to build the C++ library for your version of the compiler. Steps: 1. cd <GUROBI DIR>/linux64/src/build/ 2. make 3. mv libgurobi_c++.a ../../lib/libgurobi_c++_<some id for you, like version>.a 4. cd ../../lib/ 5. ln -sf ./libgurobi_c++<some id for you, like version>.a libgurobi_c++.a Follow this guide , specifically: 1. put the attached our custom FindGUROBI script to: - Windows: C:\\Program Files\\CMake\\share\\cmake-<your cmake version>\\Modules/ - Linux: /opt/<CMAKNAME>/share/cmake-<VERSION>/Modules 2. to your CMakeLists.txt , add: - find_package(GUROBI REQUIRED) - target_include_directories(<your executable> PRIVATE ${GUROBI_INCLUDE_DIRS}) - target_link_libraries(<your executable> PRIVATE ${GUROBI_LIBRARY}) - target_link_libraries(<your executable> PRIVATE optimized ${GUROBI_CXX_LIBRARY} debug ${GUROBI_CXX_DEBUG_LIBRARY}) 3. try to load the cmake projects (i.e., generate the build scripts using cmake). 4. if the C++ library is not found ( Gurobi c++ library not found ), check whether the correct C++ library is in the gurobi home, the file <library name>.lib has to be in the lib directory of the gurobi installation. If the file is not there, it is possible that your gurobi version is too old","title":"Gurobi"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#update-gurobi","text":"Updating is done by installing the new version and generating and using new licence key . after update, you need to delete your build dir in order to prevent using of cached path to old Gurobi install Also, you need to update the library name on line 10 of the FindGUROBI.cmake script.","title":"Update Gurobi"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#other-libraries-not-available-in-vcpkg","text":"","title":"Other Libraries Not Available in vcpkg"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#test-library-linkinginclusion","text":"For testing purposes, we can follow this simple pattern: build the library include the library: target_include_directories(<target name> PUBLIC <path to include dir>) , where include dir is the directory with the main header file of the library. if the library is not the header only library, we need to: 3.1 link the library: target_link_libraries(<target name> PUBLIC <path to lib file>) , where path to lib file is the path to the dynamic library file used for linking ( .so on Linux, .lib on Windows). 3.2. add the dynamic library to some path visible for the executable - here the library file is .so on Linux and .dll on Windows - there are plenty options for the visible path, the most common being the system PATH variable, or the directory with the executable.","title":"Test Library linking/inclusion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#dependencies-with-wsl-and-clion","text":"In WSL, when combined with CLion, some find scripts does not work, because they depend on system variables, that are not correctly passed from CLIon SSH connection to CMake. Therefore, it is necessary to add hints with absolute path to these scripts. Some of them can be downloaded here . Package that require these hints: JNI Gurobi","title":"Dependencies with WSL and CLion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#refactoring","text":"The refactoring of C++ code is a complex process, so the number of supported refactoring operations is limited. In Visual Studio, the only supported refactoring operation is renaming. In IntelliJ tools (CLion, ReSharper C++), there are more tools available, but still, the refactoring is not as powerful nor reliable as in Java or Python. Other alternative is to implement the refactoring manually, with a help of some compiler tools like clang Refactoring Engine ( example project ).","title":"Refactoring"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#changing-method-signature","text":"As of 2023-10, there is no reliable way how to change the method signature in C++. The most efficient tool is the method signature refactorin in either CLion or ReSharper C++. However, it does not work in all cases, so it is necessary to check and fix the code manually.","title":"Changing Method Signature"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#exporting-symbols-for-shared-libraries","text":"When creating a shared library, we have to specify which symbols are exported. These are the only symbols that can be directly used from the client code. This is done using special keywords. Because the keywords are different for different compilers, usually, some macros are used instead. Typically, these macros: use the correct keyword for the compiler support disabling the keyword for building static libraries or executables The macros are typically defined in a dedicated header file called export header . This file is then included in every header file that defines a symbol that should be exported. For the whole export machinery to work, we need to: create the export header file and include it in every header file that defines an exported symbol mark the symbols that should be exported with the export macro use CMake to supply the correct compiler flags used in the export header","title":"Exporting symbols for shared libraries"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#creating-the-export-header","text":"We can generate the export header file using the GenerateExportHeader module. We can get it by the following code: add_library(<target name> SHARED <files>) generate_export_header(<target name>) This will generate the export header file in the build directory. However, the export file is different for different compilers. Therefore, it is best to copy the file for each compiler and then merge the macros to create a universal export header file. Alternatively, we can use some proven export header file. Finally, we can store the export header file in the source directory and include it in every header file that defines an exported symbol.","title":"Creating the Export Header"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#marking-the-symbols","text":"Usually, we mark the following symbols for export: classes: class <export macro> MyClass{...} functions: <export macro> <return type> my_function(...) Other symbols does not have to be exported as they are automatically exported by the compiler: enums and enum classes constants For shared libraries, we have to export symbols. Becasue of the differences between the compilers, and to support using the same headers for both shared and static libraries it is better to use macros The macro from the GenerateExportHeader is named <target name>_EXPORT , we can check the exact name in the export header file. Only the symbols needed by external code should be exported. The exeption is when the the interface use templates. In this case, all symbols used by the template should be exported.","title":"Marking the Symbols"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#cmake-configuration","text":"Now the shared library build should work correctly. However, the static library build will be full of warnings, because the export macro is not intended for static libraries. The same is true for executables. Therefore, we need to set a special property for each target that uses the export header and is not a shared library: target_compile_definitions(<static/executable target name> PUBLIC <static condition macro >) The static condition macro from the GenerateExportHeader is named <target name>_STATIC_DEFINE . If not clear, we can find the exact macros' names in the export header file.","title":"CMake Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#resouces","text":"tutorial on decovar","title":"Resouces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#ensuring-the-same-runtime-library-msvc-usage","text":"Using the same runtime library is crucial when using the MSVC compiler. At lover levels, the runtime library is set by compiler flags (see the MSVC section ). These flags are automatically passed to the compiler by the build system based on the build configuration files (e.g., MSBuild files). If we generate thes files by the IDE (Visual Studio), we have to set the runtime library in the IDE. If they are generated by CMake, there are three possible situations: we use the dynamic runtime library (default): nothing has to be done the build is handled by vcpkg (libraries installed with vcpgk install ): the runtime library is set by the VCPKG_CRT_LINKAGE variable in the triplet file. Nothing has to be done. we use the static runtime library: we have to set the CMAKE_MSVC_RUNTIME_LIBRARY variable in the CMakeLists.txt file. if we use vcpgk, we should set it based on the triplet used: cmake if (VCPKG_TARGET_TRIPLET MATCHES \"-static\") set(CMAKE_MSVC_RUNTIME_LIBRARY \"MultiThreaded$<$<CONFIG:Debug>:Debug>\") endif() # Compilation for a specific CPU ## MSVC MSVC cannot compile for a specific CPU or CPU series. It can, however, use new instructions sets more efficiently if it compiles the code without the support for CPUs thad does not support these instruction sets. The command for the compiler is: `/arch: (see MSVC documentation for details). ## GCC In GCC, the march option enables compilation for a specific hardware. ml) option enables compilation for a specific hardware. pects that you use vcpkg in a per-project configuration. To make it work, add: -DCMAKE_TOOLCHAIN_FILE=<vcpkg location>/scripts/buildsystems/vcpkg.cmake - To change build options ( option in CMakeLists.txt ), run cmake with -D <option name>=<option value> <build dir> . Example: cmake -D BUILD_TESTING=OFF .","title":"Ensuring the same runtime library (MSVC) usage"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#building","text":"For building, use: cmake --build <build dir> where build dir is the directory containing the build scripts ( CmakeFiles folder). To list the build options: cmake -L","title":"Building"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#specify-the-build-type-debug-release","text":"To build in release mode, or any other build mode except for the default, we need to specify the parameters for CMake. Unfortunately, these parameters depends on the build system: Single-configuration systems (Unix, MinGW) Multi-configuration systems (Visual Studio)","title":"Specify the build type (Debug, Release)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#single-configuration-systems","text":"Single configuration systems have the build type hardcoded in the build scripts. Therefore, we need to specify the build type for CMake when we generate the build scripts: cmake ../ -DCMAKE_BUILD_TYPE=Release By default, the build type is Release .","title":"Single-configuration systems"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#multi-configuration-systems","text":"In multi-configuration systems, the -DCMAKE_BUILD_TYPE parameter is ignored, because the build configuration is supposed to be determined when building the code (i.e., same build scripts for debug and for release). Therefore, we omit it, and instead specify the --config parameter when building the code: cmake --build . --config Release","title":"Multi-configuration systems"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#specify-the-target","text":"We can use the --target parameter for that: cmake --build . --target <TARGET NAME>","title":"Specify the target"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#clean-the-source-files","text":"Run: cmake --build . --target clean","title":"Clean the source files"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#handling-case-insensitivity","text":"Windows builds are, in line with the OS, case insensitive. Moreover, the Visual Studio does some magic with names internally, so the build is case insensitive even on VS WSL builds. The case insensitivity can bring inconsistencies that later breake Unix builds. Therefore, it is desirable to have the build case sensitive even on Windows. Fortunatelly, we can toggle the case sensitivity at the OS level using this PowerShell command: Get-ChildItem <PROJECT ROOT PATH> -Recurse -Directory | ForEach-Object { fsutil.exe file setCaseSensitiveInfo $_.FullName enable } Note that this can break the git commits, so it is necessary to also configure git in your case-sensitive repo: git config core.ignorecase false","title":"Handling Case Insensitivity"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#installation-and-publishing","text":"Here, we describe how to make some library or executable available in the system ( installation ) and how to distribute it to other users ( publishing ).","title":"Installation and Publishing"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#vcpkg_1","text":"","title":"Vcpkg"},{"location":"Programming/C%2B%2B/CMake%20Manual/","text":"Introduction \u00b6 CMake is a cross-platform build system generator that generates build scripts for various build systems (e.g., make, ninja, Visual Studio, Xcode) a name of the language used to write the configuration files for the build system generator and other related scripts. CMake can run in two modes: project mode : is the standard mode. This mode is used when the project is configured using the cmake command. Therefore, the CMakeLists.txt file is executed in this mode. script mode : is used when the cmake command is run with the -P option. In this mode, the CMakeLists.txt file is not executed, but the script specified by the -P option is executed. Main resources: CMake documentation CMake tutorial Generators \u00b6 CMake not only supports multiple platforms but also multiple build systems on each platform. These build systems are called generators . To get a list of available generators, run: cmake -E capabilities Commands \u00b6 Configuration: Generating Build scripts \u00b6 General syntax is: cmake <dir> Here <dir> is the CMakeLists.txt directory. The build scripts are build in current directory. We can set any cache variable using the -D argument. Example: cmake <dir> -D <variable name>=<variable value> # or equivalently cmake <dir> -D<variable name>=<variable value> Toolchain file \u00b6 To work with package managers, a link to toolchain file has to be provided as an argument. For vcpkg , the argument is as follows: # new version cmake <dir> --toolchain <vcpkg location>/scripts/buildsystems/vcpkg.cmake # old version cmake <dir> -DCMAKE_TOOLCHAIN_FILE=<vcpkg location>/scripts/buildsystems/vcpkg.cmake Note that the toolchain is only loaded at the beginnning of the generation process. Once you forgot it, you need to delete the build scripts diectory content to make this argument work for subsequent cmake commands. Usefull arguments \u00b6 -LH to see cmake nonadvanced variables together with the description. -LHA to see also the advanced variables. Note that this prints only cached variables, to print all variables, we have to edit the CmakeLists.txt. -D : To change build options ( option in CMakeLists.txt ), run cmake with -D <option name>=<option value> <build dir> . Example: cmake -D BUILD_TESTING=OFF . Legacy arguments \u00b6 -H : to specify the source directory (where the CMakeLists.txt file is located). Now it is specified as the positional argument or using -S . Building \u00b6 For building, use: cmake --build <build dir> where build dir is the directory containing the build scripts ( CmakeFiles folder). To list the build options: cmake -L Specify the target \u00b6 By default, all targets are built. We can use the --target to specify a single target: cmake --build . --target <TARGET NAME> There is also a special target all that builds all targets, which is equivalent to not specifying the --target argument. Specify the build type (Debug, Release) \u00b6 In CMake, we use a specific build type string instead of compiler and linker flags: Debug - Debug build Release - Release build RelWithDebInfo - Release build with debug information MinSizeRel - Release build with minimal size Unfortunately, the way how the build type should be specified depends on the build system: Single-configuration systems (GCC, Clang, MinGW) Multi-configuration systems (MSVC) Single-configuration systems \u00b6 Single configuration systems have the compiler flags hardcoded in the build scripts. Therefore, we need to specify the build type for CMake when we generate the build scripts: cmake ../ -DCMAKE_BUILD_TYPE=Release By default, the build type is an empty string . This means that no extra flags are added to the compiler and linker so the compiler and linker run with their default settings. Interesting info can be found in this SO question . Multi-configuration systems \u00b6 In multi-configuration systems, the -DCMAKE_BUILD_TYPE parameter is ignored, because the build configuration is supposed to be determined when building the code (i.e., same build scripts for debug and for release). Therefore, we omit it, and instead specify the --config parameter when building the code: cmake --build . --config Release Clean the source files \u00b6 Run: cmake --build . --target clean Install \u00b6 To be able to install the project, it needs to be configured to do so. For this, check the installation configuration . To install the project, run: cmake --install <build dir> Note that the project needs to be built first. If it is not, we can build and install in one step using the --build with the --target install argument: cmake --build <build dir> --target install Usually, we want to use a different directory when testing the installation. To do that, we need to configure the project with the CMAKE_INSTALL_PREFIX variable. Example: cmake -DCMAKE_INSTALL_PREFIX=<test install dir> <source dir> CMake command-line tools \u00b6 documentation Apart from standard commands listed in previous sections, CMake provides several command-line tools that are not directly related to the build process. These tools wrap the system commands so that we are able to use them in a cross-platform way. To run these tools, execute: cmake -E <tool name> <arguments> The most useful tools are: copy - copy files and directories capabilities - print the properties of the system related to the build process Copy tool \u00b6 The copy tool has two signatures: copy <source> <destination> copy -t <destination> <source> ( only available in CMake 3.26 and later ) Here, <source> can be a directory, a file, or a list of files. The <destination> can be a directory or a file. Syntax \u00b6 Variables \u00b6 In order to use a variable in the CMakeLists.txt file, we have to use the ${} syntax: message(STATUS \"dir=${dir}\") In conditions, we can use the variable using its name: if(DEFINED <name>) ... Variables are set using the set command: set(<variable name> <variable value>) Enviromental variables \u00b6 We can use environmental variables using the ENV variable: if(DEFINED ENV{<name>}) ... Be aware that in string, we use only one pair of curly braces (see variable references manual ): message(STATUS \"dir=$ENV{dir}\") Built-in variables \u00b6 There are some variable generated by default by CMake. These are: CMAKE_CURRENT_SOURCE_DIR : the directory where the currently processed CMakeLists.txt file is located. CMAKE_CURRENT_BINARY_DIR : the directory where the build scripts are located and where the build process is executed. PROJECT_SOURCE_DIR : the nearest directory up in the directory tree where the CMakeLists.txt with the project command is located. There are also variables for installation directories typical for Unix systems. Touse them, we have to include the GNUInstallDirs module . The variables have two formats: CMAKE_INSTALL_<dir> : the directory relative to the installation prefix. These variables have to be used in the install command and other commands that use the installation prefix. CMAKE_INSTALL_FULL_<dir> : the full path to the directory. Notable variables are: BINDIR : the directory for executables ( bin ) LIBDIR : the directory for libraries ( lib ) INCLUDEDIR : the directory for headers ( include ) List variables \u00b6 List variables are defined similarly to scalar variables using the set command: set(<name> <value 1> <value 2> ...) Then, we can use the list variable in: commands that accept lists (e.g., add_executable , add_library , target_link_libraries ) for loops Print all variables \u00b6 To print all variables, the following function can be used: function(dump_cmake_variables) if (ARGV0) message(STATUS \"Printing variables matching '${ARGV0}'\") else() message(STATUS \"Printing all variables\") endif() get_cmake_property(_variableNames VARIABLES) list (SORT _variableNames) foreach (_variableName ${_variableNames}) if (ARGV0) unset(MATCHED) string(REGEX MATCH ${ARGV0} MATCHED ${_variableName}) if (NOT MATCHED) continue() endif() endif() message(STATUS \"${_variableName}=${${_variableName}}\") endforeach() message(STATUS \"Printing variables - END\") endfunction() To print all variables related to HDF5 lib, call dump_cmake_variables(HDF) after the find_package call. Control structures \u00b6 if \u00b6 The if command has the following syntax: if(<condition>) ... elseif(<condition>) ... else() ... endif() The condition can be: a variable an expression Each expression can use some of the supported operators: logical operators: AND , OR , NOT comparison operators: EQUAL , LESS , GREATER , LESS_EQUAL , GREATER_EQUAL , STREQUAL , STRLESS , STRGREATER , STRLESS_EQUAL , STRGREATER_EQUAL file operators: EXISTS , IS_DIRECTORY , IS_REGULAR_FILE , IS_SYMLINK , IS_ABSOLUTE , IS_RELATIVE , IS_NEWER_THAN , IS_OLDER_THAN string operators: MATCHES , LESS , GREATER , LESS_EQUAL , GREATER_EQUAL , STREQUAL , STRLESS , STRGREATER , STRLESS_EQUAL , STRGREATER_EQUAL version operators: VERSION_EQUAL , VERSION_LESS , VERSION_GREATER , VERSION_LESS_EQUAL , VERSION_GREATER_EQUAL these are ment to be used with version string variables created by the find_package command: cmake find_package(<package name> CONFIG REQUIRED) if(<package name>_VERSION VERSION_LESS <version>) ... endif() and more... For the full list of operators, see the if command documentation . Generator expressions \u00b6 Manual Generator expressions are a very useful tool to control the build process based on the build type, compiler type, or similar properties. CMake use them to generate mutliple build scripts from a single CMakeLists.txt file. The syntax for a basic condition expression is: \"$<$<condition>:<this will be printed if condition is satisfied>>\" Unlike, variables, the generator expressions are evaluated during the build process, not during the configuration process . Therefore, they cannot be dumped during the configuration process, and they cannot be used in the if command. However, they can be still used in variables and commands, if the evaluation is not needed during the configuration process. Notable variable expressions: $<TARGET_FILE_DIR:<target name>> - the directory where the target will be built $<TARGET_RUNTIME_DLLS:<target name>> - the list of runtime dependencies of the target Evaluating generator expressions during configuration \u00b6 In case we need to see the evaluated generator expressions during cmake configuration, we can try to cheat using the following command: file(GENERATE OUTPUT <filename> CONTENT <string-with-generator-expression>) This way, we receive the evaluated value of the generator expression in the file for one of the build configurations. File operations \u00b6 To perform file operations, use the file command. The most useful subcommands are: MAKE_DIRECTORY <directory> - create a directory RENAME <from> <to> - renames/moves a file or directory. The <from> must exist, and the parent directory of <to> must exist. REMOVE <file> - remove a file REMOVE_RECURSE <directory> - remove a directory and all its content Functions \u00b6 Functions are defined using the function command. The syntax is: function(<function name> <argument 1> <argument 2> ...) ... endfunction() This way, we have a function with simple positional arguments. These arguments can be used in the function body as variables: function(print_arguments arg1 arg2) message(STATUS \"arg1=${arg1}\") message(STATUS \"arg2=${arg2}\") endfunction() To call the function, use the following syntax: print_arguments(\"value 1\" \"value 2\") More resources: https://hsf-training.github.io/hsf-training-cmake-webpage/11-functions/index.html Named arguments \u00b6 We can notice that a typical cmake function has named arguments, e.g., add_custom_command(TARGET <target name> POST_BUILD COMMAND <command>) . To achieve this, we can use the cmake_parse_arguments command. The syntax is: function(<function name>) cmake_parse_arguments( PARSE_ARGV <required args count> <variable prefix> <options> <one_value_keywords> <multi_value_keywords> ) This way, we say that the function has: <required args count> required arguments, all argument values are stored in variables with the name <variable prefix>_<variable_name> , and the variable names available for the caller as well as the <variable_name> values are determined by the <options> , <one_value_keywords> , and <multi_value_keywords> arguments. Each of the <options> , <one_value_keywords> , and <multi_value_keywords> arguments is a list of argument names divided by a semicolon. The <options> are the arguments that can be either present or not, the <one_value_keywords> are the arguments that have a single value, and the <multi_value_keywords> are the arguments that have multiple values. Default values for arguments \u00b6 There is no specific syntax for default values for arguments. We can achieve this, for example, by using the if command: if(NOT DEFINED <variable>) set(<variable> <default value>) endif() CMakeLists.txt \u00b6 The CMakeLists.txt file is the main configuration file for any CMake project. This file is executed during the configuration step (when the cmake command is run without arguments specifying another step). It contains commands written in the CMake language that are used to configure the build process. The typical structure of the CMakeLists.txt file is as follows: Top section contains project wide setting like name, minimum cmake version, and the language specification. Targets sections containing: the target definition together with sources used target includes target linking Typical Top section content \u00b6 The typical content of the top section is: minimum cmake version: cmake_minimum_required(VERSION <version>) project name and version: project(<name> VERSION <version>) language specification: enable_language(<language>) cmake variables setup, e.g.: set(CMAKE_CXX_STANDARD <version>) compile options: add_compile_options(<option 1> <option 2> ...) cmake module inclusion: include(<module name>) Language standards \u00b6 The language standard is set using the set command together with the CMAKE_<LANG>_STANDARD variable. Example: set(CMAKE_CXX_STANDARD 17) This way, the standard is set for all targets and the compiler should be configured for that standard. However, if the compiler does not support the standard, the build script generation continues and the failure will appear later during the compilation. To avoid that, we can make the standard a requirement using the set command together with the CMAKE_<LANG>_STANDARD_REQUIRED variable. Example: set(CMAKE_CXX_STANDARD_REQUIRED ON) However, the required standard is not always correctly supported by the compiler (e.g., GCC up to version 13 does not support C++20). Therefore, we need to specify the minimum version for these compilers: if(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 13.0.0) message(FATAL_ERROR \"GCC version must be at least 13.0.0!\") endif() Set the runtime library type for MSVC \u00b6 In MSVC, it is crucial that both the target and all its dependencies are compiled with the same (standard) runtime library type. To set the library type for the target in CMake, we use the CMKAE_MSVC_RUNTIME_LIBRARY variable. The possible values are: MultiThreadedDLL - the dynamic library (default in Release mode) MultiThreadedDebugDLL - the dynamic library with debug information (default in Debug mode) MultiThreaded - the static library MultiThreadedDebug - the static library with debug information By default, the dynamic library is used. To set the static library, use: set(CMAKE_MSVC_RUNTIME_LIBRARY \"MultiThreaded$<$<CONFIG:Debug>:Debug>\") This way, the static library with debug information is used in the Debug mode, and the static library is used in the Release mode. Note that the CMAKE_MSVC_RUNTIME_LIBRARY variable was introduced in CMake 3.15. Therefore, you have to set cmake_minimum_required(VERSION 3.15) in the CMakeLists.txt file, or set the CMP0091 policy to NEW using the cmake_policy command. Compile options \u00b6 Most of the compile options are now sets automatically based on the declarations in the CMakeLists.txt file. However, some notable exceptions exists. To set such options, we have to use the add_compile_options command: add_compile_options(<option 1> <option 2> ...) MSVC \u00b6 /permissive- to enable the strictest mode of the compiler GCC \u00b6 -pedantic-errors to report all cases where non-standard GCC extension is used and treat them as errors Linker Options \u00b6 Linker options can be set with add_link_options command. Example: add_link_options(\"/STACK: 10000000\") Dependency management \u00b6 There are many ways how to manage dependencies in CMake, for complete overview, see the documentation . Although it is possible to hard-code the paths for includes and linking, it is usually better to initialize the paths automatically using a rich set of commands cmake offers. It has the following advatages: Hardcoding the paths is error-prone, while cmake commands usually deliver correct paths It boost the productivity as we do not have to investigate where each library is installed The resulting CMakeLists.txt file is more portable And most importantly, potential errors concerning missing libraries are reported prior to the compilation/linking . Most of the libraries have CMake support, so their CMake variables can be initialized simply by calling the find_package command described below. These packages have either: their own cmake config (cmake-aware libs usually installed through the package manager like vcpkg ) or they have a Find<package name> script created by someone else that heuristically search for the packege (The default location for these scripts is CMake/share/cmake-<version>/Modules ). For packages without the CMake support, we have to use lower-level cmake commands like find_path or find_libraries . For convinience, we can put these command to our own Find<name> script taht can be used by multiple project or even shared. Standard way: find_package \u00b6 The find_package command is the primary way of obtaining correct variables for a library including: include paths linking paths platform/toolchain specific enviromental variables There are two types of package (library) info search: module , which uses cmake scripts provided by CMake or OS. The modules are typically provided only for the most used libraries (e.g. boost). All modules provided by CMake are listed in the documentation . config which uses CMake scripts provided by the developers of the package. They are typically distributed with the source code and downloaded by the package manager. Unless specified, the module mode is used. To force a speciic mode, we can use the MODULE / CONFIG parameters. Config packages \u00b6 Config packages are CMake modules that were created as cmake projects by their developers. They are therefore naturally integrated into Cmake. The configuration files are executed as follows: Package version file: <package name>-config-version.cmake or <package name>ConfigVersion.cmake . This file handles the version compatibility, i.e., it ensures that the installed version of the package is compatible with the version requested in the find_package command. Package configuration file: <package name>-config.cmake or <package name>Config.cmake . Module Packages \u00b6 Module packages are packages that are not cmake projects themselves, but are hooked into cmake using custom find module scrips. These scripts are automatically executed by find_package . They are located in e.g.: CMake/share/cmake-3.22/Modules/Find<package name>.cmake . Searching for include directories with find_path \u00b6 The find_path command is intended to find the path (e.g., an include directory). A simple syntax is: find_path( <var name> NAMES <file names> PATHS <paths> ) Here: <var name> is the name of the resulting variable <file names> are all possible file names split by space. At least one of the files needs to be present in a path for it to be considered to be the found path. <paths> are candidate paths split by space Low level command: find_library \u00b6 The find_library command is used to populate a variable with a result of a specific file search optimized for libraries. The search algorithm works as follows: ? Search package paths order: <CurrentPackage>_ROOT , ENV{<CurrentPackage>_ROOT} , <ParentPackage>_ROOT , ENV{<ParentPackage>_ROOT} this only happens if the find_library command is called from within a find_<module> or find_package this step can be skipped using the NO_PACKAGE_ROOT_PATH parameter Search path from cmake cache. During a clean cmake generation, these can be only supplied by command line. Considered variables: CMAKE_LIBRARY_ARCHITECTURE CMAKE_PREFIX_PATH CMAKE_LIBRARY_PATH CMAKE_FRAMEWORK_PATH this step can be skipped using the NO_CMAKE_PATH parameter Same as step 3, but the variables are searched among system environmental variables instead this step can be skipped using the NO_CMAKE_ENVIRONMENT_PATH parameter Search paths specified by the HINTS option Search the standard system environmental paths variables considered are LIB and PATH this step can be skipped using the NO_SYSTEM_ENVIRONMENT_PATH parameter Search in system paths Considered variables: CMAKE_LIBRARY_ARCHITECTURE CMAKE_SYSTEM_PREFIX_PATH CMAKE_SYSTEM_LIBRARY_PATH CMAKE_SYSTEM_FRAMEWORK_PATH this step can be skipped using the NO_CMAKE_SYSTEM_PATH parameter Search the paths specified by the PATHS option Searching for libraries in the project dir \u00b6 Note that the project dir is not searched by default. To include in the search, use: HINTS ${PROJECT_SOURCE_DIR} . Full example on the Gurobi lib stored in <CMAKE LISTS DIR/lib/gurobi_c++.a> : find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED ) Creating a custom find script \u00b6 The structure of a simple find scripts is described in the documentation . We can either put the find script to the default location, so it will be available for all projects, or we can put it in the project directory and add that directory to the CMAKE_MODULE_PATH : list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\") The usual structure of the find script is: the comment section describing the file: #[=======================================================================[.rst: FindMOSEK ------- Finds the MOSEK library. Result Variables ^^^^^^^^^^^^^^^^ This will define the following variables: ``MOESK_FOUND`` True if the system has the MOSEK library. ``MOSEK_INCLUDE_DIRS`` Include directories needed to use MOSEK. ``MOSEK_LIBRARIES`` Libraries needed to link to MOSEK. #]=======================================================================] find commands that fils some temp variables: find_path( MOSEK_INCLUDE_DIR NAMES mosek.h PATHS \"$ENV{MOSEK_HOME}/h\" ) find_library( MOSEK_LIBRARY NAMES libmosek64.so.10.0 libfusion64.so.10.0 PATHS \"$ENV{MOSEK_HOME}/bin\" ) handeling of the result of the file commands. The standard approach is: include(FindPackageHandleStandardArgs) find_package_handle_standard_args(MOSEK FOUND_VAR MOSEK_FOUND REQUIRED_VARS MOSEK_LIBRARY MOSEK_INCLUDE_DIR ) setting the final variables: if(MOSEK_FOUND) set(MOSEK_LIBRARIES ${MOSEK_LIBRARY}) set(MOSEK_INCLUDE_DIRS ${MOSEK_INCLUDE_DIR}) endif() Downolad dependencies during configuration and build them from source \u00b6 To download and build dependencies during the configuration, we can use the FetchContent module. This way, the sources are downloaded at the configuration step, and built during the build step. Note that because of this, the usage of these dpendencies is limited, we cannot, for example, run the dependency tools at configuration time. For details, see the dependency management documentation . The usual workflow is: Download the dependency using the FetchContent_Declare command cmake FetchContent_Declare( <NAME> <SPECIFICATION> ) The specification can be either a URL or a git repository. Configure the dependency using the FetchContent_MakeAvailable command: cmake FetchContent_MakeAvailable(<NAME>) If the dependency is a CMake project, the FetchContent_MakeAvailable command will automatically configure the project by calling the add_subdirectory command with the path to the downloaded source code. Both the dependency source code and the build directory are stored in the build directory in _deps folder. This directory contains: <NAME>-src - the source code of the dependency <NAME>-build - the build directory of the dependency CMake Targets \u00b6 Targets define logical units of the build process. These can be: executables libraries custom targets doing all sorts of things sets of targets, i.e., aliases for building multiple targets at once Available targets are either user-defined or automatically generated by CMake. Executable targets \u00b6 The target definition is done using the add_executable command. The syntax is: add_executable(<target name> <source file 1> <source file 2> ...) The target name is used to refer to the target in other commands. The target name is also used to name the output file. The list of source files should contain all the source files that are needed to build the target. There are some automatic mechanisms that can be used to add the source files (discussed e.g. on cmake forums ), but they are not recommended. Library targets \u00b6 Library targets are defined using the add_library command. The syntax is: add_library(<target name> <type> <source file 1> <source file 2> ...) Targets automatically generated by CMake \u00b6 listed here . Besides the user-defined targets, CMake automatically generates some targets. These are: all : alias for building all targets. Default target if the --target argument is not specified. In Visual Studio and Xcode generators, this target is called ALL_BUILD . we can exclude some targets from the all target using the EXCLUDE_FROM_ALL target property or the EXCLUDE_FROM_ALL directory property . clean target that cleans the build directory install target that installs the project. It depends on the all target. this target is only available if the CMakelists.txt file contains the install command (see installation configuration ). and some more... Set properties for a target \u00b6 To set compile options for a target, use the target_compile_definitions command. The syntax is: target_compile_definitions(<target name> <SCOPE> <definition 1> <definition 2> ...) Include directories \u00b6 To include the headers, we need to use a inlude_directories (global), or better target_include_directories command. The difference: target specification : target_include_directories specifies the include directories for a specific target, while include_directories specifies the include directories for all targets in the current directory. mode specification : target_include_directories specifies the mode of the include directories (e.g., PUBLIC , PRIVATE , INTERFACE ), while include_directories behaves simillar to PRIVATE . Therefore, for libraries, the target_include_directories has to be used. Inspect the Include directories \u00b6 All the global include directories are stored in the INCLUDE_DIRECTORIES property, to print them, add this to the CMakeLists.txt file: get_property(dirs DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} PROPERTY INCLUDE_DIRECTORIES) foreach(dir ${dirs}) message(STATUS \"dir='${dir}'\") endforeach() Linking configuration \u00b6 For linking, use the target_link_libraries command. Make sure that you always link against all the libraries that are needed for the target to work! Do not rely on the linker errors, these may not appear due to library preloading, indirect linkage, advanced linker heuristics, etc. The result is that on one machine the code will work, but on another, it will fail. To find out if and how to link against a library, refer to the documentation of the library. Handling runtime dependencies in the output directory \u00b6 When linking dynamically (e.g., by using dynamic toochain, which is default on Windows), the runtime dependencies must be available at runtime, otherwise the program will not run. On Windows, this is usually solved by copying the runtime dependencies to the output directory. There are several ways how to do that. when using vcpkg , the runtime dependencies are copied automatically if the VCPKG_APPLOCAL_DEPS variable is set to ON otherwise, we can copy the runtime dependencies using the add_custom_command command or we can just manually copy the runtime dependencies to the output directory Copying runtime dependencies using add_custom_command \u00b6 We can use the add_custom_command command together with the $<TARGET_RUNTIME_DLLS:<target name>> generator expression. Be careful to wrap this code by a condition that checks the generator type, as the generator expression is only available for DLL-aware generators. if(CMAKE_GENERATOR MATCHES \"Visual Studio.*\") add_custom_command( TARGET <target name> POST_BUILD COMMAND ${CMAKE_COMMAND} -E copy $<TARGET_RUNTIME_DLLS:<target name>> $<TARGET_FILE_DIR:<target name>> COMMAND_EXPAND_LISTS ) endif() Here, the COMMAND_EXPAND_LISTS is used to expand the generator expressions in the command. We use it for the $<TARGET_RUNTIME_DLLS:<target name>> generator expression that returns the list of runtime dependencies of the target. Runtime dependencies and subdirectories \u00b6 When using the add_subdirectory command, a new build directory is created for the subdirectory. However, the runtime dependencies in the parent directory are not accessible and therefore it must be duplicated in the subdirectory ( VCPKG_APPLOCAL_DEPS handles this automatically). One can think that we can solve that by using the same output directory for the parent and the subdirectory (using the build dir parameter of the add_subdirectory command ). However, this is prohibited by CMake. Installation Configuration \u00b6 Importing and Exporting Guide To enable installation, we have to provide several commands and do some adjustments in the CMakeLists.txt file. Specific steps depends on what we want to achieve. The minimal installation that installs only binaries can be set up using two commands: Install binaries with the install(TARGETS... command. The basic syntax is: cmake install(TARGETS <target name 1> <target name 2>) Install headers with the install(FILES... command. The basic syntax is: cmake install(FILES <file 1> <file 2> DESTINATION <destination>) We have to install both public API headers and their internal dependencies. Usually, this means installing most of the headers. The <destination> is the directory where the files will be installed. Typically, it is ${CMAKE_INSTALL_INCLUDEDIR}/${PROJECT_NAME} for header files. We usually only want a single set of headers to be installed for both Debug and Release builds ( for vcpkg, this is required ). This can be achieved by using the CONFIGURATIONS parameter of the install command. Example: cmake install( FILES <file 1> <file 2> DESTINATION <destination> CONFIGURATIONS Release ) This command provides the install target that builds all targets (it depends on the all target) and then installs the project, which means that it copies the binary files to the installation directory. Install CMake files \u00b6 If we want our library to be used from CMake projects, the basic configuration is not enough. We have to provide the CMake configuration files that will be used by the find_package command when searching for the dependency. Usually, CMake packages have the following cmake files: <package name>Targes.cmake - contains the targets that are installed by the package <package name>Config.cmake - contains the configuration of the package, i.e., the include directories, linking directories, and other variables that are needed to use the package. <package name>ConfigVersion.cmake - contains the version of the package and the compatibility check. Installing the Targets file \u00b6 This involves two steps: use the EXPORT parameter of the install(TARGETS... command to create a reference to installed targets that we can use further cmake install(TARGETS <target name 1> <target name 2> EXPORT <export name> ) The <export name> is the name of the export set that will be used in the CMake configuration file. Typically, it is ${PROJECT_NAME}Targets . use the install(EXPORT... command to install the cmake files cmake install(EXPORT <export name> DESTINATION <destination> NAMESPACE <namespace> ) The <export name> is the name of the export set that was used in the install(TARGETS... command The <destination> is the directory where the CMake configuration files will be installed. Typically, it is ( source ): ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME} for binaries share/cmake/${PROJECT_NAME} for header-only libraries for vcpkg : share/${PROJECT_NAME} . Otherwise, we need to mess with the CONFIG_PATH parameter of the vcpkg_cmake_config_fixup function. for using the CMAKE_INSTALL_LIBDIR variable, we have to include the GNUInstallDirs module using the include command. The <namespace> is the namespace that will be used in the CMake configuration file, and later in the target_link_libraries command of the dependent project. It is not required, but it is recommended because the namespace with :: only searches between the exported targets, preventing a possible name clash with some library installed in the system. typically, it has the form of <project name>:: Installing the package configuration file and the version file \u00b6 official guide - package config file official guide - version file In order for these files to be portable, they should be generated. The appropriate functions for this are in the `CMakePackageConfigHelpers module , which has to be included. To generate the package configuration file: create the input file somewhere in the project directory with the name <package name>Config.cmake.in and fill it with the following content: ```cmake @PACKAGE_INIT@ include (\"${CMAKE_CURRENT_LIST_DIR}/ Targes.cmake\") optional content ... - The `@PACKAGE_INIT@` is a placeholder that will be replaced by the `write_basic_package_version_file` command. 1. then generate the package configuration file by ading the `configure_package_config_file` command to the `CMakeLists.txt` file: cmake configure_package_config_file( INSTALL_DESTINATION ) `` - here, the <cmake files installation path> is the same as the parameter in the install(EXPORT...` command. The version file is generated using the write_basic_package_version_file command. The syntax is: write_basic_package_version_file( <version file build path> VERSION <version> COMPATIBILITY AnyNewerVersion ) The <version file build path> is the path where the version file will be generated. The <version> is the version of the package. Typically, it is the version of the project: ${<project name>Major}.${<project name>Minor}.${<project name>Patch} . Finally, to install both files, we use the install(FILES... command: install(FILES <path to package config file in the build dir> <path to version file in the build dir> DESTINATION <cmake files installation path> ) Sanitation of the public include directories \u00b6 Additionally, an extra step is needed in case we define include directories to be accessible from other projects (i.e., PUBLIC or INTERFACE mode). In this case, we have to use a special configuration for those directories so that a different path is used when the library is installed. For example, if we include the src directory using: target_include_directories(<target name> PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/src) we have to change it to: target_include_directories(<target name> PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/src> $<INSTALL_INTERFACE:include> ) Handling dependencies \u00b6 If our library depends on other libraries, the targets depending on our library may also need to link against these dependencies. The linking itself is automatically handled by CMake. However, we need to provide the variables that are needed for the in the package configuration file ( <packag name.cmake.in ). To do so, we use the find_dependency command from the `CMakeFindDependencyMacro module : @PACKAGE_INIT@ include(CMakeFindDependencyMacro) find_dependency(<dependency 1>) ... find_dependency(<dependency n>) include(\"${CMAKE_CURRENT_LIST_DIR}/<package name>Targets.cmake\") ... The name of the dependency is the name of the package that is used in the find_package command in the CMakeLists.txt file. Other parameters ( REQUIRED , CONFIG , etc.) are not needed as they are inherited from the find_package command. Specify the targets to install \u00b6 The install command has a parameter TARGETS that can specify the targets that should be installed. The syntax is: However, the TARGETS parameter only affects the installation part of the install target . In other words, it determines what files are copied to the installation directory, but it does not affect the build process. Therefore, all targets are built regardless of the TARGETS parameter . To prevent the building of some targets when installing, we have several options: exclude selected targets from the all target using the EXCLUDE_FROM_ALL target property or the EXCLUDE_FROM_ALL directory property . disable building for the install target and use a custom wrapper target that builds only the targets that should be installed and then calls the install target. Selecting the targets to build using the EXCLUDE_FROM_ALL property \u00b6 For user targets added by the add_executable or add_library command, we can use the EXCLUDE_FROM_ALL target property: add_executable(my_target EXCLUDE_FROM_ALL <source file 1> <source file 2> ...) For targets added by the FetchContent_MakeAvailable command. The situation is more complicated. The FetchContent_Declare command provides the EXCLUDE_FROM_ALL option, but it does not work as expected. Instead, we have to set the EXCLUDE_FROM_ALL property for the target after the FetchContent_MakeAvailable command: FetchContent_Declare( <declaration name> ... ) FetchContent_MakeAvailable(<declaration name>) set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_ALL TRUE) Disabling the building of the install target and using a custom wrapper target \u00b6 For this solution,we first need to disable the building of the install using: set(CMAKE_SKIP_INSTALL_ALL_DEPENDENCY true) . Then, we need to create a target that: a) builds the selected targets, b) calls the install target. CMAKE_SKIP_INSTALL_ALL_DEPENDENCY documentation Support both shared and static libraries \u00b6 Sometimes, we want the user to be able to choose between shared and static libraries when installing the package. For vcpkg, this is required. To add this support, we have to: define option for choosing the shared or static library (most likely, we do not want to install both) install the correct library target based on the option install the export header file (in both cases, as we use the same headers for both shared and static libraries) The conditional installation can look like this: if(<package name>_BUILD_SHARED_LIBS) set_target_properties(<static lib target> PROPERTIES EXCLUDE_FROM_ALL TRUE) install(TARGETS <shared lib target> EXPORT <export name>) else() set_target_properties(<shared lib target> PROPERTIES EXCLUDE_FROM_ALL TRUE) set_target_properties(<static lib target> PROPERTIES EXPORT_NAME <shared lib target>) # this aligns the name of the exported targets install(TARGETS <static lib target> EXPORT <export name>) endif() Here, the set_target_properties command is used to exclude the other target from the all target, effectively preventing it from being built during the installation. The install command is then used to install the correct target. Finally, we just copy the export header file to the installation directory: install(FILES ${CMAKE_CURRENT_BINARY_DIR}/<export header file> DESTINATION <install include dir>) Specific configuration for frequentlly used libraries \u00b6 for google test, we want to prevent the installation of the gtest targets. To do that, turn it off before the gtets config in the CMakeLists.txt file: ```cmake # GOOGLE TEST # do not install gtest set(INSTALL_GTEST OFF) include(FetchContent) FetchContent_Declare( googletest URL https://github.com/google/googletest/archive/03597a01ee50ed33e9dfd640b249b4be3799d395.zip ) For Windows: Prevent overriding the parent project's compiler/linker settings \u00b6 set(gtest_force_shared_crt ON CACHE BOOL \"\" FORCE) FetchContent_MakeAvailable(googletest) ``` Multiple CMakeLists.txt files in subdirectories \u00b6 The configuration can be modularized by splitting the CMakeLists.txt file into multiple files, each in a separate directory. Then, these files can be executed using the add_subdirectory command in the main CMake_lists.txt file or one of the CMakeLists.txt files already added by the add_subdirectory command. This has several advantages: the configuration is more organized, it is easier to find the relevant part of the configuration or reuse the configuration in other projects the configuration is more configurable, i.e., we can turn on or off the configuration of a specific part of the project. The order of execution follows the order of the add_subdirectory commands, i.e., the processing of the CMakeLists.txt containing the add_subdirectory command is paused until the added CMakeLists.txt file is processed. Variables scope \u00b6 The variable scope for multiple CMakeLists.txt files is hierarchical. This means that the variables defined in the parent CMakeLists.txt file are visible in the child CMakeLists.txt file, but not vice versa. Decide based on the build configuration \u00b6 Sometimes is essential to decide based on the build configuration in the CMakeLists.txt file. However, this is not possible for the multi-configuration generators like Visual Studio or Xcode because the build configuration is not known during the CMake configuration step. However, there are measures that can be taken to achieve the result for particular tasks. For install command, we can use the CONFIGURATIONS parameter to install the files only for the selected build configuration. Example: install( FILES <file 1> <file 2> DESTINATION <destination> CONFIGURATIONS Release ) For building, there are ways to limit the build based on the build configuration for both single- and multi-configuration generators. For single configuration generators, we can use the CMAKE_BUILD_TYPE variable and simply exclude the target from the build using the EXCLUDE_FROM_ALL target property: if(CMAKE_BUILD_TYPE STREQUAL \"Debug\") set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_ALL TRUE) endif() For multi-configuration generators, this variable is not set, but we can use the EXCLUDE_FROM_DEFAULT_BUILD_DEBUG property: set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_DEFAULT_BUILD_DEBUG TRUE) To support both single- and multi-configuration generators, we have to first check whether we use a single- or multi-configuration generator using the CMAKE_CONFIGURATION_TYPES variable and then set the property accordingly: if(CMAKE_CONFIGURATION_TYPES) set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_DEFAULT_BUILD_DEBUG TRUE) else() if(CMAKE_BUILD_TYPE STREQUAL \"Debug\") set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_ALL TRUE) endif() endif() Executing external commands \u00b6 There are different ways to execute external commands in the CMakeLists.txt , the proper way depends on the time when we want to execute the command: configuration time : use the execute_process command build time : use the add_custom_command command can be run both before and after the build step using the PRE_BUILD and POST_BUILD options CMake Cache \u00b6 CMake has two types of variables: normal variables that are used just like in any other programming language and cache variables , which are configured in the first cmake run and then stored in the CMakeCache.txt file in the build directory. The CMake cache is an essential part of the CMake build system. It stores variables that are used to configure the build scripts. Cache variables can be set in the following ways: by the set command in the CMakeLists.txt file with the CACHE option by the -D command line argument: cmake -D<variable name>=<variable value> <dir> by the cmake GUI by using the -C option: cmake -C <cache file> <dir> The rule is that once a cache variable is set, it is not changed when the cmake command is run again (that is why it is called cache :) ). Moreover, the cache variables are not overwritten by the set command in the CMakeLists.txt file. In other words, the set command in the CMakeLists.txt is only used to set the default value of the variable. If the variable is already set in the cache, the set command is ignored. However, the cache variables can be still overridden from the CMakeLists.txt if the set command is used without the CACHE option (by normal variables). The option command \u00b6 The option command is used to define a cache variable that can be set by the user. The syntax is: option(<option name> <option description> <default value>) The behavior of the option command is as follows: If variable is already set (either a cache variable or a normal variable), the option command is ignored. Otherwise, a cache variable is created if we are in the project mode, and a normal variable is created if we are in the script mode. CMake Directory Structure \u00b6 System Find_XXX.cmake files \u00b6 The system find scripts are located in the CMake/share/cmake-<version>/Modules/ directory. Various Tasks \u00b6 Showing the generator for a configured directory \u00b6 If the configuration step is already done, we can show the generator used for the configuration by reading the CMAKE_GENERATOR variable from the CMakeCache.txt file: Get-Content CMakeCache.txt | Select-String -Pattern \"CMAKE_GENERATOR\" Debugging CMake \u00b6 Debugging CMake using CLion CMake debugger \u00b6 documentation In CLion, we can debug the CMake configuration process by: opening the CMakeLists.txt file setting the breakpoint(s) starting the debugging process by clicking on the play button which is located on the first line of the CMakeLists.txt file on the left side of the editor window. or by clicking on the debug button in the main toolbar Getting a a complete output of the CMake configuration \u00b6 We can get the complete output of the CMake configuration by running the cmake command with the --trace option. This can be especially useful when investigating included scripts, as the debuggers usually cannot step into CMake calls. Usually, the output is very long, so it is recommended to redirect it to a file: cmake --trace <dir> *> cmake_trace.txt If we want to also expand the variables, we can use the --trace-expand option.","title":"CMake Manual"},{"location":"Programming/C%2B%2B/CMake%20Manual/#introduction","text":"CMake is a cross-platform build system generator that generates build scripts for various build systems (e.g., make, ninja, Visual Studio, Xcode) a name of the language used to write the configuration files for the build system generator and other related scripts. CMake can run in two modes: project mode : is the standard mode. This mode is used when the project is configured using the cmake command. Therefore, the CMakeLists.txt file is executed in this mode. script mode : is used when the cmake command is run with the -P option. In this mode, the CMakeLists.txt file is not executed, but the script specified by the -P option is executed. Main resources: CMake documentation CMake tutorial","title":"Introduction"},{"location":"Programming/C%2B%2B/CMake%20Manual/#generators","text":"CMake not only supports multiple platforms but also multiple build systems on each platform. These build systems are called generators . To get a list of available generators, run: cmake -E capabilities","title":"Generators"},{"location":"Programming/C%2B%2B/CMake%20Manual/#commands","text":"","title":"Commands"},{"location":"Programming/C%2B%2B/CMake%20Manual/#configuration-generating-build-scripts","text":"General syntax is: cmake <dir> Here <dir> is the CMakeLists.txt directory. The build scripts are build in current directory. We can set any cache variable using the -D argument. Example: cmake <dir> -D <variable name>=<variable value> # or equivalently cmake <dir> -D<variable name>=<variable value>","title":"Configuration: Generating Build scripts"},{"location":"Programming/C%2B%2B/CMake%20Manual/#toolchain-file","text":"To work with package managers, a link to toolchain file has to be provided as an argument. For vcpkg , the argument is as follows: # new version cmake <dir> --toolchain <vcpkg location>/scripts/buildsystems/vcpkg.cmake # old version cmake <dir> -DCMAKE_TOOLCHAIN_FILE=<vcpkg location>/scripts/buildsystems/vcpkg.cmake Note that the toolchain is only loaded at the beginnning of the generation process. Once you forgot it, you need to delete the build scripts diectory content to make this argument work for subsequent cmake commands.","title":"Toolchain file"},{"location":"Programming/C%2B%2B/CMake%20Manual/#usefull-arguments","text":"-LH to see cmake nonadvanced variables together with the description. -LHA to see also the advanced variables. Note that this prints only cached variables, to print all variables, we have to edit the CmakeLists.txt. -D : To change build options ( option in CMakeLists.txt ), run cmake with -D <option name>=<option value> <build dir> . Example: cmake -D BUILD_TESTING=OFF .","title":"Usefull arguments"},{"location":"Programming/C%2B%2B/CMake%20Manual/#legacy-arguments","text":"-H : to specify the source directory (where the CMakeLists.txt file is located). Now it is specified as the positional argument or using -S .","title":"Legacy arguments"},{"location":"Programming/C%2B%2B/CMake%20Manual/#building","text":"For building, use: cmake --build <build dir> where build dir is the directory containing the build scripts ( CmakeFiles folder). To list the build options: cmake -L","title":"Building"},{"location":"Programming/C%2B%2B/CMake%20Manual/#specify-the-target","text":"By default, all targets are built. We can use the --target to specify a single target: cmake --build . --target <TARGET NAME> There is also a special target all that builds all targets, which is equivalent to not specifying the --target argument.","title":"Specify the target"},{"location":"Programming/C%2B%2B/CMake%20Manual/#specify-the-build-type-debug-release","text":"In CMake, we use a specific build type string instead of compiler and linker flags: Debug - Debug build Release - Release build RelWithDebInfo - Release build with debug information MinSizeRel - Release build with minimal size Unfortunately, the way how the build type should be specified depends on the build system: Single-configuration systems (GCC, Clang, MinGW) Multi-configuration systems (MSVC)","title":"Specify the build type (Debug, Release)"},{"location":"Programming/C%2B%2B/CMake%20Manual/#single-configuration-systems","text":"Single configuration systems have the compiler flags hardcoded in the build scripts. Therefore, we need to specify the build type for CMake when we generate the build scripts: cmake ../ -DCMAKE_BUILD_TYPE=Release By default, the build type is an empty string . This means that no extra flags are added to the compiler and linker so the compiler and linker run with their default settings. Interesting info can be found in this SO question .","title":"Single-configuration systems"},{"location":"Programming/C%2B%2B/CMake%20Manual/#multi-configuration-systems","text":"In multi-configuration systems, the -DCMAKE_BUILD_TYPE parameter is ignored, because the build configuration is supposed to be determined when building the code (i.e., same build scripts for debug and for release). Therefore, we omit it, and instead specify the --config parameter when building the code: cmake --build . --config Release","title":"Multi-configuration systems"},{"location":"Programming/C%2B%2B/CMake%20Manual/#clean-the-source-files","text":"Run: cmake --build . --target clean","title":"Clean the source files"},{"location":"Programming/C%2B%2B/CMake%20Manual/#install","text":"To be able to install the project, it needs to be configured to do so. For this, check the installation configuration . To install the project, run: cmake --install <build dir> Note that the project needs to be built first. If it is not, we can build and install in one step using the --build with the --target install argument: cmake --build <build dir> --target install Usually, we want to use a different directory when testing the installation. To do that, we need to configure the project with the CMAKE_INSTALL_PREFIX variable. Example: cmake -DCMAKE_INSTALL_PREFIX=<test install dir> <source dir>","title":"Install"},{"location":"Programming/C%2B%2B/CMake%20Manual/#cmake-command-line-tools","text":"documentation Apart from standard commands listed in previous sections, CMake provides several command-line tools that are not directly related to the build process. These tools wrap the system commands so that we are able to use them in a cross-platform way. To run these tools, execute: cmake -E <tool name> <arguments> The most useful tools are: copy - copy files and directories capabilities - print the properties of the system related to the build process","title":"CMake command-line tools"},{"location":"Programming/C%2B%2B/CMake%20Manual/#copy-tool","text":"The copy tool has two signatures: copy <source> <destination> copy -t <destination> <source> ( only available in CMake 3.26 and later ) Here, <source> can be a directory, a file, or a list of files. The <destination> can be a directory or a file.","title":"Copy tool"},{"location":"Programming/C%2B%2B/CMake%20Manual/#syntax","text":"","title":"Syntax"},{"location":"Programming/C%2B%2B/CMake%20Manual/#variables","text":"In order to use a variable in the CMakeLists.txt file, we have to use the ${} syntax: message(STATUS \"dir=${dir}\") In conditions, we can use the variable using its name: if(DEFINED <name>) ... Variables are set using the set command: set(<variable name> <variable value>)","title":"Variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#enviromental-variables","text":"We can use environmental variables using the ENV variable: if(DEFINED ENV{<name>}) ... Be aware that in string, we use only one pair of curly braces (see variable references manual ): message(STATUS \"dir=$ENV{dir}\")","title":"Enviromental variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#built-in-variables","text":"There are some variable generated by default by CMake. These are: CMAKE_CURRENT_SOURCE_DIR : the directory where the currently processed CMakeLists.txt file is located. CMAKE_CURRENT_BINARY_DIR : the directory where the build scripts are located and where the build process is executed. PROJECT_SOURCE_DIR : the nearest directory up in the directory tree where the CMakeLists.txt with the project command is located. There are also variables for installation directories typical for Unix systems. Touse them, we have to include the GNUInstallDirs module . The variables have two formats: CMAKE_INSTALL_<dir> : the directory relative to the installation prefix. These variables have to be used in the install command and other commands that use the installation prefix. CMAKE_INSTALL_FULL_<dir> : the full path to the directory. Notable variables are: BINDIR : the directory for executables ( bin ) LIBDIR : the directory for libraries ( lib ) INCLUDEDIR : the directory for headers ( include )","title":"Built-in variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#list-variables","text":"List variables are defined similarly to scalar variables using the set command: set(<name> <value 1> <value 2> ...) Then, we can use the list variable in: commands that accept lists (e.g., add_executable , add_library , target_link_libraries ) for loops","title":"List variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#print-all-variables","text":"To print all variables, the following function can be used: function(dump_cmake_variables) if (ARGV0) message(STATUS \"Printing variables matching '${ARGV0}'\") else() message(STATUS \"Printing all variables\") endif() get_cmake_property(_variableNames VARIABLES) list (SORT _variableNames) foreach (_variableName ${_variableNames}) if (ARGV0) unset(MATCHED) string(REGEX MATCH ${ARGV0} MATCHED ${_variableName}) if (NOT MATCHED) continue() endif() endif() message(STATUS \"${_variableName}=${${_variableName}}\") endforeach() message(STATUS \"Printing variables - END\") endfunction() To print all variables related to HDF5 lib, call dump_cmake_variables(HDF) after the find_package call.","title":"Print all variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#control-structures","text":"","title":"Control structures"},{"location":"Programming/C%2B%2B/CMake%20Manual/#if","text":"The if command has the following syntax: if(<condition>) ... elseif(<condition>) ... else() ... endif() The condition can be: a variable an expression Each expression can use some of the supported operators: logical operators: AND , OR , NOT comparison operators: EQUAL , LESS , GREATER , LESS_EQUAL , GREATER_EQUAL , STREQUAL , STRLESS , STRGREATER , STRLESS_EQUAL , STRGREATER_EQUAL file operators: EXISTS , IS_DIRECTORY , IS_REGULAR_FILE , IS_SYMLINK , IS_ABSOLUTE , IS_RELATIVE , IS_NEWER_THAN , IS_OLDER_THAN string operators: MATCHES , LESS , GREATER , LESS_EQUAL , GREATER_EQUAL , STREQUAL , STRLESS , STRGREATER , STRLESS_EQUAL , STRGREATER_EQUAL version operators: VERSION_EQUAL , VERSION_LESS , VERSION_GREATER , VERSION_LESS_EQUAL , VERSION_GREATER_EQUAL these are ment to be used with version string variables created by the find_package command: cmake find_package(<package name> CONFIG REQUIRED) if(<package name>_VERSION VERSION_LESS <version>) ... endif() and more... For the full list of operators, see the if command documentation .","title":"if"},{"location":"Programming/C%2B%2B/CMake%20Manual/#generator-expressions","text":"Manual Generator expressions are a very useful tool to control the build process based on the build type, compiler type, or similar properties. CMake use them to generate mutliple build scripts from a single CMakeLists.txt file. The syntax for a basic condition expression is: \"$<$<condition>:<this will be printed if condition is satisfied>>\" Unlike, variables, the generator expressions are evaluated during the build process, not during the configuration process . Therefore, they cannot be dumped during the configuration process, and they cannot be used in the if command. However, they can be still used in variables and commands, if the evaluation is not needed during the configuration process. Notable variable expressions: $<TARGET_FILE_DIR:<target name>> - the directory where the target will be built $<TARGET_RUNTIME_DLLS:<target name>> - the list of runtime dependencies of the target","title":"Generator expressions"},{"location":"Programming/C%2B%2B/CMake%20Manual/#evaluating-generator-expressions-during-configuration","text":"In case we need to see the evaluated generator expressions during cmake configuration, we can try to cheat using the following command: file(GENERATE OUTPUT <filename> CONTENT <string-with-generator-expression>) This way, we receive the evaluated value of the generator expression in the file for one of the build configurations.","title":"Evaluating generator expressions during configuration"},{"location":"Programming/C%2B%2B/CMake%20Manual/#file-operations","text":"To perform file operations, use the file command. The most useful subcommands are: MAKE_DIRECTORY <directory> - create a directory RENAME <from> <to> - renames/moves a file or directory. The <from> must exist, and the parent directory of <to> must exist. REMOVE <file> - remove a file REMOVE_RECURSE <directory> - remove a directory and all its content","title":"File operations"},{"location":"Programming/C%2B%2B/CMake%20Manual/#functions","text":"Functions are defined using the function command. The syntax is: function(<function name> <argument 1> <argument 2> ...) ... endfunction() This way, we have a function with simple positional arguments. These arguments can be used in the function body as variables: function(print_arguments arg1 arg2) message(STATUS \"arg1=${arg1}\") message(STATUS \"arg2=${arg2}\") endfunction() To call the function, use the following syntax: print_arguments(\"value 1\" \"value 2\") More resources: https://hsf-training.github.io/hsf-training-cmake-webpage/11-functions/index.html","title":"Functions"},{"location":"Programming/C%2B%2B/CMake%20Manual/#named-arguments","text":"We can notice that a typical cmake function has named arguments, e.g., add_custom_command(TARGET <target name> POST_BUILD COMMAND <command>) . To achieve this, we can use the cmake_parse_arguments command. The syntax is: function(<function name>) cmake_parse_arguments( PARSE_ARGV <required args count> <variable prefix> <options> <one_value_keywords> <multi_value_keywords> ) This way, we say that the function has: <required args count> required arguments, all argument values are stored in variables with the name <variable prefix>_<variable_name> , and the variable names available for the caller as well as the <variable_name> values are determined by the <options> , <one_value_keywords> , and <multi_value_keywords> arguments. Each of the <options> , <one_value_keywords> , and <multi_value_keywords> arguments is a list of argument names divided by a semicolon. The <options> are the arguments that can be either present or not, the <one_value_keywords> are the arguments that have a single value, and the <multi_value_keywords> are the arguments that have multiple values.","title":"Named arguments"},{"location":"Programming/C%2B%2B/CMake%20Manual/#default-values-for-arguments","text":"There is no specific syntax for default values for arguments. We can achieve this, for example, by using the if command: if(NOT DEFINED <variable>) set(<variable> <default value>) endif()","title":"Default values for arguments"},{"location":"Programming/C%2B%2B/CMake%20Manual/#cmakeliststxt","text":"The CMakeLists.txt file is the main configuration file for any CMake project. This file is executed during the configuration step (when the cmake command is run without arguments specifying another step). It contains commands written in the CMake language that are used to configure the build process. The typical structure of the CMakeLists.txt file is as follows: Top section contains project wide setting like name, minimum cmake version, and the language specification. Targets sections containing: the target definition together with sources used target includes target linking","title":"CMakeLists.txt"},{"location":"Programming/C%2B%2B/CMake%20Manual/#typical-top-section-content","text":"The typical content of the top section is: minimum cmake version: cmake_minimum_required(VERSION <version>) project name and version: project(<name> VERSION <version>) language specification: enable_language(<language>) cmake variables setup, e.g.: set(CMAKE_CXX_STANDARD <version>) compile options: add_compile_options(<option 1> <option 2> ...) cmake module inclusion: include(<module name>)","title":"Typical Top section content"},{"location":"Programming/C%2B%2B/CMake%20Manual/#language-standards","text":"The language standard is set using the set command together with the CMAKE_<LANG>_STANDARD variable. Example: set(CMAKE_CXX_STANDARD 17) This way, the standard is set for all targets and the compiler should be configured for that standard. However, if the compiler does not support the standard, the build script generation continues and the failure will appear later during the compilation. To avoid that, we can make the standard a requirement using the set command together with the CMAKE_<LANG>_STANDARD_REQUIRED variable. Example: set(CMAKE_CXX_STANDARD_REQUIRED ON) However, the required standard is not always correctly supported by the compiler (e.g., GCC up to version 13 does not support C++20). Therefore, we need to specify the minimum version for these compilers: if(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 13.0.0) message(FATAL_ERROR \"GCC version must be at least 13.0.0!\") endif()","title":"Language standards"},{"location":"Programming/C%2B%2B/CMake%20Manual/#set-the-runtime-library-type-for-msvc","text":"In MSVC, it is crucial that both the target and all its dependencies are compiled with the same (standard) runtime library type. To set the library type for the target in CMake, we use the CMKAE_MSVC_RUNTIME_LIBRARY variable. The possible values are: MultiThreadedDLL - the dynamic library (default in Release mode) MultiThreadedDebugDLL - the dynamic library with debug information (default in Debug mode) MultiThreaded - the static library MultiThreadedDebug - the static library with debug information By default, the dynamic library is used. To set the static library, use: set(CMAKE_MSVC_RUNTIME_LIBRARY \"MultiThreaded$<$<CONFIG:Debug>:Debug>\") This way, the static library with debug information is used in the Debug mode, and the static library is used in the Release mode. Note that the CMAKE_MSVC_RUNTIME_LIBRARY variable was introduced in CMake 3.15. Therefore, you have to set cmake_minimum_required(VERSION 3.15) in the CMakeLists.txt file, or set the CMP0091 policy to NEW using the cmake_policy command.","title":"Set the runtime library type for MSVC"},{"location":"Programming/C%2B%2B/CMake%20Manual/#compile-options","text":"Most of the compile options are now sets automatically based on the declarations in the CMakeLists.txt file. However, some notable exceptions exists. To set such options, we have to use the add_compile_options command: add_compile_options(<option 1> <option 2> ...)","title":"Compile options"},{"location":"Programming/C%2B%2B/CMake%20Manual/#msvc","text":"/permissive- to enable the strictest mode of the compiler","title":"MSVC"},{"location":"Programming/C%2B%2B/CMake%20Manual/#gcc","text":"-pedantic-errors to report all cases where non-standard GCC extension is used and treat them as errors","title":"GCC"},{"location":"Programming/C%2B%2B/CMake%20Manual/#linker-options","text":"Linker options can be set with add_link_options command. Example: add_link_options(\"/STACK: 10000000\")","title":"Linker Options"},{"location":"Programming/C%2B%2B/CMake%20Manual/#dependency-management","text":"There are many ways how to manage dependencies in CMake, for complete overview, see the documentation . Although it is possible to hard-code the paths for includes and linking, it is usually better to initialize the paths automatically using a rich set of commands cmake offers. It has the following advatages: Hardcoding the paths is error-prone, while cmake commands usually deliver correct paths It boost the productivity as we do not have to investigate where each library is installed The resulting CMakeLists.txt file is more portable And most importantly, potential errors concerning missing libraries are reported prior to the compilation/linking . Most of the libraries have CMake support, so their CMake variables can be initialized simply by calling the find_package command described below. These packages have either: their own cmake config (cmake-aware libs usually installed through the package manager like vcpkg ) or they have a Find<package name> script created by someone else that heuristically search for the packege (The default location for these scripts is CMake/share/cmake-<version>/Modules ). For packages without the CMake support, we have to use lower-level cmake commands like find_path or find_libraries . For convinience, we can put these command to our own Find<name> script taht can be used by multiple project or even shared.","title":"Dependency management"},{"location":"Programming/C%2B%2B/CMake%20Manual/#standard-way-find_package","text":"The find_package command is the primary way of obtaining correct variables for a library including: include paths linking paths platform/toolchain specific enviromental variables There are two types of package (library) info search: module , which uses cmake scripts provided by CMake or OS. The modules are typically provided only for the most used libraries (e.g. boost). All modules provided by CMake are listed in the documentation . config which uses CMake scripts provided by the developers of the package. They are typically distributed with the source code and downloaded by the package manager. Unless specified, the module mode is used. To force a speciic mode, we can use the MODULE / CONFIG parameters.","title":"Standard way: find_package"},{"location":"Programming/C%2B%2B/CMake%20Manual/#config-packages","text":"Config packages are CMake modules that were created as cmake projects by their developers. They are therefore naturally integrated into Cmake. The configuration files are executed as follows: Package version file: <package name>-config-version.cmake or <package name>ConfigVersion.cmake . This file handles the version compatibility, i.e., it ensures that the installed version of the package is compatible with the version requested in the find_package command. Package configuration file: <package name>-config.cmake or <package name>Config.cmake .","title":"Config packages"},{"location":"Programming/C%2B%2B/CMake%20Manual/#module-packages","text":"Module packages are packages that are not cmake projects themselves, but are hooked into cmake using custom find module scrips. These scripts are automatically executed by find_package . They are located in e.g.: CMake/share/cmake-3.22/Modules/Find<package name>.cmake .","title":"Module Packages"},{"location":"Programming/C%2B%2B/CMake%20Manual/#searching-for-include-directories-with-find_path","text":"The find_path command is intended to find the path (e.g., an include directory). A simple syntax is: find_path( <var name> NAMES <file names> PATHS <paths> ) Here: <var name> is the name of the resulting variable <file names> are all possible file names split by space. At least one of the files needs to be present in a path for it to be considered to be the found path. <paths> are candidate paths split by space","title":"Searching for include directories with find_path"},{"location":"Programming/C%2B%2B/CMake%20Manual/#low-level-command-find_library","text":"The find_library command is used to populate a variable with a result of a specific file search optimized for libraries. The search algorithm works as follows: ? Search package paths order: <CurrentPackage>_ROOT , ENV{<CurrentPackage>_ROOT} , <ParentPackage>_ROOT , ENV{<ParentPackage>_ROOT} this only happens if the find_library command is called from within a find_<module> or find_package this step can be skipped using the NO_PACKAGE_ROOT_PATH parameter Search path from cmake cache. During a clean cmake generation, these can be only supplied by command line. Considered variables: CMAKE_LIBRARY_ARCHITECTURE CMAKE_PREFIX_PATH CMAKE_LIBRARY_PATH CMAKE_FRAMEWORK_PATH this step can be skipped using the NO_CMAKE_PATH parameter Same as step 3, but the variables are searched among system environmental variables instead this step can be skipped using the NO_CMAKE_ENVIRONMENT_PATH parameter Search paths specified by the HINTS option Search the standard system environmental paths variables considered are LIB and PATH this step can be skipped using the NO_SYSTEM_ENVIRONMENT_PATH parameter Search in system paths Considered variables: CMAKE_LIBRARY_ARCHITECTURE CMAKE_SYSTEM_PREFIX_PATH CMAKE_SYSTEM_LIBRARY_PATH CMAKE_SYSTEM_FRAMEWORK_PATH this step can be skipped using the NO_CMAKE_SYSTEM_PATH parameter Search the paths specified by the PATHS option","title":"Low level command: find_library"},{"location":"Programming/C%2B%2B/CMake%20Manual/#searching-for-libraries-in-the-project-dir","text":"Note that the project dir is not searched by default. To include in the search, use: HINTS ${PROJECT_SOURCE_DIR} . Full example on the Gurobi lib stored in <CMAKE LISTS DIR/lib/gurobi_c++.a> : find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED )","title":"Searching for libraries in the project dir"},{"location":"Programming/C%2B%2B/CMake%20Manual/#creating-a-custom-find-script","text":"The structure of a simple find scripts is described in the documentation . We can either put the find script to the default location, so it will be available for all projects, or we can put it in the project directory and add that directory to the CMAKE_MODULE_PATH : list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\") The usual structure of the find script is: the comment section describing the file: #[=======================================================================[.rst: FindMOSEK ------- Finds the MOSEK library. Result Variables ^^^^^^^^^^^^^^^^ This will define the following variables: ``MOESK_FOUND`` True if the system has the MOSEK library. ``MOSEK_INCLUDE_DIRS`` Include directories needed to use MOSEK. ``MOSEK_LIBRARIES`` Libraries needed to link to MOSEK. #]=======================================================================] find commands that fils some temp variables: find_path( MOSEK_INCLUDE_DIR NAMES mosek.h PATHS \"$ENV{MOSEK_HOME}/h\" ) find_library( MOSEK_LIBRARY NAMES libmosek64.so.10.0 libfusion64.so.10.0 PATHS \"$ENV{MOSEK_HOME}/bin\" ) handeling of the result of the file commands. The standard approach is: include(FindPackageHandleStandardArgs) find_package_handle_standard_args(MOSEK FOUND_VAR MOSEK_FOUND REQUIRED_VARS MOSEK_LIBRARY MOSEK_INCLUDE_DIR ) setting the final variables: if(MOSEK_FOUND) set(MOSEK_LIBRARIES ${MOSEK_LIBRARY}) set(MOSEK_INCLUDE_DIRS ${MOSEK_INCLUDE_DIR}) endif()","title":"Creating a custom find script"},{"location":"Programming/C%2B%2B/CMake%20Manual/#downolad-dependencies-during-configuration-and-build-them-from-source","text":"To download and build dependencies during the configuration, we can use the FetchContent module. This way, the sources are downloaded at the configuration step, and built during the build step. Note that because of this, the usage of these dpendencies is limited, we cannot, for example, run the dependency tools at configuration time. For details, see the dependency management documentation . The usual workflow is: Download the dependency using the FetchContent_Declare command cmake FetchContent_Declare( <NAME> <SPECIFICATION> ) The specification can be either a URL or a git repository. Configure the dependency using the FetchContent_MakeAvailable command: cmake FetchContent_MakeAvailable(<NAME>) If the dependency is a CMake project, the FetchContent_MakeAvailable command will automatically configure the project by calling the add_subdirectory command with the path to the downloaded source code. Both the dependency source code and the build directory are stored in the build directory in _deps folder. This directory contains: <NAME>-src - the source code of the dependency <NAME>-build - the build directory of the dependency","title":"Downolad dependencies during configuration and build them from source"},{"location":"Programming/C%2B%2B/CMake%20Manual/#cmake-targets","text":"Targets define logical units of the build process. These can be: executables libraries custom targets doing all sorts of things sets of targets, i.e., aliases for building multiple targets at once Available targets are either user-defined or automatically generated by CMake.","title":"CMake Targets"},{"location":"Programming/C%2B%2B/CMake%20Manual/#executable-targets","text":"The target definition is done using the add_executable command. The syntax is: add_executable(<target name> <source file 1> <source file 2> ...) The target name is used to refer to the target in other commands. The target name is also used to name the output file. The list of source files should contain all the source files that are needed to build the target. There are some automatic mechanisms that can be used to add the source files (discussed e.g. on cmake forums ), but they are not recommended.","title":"Executable targets"},{"location":"Programming/C%2B%2B/CMake%20Manual/#library-targets","text":"Library targets are defined using the add_library command. The syntax is: add_library(<target name> <type> <source file 1> <source file 2> ...)","title":"Library targets"},{"location":"Programming/C%2B%2B/CMake%20Manual/#targets-automatically-generated-by-cmake","text":"listed here . Besides the user-defined targets, CMake automatically generates some targets. These are: all : alias for building all targets. Default target if the --target argument is not specified. In Visual Studio and Xcode generators, this target is called ALL_BUILD . we can exclude some targets from the all target using the EXCLUDE_FROM_ALL target property or the EXCLUDE_FROM_ALL directory property . clean target that cleans the build directory install target that installs the project. It depends on the all target. this target is only available if the CMakelists.txt file contains the install command (see installation configuration ). and some more...","title":"Targets automatically generated by CMake"},{"location":"Programming/C%2B%2B/CMake%20Manual/#set-properties-for-a-target","text":"To set compile options for a target, use the target_compile_definitions command. The syntax is: target_compile_definitions(<target name> <SCOPE> <definition 1> <definition 2> ...)","title":"Set properties for a target"},{"location":"Programming/C%2B%2B/CMake%20Manual/#include-directories","text":"To include the headers, we need to use a inlude_directories (global), or better target_include_directories command. The difference: target specification : target_include_directories specifies the include directories for a specific target, while include_directories specifies the include directories for all targets in the current directory. mode specification : target_include_directories specifies the mode of the include directories (e.g., PUBLIC , PRIVATE , INTERFACE ), while include_directories behaves simillar to PRIVATE . Therefore, for libraries, the target_include_directories has to be used.","title":"Include directories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#inspect-the-include-directories","text":"All the global include directories are stored in the INCLUDE_DIRECTORIES property, to print them, add this to the CMakeLists.txt file: get_property(dirs DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} PROPERTY INCLUDE_DIRECTORIES) foreach(dir ${dirs}) message(STATUS \"dir='${dir}'\") endforeach()","title":"Inspect the Include directories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#linking-configuration","text":"For linking, use the target_link_libraries command. Make sure that you always link against all the libraries that are needed for the target to work! Do not rely on the linker errors, these may not appear due to library preloading, indirect linkage, advanced linker heuristics, etc. The result is that on one machine the code will work, but on another, it will fail. To find out if and how to link against a library, refer to the documentation of the library.","title":"Linking configuration"},{"location":"Programming/C%2B%2B/CMake%20Manual/#handling-runtime-dependencies-in-the-output-directory","text":"When linking dynamically (e.g., by using dynamic toochain, which is default on Windows), the runtime dependencies must be available at runtime, otherwise the program will not run. On Windows, this is usually solved by copying the runtime dependencies to the output directory. There are several ways how to do that. when using vcpkg , the runtime dependencies are copied automatically if the VCPKG_APPLOCAL_DEPS variable is set to ON otherwise, we can copy the runtime dependencies using the add_custom_command command or we can just manually copy the runtime dependencies to the output directory","title":"Handling runtime dependencies in the output directory"},{"location":"Programming/C%2B%2B/CMake%20Manual/#copying-runtime-dependencies-using-add_custom_command","text":"We can use the add_custom_command command together with the $<TARGET_RUNTIME_DLLS:<target name>> generator expression. Be careful to wrap this code by a condition that checks the generator type, as the generator expression is only available for DLL-aware generators. if(CMAKE_GENERATOR MATCHES \"Visual Studio.*\") add_custom_command( TARGET <target name> POST_BUILD COMMAND ${CMAKE_COMMAND} -E copy $<TARGET_RUNTIME_DLLS:<target name>> $<TARGET_FILE_DIR:<target name>> COMMAND_EXPAND_LISTS ) endif() Here, the COMMAND_EXPAND_LISTS is used to expand the generator expressions in the command. We use it for the $<TARGET_RUNTIME_DLLS:<target name>> generator expression that returns the list of runtime dependencies of the target.","title":"Copying runtime dependencies using add_custom_command"},{"location":"Programming/C%2B%2B/CMake%20Manual/#runtime-dependencies-and-subdirectories","text":"When using the add_subdirectory command, a new build directory is created for the subdirectory. However, the runtime dependencies in the parent directory are not accessible and therefore it must be duplicated in the subdirectory ( VCPKG_APPLOCAL_DEPS handles this automatically). One can think that we can solve that by using the same output directory for the parent and the subdirectory (using the build dir parameter of the add_subdirectory command ). However, this is prohibited by CMake.","title":"Runtime dependencies and subdirectories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#installation-configuration","text":"Importing and Exporting Guide To enable installation, we have to provide several commands and do some adjustments in the CMakeLists.txt file. Specific steps depends on what we want to achieve. The minimal installation that installs only binaries can be set up using two commands: Install binaries with the install(TARGETS... command. The basic syntax is: cmake install(TARGETS <target name 1> <target name 2>) Install headers with the install(FILES... command. The basic syntax is: cmake install(FILES <file 1> <file 2> DESTINATION <destination>) We have to install both public API headers and their internal dependencies. Usually, this means installing most of the headers. The <destination> is the directory where the files will be installed. Typically, it is ${CMAKE_INSTALL_INCLUDEDIR}/${PROJECT_NAME} for header files. We usually only want a single set of headers to be installed for both Debug and Release builds ( for vcpkg, this is required ). This can be achieved by using the CONFIGURATIONS parameter of the install command. Example: cmake install( FILES <file 1> <file 2> DESTINATION <destination> CONFIGURATIONS Release ) This command provides the install target that builds all targets (it depends on the all target) and then installs the project, which means that it copies the binary files to the installation directory.","title":"Installation Configuration"},{"location":"Programming/C%2B%2B/CMake%20Manual/#install-cmake-files","text":"If we want our library to be used from CMake projects, the basic configuration is not enough. We have to provide the CMake configuration files that will be used by the find_package command when searching for the dependency. Usually, CMake packages have the following cmake files: <package name>Targes.cmake - contains the targets that are installed by the package <package name>Config.cmake - contains the configuration of the package, i.e., the include directories, linking directories, and other variables that are needed to use the package. <package name>ConfigVersion.cmake - contains the version of the package and the compatibility check.","title":"Install CMake files"},{"location":"Programming/C%2B%2B/CMake%20Manual/#installing-the-targets-file","text":"This involves two steps: use the EXPORT parameter of the install(TARGETS... command to create a reference to installed targets that we can use further cmake install(TARGETS <target name 1> <target name 2> EXPORT <export name> ) The <export name> is the name of the export set that will be used in the CMake configuration file. Typically, it is ${PROJECT_NAME}Targets . use the install(EXPORT... command to install the cmake files cmake install(EXPORT <export name> DESTINATION <destination> NAMESPACE <namespace> ) The <export name> is the name of the export set that was used in the install(TARGETS... command The <destination> is the directory where the CMake configuration files will be installed. Typically, it is ( source ): ${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME} for binaries share/cmake/${PROJECT_NAME} for header-only libraries for vcpkg : share/${PROJECT_NAME} . Otherwise, we need to mess with the CONFIG_PATH parameter of the vcpkg_cmake_config_fixup function. for using the CMAKE_INSTALL_LIBDIR variable, we have to include the GNUInstallDirs module using the include command. The <namespace> is the namespace that will be used in the CMake configuration file, and later in the target_link_libraries command of the dependent project. It is not required, but it is recommended because the namespace with :: only searches between the exported targets, preventing a possible name clash with some library installed in the system. typically, it has the form of <project name>::","title":"Installing the Targets file"},{"location":"Programming/C%2B%2B/CMake%20Manual/#installing-the-package-configuration-file-and-the-version-file","text":"official guide - package config file official guide - version file In order for these files to be portable, they should be generated. The appropriate functions for this are in the `CMakePackageConfigHelpers module , which has to be included. To generate the package configuration file: create the input file somewhere in the project directory with the name <package name>Config.cmake.in and fill it with the following content: ```cmake @PACKAGE_INIT@ include (\"${CMAKE_CURRENT_LIST_DIR}/ Targes.cmake\") optional content ... - The `@PACKAGE_INIT@` is a placeholder that will be replaced by the `write_basic_package_version_file` command. 1. then generate the package configuration file by ading the `configure_package_config_file` command to the `CMakeLists.txt` file: cmake configure_package_config_file( INSTALL_DESTINATION ) `` - here, the <cmake files installation path> is the same as the parameter in the install(EXPORT...` command. The version file is generated using the write_basic_package_version_file command. The syntax is: write_basic_package_version_file( <version file build path> VERSION <version> COMPATIBILITY AnyNewerVersion ) The <version file build path> is the path where the version file will be generated. The <version> is the version of the package. Typically, it is the version of the project: ${<project name>Major}.${<project name>Minor}.${<project name>Patch} . Finally, to install both files, we use the install(FILES... command: install(FILES <path to package config file in the build dir> <path to version file in the build dir> DESTINATION <cmake files installation path> )","title":"Installing the package configuration file and the version file"},{"location":"Programming/C%2B%2B/CMake%20Manual/#sanitation-of-the-public-include-directories","text":"Additionally, an extra step is needed in case we define include directories to be accessible from other projects (i.e., PUBLIC or INTERFACE mode). In this case, we have to use a special configuration for those directories so that a different path is used when the library is installed. For example, if we include the src directory using: target_include_directories(<target name> PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/src) we have to change it to: target_include_directories(<target name> PUBLIC $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/src> $<INSTALL_INTERFACE:include> )","title":"Sanitation of the public include directories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#handling-dependencies","text":"If our library depends on other libraries, the targets depending on our library may also need to link against these dependencies. The linking itself is automatically handled by CMake. However, we need to provide the variables that are needed for the in the package configuration file ( <packag name.cmake.in ). To do so, we use the find_dependency command from the `CMakeFindDependencyMacro module : @PACKAGE_INIT@ include(CMakeFindDependencyMacro) find_dependency(<dependency 1>) ... find_dependency(<dependency n>) include(\"${CMAKE_CURRENT_LIST_DIR}/<package name>Targets.cmake\") ... The name of the dependency is the name of the package that is used in the find_package command in the CMakeLists.txt file. Other parameters ( REQUIRED , CONFIG , etc.) are not needed as they are inherited from the find_package command.","title":"Handling dependencies"},{"location":"Programming/C%2B%2B/CMake%20Manual/#specify-the-targets-to-install","text":"The install command has a parameter TARGETS that can specify the targets that should be installed. The syntax is: However, the TARGETS parameter only affects the installation part of the install target . In other words, it determines what files are copied to the installation directory, but it does not affect the build process. Therefore, all targets are built regardless of the TARGETS parameter . To prevent the building of some targets when installing, we have several options: exclude selected targets from the all target using the EXCLUDE_FROM_ALL target property or the EXCLUDE_FROM_ALL directory property . disable building for the install target and use a custom wrapper target that builds only the targets that should be installed and then calls the install target.","title":"Specify the targets to install"},{"location":"Programming/C%2B%2B/CMake%20Manual/#selecting-the-targets-to-build-using-the-exclude_from_all-property","text":"For user targets added by the add_executable or add_library command, we can use the EXCLUDE_FROM_ALL target property: add_executable(my_target EXCLUDE_FROM_ALL <source file 1> <source file 2> ...) For targets added by the FetchContent_MakeAvailable command. The situation is more complicated. The FetchContent_Declare command provides the EXCLUDE_FROM_ALL option, but it does not work as expected. Instead, we have to set the EXCLUDE_FROM_ALL property for the target after the FetchContent_MakeAvailable command: FetchContent_Declare( <declaration name> ... ) FetchContent_MakeAvailable(<declaration name>) set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_ALL TRUE)","title":"Selecting the targets to build using the EXCLUDE_FROM_ALL property"},{"location":"Programming/C%2B%2B/CMake%20Manual/#disabling-the-building-of-the-install-target-and-using-a-custom-wrapper-target","text":"For this solution,we first need to disable the building of the install using: set(CMAKE_SKIP_INSTALL_ALL_DEPENDENCY true) . Then, we need to create a target that: a) builds the selected targets, b) calls the install target. CMAKE_SKIP_INSTALL_ALL_DEPENDENCY documentation","title":"Disabling the building of the install target and using a custom wrapper target"},{"location":"Programming/C%2B%2B/CMake%20Manual/#support-both-shared-and-static-libraries","text":"Sometimes, we want the user to be able to choose between shared and static libraries when installing the package. For vcpkg, this is required. To add this support, we have to: define option for choosing the shared or static library (most likely, we do not want to install both) install the correct library target based on the option install the export header file (in both cases, as we use the same headers for both shared and static libraries) The conditional installation can look like this: if(<package name>_BUILD_SHARED_LIBS) set_target_properties(<static lib target> PROPERTIES EXCLUDE_FROM_ALL TRUE) install(TARGETS <shared lib target> EXPORT <export name>) else() set_target_properties(<shared lib target> PROPERTIES EXCLUDE_FROM_ALL TRUE) set_target_properties(<static lib target> PROPERTIES EXPORT_NAME <shared lib target>) # this aligns the name of the exported targets install(TARGETS <static lib target> EXPORT <export name>) endif() Here, the set_target_properties command is used to exclude the other target from the all target, effectively preventing it from being built during the installation. The install command is then used to install the correct target. Finally, we just copy the export header file to the installation directory: install(FILES ${CMAKE_CURRENT_BINARY_DIR}/<export header file> DESTINATION <install include dir>)","title":"Support both shared and static libraries"},{"location":"Programming/C%2B%2B/CMake%20Manual/#specific-configuration-for-frequentlly-used-libraries","text":"for google test, we want to prevent the installation of the gtest targets. To do that, turn it off before the gtets config in the CMakeLists.txt file: ```cmake # GOOGLE TEST # do not install gtest set(INSTALL_GTEST OFF) include(FetchContent) FetchContent_Declare( googletest URL https://github.com/google/googletest/archive/03597a01ee50ed33e9dfd640b249b4be3799d395.zip )","title":"Specific configuration for frequentlly used libraries"},{"location":"Programming/C%2B%2B/CMake%20Manual/#for-windows-prevent-overriding-the-parent-projects-compilerlinker-settings","text":"set(gtest_force_shared_crt ON CACHE BOOL \"\" FORCE) FetchContent_MakeAvailable(googletest) ```","title":"For Windows: Prevent overriding the parent project's compiler/linker settings"},{"location":"Programming/C%2B%2B/CMake%20Manual/#multiple-cmakeliststxt-files-in-subdirectories","text":"The configuration can be modularized by splitting the CMakeLists.txt file into multiple files, each in a separate directory. Then, these files can be executed using the add_subdirectory command in the main CMake_lists.txt file or one of the CMakeLists.txt files already added by the add_subdirectory command. This has several advantages: the configuration is more organized, it is easier to find the relevant part of the configuration or reuse the configuration in other projects the configuration is more configurable, i.e., we can turn on or off the configuration of a specific part of the project. The order of execution follows the order of the add_subdirectory commands, i.e., the processing of the CMakeLists.txt containing the add_subdirectory command is paused until the added CMakeLists.txt file is processed.","title":"Multiple CMakeLists.txt files in subdirectories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#variables-scope","text":"The variable scope for multiple CMakeLists.txt files is hierarchical. This means that the variables defined in the parent CMakeLists.txt file are visible in the child CMakeLists.txt file, but not vice versa.","title":"Variables scope"},{"location":"Programming/C%2B%2B/CMake%20Manual/#decide-based-on-the-build-configuration","text":"Sometimes is essential to decide based on the build configuration in the CMakeLists.txt file. However, this is not possible for the multi-configuration generators like Visual Studio or Xcode because the build configuration is not known during the CMake configuration step. However, there are measures that can be taken to achieve the result for particular tasks. For install command, we can use the CONFIGURATIONS parameter to install the files only for the selected build configuration. Example: install( FILES <file 1> <file 2> DESTINATION <destination> CONFIGURATIONS Release ) For building, there are ways to limit the build based on the build configuration for both single- and multi-configuration generators. For single configuration generators, we can use the CMAKE_BUILD_TYPE variable and simply exclude the target from the build using the EXCLUDE_FROM_ALL target property: if(CMAKE_BUILD_TYPE STREQUAL \"Debug\") set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_ALL TRUE) endif() For multi-configuration generators, this variable is not set, but we can use the EXCLUDE_FROM_DEFAULT_BUILD_DEBUG property: set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_DEFAULT_BUILD_DEBUG TRUE) To support both single- and multi-configuration generators, we have to first check whether we use a single- or multi-configuration generator using the CMAKE_CONFIGURATION_TYPES variable and then set the property accordingly: if(CMAKE_CONFIGURATION_TYPES) set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_DEFAULT_BUILD_DEBUG TRUE) else() if(CMAKE_BUILD_TYPE STREQUAL \"Debug\") set_target_properties(<target name> PROPERTIES EXCLUDE_FROM_ALL TRUE) endif() endif()","title":"Decide based on the build configuration"},{"location":"Programming/C%2B%2B/CMake%20Manual/#executing-external-commands","text":"There are different ways to execute external commands in the CMakeLists.txt , the proper way depends on the time when we want to execute the command: configuration time : use the execute_process command build time : use the add_custom_command command can be run both before and after the build step using the PRE_BUILD and POST_BUILD options","title":"Executing external commands"},{"location":"Programming/C%2B%2B/CMake%20Manual/#cmake-cache","text":"CMake has two types of variables: normal variables that are used just like in any other programming language and cache variables , which are configured in the first cmake run and then stored in the CMakeCache.txt file in the build directory. The CMake cache is an essential part of the CMake build system. It stores variables that are used to configure the build scripts. Cache variables can be set in the following ways: by the set command in the CMakeLists.txt file with the CACHE option by the -D command line argument: cmake -D<variable name>=<variable value> <dir> by the cmake GUI by using the -C option: cmake -C <cache file> <dir> The rule is that once a cache variable is set, it is not changed when the cmake command is run again (that is why it is called cache :) ). Moreover, the cache variables are not overwritten by the set command in the CMakeLists.txt file. In other words, the set command in the CMakeLists.txt is only used to set the default value of the variable. If the variable is already set in the cache, the set command is ignored. However, the cache variables can be still overridden from the CMakeLists.txt if the set command is used without the CACHE option (by normal variables).","title":"CMake Cache"},{"location":"Programming/C%2B%2B/CMake%20Manual/#the-option-command","text":"The option command is used to define a cache variable that can be set by the user. The syntax is: option(<option name> <option description> <default value>) The behavior of the option command is as follows: If variable is already set (either a cache variable or a normal variable), the option command is ignored. Otherwise, a cache variable is created if we are in the project mode, and a normal variable is created if we are in the script mode.","title":"The option command"},{"location":"Programming/C%2B%2B/CMake%20Manual/#cmake-directory-structure","text":"","title":"CMake Directory Structure"},{"location":"Programming/C%2B%2B/CMake%20Manual/#system-find_xxxcmake-files","text":"The system find scripts are located in the CMake/share/cmake-<version>/Modules/ directory.","title":"System Find_XXX.cmake files"},{"location":"Programming/C%2B%2B/CMake%20Manual/#various-tasks","text":"","title":"Various Tasks"},{"location":"Programming/C%2B%2B/CMake%20Manual/#showing-the-generator-for-a-configured-directory","text":"If the configuration step is already done, we can show the generator used for the configuration by reading the CMAKE_GENERATOR variable from the CMakeCache.txt file: Get-Content CMakeCache.txt | Select-String -Pattern \"CMAKE_GENERATOR\"","title":"Showing the generator for a configured directory"},{"location":"Programming/C%2B%2B/CMake%20Manual/#debugging-cmake","text":"","title":"Debugging CMake"},{"location":"Programming/C%2B%2B/CMake%20Manual/#debugging-cmake-using-clion-cmake-debugger","text":"documentation In CLion, we can debug the CMake configuration process by: opening the CMakeLists.txt file setting the breakpoint(s) starting the debugging process by clicking on the play button which is located on the first line of the CMakeLists.txt file on the left side of the editor window. or by clicking on the debug button in the main toolbar","title":"Debugging CMake using CLion CMake debugger"},{"location":"Programming/C%2B%2B/CMake%20Manual/#getting-a-a-complete-output-of-the-cmake-configuration","text":"We can get the complete output of the CMake configuration by running the cmake command with the --trace option. This can be especially useful when investigating included scripts, as the debuggers usually cannot step into CMake calls. Usually, the output is very long, so it is recommended to redirect it to a file: cmake --trace <dir> *> cmake_trace.txt If we want to also expand the variables, we can use the --trace-expand option.","title":"Getting a a complete output of the CMake configuration"},{"location":"Programming/C%2B%2B/Google%20Test/","text":"Assertions \u00b6 To determine whether a test should pass or fail, we use assertion macros. There are two main types of assertions: ASSERT_* - generates a fatal failure when it fails, aborting the current function EXPECT_* - generates a nonfatal failure, allowing the function to continue running Basic Assertions \u00b6 _EQ(computed, expected) - tests that expected == actual . == must be defined for the type of the arguments.","title":"Google Test"},{"location":"Programming/C%2B%2B/Google%20Test/#assertions","text":"To determine whether a test should pass or fail, we use assertion macros. There are two main types of assertions: ASSERT_* - generates a fatal failure when it fails, aborting the current function EXPECT_* - generates a nonfatal failure, allowing the function to continue running","title":"Assertions"},{"location":"Programming/C%2B%2B/Google%20Test/#basic-assertions","text":"_EQ(computed, expected) - tests that expected == actual . == must be defined for the type of the arguments.","title":"Basic Assertions"},{"location":"Programming/Java/Debugging%20Java/","text":"General rules \u00b6 Warnings can help you to spot the problems. Check that they are enabled ( Xlint ). Sometimes, warnings may help to understand the problem. However, they are not emmited due to compilation error. Try to comment out the errorneous code and compile the code to see all warnings. Missing Resources \u00b6 First, check if the resources are where you expected in the jar or in the target folder. The structure is described here on SO . If the resources are not there, try to rebuild the project.","title":"Debugging Java"},{"location":"Programming/Java/Debugging%20Java/#general-rules","text":"Warnings can help you to spot the problems. Check that they are enabled ( Xlint ). Sometimes, warnings may help to understand the problem. However, they are not emmited due to compilation error. Try to comment out the errorneous code and compile the code to see all warnings.","title":"General rules"},{"location":"Programming/Java/Debugging%20Java/#missing-resources","text":"First, check if the resources are where you expected in the jar or in the target folder. The structure is described here on SO . If the resources are not there, try to rebuild the project.","title":"Missing Resources"},{"location":"Programming/Java/Jackson/","text":"Documentation Jackson is a (de)serialization library primarily focused to the JSON format. It supports annotations for automatic serialization and deserialization of Java objects. Jackson have two basic modes: Object mapper mode , JSON is (de)serialized from/to Java objects Parser/Generator mode , JSON is parsed/generated one token at a time To make it even more complicated, the Object mapper mode can be used in two ways: Data binding , where the JSON is (de)serialized from/to Java objects that we provide for this purpose Tree model , where the JSON is (de)serialized from/to a tree structure of Jackson objects Tutorials: Official databind tutorial Jenkov tutorial (old but gold) Object mapper with data binding \u00b6 The basic usage is: public class MyClass{ public String par1; public int par2; ... } ObjectMapper mapper = new ObjectMapper(); // Deserialization MyClass myClass = mapper.readValue(new File(\"path/to/file.json\"), MyClass.class); // Serialization MyClass myObject = new MyClass(); ... mapper.writeValue(new File(\"path/to/file.json\"), myObject); By defualt, Jackson: (de)serialize from/to public fields or public setters/getters with the same name as the JSON property all JSON properties are deserialized and all public fields or public getters are serialized if there is no matching field or setter to a JSON property, the deserialization fails However, we can customize the (de)serialization process using many annotations. This way, we can ignore some fields or JSON properties, use constructors or factory methods for deserialization, (de)serialize from/to arrays and objects, and many other things. Ignore specific fields \u00b6 If you want to ignore specific fields during serialization or deserialization, you can use the @JsonIgnoreProperties annotation . Use it as a class or type annotation. Example: @JsonIgnoreProperties({ \"par1\", \"par2\" }) public class ConfigModel { // \"par1\" and \"par2\" will be ignored } This annotation can also prevent the \"Unrecognized field\" error during deserialization, as the ignored fields does not have to be present as Java class members. we can achieve the same by field annotations, specifically the @JsonIgnore annotation. Example: public class ConfigModel { @JsonIgnore public String par1; @JsonIgnore public int par2; } Include only specific fields \u00b6 If the fields to be (de)serialized are only a minority of all fields, we can use a reverse approach: the @JsonIncludeProperties annotation. For example: @JsonIncludeProperties({ \"par1\", \"par2\" }) public class ConfigModel { // only \"par1\" and \"par2\" will be (de)serialized } Note that unlike for ignoring, there is no field annotation for including only specific fields. The @JsonInclude annotation serves a different purpose. Represent a class by a single member \u00b6 If we want the java class to be represented by a single value in the serialized file, we can achieve that by adding the @JsonValue annotation above the member or method that should represent the class. Note, however, that this only works for simple values, because the member serializers are not called, the members is serialized as a simple value instead. If you want to represent a class by a single but complex member, use a custom serializer instead . An equivalent annotation for deserialization is the @JsonCreator annotation which should be placed above a constructor or factory method. Deserialization \u00b6 The standard usage is: ObjectMapper mapper = new ObjectMapper(); // ... configure mapper File file = new File(<PATH TO FILE>); Instance instance = mapper.readValue(file, Instance.class); By default, new objects are created for all members in the object hierarchy that are either present in the serialized file. New objects are created using the setter, if exists, otherwise, the costructor is called. Multiple Setters \u00b6 If there are multiple setters, we need to specify the one that should be used for deserialization by marking it with the @JsonSetter annotation. Update existing instance \u00b6 You can update an existing instance using the readerForUpdating method: ObjectReader or = mapper.readerForUpdating(instance); // special reader or.readValue(file) // we can use the method we already know on the object reader Note that by default, the update is shalow . Only the instance object itself is updated, but its members are brand new objects. If you want to keep all objects from an existing object hierarchy, you need to use the @JsonMerge annotation. You should put this annotation above any member of the root object you want to update instead of replacing it. The @JsonUpgrade annotation is recursive : the members of the member annotated with @JsonUpgrade are updated as well and so on. Updating polymorphic types \u00b6 For updating polymorpic type, the rule is that the exact type has to match. Also, you need jackson-databind version 2.14.0 or greater. Read just part of the file \u00b6 For reading just part of the file, use the at selector taht is available in the ObjectReader class. We need to first obtain the reader from a mapper, and then use the selector: ObjectReader reader = mapper.readerFor(Instance.class); Instance instance = reader.at(\"data\").readValue(file) Note that if the path parameter of the at method is incorrect, the method throws an exception with the message: \"no content to map due to end-of-input\". Check that some node is present \u00b6 To check for presence of a node, we should use the JsonPointer class: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); We can also use the JsonPointer in the at method: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); if(found){ Instance instance = reader.at(jsonPointer).readValue(file, Instance.class) } Interface or abstract class \u00b6 When serializing interface or abstract class, it is important to include the implementation type into serialization. Otherwise, the deserialization fails, because it cannot determine the concreate type. To serialize the concrete type, we can use the @JsonRypeInfo and JsonSubTypes annotations: @JsonTypeInfo( use = JsonTypeInfo.Id.CLASS, // what value should we store, here the class name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) public interface Interface { ... } Te above code will work, the full class name will be serialized in the file, however. If we want to use a shorter syntax, i.e., some codename for the class, we need to specify a mapping between this codename and the conreate class: @JsonTypeInfo( use = JsonTypeInfo.Id.NAME, // what value should we store, here a custom name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) @JsonSubTypes({ @JsonSubTypes.Type(value = Implementation.class, name = \"Implementation name\") }) public interface Interface { ... } Custom deserializer \u00b6 An alternative to @JsonTypeInfo is to use a custom deserializer: https://stackoverflow.com/questions/44122782/jackson-deserialize-based-on-type Custom deserializer \u00b6 If our architecture is so complex or specific that none of the Jackson annotations can help us to achieve the desired behavior, we can use a custom deserializer. For that we need to: Implement a custom deserializer by extending the JsonDeserializer class Registering the deserializer in the ObjectMapper Creating a custom deserializer for class \u00b6 The only method we need to implement is the: T deserialize(JsonParser jsonParser, DeserializationContext deserializationContext) where T is the type of the object we are deserializing. To get the Jaksons representation of the JSON tree, we can call: JsonNode node = jsonParser.getCodec().readTree(jsonParser); We can get all the fields by calling using the node.fields() method. For arrays, there is a method node.elements() ; Registering the deserializer \u00b6 ObjectMapper mapper = new ObjectMapper(); SimpleModule module = new SimpleModule(); module.addDeserializer(OurClass.class, new OurClassDeserializer()); mapper.registerModule(module); Custom deserializer with generic types \u00b6 When we need a custom deserializer for a generic class, we need to use a wildcard to cover multiple values of generic argument: public class OurDeserializer extends JsonDeserializer<OurGenericClass<?>>{ ... } If we also need to get the generic argument type from JSON, we need to implement the ContextualDeserializer interface. This is discribed in a SO answer . Custom deserializer with inheritance \u00b6 We can have a common deserializer for both parent and child class, or multiple child classes. However, it is necessary to make the deserializer generic and register the deserializer for all classes, not just for the parent. Example: public class PolymorphicDeserializer<T extends Parent> extends JsonDeserializer<T> @Override public T deserialize(JsonParser p, DeserializationContext ctxt) throws IOException, JsonProcessingException { ... } } module.addDeserializer(ChildA.class, new PolymorphicDeserializer<>()); module.addDeserializer(ChildB.class, new ProcedureParameterDeserializer<>()); Serialization \u00b6 Standard serialization: // compressed mapper.writer().writeValue(file, object); // or with indents, etc. mapper.writerWithDefaultPrettyPrinter().writeValue(file, object); By default, new objects are created for all members in the object hierarchy that are either: public value mebers (fields) public getters: public methods with name get<NAME> , where <NAME> is a name of some value member Other getters with different name are called only if there is an annotation above them. A special annotation dedicated for this is @JsonProperty . Appending to an existing file \u00b6 To append to an existing file, we need to create an output stream in the append mode and then use it in jackson: ObjectMapper mapper = ... JsonGenerator generator = mapper.getFactory().createGenerator(new FileOutputStream(new File(<FILENAME>), true)); mapper.writeValue(generator, output); Complex member filters \u00b6 Insted of adding annotations to each member we want to ignore, we can also apply some more compex filters, to do that, we need to: add a @JsonFilter(\"<FILTER NAME>\") annotation to all classes for which we want to use the filter create the filter pass a set of all filters we want to use to the writer we are using for serialization The example below keeps only members inherited from MyClass : // object on which we apply the filter @JsonFilter(\"myFilter\") class targetClass{ ... } // filter PropertyFilter filter = new SimpleBeanPropertyFilter() { @Override public void serializeAsField( Object pojo, JsonGenerator jgen, SerializerProvider provider, PropertyWriter writer ) throws Exception { if(writer.getType().isTypeOrSubTypeOf(MyClass.class)){ writer.serializeAsField(pojo, jgen, provider); } } }; FilterProvider filters = new SimpleFilterProvider().addFilter(\"myFilter\", filter); // use a writer created with filters mapper.writer(filters).writeValue(generator, output); ... Flatting the hierarchy \u00b6 When we desire to simplify the object hierarchy, we can use the @JsonUnwrapped annotation above a member of a class. With this annotation, the annotated member object will be skipped while all its members will be serialized into its parent. Custom serializer \u00b6 If the serialization requirements are too complex to be expressed using Jackson annotations, we can use a custom serialzier: public class MyCustomSerializer extends JsonSerializer<MyClass> { @Override public void serialize(MyClass myClass, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException { ... } } Analogously to custom deserializer, we can register custom serializer either in the object mapper: SimpleModule module = new SimpleModule(); module.addSerializer(MyClass.class, new MyCustomSerializer()); mapper.registerModule(module); or by annotating the class: @JsonSerialize(using = MyCustomSerializer.class) public class MyClass{ ... } You can call standard serializers from custom serializers using the SerializerProvider and JsonGenerator instances supplied as a parameters of the serialize method. For example the standard serialized value of som inner/member object class can be obtained using: serializerProvider.defaultSerializeValue(myInnerInstance, jsonGenerator); Annotations \u00b6 Multiple objects \u00b6 If there are multiple objects involved in the (de)serializetion, we can use the @JsonCreator and @JsonProperty annotations to split the work: @JsonCreator public ConfigModel( @JsonProperty(\"first\") ClassA instanceA, @JsonProperty(\"second\") ClassB instanceB ) { ... } in the above examples, the first and second are keys mapping the objects in the serialized file from which instanceA and instanceB should be created. Default Serializers and Deserializers \u00b6 By default, Jackson can serialize and deserialize many types of objects: all primitive types all primitive wrappers String BigInteger BigDecimal and many others Java 8 Date and Time (de)serialization \u00b6 Among the types that are not covered in basic Jakson package are the Java 8 types for date and time. To (de)serialize them, we have two options: use the jackson-datatype-jsr310 module that adds support for the ISO-8601 format for date and time use custom serializers and deserializers if we need a different format To use the jackson-datatype-jsr310 module, we need to add it to the dependencies: <dependency> <groupId>com.fasterxml.jackson.datatype</groupId> <artifactId>jackson-datatype-jsr310</artifactId> <version>2.12.0</version> </dependency> and then register the module in the object mapper: ObjectMapper mapper = new ObjectMapper(); mapper.registerModule(new JavaTimeModule()); Object mapper with tree model \u00b6 Jenkov tutorial on JsonNode To get the tree model, we can use the readTree method of the ObjectMapper class: ObjectMapper mapper = new ObjectMapper(); JsonNode node = mapper.readTree(new File(\"path/to/file.json\")); Iterating over JSON array \u00b6 First we need to convert the node representing the array to the ArrayNode class. Then we can iterate over the array using for each or the get method: ArrayNode array = (ArrayNode) node.get(\"array\"); for(JsonNode element: array){ ... } // or for(int i = 0; i < array.size(); i++){ JsonNode element = array.get(i); ... } Iterating over JSON object \u00b6 First we need to convert the node representing the object to the ObjectNode class. Then we can iterate over the object using the fieldNames method: ObjectNode object = (ObjectNode) node.get(\"object\"); for(String fieldName: object.fieldNames()){ JsonNode value = object.get(fieldName); ... } CSV with Jackson \u00b6 documentation To read a CSV file with Jackson, we need to use the CsvMapper class. The basic usage is: CsvMapper mapper = new CsvMapper(); // iterator construction MappingIterator<<row type>> it = mapper....readValues(new File(\"path/to/file.csv\"); // iterator usage while(it.hasNext()){ <row type> row = it.next(); ... } Here the <row type is the target type that should represent a single row. Depending on it, there will be different code in place of mapper... . For example, to get a list of strings for each row, we can use: MappingIterator<List<String>> it = mapper.readerForListOf(String.class)... Use the header \u00b6 If the CSV file has a header, we can use it to load each row into a map of key-value pairs: CsvSchema schema = CsvSchema.emptySchema().withHeader(); MappingIterator<Map<String, String>> it = mapper.readerForMapOf(String.class).with(schema).readValues(new File(\"path/to/file.csv\"));","title":"Jackson"},{"location":"Programming/Java/Jackson/#object-mapper-with-data-binding","text":"The basic usage is: public class MyClass{ public String par1; public int par2; ... } ObjectMapper mapper = new ObjectMapper(); // Deserialization MyClass myClass = mapper.readValue(new File(\"path/to/file.json\"), MyClass.class); // Serialization MyClass myObject = new MyClass(); ... mapper.writeValue(new File(\"path/to/file.json\"), myObject); By defualt, Jackson: (de)serialize from/to public fields or public setters/getters with the same name as the JSON property all JSON properties are deserialized and all public fields or public getters are serialized if there is no matching field or setter to a JSON property, the deserialization fails However, we can customize the (de)serialization process using many annotations. This way, we can ignore some fields or JSON properties, use constructors or factory methods for deserialization, (de)serialize from/to arrays and objects, and many other things.","title":"Object mapper with data binding"},{"location":"Programming/Java/Jackson/#ignore-specific-fields","text":"If you want to ignore specific fields during serialization or deserialization, you can use the @JsonIgnoreProperties annotation . Use it as a class or type annotation. Example: @JsonIgnoreProperties({ \"par1\", \"par2\" }) public class ConfigModel { // \"par1\" and \"par2\" will be ignored } This annotation can also prevent the \"Unrecognized field\" error during deserialization, as the ignored fields does not have to be present as Java class members. we can achieve the same by field annotations, specifically the @JsonIgnore annotation. Example: public class ConfigModel { @JsonIgnore public String par1; @JsonIgnore public int par2; }","title":"Ignore specific fields"},{"location":"Programming/Java/Jackson/#include-only-specific-fields","text":"If the fields to be (de)serialized are only a minority of all fields, we can use a reverse approach: the @JsonIncludeProperties annotation. For example: @JsonIncludeProperties({ \"par1\", \"par2\" }) public class ConfigModel { // only \"par1\" and \"par2\" will be (de)serialized } Note that unlike for ignoring, there is no field annotation for including only specific fields. The @JsonInclude annotation serves a different purpose.","title":"Include only specific fields"},{"location":"Programming/Java/Jackson/#represent-a-class-by-a-single-member","text":"If we want the java class to be represented by a single value in the serialized file, we can achieve that by adding the @JsonValue annotation above the member or method that should represent the class. Note, however, that this only works for simple values, because the member serializers are not called, the members is serialized as a simple value instead. If you want to represent a class by a single but complex member, use a custom serializer instead . An equivalent annotation for deserialization is the @JsonCreator annotation which should be placed above a constructor or factory method.","title":"Represent a class by a single member"},{"location":"Programming/Java/Jackson/#deserialization","text":"The standard usage is: ObjectMapper mapper = new ObjectMapper(); // ... configure mapper File file = new File(<PATH TO FILE>); Instance instance = mapper.readValue(file, Instance.class); By default, new objects are created for all members in the object hierarchy that are either present in the serialized file. New objects are created using the setter, if exists, otherwise, the costructor is called.","title":"Deserialization"},{"location":"Programming/Java/Jackson/#multiple-setters","text":"If there are multiple setters, we need to specify the one that should be used for deserialization by marking it with the @JsonSetter annotation.","title":"Multiple Setters"},{"location":"Programming/Java/Jackson/#update-existing-instance","text":"You can update an existing instance using the readerForUpdating method: ObjectReader or = mapper.readerForUpdating(instance); // special reader or.readValue(file) // we can use the method we already know on the object reader Note that by default, the update is shalow . Only the instance object itself is updated, but its members are brand new objects. If you want to keep all objects from an existing object hierarchy, you need to use the @JsonMerge annotation. You should put this annotation above any member of the root object you want to update instead of replacing it. The @JsonUpgrade annotation is recursive : the members of the member annotated with @JsonUpgrade are updated as well and so on.","title":"Update existing instance"},{"location":"Programming/Java/Jackson/#updating-polymorphic-types","text":"For updating polymorpic type, the rule is that the exact type has to match. Also, you need jackson-databind version 2.14.0 or greater.","title":"Updating polymorphic types"},{"location":"Programming/Java/Jackson/#read-just-part-of-the-file","text":"For reading just part of the file, use the at selector taht is available in the ObjectReader class. We need to first obtain the reader from a mapper, and then use the selector: ObjectReader reader = mapper.readerFor(Instance.class); Instance instance = reader.at(\"data\").readValue(file) Note that if the path parameter of the at method is incorrect, the method throws an exception with the message: \"no content to map due to end-of-input\".","title":"Read just part of the file"},{"location":"Programming/Java/Jackson/#check-that-some-node-is-present","text":"To check for presence of a node, we should use the JsonPointer class: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); We can also use the JsonPointer in the at method: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); if(found){ Instance instance = reader.at(jsonPointer).readValue(file, Instance.class) }","title":"Check that some node is present"},{"location":"Programming/Java/Jackson/#interface-or-abstract-class","text":"When serializing interface or abstract class, it is important to include the implementation type into serialization. Otherwise, the deserialization fails, because it cannot determine the concreate type. To serialize the concrete type, we can use the @JsonRypeInfo and JsonSubTypes annotations: @JsonTypeInfo( use = JsonTypeInfo.Id.CLASS, // what value should we store, here the class name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) public interface Interface { ... } Te above code will work, the full class name will be serialized in the file, however. If we want to use a shorter syntax, i.e., some codename for the class, we need to specify a mapping between this codename and the conreate class: @JsonTypeInfo( use = JsonTypeInfo.Id.NAME, // what value should we store, here a custom name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) @JsonSubTypes({ @JsonSubTypes.Type(value = Implementation.class, name = \"Implementation name\") }) public interface Interface { ... }","title":"Interface or abstract class"},{"location":"Programming/Java/Jackson/#custom-deserializer","text":"An alternative to @JsonTypeInfo is to use a custom deserializer: https://stackoverflow.com/questions/44122782/jackson-deserialize-based-on-type","title":"Custom deserializer"},{"location":"Programming/Java/Jackson/#custom-deserializer_1","text":"If our architecture is so complex or specific that none of the Jackson annotations can help us to achieve the desired behavior, we can use a custom deserializer. For that we need to: Implement a custom deserializer by extending the JsonDeserializer class Registering the deserializer in the ObjectMapper","title":"Custom deserializer"},{"location":"Programming/Java/Jackson/#creating-a-custom-deserializer-for-class","text":"The only method we need to implement is the: T deserialize(JsonParser jsonParser, DeserializationContext deserializationContext) where T is the type of the object we are deserializing. To get the Jaksons representation of the JSON tree, we can call: JsonNode node = jsonParser.getCodec().readTree(jsonParser); We can get all the fields by calling using the node.fields() method. For arrays, there is a method node.elements() ;","title":"Creating a custom deserializer for class"},{"location":"Programming/Java/Jackson/#registering-the-deserializer","text":"ObjectMapper mapper = new ObjectMapper(); SimpleModule module = new SimpleModule(); module.addDeserializer(OurClass.class, new OurClassDeserializer()); mapper.registerModule(module);","title":"Registering the deserializer"},{"location":"Programming/Java/Jackson/#custom-deserializer-with-generic-types","text":"When we need a custom deserializer for a generic class, we need to use a wildcard to cover multiple values of generic argument: public class OurDeserializer extends JsonDeserializer<OurGenericClass<?>>{ ... } If we also need to get the generic argument type from JSON, we need to implement the ContextualDeserializer interface. This is discribed in a SO answer .","title":"Custom deserializer with generic types"},{"location":"Programming/Java/Jackson/#custom-deserializer-with-inheritance","text":"We can have a common deserializer for both parent and child class, or multiple child classes. However, it is necessary to make the deserializer generic and register the deserializer for all classes, not just for the parent. Example: public class PolymorphicDeserializer<T extends Parent> extends JsonDeserializer<T> @Override public T deserialize(JsonParser p, DeserializationContext ctxt) throws IOException, JsonProcessingException { ... } } module.addDeserializer(ChildA.class, new PolymorphicDeserializer<>()); module.addDeserializer(ChildB.class, new ProcedureParameterDeserializer<>());","title":"Custom deserializer with inheritance"},{"location":"Programming/Java/Jackson/#serialization","text":"Standard serialization: // compressed mapper.writer().writeValue(file, object); // or with indents, etc. mapper.writerWithDefaultPrettyPrinter().writeValue(file, object); By default, new objects are created for all members in the object hierarchy that are either: public value mebers (fields) public getters: public methods with name get<NAME> , where <NAME> is a name of some value member Other getters with different name are called only if there is an annotation above them. A special annotation dedicated for this is @JsonProperty .","title":"Serialization"},{"location":"Programming/Java/Jackson/#appending-to-an-existing-file","text":"To append to an existing file, we need to create an output stream in the append mode and then use it in jackson: ObjectMapper mapper = ... JsonGenerator generator = mapper.getFactory().createGenerator(new FileOutputStream(new File(<FILENAME>), true)); mapper.writeValue(generator, output);","title":"Appending to an existing file"},{"location":"Programming/Java/Jackson/#complex-member-filters","text":"Insted of adding annotations to each member we want to ignore, we can also apply some more compex filters, to do that, we need to: add a @JsonFilter(\"<FILTER NAME>\") annotation to all classes for which we want to use the filter create the filter pass a set of all filters we want to use to the writer we are using for serialization The example below keeps only members inherited from MyClass : // object on which we apply the filter @JsonFilter(\"myFilter\") class targetClass{ ... } // filter PropertyFilter filter = new SimpleBeanPropertyFilter() { @Override public void serializeAsField( Object pojo, JsonGenerator jgen, SerializerProvider provider, PropertyWriter writer ) throws Exception { if(writer.getType().isTypeOrSubTypeOf(MyClass.class)){ writer.serializeAsField(pojo, jgen, provider); } } }; FilterProvider filters = new SimpleFilterProvider().addFilter(\"myFilter\", filter); // use a writer created with filters mapper.writer(filters).writeValue(generator, output); ...","title":"Complex member filters"},{"location":"Programming/Java/Jackson/#flatting-the-hierarchy","text":"When we desire to simplify the object hierarchy, we can use the @JsonUnwrapped annotation above a member of a class. With this annotation, the annotated member object will be skipped while all its members will be serialized into its parent.","title":"Flatting the hierarchy"},{"location":"Programming/Java/Jackson/#custom-serializer","text":"If the serialization requirements are too complex to be expressed using Jackson annotations, we can use a custom serialzier: public class MyCustomSerializer extends JsonSerializer<MyClass> { @Override public void serialize(MyClass myClass, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException { ... } } Analogously to custom deserializer, we can register custom serializer either in the object mapper: SimpleModule module = new SimpleModule(); module.addSerializer(MyClass.class, new MyCustomSerializer()); mapper.registerModule(module); or by annotating the class: @JsonSerialize(using = MyCustomSerializer.class) public class MyClass{ ... } You can call standard serializers from custom serializers using the SerializerProvider and JsonGenerator instances supplied as a parameters of the serialize method. For example the standard serialized value of som inner/member object class can be obtained using: serializerProvider.defaultSerializeValue(myInnerInstance, jsonGenerator);","title":"Custom serializer"},{"location":"Programming/Java/Jackson/#annotations","text":"","title":"Annotations"},{"location":"Programming/Java/Jackson/#multiple-objects","text":"If there are multiple objects involved in the (de)serializetion, we can use the @JsonCreator and @JsonProperty annotations to split the work: @JsonCreator public ConfigModel( @JsonProperty(\"first\") ClassA instanceA, @JsonProperty(\"second\") ClassB instanceB ) { ... } in the above examples, the first and second are keys mapping the objects in the serialized file from which instanceA and instanceB should be created.","title":"Multiple objects"},{"location":"Programming/Java/Jackson/#default-serializers-and-deserializers","text":"By default, Jackson can serialize and deserialize many types of objects: all primitive types all primitive wrappers String BigInteger BigDecimal and many others","title":"Default Serializers and Deserializers"},{"location":"Programming/Java/Jackson/#java-8-date-and-time-deserialization","text":"Among the types that are not covered in basic Jakson package are the Java 8 types for date and time. To (de)serialize them, we have two options: use the jackson-datatype-jsr310 module that adds support for the ISO-8601 format for date and time use custom serializers and deserializers if we need a different format To use the jackson-datatype-jsr310 module, we need to add it to the dependencies: <dependency> <groupId>com.fasterxml.jackson.datatype</groupId> <artifactId>jackson-datatype-jsr310</artifactId> <version>2.12.0</version> </dependency> and then register the module in the object mapper: ObjectMapper mapper = new ObjectMapper(); mapper.registerModule(new JavaTimeModule());","title":"Java 8 Date and Time (de)serialization"},{"location":"Programming/Java/Jackson/#object-mapper-with-tree-model","text":"Jenkov tutorial on JsonNode To get the tree model, we can use the readTree method of the ObjectMapper class: ObjectMapper mapper = new ObjectMapper(); JsonNode node = mapper.readTree(new File(\"path/to/file.json\"));","title":"Object mapper with tree model"},{"location":"Programming/Java/Jackson/#iterating-over-json-array","text":"First we need to convert the node representing the array to the ArrayNode class. Then we can iterate over the array using for each or the get method: ArrayNode array = (ArrayNode) node.get(\"array\"); for(JsonNode element: array){ ... } // or for(int i = 0; i < array.size(); i++){ JsonNode element = array.get(i); ... }","title":"Iterating over JSON array"},{"location":"Programming/Java/Jackson/#iterating-over-json-object","text":"First we need to convert the node representing the object to the ObjectNode class. Then we can iterate over the object using the fieldNames method: ObjectNode object = (ObjectNode) node.get(\"object\"); for(String fieldName: object.fieldNames()){ JsonNode value = object.get(fieldName); ... }","title":"Iterating over JSON object"},{"location":"Programming/Java/Jackson/#csv-with-jackson","text":"documentation To read a CSV file with Jackson, we need to use the CsvMapper class. The basic usage is: CsvMapper mapper = new CsvMapper(); // iterator construction MappingIterator<<row type>> it = mapper....readValues(new File(\"path/to/file.csv\"); // iterator usage while(it.hasNext()){ <row type> row = it.next(); ... } Here the <row type is the target type that should represent a single row. Depending on it, there will be different code in place of mapper... . For example, to get a list of strings for each row, we can use: MappingIterator<List<String>> it = mapper.readerForListOf(String.class)...","title":"CSV with Jackson"},{"location":"Programming/Java/Jackson/#use-the-header","text":"If the CSV file has a header, we can use it to load each row into a map of key-value pairs: CsvSchema schema = CsvSchema.emptySchema().withHeader(); MappingIterator<Map<String, String>> it = mapper.readerForMapOf(String.class).with(schema).readValues(new File(\"path/to/file.csv\"));","title":"Use the header"},{"location":"Programming/Java/Java%20Programming%20Guide/","text":"Primitive types \u00b6 Java has the following primitive types: Integer types : byte , short , int , long Floating point types : float , double Character type : char Boolean type : boolean Arrays \u00b6 Arrays in Java are objects, and they are created using the new keyword. The syntax is: int[] myArray = new int[10]; The array is then filled with zeros. The length of the array is fixed and cannot be changed. The array can be initialized using the following syntax: int[] myArray = {1, 2, 3, 4, 5}; For working with arrays, the static methods from [] java.util.Arrays ](https://docs.oracle.com/javase/8/docs/api/java/util/Arrays.html) class can be used. Some of the most important methods are: copyOf and copyOfRange for copying arrays fill for filling arrays with a specific value Classes \u00b6 In java, there are the following types of classes: Standard class : similar to classes in other languages. Record class : a simple aggregate of values. Record class \u00b6 official specification Record class is a simple aggregate class. It is designed to replace the standard class in cases where the class is used only to hold the data (POJO). It is defined using the record keyword like this: record MyRecord(int par1, String par2){ ... } The advantage over the standard class is that the record class automatically declare data members for all parameters of the record header, and it generates the following methods: equals hashCode toString getters for all parameters constructor that accepts all parameters Standard Types \u00b6 Strings \u00b6 Classical string literals has to be enclosed in quotes ( \"my string\" ). They cannot contain some characters (newline, \" ). The backslash ( \\ ) character is reserved for Java escape sequences and need to be ecaped as \\\\ . In addition, since Java 15, there are text blocks, enclosed in three quotes: \"\"\" My long \"text block\" \"\"\" The properties of text blocks: can be used as standard string literals can contain newlines, quotes, and some other symbols that have to be escaped in standard string literals. The new line after opening triple quoute is obligatory. Note that backslash ( \\ ) is still interpreted as Java escape character, so we need to escape it using \\\\ . There are no fstrings in Java, the best way for using variables in strings is to use the String.format method: String s = String.format(\"My string with %s and %d\", \"text\", 5); Collections \u00b6 Java has a rich set of collections. Still, many are missing, notably some tuple class. Also, the typical tuple unpacking known from Python or C++ is not possible in Java and we have to use a verbose way of 1) creating a temporary object for all values, 2) accessing the values using the getters. List \u00b6 List initialization in Java is complicated compared to other languages. we can initialize the list as: List a = List.of(1, 2, 3); However, this method returns an immutable list. If we need a mutable list, we have to pass the list to the ArrayList constructor: List a = new ArrayList<>(List.of(1, 2, 3)); but with such a complicated syntax, we can use an input array directly: List a = new ArrayList<>(Arrays.asList(1, 2, 3)); Set \u00b6 From java 9, sets can be simply initialized as: Set<int> nums = Set.of(1,2,3); Note that this method returns an immutable set. In the earlier versions of Java, the Collections.singleton method can be used. Sorting \u00b6 Some collections can be sorted using the sort member method. It is a stable sort, which use the natural order of the elements by default, but it can be customized using a comparator. The comparator interface expects two elements and should return a negative number if the first element is smaller, a positive number if the first element is greater, and zero if the elements are equal. Overloading \u00b6 When calling an oveladed method with a null argument, we receive a compilation error: reference to <METHOD> is ambiguous . A solution to this problem is to cast the null pointer to the desired type: public set(string s){ ... } public set(MyClass c){ .... } set(null) // doesn't work set((MyClass) null) // works set((String) null) // works Regular expressions \u00b6 First thing we need to know about regexes in java is that we need to escape all backslashes (refer to the Strings chapter), there are no raw strings in Java. We can work with regexes either by using specialized high level methods that accepts regex as string, or by using the tools from the java.util.regex package. Testing if string matches a regex \u00b6 We can do it simply by: String MyString = \"tests\"; String MyRegex = \"t\\\\p+\" boolean matches = myString.matches(myRegex); Exceptions \u00b6 The exception mechanism in Java is similar to other languages. The big difference is, however, that in Java, all exception not derived from the RuntimeException class are checked , which means that their handeling is enforced by the compiler. The following code does not compile, because the java.langException class is a checked exception class. private testMethod{ ... throw new Exception(); } Therefore, we need to handle the exception using the try cache block or add throws <EXCEPTION TYPE> to the method declaration. When deciding between try/catch and throws , the rule of thumb is to use the try/cache if we can handle the exception and throws if we want to leave it for the caller. The problem arises when the method we are in implements an interface that does not have the throws declaration we need. Then the only solution is to use try/cache with a cache that does not handle the exception, and indicate the problem to the caller differently, e.g, by returning a null pointer. Logging \u00b6 Java has a built-in logging mechanism in the java.util.logging package. Apart from that, there are many other logging libraries. In this section, we focus on the SLF4J library which is an abstraction. With SLF4J, we can switch between different logging libraries without changing the code. To use SLF4J, we need to add the following dependency to the pom.xml file: <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version><version></version> </dependency> Additionally, we need to add a backend of our choice. For example, we can use the logback library: <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-classic</artifactId> <version><version></version> </dependency> The SLF4J library detects the available backend automatically. Genericity \u00b6 Oracle official tutorial for Java 8 Like many other languages, Java offers generic types with the classical syntax: class GenericClass<T>{ ... }; An object is then created as: GenericClass<OtherClass> gc = new GenericClass<OtherClass>(); Diamond operator \u00b6 At all places where the type can be inferred, we can use the dianmond operator ( <> ) without specifiyng a type: GenericClass<OtherClass> gc = new GenericClass<>(); Raw types \u00b6 specification For the backward compatibility of Java library classes that were converted to generic classes a concept of raw types was introduces. However, in addition, this concept brings a lot of very subtle compile errors, which are caused by a nonintentional creation of raw type instead of a specific type. GenericClass gc = new GenericClass(); // raw type The parameterized type can be assigned to the raw type, but when we do it the other way arround, the code emmits warning and is not runtime safe: GenericClass<OtherClass> generic = new GenericClass<>(); GenericClass raw = generic // OK GenericClass raw = new GenericClass(); GenericClass<OtherClass> generic = raw // unsafe Calling the generic methods on raw types has a simmilar consequences. But the worst property of raw types is that all generic non-static members of raw types that are not inherited and are also eraw types. Therefore, the genericity is lost even for the completely unrelated type parameters, even specific types are erased to Object . The consequence can be seen on the following example: class GenericClass<T>{ public List<String> getListofStrings() }; GenericClass raw = new GenericClass(); raw.getListofStrings(); // returns List<Object> !!! Generic method \u00b6 A generic method in Java is written as: public <<GENERIC PARAM NAME>> <RESULT TYPE> <METHOD NAME>(<PARAMS>){ ... } example: public static <T> ProcedureParameterValidationResult<T> error(String msg){ return new ProcedureParameterValidationResult<>(false, msg, null); } if the generic parameter cannot be infered from the method arguments, we can call the method with explicit generic argument like this: <<GENERIC PARAM VALUE>><METHOD NAME>(<ARGUMENTS>); example: ProcedureParameterValidationResult.<String>error(\"Value cannot be empty.\"); Retrieving the type of a generic argument \u00b6 Because the genericity in Java is resolved at runtime, it is not possible to use the generic parameter as a type. What does it mean? Whille it is possible to use Myclass.class it is not possible to use T.class . The same applies for the class methods. The usual solution is to pass the class to the generic method as a method parameter: void processGeneric(Class<T> classType){ ... } Default generic argument \u00b6 It is not possible to set the default value of a generic parameter. If we want to achive such behavior, we need to create a new class: class GeneriClass<T>{ ... } class StringClass extends GenericClass<String>{}; Wildcards \u00b6 The wildcard symbol ( ? ) represents a concrete but unspecified type. It can be bounded by superclass/interface ( ? exctends MyInterface ), or by sublcass ( ? super MyClass ). The typical use cases: single non-generic method accepting generic types of multiple generic parameter values: myMethod(Wrapper<? extends MyInterface> wrapper){ MyInterface mi = wrapper.get(); ... } method that can process generic class but does not need to work with the generic parameters at all: getLength(List<?> list){ return list.size(); } Difference between MyClass , MyClass<Object> , and MyClass<?> \u00b6 class MyClass<T>{ private T content; private set(T content){ this.content = content; } private List<String> getList; } // raw type processMyClass(MyClass myClass){...}; // My class of object types processMyClass(MyClass<Object> myClass){...}; // My class of any specific type processMyClass(MyClass<?> myClass){...}; MyClass (raw type) MyClass<Object> MyClass<?> type safe (Check types at compile time) no yes yes set can be called with any Object any Object null only getList() returns List<Object> List<String> List<String> processMyClass can be called with any List List<Object> only any List Enums \u00b6 Java enumerations are declared using the enum keyword: public enum Semaphore{ RED, ORAGNGE, GREEN } If we need to add some additional members to the enum, we need to end the list of enum items with a semicolon first: public enum Semaphore{ RED, ORAGNGE, GREEN; public void doSomething(){ ... } } Iterating over enum items \u00b6 we can iterate over enum items using the values method: for(Semaphore s: Semaphore.values()){ ... } The above code iterates over values of a specific enum. If we need an iteration over any enum (generic), we need to use a class method: Class<E> enumClass; for(E item: enumClass.getEnumConstants()){ } Converting a string to enum \u00b6 We can convert a String to enum using the valueOf method that is generated for each enum class> Semaphore sem = Semaphore.valueOf(\"RED\"); Note that the string has to match the enum value exactly, including the case. Extra whitespaces are not permited. There is also a generic static valueOf method in the Enum class, which we can use for generic enums: Class<E> enumClass; E genEnumInst = Enum.valueOf(enumClass, \"RED\"); Note that here E has to be a properly typed enum ( Enum<E> ), not the raw enum ( Enum ). File System \u00b6 The modern class for working with paths is the Path class from the java.nio.file package. Other important class is the Files class, which contains static methods for working with files and directories. Important methods: exists isDirectory isRegularFile methods for iterating over files (see next section) Iterating over files in a directory \u00b6 We can iterate the files in a directory using the Files class: Files.list : flat list of files Files.walk : recursive list of files Both methods return a Stream<Path> object. Creating a directory \u00b6 The directory can be created from any Path object using the Files.createDirectory method. We can also create a directory from a File object using the mkdir and mkdirs methods. Date and Time \u00b6 Baeldung tutorial The following classes are intended for working with date and time in Java: LocalDate : date without time LocalTime : time without date LocalDateTime : date and time The LocalDateTime als has its timezone/localization aware counterpart: ZonedDateTime : date and time with timezone Lambda expressions \u00b6 wiki An important thing about lambda expressions in Java is that we can only use them to create types satisfying some functional interface. This means that: They can be used only in a context where a functional interface is expected They need to be convertible to that interface. The example that demonstrate this behavior is bellow. Usually, we use some standard interface, but here we create a new one for clarity: interface OurFunctionalInterface(){ int operation(int a) } ... public void process(int num, OurFunctionalInterface op){ ... } With the above interface and method that uses it, we can call process(0, a -> a + 5) Which is an equivalent of writing OurFunctionalInterface addFive = a -> a + 5; process(0, addFive); Syntax \u00b6 A full syntax for lambda expressions in Java is: (<PARAMS>) -> { <EXPRESSIONS> } If there is only one expression, we can ommit the code block: (<PARAMS>) -> <EXPRESSION> And also, we can ommit the return statement from that expression. The two lambda epressions below are equivalent: (a, b) -> {return a + b;} (a, b) -> a + b If there is only one parameter, we can ommit the parantheses: <PARAM> -> <EXPRESSION> By default, the parameter types are infered from the functional interface. If we need more specific parameters for our function, we can specify the parameter type, we have to specify all of them however: (a, b) -> a + b // valid (int a, int b) -> a + b // valid (int a, b) -> a + b // invalid Also, it is necesary to use the parantheses, if we use the param type. Method references \u00b6 We can also create lambda functions from existing mehods (if they satisfy the desired interface) using a mechanism called method reference . For example, we can use the Integer.sum(int a, int b) method in conext where the IntBinaryOperator interface is required. Instead of IntBinaryOperator op = (a, b) -> a + b; // new lambda body we can write: IntBinaryOperator op = Integer::sum; // lambda from existing function Exceptions in lambda expressions \u00b6 Beware that the checked exceptions thrown inside lambda expressions has to be caught inside the lambda expression. The following code does not compile: try{ process(0, a -> a.methodThatThrows()) catch(exception e){ ... } Instead, we have to write: process(0, a -> { try{ return a.methodThatThrows()); catch(exception e){ ... } } Iteration \u00b6 Java for each loop has the following syntax: for(<TYPE> <VARNAME>: <ITERABLE>){ ... } Here, the <ITERABLE> can be either a Java Iterable , or an array. Enumerated iteration \u00b6 There is no enumerate equivalent in Java. One can use a stream API range method , however, it is less readable than standard for loop because the code execuded in loop has to be in a separate function. Iterating using an iterator \u00b6 The easiest way how to iterate if we have a given iterator is to use its forEachRemaining method. It takes a Consumer object as an argument, iterates using the iterator, and calls Consumer.accept method on each iteration. The example below uses a lambda function that satisfies the consumer interface: class Our{ void process(){ ... } ... } Iterator<Our> objectsIterator = ...; objectsIterator.forEachRemaining(o -> o.process()); Functional Programming \u00b6 Turning array to stream \u00b6 Arrays.stream(<ARRAY>); Creating stream from iterator \u00b6 The easiest way is to first create an Iterable from the iterator and then use the StreamSupport.stream method: Iterable<JsonNode> iterable = () -> iterator; var stream = StreamSupport.stream(iterable.spliterator(), false); Filtering \u00b6 A lambda function can be supplied as a filter: stream.filter(number -> number > 5) returns a stream with numbers greater then five. Transformation \u00b6 We can transform the stream with the map function: transformed = stream.map(object -> doSomething(object)) Materialize stream \u00b6 We can materrialize stream with the collect metod: List<String> result = stringStream.collect(Collectors.toList()); Which can be, in case of List shorten to: List<String> result = stringStream.toList(); var \u00b6 openjdk document Using var is similar to auto in C++. Unlike in C++, it can be used only for local variables. The var can be tricky when using together with diamond operator or generic methods. The compilation works fine, however, the raw type will be created. Type cast \u00b6 Java use a tradidional casting syntax: (<TARGET TYPE>) <VAR> There are two different types of cast: value cast , which is used for value types (only primitive types in Java) and change the data reference cast , which is used for Java objects and does not change it, it just change the objects interface Reference cast \u00b6 The upcasting (casting to a supertype) is done implicitely by the compiler, so instead of Object o = (Object) new MyClass(); we can do Object o = new MyClass(); Typically, we need to use the explicit cast for downcasting : Object o = ...; MyClass c = (MyClass) o; // when we are sure that o is an instance of MyClass... Downcasting to an incorrect type leads to a runtime ClassCastException . Casting to an unrelated type is not allowed by the compiler. Casting generic types \u00b6 Casting with generic types is notoriously dangerous due to type erasure. The real types of generic arguments are not known at runtime, therefore, an incorrect cast does not raise an error: Object o = new Object(); String s = (String) o; // ClassCatsException List<Object> lo = new ArrayList<>(); List<String> ls = (List<String>) lo; // OK at runtime, therefore, it emmits warning at compile time. Casting case expression \u00b6 Often, when we work with an interface, we have to treat each implementation differently. The best way to handle that is to use the polymorphism, i.e., add a dedicated method for treating the object to the interface and handle the differences in each implementation. However, sometimes, we cannot modify the interface as it comes from the outside of our codebase, or we do not want to put the code to the interface because it belongs to a completely different part of the application (e.g., GUI vs database connection). A typicall solution for this is to use branching on instanceof and a consecutive cast: if(obj instance of Apple){ Apple apple = (Apple) obj; ... } else if(obj instance of Peach){ Peach peach = (Peach) obj; ... } ... Casting switch \u00b6 This approach works, it is safe, but it is also error prone. A new preview swich statement is ready to replace this technique: switch(obj) case Apple apple: ... break; case Peach peach: ... break; ... } Note that unsafe type opperations are not allowed in these new switch statements, and an error is emitted instead of warning: List list = ... List<String> ls = (List<String>) list; // warning: unchecked switch(list){ case List<String> ls: // error: cannot be safely cast ... } Random numbers \u00b6 Generate random integer \u00b6 To get an infinite iterator of random integers, we can use the Random.ints method: var it = new Random().ints(0, map.nodesFromAllGraphs.size()).iterator();\\ int a = it.nextInt(); Reflection \u00b6 Reflection is a toolset for working with Java types dynamically. Get class object \u00b6 To obtain the class object, we can use: the forName method of the Class class: Java Class<?> c = Class.forName(\"java.lang.String\"); This way, we can load classes dynamically at runtime. myString.getClass() if we have an instance or String.class if we know the class at compile time. from an object of the class (e.g.,. The method can be then iterated using the getDeclaredMethods method of the Class class. Test if a class is a superclass or interface of another class \u00b6 It can be tested simply as: ClassA.isAssignableFrom(ClassB) Get field or method by name \u00b6 Field field = myClass.getDeclaredField(\"fieldName\"); Method method = myClass.getMethod(\"methodName\", <PARAM TYPES>); Here, the <PARAM TYPES> is a list of classes that represent the types of the method parameters, e.g., String.class, MyClass.class for a method that accepts a string and an instance of MyClass . Call method using Method object \u00b6 method.invoke(myObject, <PARAMS>); SQL \u00b6 Java has a build in support for SQL in package java.sql . The typical operation: create a PreparedStatement from the cursor using the SQL string as an input Fill the parameters of the PreparedStatement Execute the query Filling the query parameters \u00b6 This process consist of safe replacement of ? marks in the SQL string with real values. The PreparedStatement class has dedicated methods for that: setString for strings setObject for complex objects Each method has the index of the ? to be replaced as the first parameter. The index start from 1 . Note that these methods can be used to supply arguments for data part of the SQL statement. If the ? is used for table names or SQL keywords, the query execution will fail. Therefore, if you need to dynamically set the non-data part of an SQL query (e.g., table name), you need to sanitize the argument manually and add it to the SQl querry. Processing the result \u00b6 The java sql framework returns the result of the SQL query in form of a ResultSet . To process the result set, we need to iterate the rows using the next method and for each row, we can access the column values using one of the methods (depending on the dtat type in the column), each of which accepts either column index or label as a parameter. Example: var result = statement.executeQuery(); while(result.next()){ var str = result.getString(\"column_name\")); ... } Jackson \u00b6 Described in the Jackson manual. Dependency injection with Guice \u00b6 Dependency injection is a design pattern that allows interaction between objects without wiring them manually. It works best in large applications with many independent components. When we want to use Singletons in our application, it is time to consider using a dependency injection. Guice is very convenient as it has many features that make using DI even easier. For example, any component can be used in DI by simply annotating the constructor with @Inject . Note that there can be only one constructor annotated with @Inject in a class. To mark a component that should be used as a singleton, we annotate the class with @Singleton annotation. Assisted injection \u00b6 If the class has a constructor with some parameters that are not provided by the Guice, we can use the assisted injection Assisted Injection is a mechanism to automatically generate factories for classes that have a constructor with mixed Guice and non-Guice parameters. Instead of a factory class, we provide only a factory interface: public interface MyFactory { MyClass create(String param1, int param2); } This factory can than be used to create the object: class MyClass{ @Inject MyClass( <parameters provided by Guice>, @Assisted String param1, @Assisted int param2 ){ ... } } The non-Guice parameters are assigned using the parameters of the factory's create method. The matching is determined by the parameter type. If the parameter type is not unique, among all @Assisted parameters, we have to provide the @Assisted annotation with the value parameter: public interface MyFactory { MyClass create(@Assisted(\"param1\") String param1, @Assisted(\"param2\") String param2); } class MyClass{ @Inject MyClass( <parameters provided by Guice>, @Assisted(\"param1\") String param1, @Assisted(\"param2\") String param2 ){ ... } } Progress bar \u00b6 For the progress bar, there is a good library called progressbar The usage is simple, just wrap any iterable, iterator, stream or similar object with the ProgressBar.wrap method: ArrayList<Integer> list = ... for(int i: ProgressBar.wrap(list, \"Processing\")){ ... } // or with a stream Stream<Integer> stream = ... progressStream = ProgressBar.wrap(stream, \"Processing\"); progressStream.forEach(i -> ...); // or with an iterator Iterator<Integer> iterator = ... progressIterator = ProgressBar.wrap(iterator, \"Processing\"); while(iterator.hasNext()){ ... }","title":"Java Programming Guide"},{"location":"Programming/Java/Java%20Programming%20Guide/#primitive-types","text":"Java has the following primitive types: Integer types : byte , short , int , long Floating point types : float , double Character type : char Boolean type : boolean","title":"Primitive types"},{"location":"Programming/Java/Java%20Programming%20Guide/#arrays","text":"Arrays in Java are objects, and they are created using the new keyword. The syntax is: int[] myArray = new int[10]; The array is then filled with zeros. The length of the array is fixed and cannot be changed. The array can be initialized using the following syntax: int[] myArray = {1, 2, 3, 4, 5}; For working with arrays, the static methods from [] java.util.Arrays ](https://docs.oracle.com/javase/8/docs/api/java/util/Arrays.html) class can be used. Some of the most important methods are: copyOf and copyOfRange for copying arrays fill for filling arrays with a specific value","title":"Arrays"},{"location":"Programming/Java/Java%20Programming%20Guide/#classes","text":"In java, there are the following types of classes: Standard class : similar to classes in other languages. Record class : a simple aggregate of values.","title":"Classes"},{"location":"Programming/Java/Java%20Programming%20Guide/#record-class","text":"official specification Record class is a simple aggregate class. It is designed to replace the standard class in cases where the class is used only to hold the data (POJO). It is defined using the record keyword like this: record MyRecord(int par1, String par2){ ... } The advantage over the standard class is that the record class automatically declare data members for all parameters of the record header, and it generates the following methods: equals hashCode toString getters for all parameters constructor that accepts all parameters","title":"Record class"},{"location":"Programming/Java/Java%20Programming%20Guide/#standard-types","text":"","title":"Standard Types"},{"location":"Programming/Java/Java%20Programming%20Guide/#strings","text":"Classical string literals has to be enclosed in quotes ( \"my string\" ). They cannot contain some characters (newline, \" ). The backslash ( \\ ) character is reserved for Java escape sequences and need to be ecaped as \\\\ . In addition, since Java 15, there are text blocks, enclosed in three quotes: \"\"\" My long \"text block\" \"\"\" The properties of text blocks: can be used as standard string literals can contain newlines, quotes, and some other symbols that have to be escaped in standard string literals. The new line after opening triple quoute is obligatory. Note that backslash ( \\ ) is still interpreted as Java escape character, so we need to escape it using \\\\ . There are no fstrings in Java, the best way for using variables in strings is to use the String.format method: String s = String.format(\"My string with %s and %d\", \"text\", 5);","title":"Strings"},{"location":"Programming/Java/Java%20Programming%20Guide/#collections","text":"Java has a rich set of collections. Still, many are missing, notably some tuple class. Also, the typical tuple unpacking known from Python or C++ is not possible in Java and we have to use a verbose way of 1) creating a temporary object for all values, 2) accessing the values using the getters.","title":"Collections"},{"location":"Programming/Java/Java%20Programming%20Guide/#list","text":"List initialization in Java is complicated compared to other languages. we can initialize the list as: List a = List.of(1, 2, 3); However, this method returns an immutable list. If we need a mutable list, we have to pass the list to the ArrayList constructor: List a = new ArrayList<>(List.of(1, 2, 3)); but with such a complicated syntax, we can use an input array directly: List a = new ArrayList<>(Arrays.asList(1, 2, 3));","title":"List"},{"location":"Programming/Java/Java%20Programming%20Guide/#set","text":"From java 9, sets can be simply initialized as: Set<int> nums = Set.of(1,2,3); Note that this method returns an immutable set. In the earlier versions of Java, the Collections.singleton method can be used.","title":"Set"},{"location":"Programming/Java/Java%20Programming%20Guide/#sorting","text":"Some collections can be sorted using the sort member method. It is a stable sort, which use the natural order of the elements by default, but it can be customized using a comparator. The comparator interface expects two elements and should return a negative number if the first element is smaller, a positive number if the first element is greater, and zero if the elements are equal.","title":"Sorting"},{"location":"Programming/Java/Java%20Programming%20Guide/#overloading","text":"When calling an oveladed method with a null argument, we receive a compilation error: reference to <METHOD> is ambiguous . A solution to this problem is to cast the null pointer to the desired type: public set(string s){ ... } public set(MyClass c){ .... } set(null) // doesn't work set((MyClass) null) // works set((String) null) // works","title":"Overloading"},{"location":"Programming/Java/Java%20Programming%20Guide/#regular-expressions","text":"First thing we need to know about regexes in java is that we need to escape all backslashes (refer to the Strings chapter), there are no raw strings in Java. We can work with regexes either by using specialized high level methods that accepts regex as string, or by using the tools from the java.util.regex package.","title":"Regular expressions"},{"location":"Programming/Java/Java%20Programming%20Guide/#testing-if-string-matches-a-regex","text":"We can do it simply by: String MyString = \"tests\"; String MyRegex = \"t\\\\p+\" boolean matches = myString.matches(myRegex);","title":"Testing if string matches a regex"},{"location":"Programming/Java/Java%20Programming%20Guide/#exceptions","text":"The exception mechanism in Java is similar to other languages. The big difference is, however, that in Java, all exception not derived from the RuntimeException class are checked , which means that their handeling is enforced by the compiler. The following code does not compile, because the java.langException class is a checked exception class. private testMethod{ ... throw new Exception(); } Therefore, we need to handle the exception using the try cache block or add throws <EXCEPTION TYPE> to the method declaration. When deciding between try/catch and throws , the rule of thumb is to use the try/cache if we can handle the exception and throws if we want to leave it for the caller. The problem arises when the method we are in implements an interface that does not have the throws declaration we need. Then the only solution is to use try/cache with a cache that does not handle the exception, and indicate the problem to the caller differently, e.g, by returning a null pointer.","title":"Exceptions"},{"location":"Programming/Java/Java%20Programming%20Guide/#logging","text":"Java has a built-in logging mechanism in the java.util.logging package. Apart from that, there are many other logging libraries. In this section, we focus on the SLF4J library which is an abstraction. With SLF4J, we can switch between different logging libraries without changing the code. To use SLF4J, we need to add the following dependency to the pom.xml file: <dependency> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> <version><version></version> </dependency> Additionally, we need to add a backend of our choice. For example, we can use the logback library: <dependency> <groupId>ch.qos.logback</groupId> <artifactId>logback-classic</artifactId> <version><version></version> </dependency> The SLF4J library detects the available backend automatically.","title":"Logging"},{"location":"Programming/Java/Java%20Programming%20Guide/#genericity","text":"Oracle official tutorial for Java 8 Like many other languages, Java offers generic types with the classical syntax: class GenericClass<T>{ ... }; An object is then created as: GenericClass<OtherClass> gc = new GenericClass<OtherClass>();","title":"Genericity"},{"location":"Programming/Java/Java%20Programming%20Guide/#diamond-operator","text":"At all places where the type can be inferred, we can use the dianmond operator ( <> ) without specifiyng a type: GenericClass<OtherClass> gc = new GenericClass<>();","title":"Diamond operator"},{"location":"Programming/Java/Java%20Programming%20Guide/#raw-types","text":"specification For the backward compatibility of Java library classes that were converted to generic classes a concept of raw types was introduces. However, in addition, this concept brings a lot of very subtle compile errors, which are caused by a nonintentional creation of raw type instead of a specific type. GenericClass gc = new GenericClass(); // raw type The parameterized type can be assigned to the raw type, but when we do it the other way arround, the code emmits warning and is not runtime safe: GenericClass<OtherClass> generic = new GenericClass<>(); GenericClass raw = generic // OK GenericClass raw = new GenericClass(); GenericClass<OtherClass> generic = raw // unsafe Calling the generic methods on raw types has a simmilar consequences. But the worst property of raw types is that all generic non-static members of raw types that are not inherited and are also eraw types. Therefore, the genericity is lost even for the completely unrelated type parameters, even specific types are erased to Object . The consequence can be seen on the following example: class GenericClass<T>{ public List<String> getListofStrings() }; GenericClass raw = new GenericClass(); raw.getListofStrings(); // returns List<Object> !!!","title":"Raw types"},{"location":"Programming/Java/Java%20Programming%20Guide/#generic-method","text":"A generic method in Java is written as: public <<GENERIC PARAM NAME>> <RESULT TYPE> <METHOD NAME>(<PARAMS>){ ... } example: public static <T> ProcedureParameterValidationResult<T> error(String msg){ return new ProcedureParameterValidationResult<>(false, msg, null); } if the generic parameter cannot be infered from the method arguments, we can call the method with explicit generic argument like this: <<GENERIC PARAM VALUE>><METHOD NAME>(<ARGUMENTS>); example: ProcedureParameterValidationResult.<String>error(\"Value cannot be empty.\");","title":"Generic method"},{"location":"Programming/Java/Java%20Programming%20Guide/#retrieving-the-type-of-a-generic-argument","text":"Because the genericity in Java is resolved at runtime, it is not possible to use the generic parameter as a type. What does it mean? Whille it is possible to use Myclass.class it is not possible to use T.class . The same applies for the class methods. The usual solution is to pass the class to the generic method as a method parameter: void processGeneric(Class<T> classType){ ... }","title":"Retrieving the type of a generic argument"},{"location":"Programming/Java/Java%20Programming%20Guide/#default-generic-argument","text":"It is not possible to set the default value of a generic parameter. If we want to achive such behavior, we need to create a new class: class GeneriClass<T>{ ... } class StringClass extends GenericClass<String>{};","title":"Default generic argument"},{"location":"Programming/Java/Java%20Programming%20Guide/#wildcards","text":"The wildcard symbol ( ? ) represents a concrete but unspecified type. It can be bounded by superclass/interface ( ? exctends MyInterface ), or by sublcass ( ? super MyClass ). The typical use cases: single non-generic method accepting generic types of multiple generic parameter values: myMethod(Wrapper<? extends MyInterface> wrapper){ MyInterface mi = wrapper.get(); ... } method that can process generic class but does not need to work with the generic parameters at all: getLength(List<?> list){ return list.size(); }","title":"Wildcards"},{"location":"Programming/Java/Java%20Programming%20Guide/#difference-between-myclass-myclassobject-and-myclass","text":"class MyClass<T>{ private T content; private set(T content){ this.content = content; } private List<String> getList; } // raw type processMyClass(MyClass myClass){...}; // My class of object types processMyClass(MyClass<Object> myClass){...}; // My class of any specific type processMyClass(MyClass<?> myClass){...}; MyClass (raw type) MyClass<Object> MyClass<?> type safe (Check types at compile time) no yes yes set can be called with any Object any Object null only getList() returns List<Object> List<String> List<String> processMyClass can be called with any List List<Object> only any List","title":"Difference between MyClass, MyClass&lt;Object&gt;, and MyClass&lt;?&gt;"},{"location":"Programming/Java/Java%20Programming%20Guide/#enums","text":"Java enumerations are declared using the enum keyword: public enum Semaphore{ RED, ORAGNGE, GREEN } If we need to add some additional members to the enum, we need to end the list of enum items with a semicolon first: public enum Semaphore{ RED, ORAGNGE, GREEN; public void doSomething(){ ... } }","title":"Enums"},{"location":"Programming/Java/Java%20Programming%20Guide/#iterating-over-enum-items","text":"we can iterate over enum items using the values method: for(Semaphore s: Semaphore.values()){ ... } The above code iterates over values of a specific enum. If we need an iteration over any enum (generic), we need to use a class method: Class<E> enumClass; for(E item: enumClass.getEnumConstants()){ }","title":"Iterating over enum items"},{"location":"Programming/Java/Java%20Programming%20Guide/#converting-a-string-to-enum","text":"We can convert a String to enum using the valueOf method that is generated for each enum class> Semaphore sem = Semaphore.valueOf(\"RED\"); Note that the string has to match the enum value exactly, including the case. Extra whitespaces are not permited. There is also a generic static valueOf method in the Enum class, which we can use for generic enums: Class<E> enumClass; E genEnumInst = Enum.valueOf(enumClass, \"RED\"); Note that here E has to be a properly typed enum ( Enum<E> ), not the raw enum ( Enum ).","title":"Converting a string to enum"},{"location":"Programming/Java/Java%20Programming%20Guide/#file-system","text":"The modern class for working with paths is the Path class from the java.nio.file package. Other important class is the Files class, which contains static methods for working with files and directories. Important methods: exists isDirectory isRegularFile methods for iterating over files (see next section)","title":"File System"},{"location":"Programming/Java/Java%20Programming%20Guide/#iterating-over-files-in-a-directory","text":"We can iterate the files in a directory using the Files class: Files.list : flat list of files Files.walk : recursive list of files Both methods return a Stream<Path> object.","title":"Iterating over files in a directory"},{"location":"Programming/Java/Java%20Programming%20Guide/#creating-a-directory","text":"The directory can be created from any Path object using the Files.createDirectory method. We can also create a directory from a File object using the mkdir and mkdirs methods.","title":"Creating a directory"},{"location":"Programming/Java/Java%20Programming%20Guide/#date-and-time","text":"Baeldung tutorial The following classes are intended for working with date and time in Java: LocalDate : date without time LocalTime : time without date LocalDateTime : date and time The LocalDateTime als has its timezone/localization aware counterpart: ZonedDateTime : date and time with timezone","title":"Date and Time"},{"location":"Programming/Java/Java%20Programming%20Guide/#lambda-expressions","text":"wiki An important thing about lambda expressions in Java is that we can only use them to create types satisfying some functional interface. This means that: They can be used only in a context where a functional interface is expected They need to be convertible to that interface. The example that demonstrate this behavior is bellow. Usually, we use some standard interface, but here we create a new one for clarity: interface OurFunctionalInterface(){ int operation(int a) } ... public void process(int num, OurFunctionalInterface op){ ... } With the above interface and method that uses it, we can call process(0, a -> a + 5) Which is an equivalent of writing OurFunctionalInterface addFive = a -> a + 5; process(0, addFive);","title":"Lambda expressions"},{"location":"Programming/Java/Java%20Programming%20Guide/#syntax","text":"A full syntax for lambda expressions in Java is: (<PARAMS>) -> { <EXPRESSIONS> } If there is only one expression, we can ommit the code block: (<PARAMS>) -> <EXPRESSION> And also, we can ommit the return statement from that expression. The two lambda epressions below are equivalent: (a, b) -> {return a + b;} (a, b) -> a + b If there is only one parameter, we can ommit the parantheses: <PARAM> -> <EXPRESSION> By default, the parameter types are infered from the functional interface. If we need more specific parameters for our function, we can specify the parameter type, we have to specify all of them however: (a, b) -> a + b // valid (int a, int b) -> a + b // valid (int a, b) -> a + b // invalid Also, it is necesary to use the parantheses, if we use the param type.","title":"Syntax"},{"location":"Programming/Java/Java%20Programming%20Guide/#method-references","text":"We can also create lambda functions from existing mehods (if they satisfy the desired interface) using a mechanism called method reference . For example, we can use the Integer.sum(int a, int b) method in conext where the IntBinaryOperator interface is required. Instead of IntBinaryOperator op = (a, b) -> a + b; // new lambda body we can write: IntBinaryOperator op = Integer::sum; // lambda from existing function","title":"Method references"},{"location":"Programming/Java/Java%20Programming%20Guide/#exceptions-in-lambda-expressions","text":"Beware that the checked exceptions thrown inside lambda expressions has to be caught inside the lambda expression. The following code does not compile: try{ process(0, a -> a.methodThatThrows()) catch(exception e){ ... } Instead, we have to write: process(0, a -> { try{ return a.methodThatThrows()); catch(exception e){ ... } }","title":"Exceptions in lambda expressions"},{"location":"Programming/Java/Java%20Programming%20Guide/#iteration","text":"Java for each loop has the following syntax: for(<TYPE> <VARNAME>: <ITERABLE>){ ... } Here, the <ITERABLE> can be either a Java Iterable , or an array.","title":"Iteration"},{"location":"Programming/Java/Java%20Programming%20Guide/#enumerated-iteration","text":"There is no enumerate equivalent in Java. One can use a stream API range method , however, it is less readable than standard for loop because the code execuded in loop has to be in a separate function.","title":"Enumerated iteration"},{"location":"Programming/Java/Java%20Programming%20Guide/#iterating-using-an-iterator","text":"The easiest way how to iterate if we have a given iterator is to use its forEachRemaining method. It takes a Consumer object as an argument, iterates using the iterator, and calls Consumer.accept method on each iteration. The example below uses a lambda function that satisfies the consumer interface: class Our{ void process(){ ... } ... } Iterator<Our> objectsIterator = ...; objectsIterator.forEachRemaining(o -> o.process());","title":"Iterating using an iterator"},{"location":"Programming/Java/Java%20Programming%20Guide/#functional-programming","text":"","title":"Functional Programming"},{"location":"Programming/Java/Java%20Programming%20Guide/#turning-array-to-stream","text":"Arrays.stream(<ARRAY>);","title":"Turning array to stream"},{"location":"Programming/Java/Java%20Programming%20Guide/#creating-stream-from-iterator","text":"The easiest way is to first create an Iterable from the iterator and then use the StreamSupport.stream method: Iterable<JsonNode> iterable = () -> iterator; var stream = StreamSupport.stream(iterable.spliterator(), false);","title":"Creating stream from iterator"},{"location":"Programming/Java/Java%20Programming%20Guide/#filtering","text":"A lambda function can be supplied as a filter: stream.filter(number -> number > 5) returns a stream with numbers greater then five.","title":"Filtering"},{"location":"Programming/Java/Java%20Programming%20Guide/#transformation","text":"We can transform the stream with the map function: transformed = stream.map(object -> doSomething(object))","title":"Transformation"},{"location":"Programming/Java/Java%20Programming%20Guide/#materialize-stream","text":"We can materrialize stream with the collect metod: List<String> result = stringStream.collect(Collectors.toList()); Which can be, in case of List shorten to: List<String> result = stringStream.toList();","title":"Materialize stream"},{"location":"Programming/Java/Java%20Programming%20Guide/#var","text":"openjdk document Using var is similar to auto in C++. Unlike in C++, it can be used only for local variables. The var can be tricky when using together with diamond operator or generic methods. The compilation works fine, however, the raw type will be created.","title":"var"},{"location":"Programming/Java/Java%20Programming%20Guide/#type-cast","text":"Java use a tradidional casting syntax: (<TARGET TYPE>) <VAR> There are two different types of cast: value cast , which is used for value types (only primitive types in Java) and change the data reference cast , which is used for Java objects and does not change it, it just change the objects interface","title":"Type cast"},{"location":"Programming/Java/Java%20Programming%20Guide/#reference-cast","text":"The upcasting (casting to a supertype) is done implicitely by the compiler, so instead of Object o = (Object) new MyClass(); we can do Object o = new MyClass(); Typically, we need to use the explicit cast for downcasting : Object o = ...; MyClass c = (MyClass) o; // when we are sure that o is an instance of MyClass... Downcasting to an incorrect type leads to a runtime ClassCastException . Casting to an unrelated type is not allowed by the compiler.","title":"Reference cast"},{"location":"Programming/Java/Java%20Programming%20Guide/#casting-generic-types","text":"Casting with generic types is notoriously dangerous due to type erasure. The real types of generic arguments are not known at runtime, therefore, an incorrect cast does not raise an error: Object o = new Object(); String s = (String) o; // ClassCatsException List<Object> lo = new ArrayList<>(); List<String> ls = (List<String>) lo; // OK at runtime, therefore, it emmits warning at compile time.","title":"Casting generic types"},{"location":"Programming/Java/Java%20Programming%20Guide/#casting-case-expression","text":"Often, when we work with an interface, we have to treat each implementation differently. The best way to handle that is to use the polymorphism, i.e., add a dedicated method for treating the object to the interface and handle the differences in each implementation. However, sometimes, we cannot modify the interface as it comes from the outside of our codebase, or we do not want to put the code to the interface because it belongs to a completely different part of the application (e.g., GUI vs database connection). A typicall solution for this is to use branching on instanceof and a consecutive cast: if(obj instance of Apple){ Apple apple = (Apple) obj; ... } else if(obj instance of Peach){ Peach peach = (Peach) obj; ... } ...","title":"Casting case expression"},{"location":"Programming/Java/Java%20Programming%20Guide/#casting-switch","text":"This approach works, it is safe, but it is also error prone. A new preview swich statement is ready to replace this technique: switch(obj) case Apple apple: ... break; case Peach peach: ... break; ... } Note that unsafe type opperations are not allowed in these new switch statements, and an error is emitted instead of warning: List list = ... List<String> ls = (List<String>) list; // warning: unchecked switch(list){ case List<String> ls: // error: cannot be safely cast ... }","title":"Casting switch"},{"location":"Programming/Java/Java%20Programming%20Guide/#random-numbers","text":"","title":"Random numbers"},{"location":"Programming/Java/Java%20Programming%20Guide/#generate-random-integer","text":"To get an infinite iterator of random integers, we can use the Random.ints method: var it = new Random().ints(0, map.nodesFromAllGraphs.size()).iterator();\\ int a = it.nextInt();","title":"Generate random integer"},{"location":"Programming/Java/Java%20Programming%20Guide/#reflection","text":"Reflection is a toolset for working with Java types dynamically.","title":"Reflection"},{"location":"Programming/Java/Java%20Programming%20Guide/#get-class-object","text":"To obtain the class object, we can use: the forName method of the Class class: Java Class<?> c = Class.forName(\"java.lang.String\"); This way, we can load classes dynamically at runtime. myString.getClass() if we have an instance or String.class if we know the class at compile time. from an object of the class (e.g.,. The method can be then iterated using the getDeclaredMethods method of the Class class.","title":"Get class object"},{"location":"Programming/Java/Java%20Programming%20Guide/#test-if-a-class-is-a-superclass-or-interface-of-another-class","text":"It can be tested simply as: ClassA.isAssignableFrom(ClassB)","title":"Test if a class is a superclass or interface of another class"},{"location":"Programming/Java/Java%20Programming%20Guide/#get-field-or-method-by-name","text":"Field field = myClass.getDeclaredField(\"fieldName\"); Method method = myClass.getMethod(\"methodName\", <PARAM TYPES>); Here, the <PARAM TYPES> is a list of classes that represent the types of the method parameters, e.g., String.class, MyClass.class for a method that accepts a string and an instance of MyClass .","title":"Get field or method by name"},{"location":"Programming/Java/Java%20Programming%20Guide/#call-method-using-method-object","text":"method.invoke(myObject, <PARAMS>);","title":"Call method using Method object"},{"location":"Programming/Java/Java%20Programming%20Guide/#sql","text":"Java has a build in support for SQL in package java.sql . The typical operation: create a PreparedStatement from the cursor using the SQL string as an input Fill the parameters of the PreparedStatement Execute the query","title":"SQL"},{"location":"Programming/Java/Java%20Programming%20Guide/#filling-the-query-parameters","text":"This process consist of safe replacement of ? marks in the SQL string with real values. The PreparedStatement class has dedicated methods for that: setString for strings setObject for complex objects Each method has the index of the ? to be replaced as the first parameter. The index start from 1 . Note that these methods can be used to supply arguments for data part of the SQL statement. If the ? is used for table names or SQL keywords, the query execution will fail. Therefore, if you need to dynamically set the non-data part of an SQL query (e.g., table name), you need to sanitize the argument manually and add it to the SQl querry.","title":"Filling the query parameters"},{"location":"Programming/Java/Java%20Programming%20Guide/#processing-the-result","text":"The java sql framework returns the result of the SQL query in form of a ResultSet . To process the result set, we need to iterate the rows using the next method and for each row, we can access the column values using one of the methods (depending on the dtat type in the column), each of which accepts either column index or label as a parameter. Example: var result = statement.executeQuery(); while(result.next()){ var str = result.getString(\"column_name\")); ... }","title":"Processing the result"},{"location":"Programming/Java/Java%20Programming%20Guide/#jackson","text":"Described in the Jackson manual.","title":"Jackson"},{"location":"Programming/Java/Java%20Programming%20Guide/#dependency-injection-with-guice","text":"Dependency injection is a design pattern that allows interaction between objects without wiring them manually. It works best in large applications with many independent components. When we want to use Singletons in our application, it is time to consider using a dependency injection. Guice is very convenient as it has many features that make using DI even easier. For example, any component can be used in DI by simply annotating the constructor with @Inject . Note that there can be only one constructor annotated with @Inject in a class. To mark a component that should be used as a singleton, we annotate the class with @Singleton annotation.","title":"Dependency injection with Guice"},{"location":"Programming/Java/Java%20Programming%20Guide/#assisted-injection","text":"If the class has a constructor with some parameters that are not provided by the Guice, we can use the assisted injection Assisted Injection is a mechanism to automatically generate factories for classes that have a constructor with mixed Guice and non-Guice parameters. Instead of a factory class, we provide only a factory interface: public interface MyFactory { MyClass create(String param1, int param2); } This factory can than be used to create the object: class MyClass{ @Inject MyClass( <parameters provided by Guice>, @Assisted String param1, @Assisted int param2 ){ ... } } The non-Guice parameters are assigned using the parameters of the factory's create method. The matching is determined by the parameter type. If the parameter type is not unique, among all @Assisted parameters, we have to provide the @Assisted annotation with the value parameter: public interface MyFactory { MyClass create(@Assisted(\"param1\") String param1, @Assisted(\"param2\") String param2); } class MyClass{ @Inject MyClass( <parameters provided by Guice>, @Assisted(\"param1\") String param1, @Assisted(\"param2\") String param2 ){ ... } }","title":"Assisted injection"},{"location":"Programming/Java/Java%20Programming%20Guide/#progress-bar","text":"For the progress bar, there is a good library called progressbar The usage is simple, just wrap any iterable, iterator, stream or similar object with the ProgressBar.wrap method: ArrayList<Integer> list = ... for(int i: ProgressBar.wrap(list, \"Processing\")){ ... } // or with a stream Stream<Integer> stream = ... progressStream = ProgressBar.wrap(stream, \"Processing\"); progressStream.forEach(i -> ...); // or with an iterator Iterator<Integer> iterator = ... progressIterator = ProgressBar.wrap(iterator, \"Processing\"); while(iterator.hasNext()){ ... }","title":"Progress bar"},{"location":"Programming/Java/Java%20Workflow/","text":"Java developement stack \u00b6 We use this stack: Toolchain: JDK Package manager: standalone Maven IDE: Netbeans, Idea JDK \u00b6 Java developem kit is the standard Java toolchain. Most comon tools are: javac for compilation java for execution javac \u00b6 The syntax is: javac [options] [sourcefiles] Warning control \u00b6 By default, all warnings are disabled and only a summary of the warnings is displayed in the output (i.e., all warning types encountered, without specific lines where they occur). To enable all warnings use the -Xlint argument. To enable/disable specific warnings, use -Xlint:<warning name> and -Xlint:-<warning name> , respectively. Maven \u00b6 Described in the Maven manual. Netbeans \u00b6 Install the latest version of Apache Netbeans Configuration: Autosaving: Editor -> Autosave Tab size, tabs instead of spaces, 120 char marker line: Editor -> Formatting -> All Languages Multi-row tabs: Appearance -> DocumentTabs Git labels on projects: Team -> Versioning -> Show Versioning Labels Internet Browser: General -> web browser Javadoc config: if the javadoc for java SE does not work out of the box, maybe there is a wrong URL. Go to Tools -> Java Platforms -> Javadoc and enter there the path where the Javadoc is accessible online Install the basic plugins Markdown Project configuration \u00b6 Configure Maven Goals \u00b6 These can be configured in Project properties -> Actions Troubleshooting \u00b6 If there is a serious problem, one way to solve it can be to delete the Netbeans cache located in ~\\AppData\\Local\\NetBeans\\Cache . Idea \u00b6 Configuration \u00b6 Configuration is done in File -> Settings . A lot of configuration we do in Idea Settings only applies to the current project. Therefore, it is a good idea to check whether the settings is not present in the template for new projects, so that we do not have to set it up for every new project. The settings template is located in File -> New Project Setup . Settings synchronization \u00b6 Log in into JetBrains Toolbox or to the App Click on the gear icon on the top-right and choose Sync Check all categories and click on pull settings from the cloud Resatart Ida to update all the settings More on Jetbrains Maven configuration \u00b6 By default, Idea uses the bundled Maven, which is almost never desired. It is best to swich to the system Maven, which we should done in: File -> Settings -> Build, Execution, Deployment -> Build Tools -> Maven -> Maven home directory for existing projects File -> New Project Setup -> Maven -> Maven home directory for new projects JDK configuration \u00b6 The correct JDK has to be set up in various places: compielr has to be at least target jdk, set it in: File -> Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Project bytecode version and Per-module bytecode version language level should be the same as target jdk: File -> Project Structure... -> Modules -> Language Level Compilation \u00b6 Everything is compiled in the background automatically. However, if we need to compile manually using maven, e.g., to activate certain plugins, we can compile using the maven tab on the right. Running Projects \u00b6 To add or edit run configurations, click on the run configuration dropdown left of the run button and choose Edit Configurations... . If the required configuration field is not present, it may be necessary to activate it by clicking on Modify options and choosing the desired option. Developing the whole project stack at once \u00b6 If we are developing a whole stack of projects at once, it is best if we can navigate between them easily. However, it is not possible to open multiple projects in the same window in Idea (like in Netbeans). Instead, we need to open the project on the top of the stack and then add the other projects as modules. To add a module, click File -> Project Structure -> Modules and add the module. Running Maven Goals \u00b6 Maven goals can be run from a dedicated tab on the right. The goals in the tab are divided into two categories: Lifecycle goals are the most common goals, which are used to build the project Plugins goals are the goals of the plugins used in the project To run the goal, just double-click on it. If the run need some special configuration, right-click on the goal and choose Modify Run Configuration... If the plugin is missing from the list, it may be necessary to reload the plugins. Click the Reload All Maven Projects button in the top left corner of the maven tab. If the goal we want to run is neither a Lifecycle goal nor a Plugin goal (e.g., exec ), we can run it by creating a run configuration and selecting maven type. Than, we select the maven goal by inserting it as a first argument in the Command line field. Maven goal configuration \u00b6 To run a maven goal with a specific profile, add the profile name to the Profiles field. Troubleshooting \u00b6 Idea does not detect manually installed Maven artifact \u00b6 Sometimes, idea cannot recognize installed maven artifact and marks it as missing (red). Fix: right click on the project or pom file -> Maven -> Reload project Idea does not see environment variables \u00b6 These may be new environment variables, which were not present when Idea was started. Restart Idea to see the new environment variables. Set Java version for project \u00b6 There are two options to set: source version determines the least version of java that supports all language tools used in the source code. It is the least version that can be used to compile the source code. target version determines the least version of java that can be used to run your java application. The target version can be higher than the source version, but not he other way around. most of the time, however, we use the same version for source and for target. Usually, these versions needs to be set: in the project compilation tool (maven) to configure the project compilation in the IDE project properties, to configure the compilation of individual files, which is executed in the background to report the compilation errors at real time. Sometimes, the Java version used for running Maven has to be also set because it cannot be lower then the Java version used for project compilation using Maven. SO explanation Setting Java version in Maven \u00b6 Since Java 9, both Java versions can be set at once with the release option: <properties> <maven.compiler.release>10</maven.compiler.release> </properties> or equivalently <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> <release>10</release> </configuration> </plugin> The old way is to use separate properties: <properties> <maven.compiler.target>10</maven.compiler.target> <maven.compiler.source>10</maven.compiler.source> </properties> Note that the Java version configured in the pom file cannot be greater then the Java version used to run the Maven. Setting Java version in Netbeans \u00b6 Note that for maven projects, this is automatically set to match the properties in the pom file. For the Netbeans real-time compiler, the cross compilation does not make sense, so both source and target Java version is set in one place: Right click on project -> Properties -> Sources In the bottom, change the Source/Binary Format Setting Java version in IDEA \u00b6 In idea, an extra step is sometimes necessary: to set the Java Language Level in the project settings: File -> Project Structure... -> Modules -> Language Level . The language level has to be lower or equal to the target version set in the maven configuration. Setting Java version used for executing Maven \u00b6 If the Maven is executed from command line, edit the JAVA_HOME system property. If the Maven is executed from Netbeans, edit Project Properties -> Build -> Compile -> Java Platform Enabling preview features \u00b6 The preview features can be enabled using the --enable-preview argument. In Maven, this has to be passed to the compiler plugin: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <enablePreview>true</enablePreview> </configuration> </plugin> </plugins> </build> For Maven compiler plugin older then version 3.10.1, use: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <compilerArgs> <arg>--enable-preview</arg> </compilerArgs> </configuration> </plugin> </plugins> </build> Unfortunatelly, it is also necessary to enable preview features in the IDE configuration: In Netbeans, add --enable-preview to Project Properties -> Run -> VM options . In IDEA: add --enable-preview to Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Override compiler parameters per-module add --enable-preview to run configuration -> Build and run vm options Gurobi \u00b6 Gurobi is a commercial project not contained in any public maven repositories. It is necessary to install the gurobi maven artifact manualy. Potential problems \u00b6 unsatisfied linker error : Check if the gurobi version in the error log matches the gurobi version installed. Make part of the project optional \u00b6 Sometimes we want to make a part of the project optional, so that it is not required at runtime or even at compile time. This is useful if that part of the project is: not essential for the project to work dependent on some external library, which can be cumbersome to install The required steps are usually: Create a maven profile for the optional part of the project Move optional dependencies to the profile Move optional source files outside the src/main/java directory Use the build-helper-maven-plugin in the profile to add the optional source files to the project At runtime, load the optional part of the project using the reflection We now describe these steps in detail. Maven profiles and optional dependencies \u00b6 A maven profile belongs to the profiles section of the pom.xml file. The structure is simple: <profiles> <profile> <id>optional</id> <dependencies> <!-- optional dependencies --> </dependencies> <build> <plugins> <!-- optional plugins --> </plugins> </build> </profile> Moving optional source files outside the src/main/java directory \u00b6 The optional source files can be moved anywhere, but a logical place is to create a new directory under src/main , e.g., src/main/<lib>-optional . Then we need to tell maven to include these files in the project if the optional profile is activated. This is done using the build-helper-maven-plugin in the optional profile: <profiles> <profile> <id>optional</id> <build> <plugins> build-helper-maven-plugin here </plugins> </build> </profile> </profiles> For the build-helper-maven-plugin configuration, see the Maven manual . Loading the optional part of the project at runtime \u00b6 First, we should check if the optional part of the project is present at runtime: public static boolean libAvailable(){ try { Class.forName(\"com.example.lib\"); return true; } catch (ClassNotFoundException e) { return false; } } Then, we can load the optional part of the project using reflection: if(libAvailable()){ Class<?> libClass = Class.forName(\"com.example.lib\"); Object lib = libClass.newInstance(); Method method = libClass.getMethod(\"someMethod\"); method.invoke(lib); } else{ // handle missing library } Archive \u00b6 AIC maven repo access \u00b6 To Access the AIC maven repo, copy maven settings from another computer (located in ~/.m2)","title":"Java Workflow"},{"location":"Programming/Java/Java%20Workflow/#java-developement-stack","text":"We use this stack: Toolchain: JDK Package manager: standalone Maven IDE: Netbeans, Idea","title":"Java developement stack"},{"location":"Programming/Java/Java%20Workflow/#jdk","text":"Java developem kit is the standard Java toolchain. Most comon tools are: javac for compilation java for execution","title":"JDK"},{"location":"Programming/Java/Java%20Workflow/#javac","text":"The syntax is: javac [options] [sourcefiles]","title":"javac"},{"location":"Programming/Java/Java%20Workflow/#warning-control","text":"By default, all warnings are disabled and only a summary of the warnings is displayed in the output (i.e., all warning types encountered, without specific lines where they occur). To enable all warnings use the -Xlint argument. To enable/disable specific warnings, use -Xlint:<warning name> and -Xlint:-<warning name> , respectively.","title":"Warning control"},{"location":"Programming/Java/Java%20Workflow/#maven","text":"Described in the Maven manual.","title":"Maven"},{"location":"Programming/Java/Java%20Workflow/#netbeans","text":"Install the latest version of Apache Netbeans Configuration: Autosaving: Editor -> Autosave Tab size, tabs instead of spaces, 120 char marker line: Editor -> Formatting -> All Languages Multi-row tabs: Appearance -> DocumentTabs Git labels on projects: Team -> Versioning -> Show Versioning Labels Internet Browser: General -> web browser Javadoc config: if the javadoc for java SE does not work out of the box, maybe there is a wrong URL. Go to Tools -> Java Platforms -> Javadoc and enter there the path where the Javadoc is accessible online Install the basic plugins Markdown","title":"Netbeans"},{"location":"Programming/Java/Java%20Workflow/#project-configuration","text":"","title":"Project configuration"},{"location":"Programming/Java/Java%20Workflow/#configure-maven-goals","text":"These can be configured in Project properties -> Actions","title":"Configure Maven Goals"},{"location":"Programming/Java/Java%20Workflow/#troubleshooting","text":"If there is a serious problem, one way to solve it can be to delete the Netbeans cache located in ~\\AppData\\Local\\NetBeans\\Cache .","title":"Troubleshooting"},{"location":"Programming/Java/Java%20Workflow/#idea","text":"","title":"Idea"},{"location":"Programming/Java/Java%20Workflow/#configuration","text":"Configuration is done in File -> Settings . A lot of configuration we do in Idea Settings only applies to the current project. Therefore, it is a good idea to check whether the settings is not present in the template for new projects, so that we do not have to set it up for every new project. The settings template is located in File -> New Project Setup .","title":"Configuration"},{"location":"Programming/Java/Java%20Workflow/#settings-synchronization","text":"Log in into JetBrains Toolbox or to the App Click on the gear icon on the top-right and choose Sync Check all categories and click on pull settings from the cloud Resatart Ida to update all the settings More on Jetbrains","title":"Settings synchronization"},{"location":"Programming/Java/Java%20Workflow/#maven-configuration","text":"By default, Idea uses the bundled Maven, which is almost never desired. It is best to swich to the system Maven, which we should done in: File -> Settings -> Build, Execution, Deployment -> Build Tools -> Maven -> Maven home directory for existing projects File -> New Project Setup -> Maven -> Maven home directory for new projects","title":"Maven configuration"},{"location":"Programming/Java/Java%20Workflow/#jdk-configuration","text":"The correct JDK has to be set up in various places: compielr has to be at least target jdk, set it in: File -> Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Project bytecode version and Per-module bytecode version language level should be the same as target jdk: File -> Project Structure... -> Modules -> Language Level","title":"JDK configuration"},{"location":"Programming/Java/Java%20Workflow/#compilation","text":"Everything is compiled in the background automatically. However, if we need to compile manually using maven, e.g., to activate certain plugins, we can compile using the maven tab on the right.","title":"Compilation"},{"location":"Programming/Java/Java%20Workflow/#running-projects","text":"To add or edit run configurations, click on the run configuration dropdown left of the run button and choose Edit Configurations... . If the required configuration field is not present, it may be necessary to activate it by clicking on Modify options and choosing the desired option.","title":"Running Projects"},{"location":"Programming/Java/Java%20Workflow/#developing-the-whole-project-stack-at-once","text":"If we are developing a whole stack of projects at once, it is best if we can navigate between them easily. However, it is not possible to open multiple projects in the same window in Idea (like in Netbeans). Instead, we need to open the project on the top of the stack and then add the other projects as modules. To add a module, click File -> Project Structure -> Modules and add the module.","title":"Developing the whole project stack at once"},{"location":"Programming/Java/Java%20Workflow/#running-maven-goals","text":"Maven goals can be run from a dedicated tab on the right. The goals in the tab are divided into two categories: Lifecycle goals are the most common goals, which are used to build the project Plugins goals are the goals of the plugins used in the project To run the goal, just double-click on it. If the run need some special configuration, right-click on the goal and choose Modify Run Configuration... If the plugin is missing from the list, it may be necessary to reload the plugins. Click the Reload All Maven Projects button in the top left corner of the maven tab. If the goal we want to run is neither a Lifecycle goal nor a Plugin goal (e.g., exec ), we can run it by creating a run configuration and selecting maven type. Than, we select the maven goal by inserting it as a first argument in the Command line field.","title":"Running Maven Goals"},{"location":"Programming/Java/Java%20Workflow/#maven-goal-configuration","text":"To run a maven goal with a specific profile, add the profile name to the Profiles field.","title":"Maven goal configuration"},{"location":"Programming/Java/Java%20Workflow/#troubleshooting_1","text":"","title":"Troubleshooting"},{"location":"Programming/Java/Java%20Workflow/#idea-does-not-detect-manually-installed-maven-artifact","text":"Sometimes, idea cannot recognize installed maven artifact and marks it as missing (red). Fix: right click on the project or pom file -> Maven -> Reload project","title":"Idea does not detect manually installed Maven artifact"},{"location":"Programming/Java/Java%20Workflow/#idea-does-not-see-environment-variables","text":"These may be new environment variables, which were not present when Idea was started. Restart Idea to see the new environment variables.","title":"Idea does not see environment variables"},{"location":"Programming/Java/Java%20Workflow/#set-java-version-for-project","text":"There are two options to set: source version determines the least version of java that supports all language tools used in the source code. It is the least version that can be used to compile the source code. target version determines the least version of java that can be used to run your java application. The target version can be higher than the source version, but not he other way around. most of the time, however, we use the same version for source and for target. Usually, these versions needs to be set: in the project compilation tool (maven) to configure the project compilation in the IDE project properties, to configure the compilation of individual files, which is executed in the background to report the compilation errors at real time. Sometimes, the Java version used for running Maven has to be also set because it cannot be lower then the Java version used for project compilation using Maven. SO explanation","title":"Set Java version for project"},{"location":"Programming/Java/Java%20Workflow/#setting-java-version-in-maven","text":"Since Java 9, both Java versions can be set at once with the release option: <properties> <maven.compiler.release>10</maven.compiler.release> </properties> or equivalently <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> <release>10</release> </configuration> </plugin> The old way is to use separate properties: <properties> <maven.compiler.target>10</maven.compiler.target> <maven.compiler.source>10</maven.compiler.source> </properties> Note that the Java version configured in the pom file cannot be greater then the Java version used to run the Maven.","title":"Setting Java version in Maven"},{"location":"Programming/Java/Java%20Workflow/#setting-java-version-in-netbeans","text":"Note that for maven projects, this is automatically set to match the properties in the pom file. For the Netbeans real-time compiler, the cross compilation does not make sense, so both source and target Java version is set in one place: Right click on project -> Properties -> Sources In the bottom, change the Source/Binary Format","title":"Setting Java version in Netbeans"},{"location":"Programming/Java/Java%20Workflow/#setting-java-version-in-idea","text":"In idea, an extra step is sometimes necessary: to set the Java Language Level in the project settings: File -> Project Structure... -> Modules -> Language Level . The language level has to be lower or equal to the target version set in the maven configuration.","title":"Setting Java version in IDEA"},{"location":"Programming/Java/Java%20Workflow/#setting-java-version-used-for-executing-maven","text":"If the Maven is executed from command line, edit the JAVA_HOME system property. If the Maven is executed from Netbeans, edit Project Properties -> Build -> Compile -> Java Platform","title":"Setting Java version used for executing Maven"},{"location":"Programming/Java/Java%20Workflow/#enabling-preview-features","text":"The preview features can be enabled using the --enable-preview argument. In Maven, this has to be passed to the compiler plugin: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <enablePreview>true</enablePreview> </configuration> </plugin> </plugins> </build> For Maven compiler plugin older then version 3.10.1, use: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <compilerArgs> <arg>--enable-preview</arg> </compilerArgs> </configuration> </plugin> </plugins> </build> Unfortunatelly, it is also necessary to enable preview features in the IDE configuration: In Netbeans, add --enable-preview to Project Properties -> Run -> VM options . In IDEA: add --enable-preview to Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Override compiler parameters per-module add --enable-preview to run configuration -> Build and run vm options","title":"Enabling preview features"},{"location":"Programming/Java/Java%20Workflow/#gurobi","text":"Gurobi is a commercial project not contained in any public maven repositories. It is necessary to install the gurobi maven artifact manualy.","title":"Gurobi"},{"location":"Programming/Java/Java%20Workflow/#potential-problems","text":"unsatisfied linker error : Check if the gurobi version in the error log matches the gurobi version installed.","title":"Potential problems"},{"location":"Programming/Java/Java%20Workflow/#make-part-of-the-project-optional","text":"Sometimes we want to make a part of the project optional, so that it is not required at runtime or even at compile time. This is useful if that part of the project is: not essential for the project to work dependent on some external library, which can be cumbersome to install The required steps are usually: Create a maven profile for the optional part of the project Move optional dependencies to the profile Move optional source files outside the src/main/java directory Use the build-helper-maven-plugin in the profile to add the optional source files to the project At runtime, load the optional part of the project using the reflection We now describe these steps in detail.","title":"Make part of the project optional"},{"location":"Programming/Java/Java%20Workflow/#maven-profiles-and-optional-dependencies","text":"A maven profile belongs to the profiles section of the pom.xml file. The structure is simple: <profiles> <profile> <id>optional</id> <dependencies> <!-- optional dependencies --> </dependencies> <build> <plugins> <!-- optional plugins --> </plugins> </build> </profile>","title":"Maven profiles and optional dependencies"},{"location":"Programming/Java/Java%20Workflow/#moving-optional-source-files-outside-the-srcmainjava-directory","text":"The optional source files can be moved anywhere, but a logical place is to create a new directory under src/main , e.g., src/main/<lib>-optional . Then we need to tell maven to include these files in the project if the optional profile is activated. This is done using the build-helper-maven-plugin in the optional profile: <profiles> <profile> <id>optional</id> <build> <plugins> build-helper-maven-plugin here </plugins> </build> </profile> </profiles> For the build-helper-maven-plugin configuration, see the Maven manual .","title":"Moving optional source files outside the src/main/java directory"},{"location":"Programming/Java/Java%20Workflow/#loading-the-optional-part-of-the-project-at-runtime","text":"First, we should check if the optional part of the project is present at runtime: public static boolean libAvailable(){ try { Class.forName(\"com.example.lib\"); return true; } catch (ClassNotFoundException e) { return false; } } Then, we can load the optional part of the project using reflection: if(libAvailable()){ Class<?> libClass = Class.forName(\"com.example.lib\"); Object lib = libClass.newInstance(); Method method = libClass.getMethod(\"someMethod\"); method.invoke(lib); } else{ // handle missing library }","title":"Loading the optional part of the project at runtime"},{"location":"Programming/Java/Java%20Workflow/#archive","text":"","title":"Archive"},{"location":"Programming/Java/Java%20Workflow/#aic-maven-repo-access","text":"To Access the AIC maven repo, copy maven settings from another computer (located in ~/.m2)","title":"AIC maven repo access"},{"location":"Programming/Java/Maven/","text":"Maven is a dependency management and build tool for Java. The project configuration for Maven is stored in a single file called pom.xml . Both the project dependencies and the build configuration are stored in this file. The maven is executed using the as mvn <goal> . Typical goals are compile : compile the project test : run tests package : package the project into a jar or war file install : install the project into the local maven repository deploy : deploy the project to a remote repository Maven Packages: Artifacts \u00b6 All Maven dependencies and plugins installed in local system repositories can be either: downloaded from a remote repository installed from a local file or project Note that if we want the artifact to be downloaded from a remote repository, we do not have to take any action. It will be downloaded automatically when needed. By default, Maven searches for artifacts in the Maven Central repository . If we want to use a different repository, we have to add it manually. List locally installed artifacts \u00b6 There is no built-in command to list all locally installed artifacts. However, we can use the info plugin : mvn ninja.fido:info-maven-plugin:list Install artifacts from a remote repository manually \u00b6 If we want to install an artifact from a remote repository manually, we can use the dependency:get goal. The syntax is as follows: mvn dependency:get -Dartifact=<GROUP ID>:<ARTIFACT ID>:<VERSION> Debugging missing Artifacts \u00b6 All Maven dependencies should work out of the box. If some dependencies cannot be resolved: check that the dependencies are on the maven central. if not, check that they are in some special repo and check that the repo is present in the pom of the project that requires the dependency Variables \u00b6 Variables in the pom.xml file can be used using the ${variable} syntax. To use some environment variables, we can use the ${env.variable} . Note that contrary to all expectations, maven goals do not fail if a variable is not defined . Instead, the variable is replaced with an empty string. This can lead to unexpected behavior, so it is usually preferable to fail. For that, we can use the enforcer plugin with the requireProperty rule: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-enforcer-plugin</artifactId> <version>3.4.1</version> <executions> <execution> <id>enforce-property</id> <goals> <goal>enforce</goal> </goals> <configuration> <rules> <requireProperty> <property>basedir</property> <message>You must set a basedir property!</message> </requireProperty> </rules> <fail>true</fail> </configuration> </execution> </executions> </plugin> Compilation \u00b6 reference Compilation is handeled by the Maven compiler plugin. Usually, typing mvn compile is enough to compile the project. If you need any customization, add it to the compiler plugin configuration <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> .... </configuration> </plugin> Passing arguments to javac \u00b6 A single argument can be passed using the <compilerArgument> property. For more compiler arguments, use <compilerArgs> : <compilerArgs> <arg>-Xmaxerrs=1000</arg> <arg>-Xlint</arg> </compilerArgs> Dependencies \u00b6 Maven dependencies are defined in the pom.xml file, in the <dependencies> section. Each dependency has the following structure: <dependency> <groupId>org.apache.maven</groupId> <artifactId>maven-core</artifactId> <version>3.0</version> <scope>compile</scope> </dependency> The scope property is optional and the default value is compile . The dependencies are automatically downloaded if we run a maven goal that requires them (e.g., compile , test , install ). If we want to download them manually, we can use the dependency:resolve goal. To list all dependencies of a project, we can use the dependency:list goal. Using dependencies distributed as a jar \u00b6 Sometimes, we possess a jar file that is not present in any maven repository. We can still install it to the local repository as a new artifact using the install:install-file goal: mvn install:install-file -Dfile=<PATH TO JAR> -DgroupId=<GROUP ID> -DartifactId=<ARTEIFACT ID> -Dversion=<VERSION> -Dpackaging=jar Here the <PATH TO JAR> is a path to the jar library. The <GROUP ID> , <ARTEIFACT ID> , and <VERSION> can have arbitrary values, they just have to correspond with the values specified in the pom dependency. Be careful with PowerShell: in PowerShell, program parameters starting with minus and containing dot need to be quoted, for example: '-DgroupId=com.example.app' instead of -DgroupId=com.example.app Using dependencies distributed as a zip \u00b6 Sometimes, the dependency is distributed as a zip file containing sources. Typically, the manual for such a library tells us to modify the classpath to include the sources. However, Maven handles the classpath automatically, so it is not wise to modify it manually. The best way to use it is to unpack the zip and add the sources to the project during compilation. For that, we can use the build-helper-maven-plugin plugin: <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>build-helper-maven-plugin</artifactId> <version>3.2.0</version> <executions> <execution> <id>add-lib-sources</id> <phase>generate-sources</phase> <goals> <goal>add-source</goal> </goals> <configuration> <sources> <source>path/to/lib/folder</source> </sources> </configuration> </execution> </executions> </plugin> Tests \u00b6 Tests are usually executed with the Maven Surefire plugin using the test goal. Test configuration \u00b6 Tests are run using the Maven Surefire plugin . Usually, the default configuration is enough. However, if we want to customize the test execution, we can add the plugin to our pom and configure it: <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>3.0.0-M5</version> <configuration> ... configuration ... </configuration> </plugin> </plugins> ... other plugins ... </build> For example, to exclude some tests, the following configuration can be used: <configuration> <excludes> <exclude>**/IncompleteTest.java</exclude> </excludes> </configuration> Run a single tests file \u00b6 To run a single test file, use: mvn test -Dtest=\"<TEST CLASS NAME>\" The name should be just a class name without the package and without file extension: mvn test -Dtest=\"OSMFileTest\" We can also use a fully qualified name, if there are more test classes with the same name: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.OSMFileTest\" Run tests using a pattern \u00b6 More test can be run with a pattern, for example: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.**\" runs all tests within the input package. Execute Programs from Maven \u00b6 For executing programs, Maven has the exec plugin. This plugin has two goals: exec:java : for Java programs exec:exec : for any program However, the exec:java goal is not very flexible. It uses the same JVM as the calling Maven process, so the JVM cannot be configured in any way. Notably, with the exec:java goal, it is not possible to: pass JVM arguments like -Xmx set the library path using -Djava.library.path Note that when working with PowerShell, we typically encounter the problem with arguments that start with - and contain a dot. For guide how to solve this, refer to the PowerShell manual . exec:java \u00b6 Basic example: mvn exec:java -Dexec.mainClass=test.Main Other usefull arguments: - -Dexec.args=\"arg1 arg2 arg3\" exec:exec \u00b6 Basic example: mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"Xmx30g -classpath %classpath test.Main\" The -classpath %classpath argument is obligatory and it is used to pass the project classpath to the program. Note that here, the -Dexec.args parameter is used both for vm and program arguments. The order is: JVM arguments, classpath main class program arguments Example: mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"-Xmx30g -classpath %classpath test.Main arg1 arg2\" We can also use -Dexec.mainClass with exec:exec , but we need to refer it in the -classpath argument. The following three maven commands run the same Java program: mvn exec:java -Dexec.mainClass=test.Main mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"-classpath %classpath test.Main\" mvn exec:exec -Dexec.executable=\"java\" -Dexec.mainClass=test.Main -Dexec.args=\"-classpath %classpath ${exec.mainClass}\" Configure the exec plugin in the pom \u00b6 We can add a configuration of the exec plugin to the pom.xml , so we do not have to type the arguments or the main class every time we run the program. However, this way, we have to supply all program arguments in the pom.xml file. It is not possible to pass some parameters from the command line and some from the pom.xml file configuration. Note that when using the exec:ecec goal, this includes the JVM arguments as well. HTTPS certificates \u00b6 Sometimes, it can happen that maven cannot connect to a repository with this error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target This error signals that the server SSL certificate of the maven repo (when using HTTPS) is not present in the local SSL certificate keystore. This can have two reasons, to disintiguish between them, try to access the repo from your browser over https: If you can access the repo from your browser, it means that the server has a valid SSL certificate, but it is not in zour local keystore (just in the browser keystore). You can solve this problem by adding the certificate to your java SSL keystore (see below). if you cannot access the server from your browser, it is likely that the server does not have a valid SSL certificate, and you have to solve it on the serer side. Adding a new SSL certificate to your keystore \u00b6 Open the repo URL in your browser Export the certificate: Chrome: click on the padlock icon left to address in address bar, select Certificate -> Details -> Copy to File and save in format \"Der-encoded binary, single certificate\". Firefox: click on HTTPS certificate chain (the lock icon right next to URL address). Click more info -> security -> show certificate -> details -> export.. . Pickup the name and choose file type *.cer Determine the keystore location: 3.1 Using maven --version , find out the location of the java used by Maven 3.2 The keystore is saved in file: <JAVA LOCATION>/lib/security/cacerts Open console as administrator and add the certificate to the keystore using: keytool -import -keystore \"<PATH TO cacerts>\" -file \"PATH TO TH EXPORTED *.cer FILE\" You can check that the operation was sucessful by listing all certificates: keytool -keystore \"<PATH TO cacerts>\" -list Debugging maven \u00b6 First, try to look at the versions of related dependencies and plugins. Old versions of these can cause many problems. No tests found using the Dtest argument of the test goal \u00b6 Check the class name/path/pattern If the name works, but pattern does not, it can be caused by an old version of the surefire plugin that use a different patten syntax. uncompilable source code \u00b6 Try to clean and compile again Project Publishing \u00b6 This section s focused on Maven projects but most of the steps should apply to all Java projects. Steps: Cleanup Add License to all files Test Update Changelog Update pom of the project and related projects Install Locally Deploy Cleanup \u00b6 The following cleanup steps should be done: clean garbage/IDE files check that they are in gitignore remove already tracked files by git rm -r --cached <path> remove unused imports In Netbeans: Refactor -> Inspect and Transform -> browse -> imports -> organize imports Add license information: \u00b6 There is a nice program called licenseheaders that can be used for that. Unfortunatelly, the program is currently broken and a fork must be used . Syntax: licenseheaders -t <license type> -o \"<Owner>\" -cy -n <Software name> -u \"<link to github>\" A full path to a license template has to be given. Updating poms \u00b6 In general, it is desirable to update the version of each related project from SNAPSHOT to the release version and then test the whole setup. In complex setups, we need to: Change the version of the parent project Change the version of the project's dependencies Change the version of the project itself Update the versions of projects in parent pom Update the versions of projects in the dependency section of each related project (if this is not done by the parent pom) If we fullfill all these steps, we can deploy the project. Both SNAPSHOTs and releases can be deployed using the deploy goal. The repository is chosen based on the version of the artifact (SNAPSHOT or release). Deploying the project to Maven Central \u00b6 Deploying to Maven Central has many advantages: it is free, reliable, and every maven user can get our artifact without any additional configuration. However, it is also a bit complicated due to security requirements and high quality standards. The process is usually as follows: namespace (groupId) registration deploying of artifacts to the namespace Note that for one group id registration, we can deploy multiple artifacts, as long as the registered group id is equal to, or a prefix of, the group id of the artifact . The whole process is described in detail in the Central Repository Documentation . As of 2024-02-15, there is an ongoing migration from OSSRH to the new Central Portal. Because of that, we will describe the current (OSSRH) process only briefly, as it is likely to change in the future. Also, we will describe only the process for deploying artifacts, as the namespace registration in OSSRH is not possible anymore. To see your currently registered namespaces, go to the your profile at sonatype Jira portal and look insides the issues listed on right in the Activity Stream. Deploying artifacts to OSSRH \u00b6 Requirements: required metadata in the pom javadoc and sources must be supplied all files must be signed with GPG various plugins and configuration need to be set up in the pom user authentication for the OSSRH must be set up in the settings.xml file Required metadata \u00b6 Official documentation (at the end) The following metadata must be present in the pom: project tags: name description url license information: ```XML MIT License https://opensource.org/licenses/MIT developer information: XML <developers> <developer> <name>Full Name</name> <email>user email</email> <url>user url</url> </developer> </developers> scm information: XML <scm> <connection>scm:git:git://github.com/simpligility/ossrh-demo.git</connection> <developerConnection>scm:git:ssh://github.com:simpligility/ossrh-demo.git</developerConnection> <url>http://github.com/simpligility/ossrh-demo/tree/master</url> </scm> Javadoc and sources setup \u00b6 The following configuration is suitable for deploying with sources and javadoc: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-source-plugin</artifactId> <version>2.2.1</version> <executions> <execution> <id>attach-sources</id> <goals> <goal>jar-no-fork</goal> </goals> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-javadoc-plugin</artifactId> <version>2.9.1</version> <executions> <execution> <id>attach-javadocs</id> <goals> <goal>jar</goal> </goals> </execution> </executions> </plugin> Signing the artifacts with GPG \u00b6 To sign the artifacts with GPG, we need to add the gp plugin to the pom: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-gpg-plugin</artifactId> <version>1.5</version> <executions> <execution> <id>sign-artifacts</id> <phase>verify</phase> <goals> <goal>sign</goal> </goals> </execution> </executions> </plugin> For this plugin to work, the GPG must be installed and available in the system path. Storing the GPG passphrase \u00b6 To make our life easier, we can store the GPG passphrase in the settings.xml file. The following configuration must be present in the profiles section: <profile> <id>ossrh</id> <activation> <activeByDefault>true</activeByDefault> </activation> <properties> <gpg.executable>gpg</gpg.executable> <!-- this should not be needed as gpg is default... --> <gpg.passphrase>F1D06949</gpg.passphrase> </properties> </profile> Required plugins and configuration \u00b6 For the deployment to OSSRH, we need to use several plugins and add some configuration to the pom. First, we need to set up the distribution management: <distributionManagement> <snapshotRepository> <id>ossrh</id> <url><SNAPSHOT URL></url> </snapshotRepository> <repository> <id>ossrh</id> <url><RELEASE URL></url> </repository> </distributionManagement> The and are links to the repo we have received after the namespace registration approval. Note that we need to use the original URLs, even if they do not match the links in the documentation. The current (2024-02-15) URLs are: https://s01.oss.sonatype.org/content/repositories/snapshots https://s01.oss.sonatype.org/service/local/staging/deploy/maven2/ Next, we need a staging plugin to deploy the artifacts to the OSSRH staging repository. The plugin is configured as follows: <plugin> <groupId>org.sonatype.plugins</groupId> <artifactId>nexus-staging-maven-plugin</artifactId> <version><latest version here></version> <extensions>true</extensions> <configuration> <serverId>ossrh</serverId> <nexusUrl><REPO ROOT URL></nexusUrl> <autoReleaseAfterClose>true</autoReleaseAfterClose> </configuration> </plugin> Here, the <REPO ROOT URL> is the root URL of the repositories above, e.g., for recent versions, it is https://s01.oss.sonatype.org/ . Setting up user authentication for the OSSRH \u00b6 The following configuration must be present in the settings.xml file: <servers> <server> <id>ossrh</id> <username>your username</username> <password>your password</password> </server> </servers> Profiles \u00b6 Maven profiles can be used to supply different configurations in a single pom.xml file. This can be used for example to: support different build environments (e.g., development, testing, production) support different build targets (e.g., different Java versions) support different build configurations (e.g., optional dependencies) Profiles are defined in the profiles section of the pom.xml file. Each profile has a unique id and can contain any configuration that can be present in the pom.xml file. To manually activate a profile, we can use the -P argument of the mvn command: mvn <goal> -P <profile id> Note that the profile needs to be selected for all relevant goals, not just for the compilation . For example, if we have and optional dependency in the profile, we need to select the profile for compile , but also for exec , install , etc, otherwise the dependency will not be found at runtime. Displaying active profiles \u00b6 To display the active profiles, use the following command: mvn help:active-profiles Note that again, this will only show the profiles that are activated in the settings.xml file or the profiles that are activated by default. To test an optional profile, we need to activate it even for the help:active-profiles goal: mvn help:active-profiles -P <profile id> Creating maven plugins \u00b6 Official guide Maven plugins can be created as maven projects. Specifics pom configuration: The packaging is maven-plugin The name of the plugin is <artifactId>-maven-plugin among the dependencies, there should be maven-plugin-api and maven-plugin-annotations To use a cclass method as an entry point for a plugin goal, we annotate it with @Mojo(name = \"<goal name>\") . Testing a maven plugin \u00b6 The best way to test the plugin is to run it separately, even if it should be later bound to a lifecycle phase. To run the plugin, we use the following syntax: mvn <group id>:<artifact id>:<goal name> # Example: mvn com.test:example-maven-plugin:example This is indeed very verbose. To shorten it, we can add a special configuration to the system settings.xml file: <pluginGroups> <pluginGroup>com.test</pluginGroup> </pluginGroups> Now, we can run the plugin using: mvn example:example Debugging a maven plugin in IntelliJ IDEA \u00b6 Maven plugins can be easily debugged in IntelliJ IDEA. However, it is important to understand that unlike maven plugins are run from the local repository, not from the project source code. Therefore, we need to install the plugin after each change . Make the plugin runnable from any directory \u00b6 Normally, the goals can be run only from project directories (where the pom.xml is present). To make the plugin runnable from any directory, we need to annotate the goal with @requiresProject = false : @Mojo(name = \"example\", requiresProject = false) Various useful tasks \u00b6 Displaying the classpath \u00b6 To display the classpath, use the following command: mvn dependency:build-classpath","title":"Maven"},{"location":"Programming/Java/Maven/#maven-packages-artifacts","text":"All Maven dependencies and plugins installed in local system repositories can be either: downloaded from a remote repository installed from a local file or project Note that if we want the artifact to be downloaded from a remote repository, we do not have to take any action. It will be downloaded automatically when needed. By default, Maven searches for artifacts in the Maven Central repository . If we want to use a different repository, we have to add it manually.","title":"Maven Packages: Artifacts"},{"location":"Programming/Java/Maven/#list-locally-installed-artifacts","text":"There is no built-in command to list all locally installed artifacts. However, we can use the info plugin : mvn ninja.fido:info-maven-plugin:list","title":"List locally installed artifacts"},{"location":"Programming/Java/Maven/#install-artifacts-from-a-remote-repository-manually","text":"If we want to install an artifact from a remote repository manually, we can use the dependency:get goal. The syntax is as follows: mvn dependency:get -Dartifact=<GROUP ID>:<ARTIFACT ID>:<VERSION>","title":"Install artifacts from a remote repository manually"},{"location":"Programming/Java/Maven/#debugging-missing-artifacts","text":"All Maven dependencies should work out of the box. If some dependencies cannot be resolved: check that the dependencies are on the maven central. if not, check that they are in some special repo and check that the repo is present in the pom of the project that requires the dependency","title":"Debugging missing Artifacts"},{"location":"Programming/Java/Maven/#variables","text":"Variables in the pom.xml file can be used using the ${variable} syntax. To use some environment variables, we can use the ${env.variable} . Note that contrary to all expectations, maven goals do not fail if a variable is not defined . Instead, the variable is replaced with an empty string. This can lead to unexpected behavior, so it is usually preferable to fail. For that, we can use the enforcer plugin with the requireProperty rule: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-enforcer-plugin</artifactId> <version>3.4.1</version> <executions> <execution> <id>enforce-property</id> <goals> <goal>enforce</goal> </goals> <configuration> <rules> <requireProperty> <property>basedir</property> <message>You must set a basedir property!</message> </requireProperty> </rules> <fail>true</fail> </configuration> </execution> </executions> </plugin>","title":"Variables"},{"location":"Programming/Java/Maven/#compilation","text":"reference Compilation is handeled by the Maven compiler plugin. Usually, typing mvn compile is enough to compile the project. If you need any customization, add it to the compiler plugin configuration <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> .... </configuration> </plugin>","title":"Compilation"},{"location":"Programming/Java/Maven/#passing-arguments-to-javac","text":"A single argument can be passed using the <compilerArgument> property. For more compiler arguments, use <compilerArgs> : <compilerArgs> <arg>-Xmaxerrs=1000</arg> <arg>-Xlint</arg> </compilerArgs>","title":"Passing arguments to javac"},{"location":"Programming/Java/Maven/#dependencies","text":"Maven dependencies are defined in the pom.xml file, in the <dependencies> section. Each dependency has the following structure: <dependency> <groupId>org.apache.maven</groupId> <artifactId>maven-core</artifactId> <version>3.0</version> <scope>compile</scope> </dependency> The scope property is optional and the default value is compile . The dependencies are automatically downloaded if we run a maven goal that requires them (e.g., compile , test , install ). If we want to download them manually, we can use the dependency:resolve goal. To list all dependencies of a project, we can use the dependency:list goal.","title":"Dependencies"},{"location":"Programming/Java/Maven/#using-dependencies-distributed-as-a-jar","text":"Sometimes, we possess a jar file that is not present in any maven repository. We can still install it to the local repository as a new artifact using the install:install-file goal: mvn install:install-file -Dfile=<PATH TO JAR> -DgroupId=<GROUP ID> -DartifactId=<ARTEIFACT ID> -Dversion=<VERSION> -Dpackaging=jar Here the <PATH TO JAR> is a path to the jar library. The <GROUP ID> , <ARTEIFACT ID> , and <VERSION> can have arbitrary values, they just have to correspond with the values specified in the pom dependency. Be careful with PowerShell: in PowerShell, program parameters starting with minus and containing dot need to be quoted, for example: '-DgroupId=com.example.app' instead of -DgroupId=com.example.app","title":"Using dependencies distributed as a jar"},{"location":"Programming/Java/Maven/#using-dependencies-distributed-as-a-zip","text":"Sometimes, the dependency is distributed as a zip file containing sources. Typically, the manual for such a library tells us to modify the classpath to include the sources. However, Maven handles the classpath automatically, so it is not wise to modify it manually. The best way to use it is to unpack the zip and add the sources to the project during compilation. For that, we can use the build-helper-maven-plugin plugin: <plugin> <groupId>org.codehaus.mojo</groupId> <artifactId>build-helper-maven-plugin</artifactId> <version>3.2.0</version> <executions> <execution> <id>add-lib-sources</id> <phase>generate-sources</phase> <goals> <goal>add-source</goal> </goals> <configuration> <sources> <source>path/to/lib/folder</source> </sources> </configuration> </execution> </executions> </plugin>","title":"Using dependencies distributed as a zip"},{"location":"Programming/Java/Maven/#tests","text":"Tests are usually executed with the Maven Surefire plugin using the test goal.","title":"Tests"},{"location":"Programming/Java/Maven/#test-configuration","text":"Tests are run using the Maven Surefire plugin . Usually, the default configuration is enough. However, if we want to customize the test execution, we can add the plugin to our pom and configure it: <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-surefire-plugin</artifactId> <version>3.0.0-M5</version> <configuration> ... configuration ... </configuration> </plugin> </plugins> ... other plugins ... </build> For example, to exclude some tests, the following configuration can be used: <configuration> <excludes> <exclude>**/IncompleteTest.java</exclude> </excludes> </configuration>","title":"Test configuration"},{"location":"Programming/Java/Maven/#run-a-single-tests-file","text":"To run a single test file, use: mvn test -Dtest=\"<TEST CLASS NAME>\" The name should be just a class name without the package and without file extension: mvn test -Dtest=\"OSMFileTest\" We can also use a fully qualified name, if there are more test classes with the same name: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.OSMFileTest\"","title":"Run a single tests file"},{"location":"Programming/Java/Maven/#run-tests-using-a-pattern","text":"More test can be run with a pattern, for example: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.**\" runs all tests within the input package.","title":"Run tests using a pattern"},{"location":"Programming/Java/Maven/#execute-programs-from-maven","text":"For executing programs, Maven has the exec plugin. This plugin has two goals: exec:java : for Java programs exec:exec : for any program However, the exec:java goal is not very flexible. It uses the same JVM as the calling Maven process, so the JVM cannot be configured in any way. Notably, with the exec:java goal, it is not possible to: pass JVM arguments like -Xmx set the library path using -Djava.library.path Note that when working with PowerShell, we typically encounter the problem with arguments that start with - and contain a dot. For guide how to solve this, refer to the PowerShell manual .","title":"Execute Programs from Maven"},{"location":"Programming/Java/Maven/#execjava","text":"Basic example: mvn exec:java -Dexec.mainClass=test.Main Other usefull arguments: - -Dexec.args=\"arg1 arg2 arg3\"","title":"exec:java"},{"location":"Programming/Java/Maven/#execexec","text":"Basic example: mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"Xmx30g -classpath %classpath test.Main\" The -classpath %classpath argument is obligatory and it is used to pass the project classpath to the program. Note that here, the -Dexec.args parameter is used both for vm and program arguments. The order is: JVM arguments, classpath main class program arguments Example: mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"-Xmx30g -classpath %classpath test.Main arg1 arg2\" We can also use -Dexec.mainClass with exec:exec , but we need to refer it in the -classpath argument. The following three maven commands run the same Java program: mvn exec:java -Dexec.mainClass=test.Main mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"-classpath %classpath test.Main\" mvn exec:exec -Dexec.executable=\"java\" -Dexec.mainClass=test.Main -Dexec.args=\"-classpath %classpath ${exec.mainClass}\"","title":"exec:exec"},{"location":"Programming/Java/Maven/#configure-the-exec-plugin-in-the-pom","text":"We can add a configuration of the exec plugin to the pom.xml , so we do not have to type the arguments or the main class every time we run the program. However, this way, we have to supply all program arguments in the pom.xml file. It is not possible to pass some parameters from the command line and some from the pom.xml file configuration. Note that when using the exec:ecec goal, this includes the JVM arguments as well.","title":"Configure the exec plugin in the pom"},{"location":"Programming/Java/Maven/#https-certificates","text":"Sometimes, it can happen that maven cannot connect to a repository with this error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target This error signals that the server SSL certificate of the maven repo (when using HTTPS) is not present in the local SSL certificate keystore. This can have two reasons, to disintiguish between them, try to access the repo from your browser over https: If you can access the repo from your browser, it means that the server has a valid SSL certificate, but it is not in zour local keystore (just in the browser keystore). You can solve this problem by adding the certificate to your java SSL keystore (see below). if you cannot access the server from your browser, it is likely that the server does not have a valid SSL certificate, and you have to solve it on the serer side.","title":"HTTPS certificates"},{"location":"Programming/Java/Maven/#adding-a-new-ssl-certificate-to-your-keystore","text":"Open the repo URL in your browser Export the certificate: Chrome: click on the padlock icon left to address in address bar, select Certificate -> Details -> Copy to File and save in format \"Der-encoded binary, single certificate\". Firefox: click on HTTPS certificate chain (the lock icon right next to URL address). Click more info -> security -> show certificate -> details -> export.. . Pickup the name and choose file type *.cer Determine the keystore location: 3.1 Using maven --version , find out the location of the java used by Maven 3.2 The keystore is saved in file: <JAVA LOCATION>/lib/security/cacerts Open console as administrator and add the certificate to the keystore using: keytool -import -keystore \"<PATH TO cacerts>\" -file \"PATH TO TH EXPORTED *.cer FILE\" You can check that the operation was sucessful by listing all certificates: keytool -keystore \"<PATH TO cacerts>\" -list","title":"Adding a new SSL certificate to your keystore"},{"location":"Programming/Java/Maven/#debugging-maven","text":"First, try to look at the versions of related dependencies and plugins. Old versions of these can cause many problems.","title":"Debugging maven"},{"location":"Programming/Java/Maven/#no-tests-found-using-the-dtest-argument-of-the-test-goal","text":"Check the class name/path/pattern If the name works, but pattern does not, it can be caused by an old version of the surefire plugin that use a different patten syntax.","title":"No tests found using the Dtest argument of the test goal"},{"location":"Programming/Java/Maven/#uncompilable-source-code","text":"Try to clean and compile again","title":"uncompilable  source code"},{"location":"Programming/Java/Maven/#project-publishing","text":"This section s focused on Maven projects but most of the steps should apply to all Java projects. Steps: Cleanup Add License to all files Test Update Changelog Update pom of the project and related projects Install Locally Deploy","title":"Project Publishing"},{"location":"Programming/Java/Maven/#cleanup","text":"The following cleanup steps should be done: clean garbage/IDE files check that they are in gitignore remove already tracked files by git rm -r --cached <path> remove unused imports In Netbeans: Refactor -> Inspect and Transform -> browse -> imports -> organize imports","title":"Cleanup"},{"location":"Programming/Java/Maven/#add-license-information","text":"There is a nice program called licenseheaders that can be used for that. Unfortunatelly, the program is currently broken and a fork must be used . Syntax: licenseheaders -t <license type> -o \"<Owner>\" -cy -n <Software name> -u \"<link to github>\" A full path to a license template has to be given.","title":"Add license information:"},{"location":"Programming/Java/Maven/#updating-poms","text":"In general, it is desirable to update the version of each related project from SNAPSHOT to the release version and then test the whole setup. In complex setups, we need to: Change the version of the parent project Change the version of the project's dependencies Change the version of the project itself Update the versions of projects in parent pom Update the versions of projects in the dependency section of each related project (if this is not done by the parent pom) If we fullfill all these steps, we can deploy the project. Both SNAPSHOTs and releases can be deployed using the deploy goal. The repository is chosen based on the version of the artifact (SNAPSHOT or release).","title":"Updating poms"},{"location":"Programming/Java/Maven/#deploying-the-project-to-maven-central","text":"Deploying to Maven Central has many advantages: it is free, reliable, and every maven user can get our artifact without any additional configuration. However, it is also a bit complicated due to security requirements and high quality standards. The process is usually as follows: namespace (groupId) registration deploying of artifacts to the namespace Note that for one group id registration, we can deploy multiple artifacts, as long as the registered group id is equal to, or a prefix of, the group id of the artifact . The whole process is described in detail in the Central Repository Documentation . As of 2024-02-15, there is an ongoing migration from OSSRH to the new Central Portal. Because of that, we will describe the current (OSSRH) process only briefly, as it is likely to change in the future. Also, we will describe only the process for deploying artifacts, as the namespace registration in OSSRH is not possible anymore. To see your currently registered namespaces, go to the your profile at sonatype Jira portal and look insides the issues listed on right in the Activity Stream.","title":"Deploying the project to Maven Central"},{"location":"Programming/Java/Maven/#deploying-artifacts-to-ossrh","text":"Requirements: required metadata in the pom javadoc and sources must be supplied all files must be signed with GPG various plugins and configuration need to be set up in the pom user authentication for the OSSRH must be set up in the settings.xml file","title":"Deploying artifacts to OSSRH"},{"location":"Programming/Java/Maven/#required-metadata","text":"Official documentation (at the end) The following metadata must be present in the pom: project tags: name description url license information: ```XML MIT License https://opensource.org/licenses/MIT developer information: XML <developers> <developer> <name>Full Name</name> <email>user email</email> <url>user url</url> </developer> </developers> scm information: XML <scm> <connection>scm:git:git://github.com/simpligility/ossrh-demo.git</connection> <developerConnection>scm:git:ssh://github.com:simpligility/ossrh-demo.git</developerConnection> <url>http://github.com/simpligility/ossrh-demo/tree/master</url> </scm>","title":"Required metadata"},{"location":"Programming/Java/Maven/#javadoc-and-sources-setup","text":"The following configuration is suitable for deploying with sources and javadoc: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-source-plugin</artifactId> <version>2.2.1</version> <executions> <execution> <id>attach-sources</id> <goals> <goal>jar-no-fork</goal> </goals> </execution> </executions> </plugin> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-javadoc-plugin</artifactId> <version>2.9.1</version> <executions> <execution> <id>attach-javadocs</id> <goals> <goal>jar</goal> </goals> </execution> </executions> </plugin>","title":"Javadoc and sources setup"},{"location":"Programming/Java/Maven/#signing-the-artifacts-with-gpg","text":"To sign the artifacts with GPG, we need to add the gp plugin to the pom: <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-gpg-plugin</artifactId> <version>1.5</version> <executions> <execution> <id>sign-artifacts</id> <phase>verify</phase> <goals> <goal>sign</goal> </goals> </execution> </executions> </plugin> For this plugin to work, the GPG must be installed and available in the system path.","title":"Signing the artifacts with GPG"},{"location":"Programming/Java/Maven/#storing-the-gpg-passphrase","text":"To make our life easier, we can store the GPG passphrase in the settings.xml file. The following configuration must be present in the profiles section: <profile> <id>ossrh</id> <activation> <activeByDefault>true</activeByDefault> </activation> <properties> <gpg.executable>gpg</gpg.executable> <!-- this should not be needed as gpg is default... --> <gpg.passphrase>F1D06949</gpg.passphrase> </properties> </profile>","title":"Storing the GPG passphrase"},{"location":"Programming/Java/Maven/#required-plugins-and-configuration","text":"For the deployment to OSSRH, we need to use several plugins and add some configuration to the pom. First, we need to set up the distribution management: <distributionManagement> <snapshotRepository> <id>ossrh</id> <url><SNAPSHOT URL></url> </snapshotRepository> <repository> <id>ossrh</id> <url><RELEASE URL></url> </repository> </distributionManagement> The and are links to the repo we have received after the namespace registration approval. Note that we need to use the original URLs, even if they do not match the links in the documentation. The current (2024-02-15) URLs are: https://s01.oss.sonatype.org/content/repositories/snapshots https://s01.oss.sonatype.org/service/local/staging/deploy/maven2/ Next, we need a staging plugin to deploy the artifacts to the OSSRH staging repository. The plugin is configured as follows: <plugin> <groupId>org.sonatype.plugins</groupId> <artifactId>nexus-staging-maven-plugin</artifactId> <version><latest version here></version> <extensions>true</extensions> <configuration> <serverId>ossrh</serverId> <nexusUrl><REPO ROOT URL></nexusUrl> <autoReleaseAfterClose>true</autoReleaseAfterClose> </configuration> </plugin> Here, the <REPO ROOT URL> is the root URL of the repositories above, e.g., for recent versions, it is https://s01.oss.sonatype.org/ .","title":"Required plugins and configuration"},{"location":"Programming/Java/Maven/#setting-up-user-authentication-for-the-ossrh","text":"The following configuration must be present in the settings.xml file: <servers> <server> <id>ossrh</id> <username>your username</username> <password>your password</password> </server> </servers>","title":"Setting up user authentication for the OSSRH"},{"location":"Programming/Java/Maven/#profiles","text":"Maven profiles can be used to supply different configurations in a single pom.xml file. This can be used for example to: support different build environments (e.g., development, testing, production) support different build targets (e.g., different Java versions) support different build configurations (e.g., optional dependencies) Profiles are defined in the profiles section of the pom.xml file. Each profile has a unique id and can contain any configuration that can be present in the pom.xml file. To manually activate a profile, we can use the -P argument of the mvn command: mvn <goal> -P <profile id> Note that the profile needs to be selected for all relevant goals, not just for the compilation . For example, if we have and optional dependency in the profile, we need to select the profile for compile , but also for exec , install , etc, otherwise the dependency will not be found at runtime.","title":"Profiles"},{"location":"Programming/Java/Maven/#displaying-active-profiles","text":"To display the active profiles, use the following command: mvn help:active-profiles Note that again, this will only show the profiles that are activated in the settings.xml file or the profiles that are activated by default. To test an optional profile, we need to activate it even for the help:active-profiles goal: mvn help:active-profiles -P <profile id>","title":"Displaying active profiles"},{"location":"Programming/Java/Maven/#creating-maven-plugins","text":"Official guide Maven plugins can be created as maven projects. Specifics pom configuration: The packaging is maven-plugin The name of the plugin is <artifactId>-maven-plugin among the dependencies, there should be maven-plugin-api and maven-plugin-annotations To use a cclass method as an entry point for a plugin goal, we annotate it with @Mojo(name = \"<goal name>\") .","title":"Creating maven plugins"},{"location":"Programming/Java/Maven/#testing-a-maven-plugin","text":"The best way to test the plugin is to run it separately, even if it should be later bound to a lifecycle phase. To run the plugin, we use the following syntax: mvn <group id>:<artifact id>:<goal name> # Example: mvn com.test:example-maven-plugin:example This is indeed very verbose. To shorten it, we can add a special configuration to the system settings.xml file: <pluginGroups> <pluginGroup>com.test</pluginGroup> </pluginGroups> Now, we can run the plugin using: mvn example:example","title":"Testing a maven plugin"},{"location":"Programming/Java/Maven/#debugging-a-maven-plugin-in-intellij-idea","text":"Maven plugins can be easily debugged in IntelliJ IDEA. However, it is important to understand that unlike maven plugins are run from the local repository, not from the project source code. Therefore, we need to install the plugin after each change .","title":"Debugging a maven plugin in IntelliJ IDEA"},{"location":"Programming/Java/Maven/#make-the-plugin-runnable-from-any-directory","text":"Normally, the goals can be run only from project directories (where the pom.xml is present). To make the plugin runnable from any directory, we need to annotate the goal with @requiresProject = false : @Mojo(name = \"example\", requiresProject = false)","title":"Make the plugin runnable from any directory"},{"location":"Programming/Java/Maven/#various-useful-tasks","text":"","title":"Various useful tasks"},{"location":"Programming/Java/Maven/#displaying-the-classpath","text":"To display the classpath, use the following command: mvn dependency:build-classpath","title":"Displaying the classpath"},{"location":"Programming/Python/Matplotlib%20Manual/","text":"","title":"Matplotlib Manual"},{"location":"Programming/Python/Pandas%20Manual/","text":"Main principles \u00b6 Pandas extensively uses the term axis. In Pandas, axis 0 is vertical (rows) and axis 1 is horizontal (columns). Creating a DataFrame \u00b6 The DataFrame class has a constructor that supports multiple formats of input data as well as many configuration parameters. Therefore , for most formats of input data, we can create a dataframe using the constructor. However, we can also crete a dataframe using the from_* functions, and for some formats, these functions are the only way to create a dataframe. From a dictionary \u00b6 When having a dictionary, we can choose between two options the constructor and the from_dict function. The required syntax depend on the shape of the dictionary with respect to the required dataframe. Keys are column names, values are list of column values \u00b6 df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) # or equivalently df.DataFrame.from_dict({'col1': [1, 2], 'col2': [3, 4]}) Note that the values of the dictionary have to be lists. If we have a dictionary with values that are not lists (i.e., only one row), we have to use the orient parameter to specify the orientation of the data and then transpose the dataframe: d = {'col1': 1, 'col2': 2} df = pd.DataFrame.from_dict(d, orient='index').T # or equivalently df = pd.DataFrame([d], columns=d.keys()) Keys are indices, values are values of a single column \u00b6 df = pd.DataFrame.from_dict({'row1': 1, 'row2': 2}, orient='index', columns=['Values']) Keys are indices, values are values of single row \u00b6 df = pd.DataFrame.from_dict({'row1': [1, 2], 'row2': [3, 4]}, orient='index') Keys are one column, values are another column \u00b6 d = {'row1 col1': 'row1 col2', 'row2 col1': 'row2 col2' df = pd.DataFrame.from_dict(d.items()) # or equivalently df = pd.DataFrame({'col1': d.keys(), 'col2': d.values()}) From a list of dictionaries \u00b6 df = pd.DataFrame([{'col1': 1, 'col2': 3}, {'col1': 2, 'col2': 4}]) From a list of lists \u00b6 df = pd.DataFrame([[1, 3], [2, 4]], columns=['col1', 'col2']) Creating a zero or constant-filled dataframe \u00b6 To create a dataframe filled with a constant, we can use the dataframe constructor and pass the constant as the first (data) argument: df = pd.DataFrame(0, index=range(10), columns=['col1', 'col2']) Generating the index \u00b6 As displayed in the above example, we can generate a numerical index using the range function. However, there are more options: date index with date_range pd.date_range(<start date>, <end date>, freq=<frequency>) Obtaining info about dataset \u00b6 For a DataFrame df : column names: df.columns column types: df.dtypes number of rows: len(df) Iteration \u00b6 Standard Iteration \u00b6 https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas Iteration without modifying the dataframe. From the fastest to the slowest. Vector operations \u00b6 List Comprehensions \u00b6 Apply \u00b6 The apply function can be used to apply a function to each row or column of the dataframe. For iterating over rows, we need to set the axis parameter to 1. Example: df['new_col'] = df.apply(lambda row: row['col1'] + row['col2'], axis=1) itertuples() \u00b6 https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.html Returns dataframe rows as pandas named tuples with index as the first member of the tuple. iterrows() \u00b6 https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html returns a tuple (index, data) it does not preserve the dtype items() \u00b6 https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.items.html Iterates over columns Iteration with modification \u00b6 For modification, the best strategy is to: select what we want to modify (see selection ) modify the selection with the assignment operator. The right side of the assignment operator can be the result of an iteration. Filtration \u00b6 filtered = df[df['max_delay'] == x] or equivalently: filtered = df[df.max_delay == x] Filtration by Multiple Columns \u00b6 Example: filtered = df[(df['max_delay'] == x) & (df['exp_length'] == y)] Accept multiple values \u00b6 For that, we can use the isin function: filtered = df[df['max_delay'].isin([x, y])] Using the query function \u00b6 The query function can be used for more complicated filters. It is more flexible and the syntax is less verbose. The above filter can be rewriten as: filtered = df.query('max_delay == x and exp_length == y'] Filtering Series \u00b6 A seris can be filtered even simpler then the dataframe: s = df['col'] sf = s[s <= 10] # now we have a Series with values from df['col'] less than 10 Useful filter functions \u00b6 null values: <column selection>.isnull() non null/nan values: <column selection>.notnull() filtring using the string value: <column selection>.str.<string function> filtering dates: <column selection>.dt.<date function> Selection \u00b6 If we want to select a part of the dataframe (a set of rows and columns) independently of the values of the dataframe (for that, see filtration ), we can use these methods: loc : select by index, works for both rows and columns iloc : select by position, works for both rows and columns [] : select by index, works only for columns There are also other methods that works for selection but does not work for setting values, such as: xs : select by label, works for both rows and columns The return type of the selection is determined by the number of selected rows and columns. For a single row or column, the result is a series, for multiple rows and columns, the result is a dataframe. If we want to get a dataframe for a single row or column, we can use the [] operator with a list of values: df[['col1']] # or df.loc[['row1']] # or df.iloc[[0]] loc \u00b6 The operator loc has many possible input parameters, the most common syntax is df.loc[<row selection>, <column selection>] each selection has the form of <start label>:<end label> . For the whole column, we therefore use: df.loc[:, <column name>] Difference between array operator on dataframe and on loc \u00b6 Both methods can be used both for getting and setting the column: a = df['col'] # or equivalently a = df.loc[:, 'col'] df2['col'] = a # or equivalently df2.loc[:, 'col'] = a The difference between these two methods is apparent when we want to use a chained selection, i.e., selecting from a selection. While the loc selects the appropriate columns in one step, so we know that we still refer to the original dataframe, the array operator operations are separate, and therefore, the result value can refer to a temporary: dfmi.loc[:, ('one', 'second')] = value # we set a value of a part of dfmi dfmi['one']['second'] = value # can be dangerous, we can set value to a temporary This problem is indicated by a SettingWithCopy warning. Sometimes it is not obvious that we use a chain of array operator selections, e.g.: sel = df[['a', 'b']] ..... sel['a'] = ... # we possibly edit a temporary! For more, see the dovumentation . iloc \u00b6 The iloc method works similarly to the loc method, but it uses the position instead of the label. To select more values, we can use the slice syntax df.iloc[<start position>:<end position>:<step>,<collumn slicing...>] Be aware that if the iloc operator selects by single value (e.g.: df.iloc[3] ), it returns the single row as series . To get a dataframe slice, we need to use a list of values (e.g.: df.iloc[[3]] ). Selecting all columns but one \u00b6 If we do not mind copying the dataframe, we can use the drop function. Otherwise, we can use the loc method and supply the filtered column lables obtained using the columns property: df.loc[:, df.columns != '<column to skip>'] Multi-index selection \u00b6 documentation When selecting from a dataframe with a multi-index, things get a bit more complicated. There are three ways how to select from a multi-index dataframe: using loc with slices: simple, but verbose using loc with IndexSlice object: more readable, but requires the IndexSlice object to be created first using xs function, neat, but does not support all the features, e.g., it does not support ranges Using loc \u00b6 The general loc usage is the same as for a single index dataframe: df.loc[<row selection>, <column selection>] However, each selection is now a tuple, where each element of the tuple corresponds to one level of the multi-index: df.loc[(<row selection level 1>, <row selection level 2>, ...), (<column selection level 1>, <column selection level 2>, ...)] The <row selection> can be a specifica value, a list of values, or a slice. Note that we have to use the slice function, as pandas uses the standard slice syntax for something else. We can skip lower levels to select all values from those levels. However, we cannot skip upper levels. If we want to select all values from the upper level, we need to use the slice(None) for that level: df.loc[(slice(None), slice(15, 30)), ...] Note that for multi-index slicing, the index needs to be sorted. If it is not, we can use the sort_index function. pandas slicing documentation Using IndexSlice for more readable syntax \u00b6 We can obtain the same result with a more readable syntax using the IndexSlice object: idx = pd.IndexSlice dft.loc[idx[:, 15:30], ...] Handeling the too many indexers error \u00b6 Sometimes, when using the loc method, the selection can fail with the too many indexers error, because it is ambiguous whether we select by rows or by columns. In that case, we can either use the axis parameter to specify the axis to select from: python df.loc(axis=0)[<row selection>] or use the IndexSlice instead. Using xs \u00b6 The xs function can be used to select from a multi-index dataframe. However, slices (ranges) are not supported. Example: df.xs(15, level=1) # selects all rows with level 1 equal to 15 Select row with a maximum value in a column \u00b6 To get the index of the row with the maximum value in a column, we can use the idxmax function: df['col'].idxmax() Then we can use the loc method to get the row. Selecting a single value (cell, scalar) \u00b6 When we select a single value from a dataframe, the result is sometimes a series, especially when we use a filtration. To get a scalar, we can use the item() method: df.loc[<row>, <column>].item() Sorting \u00b6 for sorting the dataframe, we can use the sort_values function. The first argument is the list of columns to sort by, starting with the most important column. Example: df.sort_values(['col1', 'col2']) If we want to use a custom sorting function, we can use the key argument. The key function should satisfy the classical python sorting interface (see Python manual ) and it should be a vector function, i.e., instead of returning a single position for a given value, it should return a vector of positions for a given vector of values. Example: def key_fn(column: list): return [len(x) for x in l] df.sort_values('col', key=key_fn) Working with columns \u00b6 Adding a column \u00b6 The preferable way is to use the assign function: # adds a column named 'instance_length' with constant value result_df_5_nyc_mv.assign(instance_length = 5) Multiple columns can be added at once: trips = trips.assign(dropoff_datetime = 0, dropoff_GPS_lon = 0, dropoff_GPS_lat = 0, pickup_GPS_lon = 0, pickup_GPS_lat = 0) Rename a column \u00b6 To rename a column, we can use the pandas rename function: df.rename(columns={'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}) # or equivalently df.rename({'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}, axis='columns') Rename a Series (column) \u00b6 The column name in the series object is actually the name of the series. To rename the series, we can use the rename function, or we can set the name property of the series: s.rename('<new name>') # or equivalently s.name = '<new name>' Working with the index \u00b6 Index of a dataframe df can be accessed by df.index . Standard range operation can be applied to index. Selecting just a single index level from a multi-index \u00b6 If we want to select just a single index level, we can use the get_level_values function: df.index.get_level_values(<level>) Note however, that this function returns duplicated values when there are multiple values in other levels. To get unique values, we can use the unique function. There is also another method, that returns unique values: the level property: df.index.levels[<level>] However, this way, we can get outdated values , as the values are not always updated when the index is changed. To get the updated values, we need to call the method remove_unused_levels after each change of the index. Changing the index \u00b6 Using columns as a new index \u00b6 For that, we can use the set_index function. Using an existing index to create a new index \u00b6 For that, we can use the reindex function. The first parameter is the new index. Example: df.reindex(df.index + 1) # creates a new index by adding 1 to the old index Important parameters: fill_value : the value to use for missing values. By default, the missing values are filled with NaN . Creating index from scratch \u00b6 To create an index from scratch, we just assign the index to the dataframe index property: df.index = pd.Index([1, 2, 3, 4, 5]) We can also assign a range directly to the index: df.index = range(5) To create more complicated indices, dedicated functions can be used: MultiIndex.from_product : creates a multi-index from the cartesian product of the given iterables Renaming the index \u00b6 The Index.rename function can be used for that. Aggregation \u00b6 Analogously to SQL, pandas has a groupby function for aggreagting rows. The usage is as follows: group = df.groupby(<columns>) # returns a groupby object grouped by the columns sel = group[<columns>] # we can select only some columns from the groupby object agg = sel.<aggregation function> # we apply an aggregation function to the selected columns We can skip the sel step and apply the aggregation function directly to the groupby object. This way, the aggregation function is applied to all columns. Example (sum): df.groupby('col').sum() Sums the results for each group (column by column) To get a count, we can call the size function: df.groupby('col').size() Note that unlike in SQL, the aggregation function does not have to return a single value. It can return a series or a dataframe. In that case, the result is a dataframe with the columns corresponding to the returned series/dataframe. In other words, the aggregation does not have to actually aggregate the data, it can also transform it . In the groupby object, the columns used for grouping are omitted if each group is aggregated to exactly one row. To keep them, we can use the group_keys parameter of the groupby function. Aggregate functions \u00b6 For the aggregate function, we can use one of the prepared aggregation functions. Classical functions(single value per group): sum mean median min max count Transformation functions (value for each row): cumsum : cumulative sum diff : difference between the current and the previous row. the periods parameter specifies which row to use for the difference. By default, it is the previous row (periods=1). For next row, use periods=-1, but note that the result is then negative. We can use the abs function to get the absolute value. Custom aggegate function \u00b6 Also, there are more general aggregate functions: agg function that is usefull for applying different functions for different columns and apply : the most flexible function that can be used for custom aggregation and transformation operations. These two functions have different interfaces for the custom aggregation functions they call. These are summarized in the following table: property agg apply can just transform the data no yes can use data from one column in another column no yes applied to each specified column the whole dataframe representing single group output dataframe scalar, series, or dataframe can use multiple aggregate functions yes no agg \u00b6 Example: df.groupby('col').agg({'col1': 'sum', 'col2': 'mean'}) apply \u00b6 The apply function takes a custom function as an argument. That custom aggregation function: takes a DataFrame/Series (depending on the source object) as the first argument this dataframe/series contains the data for the group (all columns) returns a Series, DataFrame, or a scalar when a scalar is returned, the result is a series with the scalar value for each group we do not have to reduce the data to a single value or a single row, we can just transform the data arbitrarily. The process works as follows: The dataframe is split into groups according to the groupby function. The custom function is applied to each group. The results are combined into a single dataframe. In other words, the custom function only sees the dataframe/series representing the group, not the whole dataframe/series. The grouping and compining aggreate results is done by the apply function. Time aggregation \u00b6 We can also aggregate by time. For that, we need an index or column with datetime values. Then, we can use the resample function. Example: df = pd.DataFrame({'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, index=pd.date_range('2021-01-01', periods=10, freq='1D')) df.resample('1H').sum() The aggregate function is applied to each group in case of multiple values in the same time slot ( downsampling ). In case of no values in the time slot ( upsampling ), the value is filled with NaN . We can use the ffill function to fill in the missing values. Example: df.resample('1H').sum().ffill() Joins \u00b6 Similarly to SQL, Pandas has a way to join two dataframes. There are two functions for that: merge : the most general function that has the behavior known from SQL join : a more specialized function, The following table lists the most important differences between the two functions: property merge join default join type inner left join right table via column (default) or index ( right_index=True ) index join left table via column (default) or index ( left_index=True ) index, or column ( on=key_or_keys ) There is also a static pd.merge function. All merge and join methods are just wrappers around this function. The indexes are lost after the join (if not used for the join). To keep an index, we can store it as a column before the join. Appending and Concatenating data \u00b6 In pandas, there is a concat function that can be used to concatenate data: pd.concat([df1, df2]) It can concatenate dataframes or series and it can concatenate vertically (by rows, default) or horizontally (by columns) By default, the indices from both input parameters are preserved. To reset the index, we can use the ignore_index parameter. Alternatively, to preserve one of the indices, we can set the index of the other dataframe to the index of the first dataframe before the concatenation using the set_index function. Pandas Data Types \u00b6 Object \u00b6 If pandas does not recognize the type of the column, or there are multiple types in the column, it uses the object type. However this may sound like a wonderful solution, it causes many problems, so be sure to avoid object type columns at all costs. Typically, the problem arises when we try to apply a vector operation to the column: we round a column with mix of floats and ints: fail ( loop of ufunc does not support argument 0 of type float which has no callable rint method ) we need to apply string functions, but the column contains numbers as well The solution is usually: fill the missing values with the fillna function convert the column to str type using the astype function apply string functions to clear the data convert the column to the desired type Categorical data \u00b6 Sometimes, it can be usefull to treat a column as a categorical variable instead of a string or a number. For that, we can use the Categorical class. The constructor accepts the values of to be converted to categorical variable (list, column,...) and optional parameters. The most important parameters are: categories : the list of categories. If not specified, the categories are inferred from the data. If specified, the categories are used as the categories of the categorical variable. If the data contains values that are not in the categories, the Categorical constructor raises an error. If the categories contain values that are not in the data, the values are converted to NaN . ordered : if True , the categories are ordered in the order of the categories parameter. Datetime \u00b6 Pandas has a special type for datetime values. One of its dangerous properties is that zero parts of the datetime are truncated both when displaying and on export: df = pd.DataFrame({'date': pd.to_datetime(['2021-01-01 00:00:00', '2021-01-01 00:00:00'])}) print(df) # output: # '2021-01-01' # '2021-01-01' I/O \u00b6 csv \u00b6 For reading csv files, we can use the read_csv function. Important params: sep : separator header : row number to use as column names. If None , no header is used skiprows : number of rows to skip from the beginning delim_whitespace : if True , the whitespace is used as a separator. The sep parameter is ignored in that case. This is a way how to read a file with variable number of whitespaces between columns. For export, we can use the to_csv method: df.to_csv(<file name> [, <other params>]) Useful parameters: index : if False , the index is not exported index_label : the name of the index column Json \u00b6 For exporting to json, we can use the to_json function. By default, the data are exported as a list of columns. To export the data as a list of rows, we can use the orient parameter: df.to_json(<file name>, orient='records') Other important parameters: indent : the number of spaces to use for indentation Insert dataframe into db \u00b6 We can use the to_sql method for that: df.to_sql(<table name>, <sql alchemy engine> [, <other params>]) Important params: to append, not replace existing records: if_exists='append' do not import dataframe index: index=False For larger datasets, it is important to not insert everything at once, while also tracking the progress. The following code does exactly that def chunker(seq, size): return (seq[pos:pos + size] for pos in range(0, len(seq), size)) chunksize = int(len(data) / 1000) # 0.1% with tqdm(total=len(data)) as pbar: for i, cdf in enumerate(chunker(data, chunksize)): cdf.to_sql(<table name>, <sqlalchemy_engine>) pbar.update(chunksize) If the speed is slow, it can be caused by a low upload speed of your internet connection. Note that due to the SQL syntax, the size of the SQL strings may be much larger than the size of the dataframe. Latex export \u00b6 Currently, the to_latex function is deprecated. The Styler class should be used for latex exports instead. You can get the Styler from the DataFrame using the style property. The usual workfolow is: Create a Styler object from the dataframe using the style property. Apply the desired formatting to the styler object. Export DataFrame to latex using the to_latex method. Keep in mind that the Styler object is immutable, so you need to assign the result of each formatting operation to a new variable or chain the calls . Example: # wrong, the format is not apllied df.style.format(...) df.style.to_latex(...) # correct: temp var s = df.style.format(...) s.to_latex(...) # correct: chain calls df.style.format(...).to_latex(...) Formatting the index: columns and row labels \u00b6 The columns' and row labels' format is configures by the format_index function. Important parameters: axis : 0 for rows, 1 for columns (cannot be both) escape : by default, the index is not escaped, to do so, we need to set escape to 'latex' . Formatting or changing the values \u00b6 The values are formated by the format function. Important parameters: escape : by default, the values are not escaped, to do so, we need to set escape to 'latex' . na_rep : the string to use for missing values precision : the number of decimal places to use for floats Replacing values \u00b6 For replace some values for the presentation with something else, we can also use the format function.For example, to change the boolean presentation in column col we call: df.style.format({'col': lambda x: 'yes' if x else 'no'}) Hihglighting min/max values \u00b6 For highlighting the min/max values, we can use the highlight_min and highlight_max functions. Important parameters: subset : the columns in which the highlighting should be applied props : the css properties to apply to the highlighted cells Hiding some columns, rows, or indices \u00b6 For hiding some columns, rows, or indices, we can use the hide function. Important Parameters: axis : 0 for hiding row indices (default), 1 for hiding column names level : the level of the multi-index to hide (default is all levels) subset : the columns or rows to hide (default is all columns or rows) When used without the subset parameter, the hide function hides the whole index. To hide just a selected row or column from the data, the subset parameter has to be used. By default, the <index_name> refers to the row index. To hide a column : df.style.hide_columns(<column name>, axis=1) Changing the header (column labels) \u00b6 There is no equivalent to the header parameter of the old to_latex function in the new style system. Instead, it is necessary to change the column names of the dataframe. Exporting to latex \u00b6 For the export, we use the to_latex function. Important parameters: convert_css : if True , the css properties are converted to latex commands multirow_align : the alignment of the multirow cells. Options are t , c , b hrules : if set to True , the horizontal lines are added to the table, specifically to the top, bottom, and between the header and the body. Note that these hrules are realized as the \\toprule , \\midrule , and \\bottomrule commands from the booktabs package, so the package has to be imported . clines : configuration for hlines between rows. It is a string composed of two parts divided by ; (e.g.: skip-last;data ). The parts are: whether to skip last row or not ( skip-last or all ) whether to draw the lines between indices or the whole rows ( index or data ) Displaying the dataframe in console \u00b6 We can display the dataframe in the conslo print or int the log just by supplying the dataframe as an argument because it implements the __repr__ method. Sometimes, however, the default display parameters are not sufficient. In that case, we can use the set_option function to change the display parameters: pd.set_option('display.max_rows', 1000) Important parameters: display.max_rows : the maximum number of rows to display display.max_columns : the maximum number of columns to display display.max_colwidth : the maximum width of a column Other useful functions \u00b6 drop_duplicates to quickly drop duplicate rows based on a subset of columns. factorize to encode a series values as a categorical variable, i.e., assigns a different number to each unique value in series. pivot_table : function that can aggragate and transform a dataframe in one step. with this function, one can create a pivot table, but also a lot more. cut : function that can be used to discretize a continuous variable into bins. pivot_table \u00b6 The pivot table (mega)function do a lot of things at once: it aggregates the data it transforms the data it sorts the data due to reindexing Although this function is very powerfall there are also many pitfalls. The most important ones are: column data type change for columns with missing values Column data type change for columns with missing values \u00b6 The tranformation often creates row-column combinations that do not exist in the original data. These are filled with NaN values. But some data types does not support NaN values, and in conclusion, the data type of the columns with missing values is changed to float . Possible solutions: we can use the fill_value parameter to fill the missing values with some value that is supported by the data type (e.g. -1 for integers) we can use the dropna parameter to drop the rows with missing values we can change the data type of the columns with missing values prior to calling the pivot_table function. For example, the pandas integer data types support NaN values. to_datetime \u00b6 The to_datetime function can convert various inputs to datetime. It can be used to both scalars and vectors. Important parameters: unit : the unit of the input, e.g., s for seconds. origin : the origin of the input, e.g., unix for unix timestamps. It can be also any specific datetime object. squeeze \u00b6 The squeeze function removes the unnecessary dimension from a dataframe or series. It is usefull when we want to convert a dataframe with a single column to a series, or a series with a single value to a scalar. Geopandas \u00b6 Geopandas is a GIS addon to pandas, an equivalent to PostGIS. Unfortunately, it currently supports only one geometry column per table. Do not ever copy paste the geometries from jupyter notebook as the coordinates are rounded! Use the to_wkt function instead. Create a geodataframe from CSV \u00b6 Geopandas has it's own read_csv function, however, it requires a very specific csv format, so it is usually easier to first import csv to pandas and then create geopandas dataframe from pandas dataframe. Converting pandas Dataframe to geopandas Dataframe \u00b6 The geopandas dataframe constructor accepts pandas dataframes, we just need to specify the geometry column and the coordinate system: gdf = gpd.GeoDataFrame( <PANDAS DATAFRAME> geometry=gpd.points_from_xy(<X COLUMN>, <Y COLUMN>), crs=<SRID> ) Create geodataframe from shapely \u00b6 To load data from shapely, execute gdf = gpd.read_file(<PATH TO FOLDER WITH SHAPEFILES>) Working with the geometry \u00b6 The geometry can be accessed using the geometry property of the geodataframe. Spliting multi-geometry columns \u00b6 If the geometry column contains multi-geometries, we can split them into separate rows using the explode function: gdf = gdf.explode() Insert geodataframe into db \u00b6 preprocesssing \u00b6 Before inserting a geodataframe into the database, we need to process it a little bit: set the SRID: gdf.set_crs(epsg=<SRID>, allow_override=True, inplace=True) set the geometry: gdf.set_geometry('geom', inplace=True) select, rename, or add columns so that the resulting geodataframe match the corresponding database table. This process is same as when working with pandas Simple insertion \u00b6 When the data are in the correct format and we don|t need any customization for the db query, we can use the to_postgis method: gdf.to_postgis(<TABLE NAME>, <SQL ALCHEMY CONNECTION>, if_exists='append') Customized Insertion: geoalchemy \u00b6 If we need some special insert statement, we cannot rely on the geodataframe.to_postgis function, as it is not flexible enough. The pandas dataframe.to_sql function is more flexible, however, it has trouble when working with geodata. The easiest options is therefore to use geoalchemy , the database wraper used in geopandas (extension of sqlalchemy , which is a database wrapper for pandas ). First, we need to create the insert statement. The example here uses a modification for handeling duplicite elements. meta = sqlalchemy.MetaData() # create a collection for geoalchemy database # objects table = geoalchemy2.Table( '<TABLE NAME>', meta, autoload_with=<SQL ALCHEMY CONNECTION>) insert_statement = sqlalchemy.dialects.postgresql.insert(table).on_conflict_do_nothing() In the above example, we create a geoalchemy representation of a table and then we use this representation to create a customized insert statement (the on_conflict_do_nothing is the speciality here.). Note that we use a speciatl PostgreSQL insert statement instead of the standard SQLAlchemy insert statement. Second, we need to prepare the data as a list of dictionary entries: list_to_insert = [ {'id': 0, 'geom': <GEOM>, ...}, {'id': 0, 'geom': <GEOM>, ...}, .... ] Note that the geometry in the geodataframe is in the shapely format. Therefore, we need to convert it to string using the geoalchemy from_shape function: geoalchemy2.shape.from_shape(<GEOMETRY>, srid=<SRID>) Finally, we can execute the query using an sqlalchemy connection: sqlalchemy_connection.execute(insert_statement, list_to_insert)","title":"Pandas Manual"},{"location":"Programming/Python/Pandas%20Manual/#main-principles","text":"Pandas extensively uses the term axis. In Pandas, axis 0 is vertical (rows) and axis 1 is horizontal (columns).","title":"Main principles"},{"location":"Programming/Python/Pandas%20Manual/#creating-a-dataframe","text":"The DataFrame class has a constructor that supports multiple formats of input data as well as many configuration parameters. Therefore , for most formats of input data, we can create a dataframe using the constructor. However, we can also crete a dataframe using the from_* functions, and for some formats, these functions are the only way to create a dataframe.","title":"Creating a DataFrame"},{"location":"Programming/Python/Pandas%20Manual/#from-a-dictionary","text":"When having a dictionary, we can choose between two options the constructor and the from_dict function. The required syntax depend on the shape of the dictionary with respect to the required dataframe.","title":"From a dictionary"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-column-names-values-are-list-of-column-values","text":"df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) # or equivalently df.DataFrame.from_dict({'col1': [1, 2], 'col2': [3, 4]}) Note that the values of the dictionary have to be lists. If we have a dictionary with values that are not lists (i.e., only one row), we have to use the orient parameter to specify the orientation of the data and then transpose the dataframe: d = {'col1': 1, 'col2': 2} df = pd.DataFrame.from_dict(d, orient='index').T # or equivalently df = pd.DataFrame([d], columns=d.keys())","title":"Keys are column names, values are list of column values"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-indices-values-are-values-of-a-single-column","text":"df = pd.DataFrame.from_dict({'row1': 1, 'row2': 2}, orient='index', columns=['Values'])","title":"Keys are indices, values are values of a single column"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-indices-values-are-values-of-single-row","text":"df = pd.DataFrame.from_dict({'row1': [1, 2], 'row2': [3, 4]}, orient='index')","title":"Keys are indices, values are values of single row"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-one-column-values-are-another-column","text":"d = {'row1 col1': 'row1 col2', 'row2 col1': 'row2 col2' df = pd.DataFrame.from_dict(d.items()) # or equivalently df = pd.DataFrame({'col1': d.keys(), 'col2': d.values()})","title":"Keys are one column, values are another column"},{"location":"Programming/Python/Pandas%20Manual/#from-a-list-of-dictionaries","text":"df = pd.DataFrame([{'col1': 1, 'col2': 3}, {'col1': 2, 'col2': 4}])","title":"From a list of dictionaries"},{"location":"Programming/Python/Pandas%20Manual/#from-a-list-of-lists","text":"df = pd.DataFrame([[1, 3], [2, 4]], columns=['col1', 'col2'])","title":"From a list of lists"},{"location":"Programming/Python/Pandas%20Manual/#creating-a-zero-or-constant-filled-dataframe","text":"To create a dataframe filled with a constant, we can use the dataframe constructor and pass the constant as the first (data) argument: df = pd.DataFrame(0, index=range(10), columns=['col1', 'col2'])","title":"Creating a zero or constant-filled dataframe"},{"location":"Programming/Python/Pandas%20Manual/#generating-the-index","text":"As displayed in the above example, we can generate a numerical index using the range function. However, there are more options: date index with date_range pd.date_range(<start date>, <end date>, freq=<frequency>)","title":"Generating the index"},{"location":"Programming/Python/Pandas%20Manual/#obtaining-info-about-dataset","text":"For a DataFrame df : column names: df.columns column types: df.dtypes number of rows: len(df)","title":"Obtaining info about dataset"},{"location":"Programming/Python/Pandas%20Manual/#iteration","text":"","title":"Iteration"},{"location":"Programming/Python/Pandas%20Manual/#standard-iteration","text":"https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas Iteration without modifying the dataframe. From the fastest to the slowest.","title":"Standard Iteration"},{"location":"Programming/Python/Pandas%20Manual/#vector-operations","text":"","title":"Vector operations"},{"location":"Programming/Python/Pandas%20Manual/#list-comprehensions","text":"","title":"List Comprehensions"},{"location":"Programming/Python/Pandas%20Manual/#apply","text":"The apply function can be used to apply a function to each row or column of the dataframe. For iterating over rows, we need to set the axis parameter to 1. Example: df['new_col'] = df.apply(lambda row: row['col1'] + row['col2'], axis=1)","title":"Apply"},{"location":"Programming/Python/Pandas%20Manual/#itertuples","text":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.html Returns dataframe rows as pandas named tuples with index as the first member of the tuple.","title":"itertuples()"},{"location":"Programming/Python/Pandas%20Manual/#iterrows","text":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html returns a tuple (index, data) it does not preserve the dtype","title":"iterrows()"},{"location":"Programming/Python/Pandas%20Manual/#items","text":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.items.html Iterates over columns","title":"items()"},{"location":"Programming/Python/Pandas%20Manual/#iteration-with-modification","text":"For modification, the best strategy is to: select what we want to modify (see selection ) modify the selection with the assignment operator. The right side of the assignment operator can be the result of an iteration.","title":"Iteration with modification"},{"location":"Programming/Python/Pandas%20Manual/#filtration","text":"filtered = df[df['max_delay'] == x] or equivalently: filtered = df[df.max_delay == x]","title":"Filtration"},{"location":"Programming/Python/Pandas%20Manual/#filtration-by-multiple-columns","text":"Example: filtered = df[(df['max_delay'] == x) & (df['exp_length'] == y)]","title":"Filtration by Multiple Columns"},{"location":"Programming/Python/Pandas%20Manual/#accept-multiple-values","text":"For that, we can use the isin function: filtered = df[df['max_delay'].isin([x, y])]","title":"Accept multiple values"},{"location":"Programming/Python/Pandas%20Manual/#using-the-query-function","text":"The query function can be used for more complicated filters. It is more flexible and the syntax is less verbose. The above filter can be rewriten as: filtered = df.query('max_delay == x and exp_length == y']","title":"Using the query function"},{"location":"Programming/Python/Pandas%20Manual/#filtering-series","text":"A seris can be filtered even simpler then the dataframe: s = df['col'] sf = s[s <= 10] # now we have a Series with values from df['col'] less than 10","title":"Filtering Series"},{"location":"Programming/Python/Pandas%20Manual/#useful-filter-functions","text":"null values: <column selection>.isnull() non null/nan values: <column selection>.notnull() filtring using the string value: <column selection>.str.<string function> filtering dates: <column selection>.dt.<date function>","title":"Useful filter functions"},{"location":"Programming/Python/Pandas%20Manual/#selection","text":"If we want to select a part of the dataframe (a set of rows and columns) independently of the values of the dataframe (for that, see filtration ), we can use these methods: loc : select by index, works for both rows and columns iloc : select by position, works for both rows and columns [] : select by index, works only for columns There are also other methods that works for selection but does not work for setting values, such as: xs : select by label, works for both rows and columns The return type of the selection is determined by the number of selected rows and columns. For a single row or column, the result is a series, for multiple rows and columns, the result is a dataframe. If we want to get a dataframe for a single row or column, we can use the [] operator with a list of values: df[['col1']] # or df.loc[['row1']] # or df.iloc[[0]]","title":"Selection"},{"location":"Programming/Python/Pandas%20Manual/#loc","text":"The operator loc has many possible input parameters, the most common syntax is df.loc[<row selection>, <column selection>] each selection has the form of <start label>:<end label> . For the whole column, we therefore use: df.loc[:, <column name>]","title":"loc"},{"location":"Programming/Python/Pandas%20Manual/#difference-between-array-operator-on-dataframe-and-on-loc","text":"Both methods can be used both for getting and setting the column: a = df['col'] # or equivalently a = df.loc[:, 'col'] df2['col'] = a # or equivalently df2.loc[:, 'col'] = a The difference between these two methods is apparent when we want to use a chained selection, i.e., selecting from a selection. While the loc selects the appropriate columns in one step, so we know that we still refer to the original dataframe, the array operator operations are separate, and therefore, the result value can refer to a temporary: dfmi.loc[:, ('one', 'second')] = value # we set a value of a part of dfmi dfmi['one']['second'] = value # can be dangerous, we can set value to a temporary This problem is indicated by a SettingWithCopy warning. Sometimes it is not obvious that we use a chain of array operator selections, e.g.: sel = df[['a', 'b']] ..... sel['a'] = ... # we possibly edit a temporary! For more, see the dovumentation .","title":"Difference between array operator on dataframe and on loc"},{"location":"Programming/Python/Pandas%20Manual/#iloc","text":"The iloc method works similarly to the loc method, but it uses the position instead of the label. To select more values, we can use the slice syntax df.iloc[<start position>:<end position>:<step>,<collumn slicing...>] Be aware that if the iloc operator selects by single value (e.g.: df.iloc[3] ), it returns the single row as series . To get a dataframe slice, we need to use a list of values (e.g.: df.iloc[[3]] ).","title":"iloc"},{"location":"Programming/Python/Pandas%20Manual/#selecting-all-columns-but-one","text":"If we do not mind copying the dataframe, we can use the drop function. Otherwise, we can use the loc method and supply the filtered column lables obtained using the columns property: df.loc[:, df.columns != '<column to skip>']","title":"Selecting all columns but one"},{"location":"Programming/Python/Pandas%20Manual/#multi-index-selection","text":"documentation When selecting from a dataframe with a multi-index, things get a bit more complicated. There are three ways how to select from a multi-index dataframe: using loc with slices: simple, but verbose using loc with IndexSlice object: more readable, but requires the IndexSlice object to be created first using xs function, neat, but does not support all the features, e.g., it does not support ranges","title":"Multi-index selection"},{"location":"Programming/Python/Pandas%20Manual/#using-loc","text":"The general loc usage is the same as for a single index dataframe: df.loc[<row selection>, <column selection>] However, each selection is now a tuple, where each element of the tuple corresponds to one level of the multi-index: df.loc[(<row selection level 1>, <row selection level 2>, ...), (<column selection level 1>, <column selection level 2>, ...)] The <row selection> can be a specifica value, a list of values, or a slice. Note that we have to use the slice function, as pandas uses the standard slice syntax for something else. We can skip lower levels to select all values from those levels. However, we cannot skip upper levels. If we want to select all values from the upper level, we need to use the slice(None) for that level: df.loc[(slice(None), slice(15, 30)), ...] Note that for multi-index slicing, the index needs to be sorted. If it is not, we can use the sort_index function. pandas slicing documentation","title":"Using loc"},{"location":"Programming/Python/Pandas%20Manual/#using-indexslice-for-more-readable-syntax","text":"We can obtain the same result with a more readable syntax using the IndexSlice object: idx = pd.IndexSlice dft.loc[idx[:, 15:30], ...]","title":"Using IndexSlice for more readable syntax"},{"location":"Programming/Python/Pandas%20Manual/#handeling-the-too-many-indexers-error","text":"Sometimes, when using the loc method, the selection can fail with the too many indexers error, because it is ambiguous whether we select by rows or by columns. In that case, we can either use the axis parameter to specify the axis to select from: python df.loc(axis=0)[<row selection>] or use the IndexSlice instead.","title":"Handeling the too many indexers error"},{"location":"Programming/Python/Pandas%20Manual/#using-xs","text":"The xs function can be used to select from a multi-index dataframe. However, slices (ranges) are not supported. Example: df.xs(15, level=1) # selects all rows with level 1 equal to 15","title":"Using xs"},{"location":"Programming/Python/Pandas%20Manual/#select-row-with-a-maximum-value-in-a-column","text":"To get the index of the row with the maximum value in a column, we can use the idxmax function: df['col'].idxmax() Then we can use the loc method to get the row.","title":"Select row with a maximum value in a column"},{"location":"Programming/Python/Pandas%20Manual/#selecting-a-single-value-cell-scalar","text":"When we select a single value from a dataframe, the result is sometimes a series, especially when we use a filtration. To get a scalar, we can use the item() method: df.loc[<row>, <column>].item()","title":"Selecting a single value (cell, scalar)"},{"location":"Programming/Python/Pandas%20Manual/#sorting","text":"for sorting the dataframe, we can use the sort_values function. The first argument is the list of columns to sort by, starting with the most important column. Example: df.sort_values(['col1', 'col2']) If we want to use a custom sorting function, we can use the key argument. The key function should satisfy the classical python sorting interface (see Python manual ) and it should be a vector function, i.e., instead of returning a single position for a given value, it should return a vector of positions for a given vector of values. Example: def key_fn(column: list): return [len(x) for x in l] df.sort_values('col', key=key_fn)","title":"Sorting"},{"location":"Programming/Python/Pandas%20Manual/#working-with-columns","text":"","title":"Working with columns"},{"location":"Programming/Python/Pandas%20Manual/#adding-a-column","text":"The preferable way is to use the assign function: # adds a column named 'instance_length' with constant value result_df_5_nyc_mv.assign(instance_length = 5) Multiple columns can be added at once: trips = trips.assign(dropoff_datetime = 0, dropoff_GPS_lon = 0, dropoff_GPS_lat = 0, pickup_GPS_lon = 0, pickup_GPS_lat = 0)","title":"Adding a column"},{"location":"Programming/Python/Pandas%20Manual/#rename-a-column","text":"To rename a column, we can use the pandas rename function: df.rename(columns={'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}) # or equivalently df.rename({'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}, axis='columns')","title":"Rename a column"},{"location":"Programming/Python/Pandas%20Manual/#rename-a-series-column","text":"The column name in the series object is actually the name of the series. To rename the series, we can use the rename function, or we can set the name property of the series: s.rename('<new name>') # or equivalently s.name = '<new name>'","title":"Rename a Series (column)"},{"location":"Programming/Python/Pandas%20Manual/#working-with-the-index","text":"Index of a dataframe df can be accessed by df.index . Standard range operation can be applied to index.","title":"Working with the index"},{"location":"Programming/Python/Pandas%20Manual/#selecting-just-a-single-index-level-from-a-multi-index","text":"If we want to select just a single index level, we can use the get_level_values function: df.index.get_level_values(<level>) Note however, that this function returns duplicated values when there are multiple values in other levels. To get unique values, we can use the unique function. There is also another method, that returns unique values: the level property: df.index.levels[<level>] However, this way, we can get outdated values , as the values are not always updated when the index is changed. To get the updated values, we need to call the method remove_unused_levels after each change of the index.","title":"Selecting just a single index level from a multi-index"},{"location":"Programming/Python/Pandas%20Manual/#changing-the-index","text":"","title":"Changing the index"},{"location":"Programming/Python/Pandas%20Manual/#using-columns-as-a-new-index","text":"For that, we can use the set_index function.","title":"Using columns as a new index"},{"location":"Programming/Python/Pandas%20Manual/#using-an-existing-index-to-create-a-new-index","text":"For that, we can use the reindex function. The first parameter is the new index. Example: df.reindex(df.index + 1) # creates a new index by adding 1 to the old index Important parameters: fill_value : the value to use for missing values. By default, the missing values are filled with NaN .","title":"Using an existing index to create a new index"},{"location":"Programming/Python/Pandas%20Manual/#creating-index-from-scratch","text":"To create an index from scratch, we just assign the index to the dataframe index property: df.index = pd.Index([1, 2, 3, 4, 5]) We can also assign a range directly to the index: df.index = range(5) To create more complicated indices, dedicated functions can be used: MultiIndex.from_product : creates a multi-index from the cartesian product of the given iterables","title":"Creating index from scratch"},{"location":"Programming/Python/Pandas%20Manual/#renaming-the-index","text":"The Index.rename function can be used for that.","title":"Renaming the index"},{"location":"Programming/Python/Pandas%20Manual/#aggregation","text":"Analogously to SQL, pandas has a groupby function for aggreagting rows. The usage is as follows: group = df.groupby(<columns>) # returns a groupby object grouped by the columns sel = group[<columns>] # we can select only some columns from the groupby object agg = sel.<aggregation function> # we apply an aggregation function to the selected columns We can skip the sel step and apply the aggregation function directly to the groupby object. This way, the aggregation function is applied to all columns. Example (sum): df.groupby('col').sum() Sums the results for each group (column by column) To get a count, we can call the size function: df.groupby('col').size() Note that unlike in SQL, the aggregation function does not have to return a single value. It can return a series or a dataframe. In that case, the result is a dataframe with the columns corresponding to the returned series/dataframe. In other words, the aggregation does not have to actually aggregate the data, it can also transform it . In the groupby object, the columns used for grouping are omitted if each group is aggregated to exactly one row. To keep them, we can use the group_keys parameter of the groupby function.","title":"Aggregation"},{"location":"Programming/Python/Pandas%20Manual/#aggregate-functions","text":"For the aggregate function, we can use one of the prepared aggregation functions. Classical functions(single value per group): sum mean median min max count Transformation functions (value for each row): cumsum : cumulative sum diff : difference between the current and the previous row. the periods parameter specifies which row to use for the difference. By default, it is the previous row (periods=1). For next row, use periods=-1, but note that the result is then negative. We can use the abs function to get the absolute value.","title":"Aggregate functions"},{"location":"Programming/Python/Pandas%20Manual/#custom-aggegate-function","text":"Also, there are more general aggregate functions: agg function that is usefull for applying different functions for different columns and apply : the most flexible function that can be used for custom aggregation and transformation operations. These two functions have different interfaces for the custom aggregation functions they call. These are summarized in the following table: property agg apply can just transform the data no yes can use data from one column in another column no yes applied to each specified column the whole dataframe representing single group output dataframe scalar, series, or dataframe can use multiple aggregate functions yes no","title":"Custom aggegate function"},{"location":"Programming/Python/Pandas%20Manual/#agg","text":"Example: df.groupby('col').agg({'col1': 'sum', 'col2': 'mean'})","title":"agg"},{"location":"Programming/Python/Pandas%20Manual/#apply_1","text":"The apply function takes a custom function as an argument. That custom aggregation function: takes a DataFrame/Series (depending on the source object) as the first argument this dataframe/series contains the data for the group (all columns) returns a Series, DataFrame, or a scalar when a scalar is returned, the result is a series with the scalar value for each group we do not have to reduce the data to a single value or a single row, we can just transform the data arbitrarily. The process works as follows: The dataframe is split into groups according to the groupby function. The custom function is applied to each group. The results are combined into a single dataframe. In other words, the custom function only sees the dataframe/series representing the group, not the whole dataframe/series. The grouping and compining aggreate results is done by the apply function.","title":"apply"},{"location":"Programming/Python/Pandas%20Manual/#time-aggregation","text":"We can also aggregate by time. For that, we need an index or column with datetime values. Then, we can use the resample function. Example: df = pd.DataFrame({'col1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, index=pd.date_range('2021-01-01', periods=10, freq='1D')) df.resample('1H').sum() The aggregate function is applied to each group in case of multiple values in the same time slot ( downsampling ). In case of no values in the time slot ( upsampling ), the value is filled with NaN . We can use the ffill function to fill in the missing values. Example: df.resample('1H').sum().ffill()","title":"Time aggregation"},{"location":"Programming/Python/Pandas%20Manual/#joins","text":"Similarly to SQL, Pandas has a way to join two dataframes. There are two functions for that: merge : the most general function that has the behavior known from SQL join : a more specialized function, The following table lists the most important differences between the two functions: property merge join default join type inner left join right table via column (default) or index ( right_index=True ) index join left table via column (default) or index ( left_index=True ) index, or column ( on=key_or_keys ) There is also a static pd.merge function. All merge and join methods are just wrappers around this function. The indexes are lost after the join (if not used for the join). To keep an index, we can store it as a column before the join.","title":"Joins"},{"location":"Programming/Python/Pandas%20Manual/#appending-and-concatenating-data","text":"In pandas, there is a concat function that can be used to concatenate data: pd.concat([df1, df2]) It can concatenate dataframes or series and it can concatenate vertically (by rows, default) or horizontally (by columns) By default, the indices from both input parameters are preserved. To reset the index, we can use the ignore_index parameter. Alternatively, to preserve one of the indices, we can set the index of the other dataframe to the index of the first dataframe before the concatenation using the set_index function.","title":"Appending and Concatenating data"},{"location":"Programming/Python/Pandas%20Manual/#pandas-data-types","text":"","title":"Pandas Data Types"},{"location":"Programming/Python/Pandas%20Manual/#object","text":"If pandas does not recognize the type of the column, or there are multiple types in the column, it uses the object type. However this may sound like a wonderful solution, it causes many problems, so be sure to avoid object type columns at all costs. Typically, the problem arises when we try to apply a vector operation to the column: we round a column with mix of floats and ints: fail ( loop of ufunc does not support argument 0 of type float which has no callable rint method ) we need to apply string functions, but the column contains numbers as well The solution is usually: fill the missing values with the fillna function convert the column to str type using the astype function apply string functions to clear the data convert the column to the desired type","title":"Object"},{"location":"Programming/Python/Pandas%20Manual/#categorical-data","text":"Sometimes, it can be usefull to treat a column as a categorical variable instead of a string or a number. For that, we can use the Categorical class. The constructor accepts the values of to be converted to categorical variable (list, column,...) and optional parameters. The most important parameters are: categories : the list of categories. If not specified, the categories are inferred from the data. If specified, the categories are used as the categories of the categorical variable. If the data contains values that are not in the categories, the Categorical constructor raises an error. If the categories contain values that are not in the data, the values are converted to NaN . ordered : if True , the categories are ordered in the order of the categories parameter.","title":"Categorical data"},{"location":"Programming/Python/Pandas%20Manual/#datetime","text":"Pandas has a special type for datetime values. One of its dangerous properties is that zero parts of the datetime are truncated both when displaying and on export: df = pd.DataFrame({'date': pd.to_datetime(['2021-01-01 00:00:00', '2021-01-01 00:00:00'])}) print(df) # output: # '2021-01-01' # '2021-01-01'","title":"Datetime"},{"location":"Programming/Python/Pandas%20Manual/#io","text":"","title":"I/O"},{"location":"Programming/Python/Pandas%20Manual/#csv","text":"For reading csv files, we can use the read_csv function. Important params: sep : separator header : row number to use as column names. If None , no header is used skiprows : number of rows to skip from the beginning delim_whitespace : if True , the whitespace is used as a separator. The sep parameter is ignored in that case. This is a way how to read a file with variable number of whitespaces between columns. For export, we can use the to_csv method: df.to_csv(<file name> [, <other params>]) Useful parameters: index : if False , the index is not exported index_label : the name of the index column","title":"csv"},{"location":"Programming/Python/Pandas%20Manual/#json","text":"For exporting to json, we can use the to_json function. By default, the data are exported as a list of columns. To export the data as a list of rows, we can use the orient parameter: df.to_json(<file name>, orient='records') Other important parameters: indent : the number of spaces to use for indentation","title":"Json"},{"location":"Programming/Python/Pandas%20Manual/#insert-dataframe-into-db","text":"We can use the to_sql method for that: df.to_sql(<table name>, <sql alchemy engine> [, <other params>]) Important params: to append, not replace existing records: if_exists='append' do not import dataframe index: index=False For larger datasets, it is important to not insert everything at once, while also tracking the progress. The following code does exactly that def chunker(seq, size): return (seq[pos:pos + size] for pos in range(0, len(seq), size)) chunksize = int(len(data) / 1000) # 0.1% with tqdm(total=len(data)) as pbar: for i, cdf in enumerate(chunker(data, chunksize)): cdf.to_sql(<table name>, <sqlalchemy_engine>) pbar.update(chunksize) If the speed is slow, it can be caused by a low upload speed of your internet connection. Note that due to the SQL syntax, the size of the SQL strings may be much larger than the size of the dataframe.","title":"Insert dataframe into db"},{"location":"Programming/Python/Pandas%20Manual/#latex-export","text":"Currently, the to_latex function is deprecated. The Styler class should be used for latex exports instead. You can get the Styler from the DataFrame using the style property. The usual workfolow is: Create a Styler object from the dataframe using the style property. Apply the desired formatting to the styler object. Export DataFrame to latex using the to_latex method. Keep in mind that the Styler object is immutable, so you need to assign the result of each formatting operation to a new variable or chain the calls . Example: # wrong, the format is not apllied df.style.format(...) df.style.to_latex(...) # correct: temp var s = df.style.format(...) s.to_latex(...) # correct: chain calls df.style.format(...).to_latex(...)","title":"Latex export"},{"location":"Programming/Python/Pandas%20Manual/#formatting-the-index-columns-and-row-labels","text":"The columns' and row labels' format is configures by the format_index function. Important parameters: axis : 0 for rows, 1 for columns (cannot be both) escape : by default, the index is not escaped, to do so, we need to set escape to 'latex' .","title":"Formatting the index: columns and row labels"},{"location":"Programming/Python/Pandas%20Manual/#formatting-or-changing-the-values","text":"The values are formated by the format function. Important parameters: escape : by default, the values are not escaped, to do so, we need to set escape to 'latex' . na_rep : the string to use for missing values precision : the number of decimal places to use for floats","title":"Formatting or changing the values"},{"location":"Programming/Python/Pandas%20Manual/#replacing-values","text":"For replace some values for the presentation with something else, we can also use the format function.For example, to change the boolean presentation in column col we call: df.style.format({'col': lambda x: 'yes' if x else 'no'})","title":"Replacing values"},{"location":"Programming/Python/Pandas%20Manual/#hihglighting-minmax-values","text":"For highlighting the min/max values, we can use the highlight_min and highlight_max functions. Important parameters: subset : the columns in which the highlighting should be applied props : the css properties to apply to the highlighted cells","title":"Hihglighting min/max values"},{"location":"Programming/Python/Pandas%20Manual/#hiding-some-columns-rows-or-indices","text":"For hiding some columns, rows, or indices, we can use the hide function. Important Parameters: axis : 0 for hiding row indices (default), 1 for hiding column names level : the level of the multi-index to hide (default is all levels) subset : the columns or rows to hide (default is all columns or rows) When used without the subset parameter, the hide function hides the whole index. To hide just a selected row or column from the data, the subset parameter has to be used. By default, the <index_name> refers to the row index. To hide a column : df.style.hide_columns(<column name>, axis=1)","title":"Hiding some columns, rows, or indices"},{"location":"Programming/Python/Pandas%20Manual/#changing-the-header-column-labels","text":"There is no equivalent to the header parameter of the old to_latex function in the new style system. Instead, it is necessary to change the column names of the dataframe.","title":"Changing the header (column labels)"},{"location":"Programming/Python/Pandas%20Manual/#exporting-to-latex","text":"For the export, we use the to_latex function. Important parameters: convert_css : if True , the css properties are converted to latex commands multirow_align : the alignment of the multirow cells. Options are t , c , b hrules : if set to True , the horizontal lines are added to the table, specifically to the top, bottom, and between the header and the body. Note that these hrules are realized as the \\toprule , \\midrule , and \\bottomrule commands from the booktabs package, so the package has to be imported . clines : configuration for hlines between rows. It is a string composed of two parts divided by ; (e.g.: skip-last;data ). The parts are: whether to skip last row or not ( skip-last or all ) whether to draw the lines between indices or the whole rows ( index or data )","title":"Exporting to latex"},{"location":"Programming/Python/Pandas%20Manual/#displaying-the-dataframe-in-console","text":"We can display the dataframe in the conslo print or int the log just by supplying the dataframe as an argument because it implements the __repr__ method. Sometimes, however, the default display parameters are not sufficient. In that case, we can use the set_option function to change the display parameters: pd.set_option('display.max_rows', 1000) Important parameters: display.max_rows : the maximum number of rows to display display.max_columns : the maximum number of columns to display display.max_colwidth : the maximum width of a column","title":"Displaying the dataframe in console"},{"location":"Programming/Python/Pandas%20Manual/#other-useful-functions","text":"drop_duplicates to quickly drop duplicate rows based on a subset of columns. factorize to encode a series values as a categorical variable, i.e., assigns a different number to each unique value in series. pivot_table : function that can aggragate and transform a dataframe in one step. with this function, one can create a pivot table, but also a lot more. cut : function that can be used to discretize a continuous variable into bins.","title":"Other useful functions"},{"location":"Programming/Python/Pandas%20Manual/#pivot_table","text":"The pivot table (mega)function do a lot of things at once: it aggregates the data it transforms the data it sorts the data due to reindexing Although this function is very powerfall there are also many pitfalls. The most important ones are: column data type change for columns with missing values","title":"pivot_table"},{"location":"Programming/Python/Pandas%20Manual/#column-data-type-change-for-columns-with-missing-values","text":"The tranformation often creates row-column combinations that do not exist in the original data. These are filled with NaN values. But some data types does not support NaN values, and in conclusion, the data type of the columns with missing values is changed to float . Possible solutions: we can use the fill_value parameter to fill the missing values with some value that is supported by the data type (e.g. -1 for integers) we can use the dropna parameter to drop the rows with missing values we can change the data type of the columns with missing values prior to calling the pivot_table function. For example, the pandas integer data types support NaN values.","title":"Column data type change for columns with missing values"},{"location":"Programming/Python/Pandas%20Manual/#to_datetime","text":"The to_datetime function can convert various inputs to datetime. It can be used to both scalars and vectors. Important parameters: unit : the unit of the input, e.g., s for seconds. origin : the origin of the input, e.g., unix for unix timestamps. It can be also any specific datetime object.","title":"to_datetime"},{"location":"Programming/Python/Pandas%20Manual/#squeeze","text":"The squeeze function removes the unnecessary dimension from a dataframe or series. It is usefull when we want to convert a dataframe with a single column to a series, or a series with a single value to a scalar.","title":"squeeze"},{"location":"Programming/Python/Pandas%20Manual/#geopandas","text":"Geopandas is a GIS addon to pandas, an equivalent to PostGIS. Unfortunately, it currently supports only one geometry column per table. Do not ever copy paste the geometries from jupyter notebook as the coordinates are rounded! Use the to_wkt function instead.","title":"Geopandas"},{"location":"Programming/Python/Pandas%20Manual/#create-a-geodataframe-from-csv","text":"Geopandas has it's own read_csv function, however, it requires a very specific csv format, so it is usually easier to first import csv to pandas and then create geopandas dataframe from pandas dataframe.","title":"Create a geodataframe from CSV"},{"location":"Programming/Python/Pandas%20Manual/#converting-pandas-dataframe-to-geopandas-dataframe","text":"The geopandas dataframe constructor accepts pandas dataframes, we just need to specify the geometry column and the coordinate system: gdf = gpd.GeoDataFrame( <PANDAS DATAFRAME> geometry=gpd.points_from_xy(<X COLUMN>, <Y COLUMN>), crs=<SRID> )","title":"Converting pandas Dataframe to geopandas Dataframe"},{"location":"Programming/Python/Pandas%20Manual/#create-geodataframe-from-shapely","text":"To load data from shapely, execute gdf = gpd.read_file(<PATH TO FOLDER WITH SHAPEFILES>)","title":"Create geodataframe from shapely"},{"location":"Programming/Python/Pandas%20Manual/#working-with-the-geometry","text":"The geometry can be accessed using the geometry property of the geodataframe.","title":"Working with the geometry"},{"location":"Programming/Python/Pandas%20Manual/#spliting-multi-geometry-columns","text":"If the geometry column contains multi-geometries, we can split them into separate rows using the explode function: gdf = gdf.explode()","title":"Spliting multi-geometry columns"},{"location":"Programming/Python/Pandas%20Manual/#insert-geodataframe-into-db","text":"","title":"Insert geodataframe into db"},{"location":"Programming/Python/Pandas%20Manual/#preprocesssing","text":"Before inserting a geodataframe into the database, we need to process it a little bit: set the SRID: gdf.set_crs(epsg=<SRID>, allow_override=True, inplace=True) set the geometry: gdf.set_geometry('geom', inplace=True) select, rename, or add columns so that the resulting geodataframe match the corresponding database table. This process is same as when working with pandas","title":"preprocesssing"},{"location":"Programming/Python/Pandas%20Manual/#simple-insertion","text":"When the data are in the correct format and we don|t need any customization for the db query, we can use the to_postgis method: gdf.to_postgis(<TABLE NAME>, <SQL ALCHEMY CONNECTION>, if_exists='append')","title":"Simple insertion"},{"location":"Programming/Python/Pandas%20Manual/#customized-insertion-geoalchemy","text":"If we need some special insert statement, we cannot rely on the geodataframe.to_postgis function, as it is not flexible enough. The pandas dataframe.to_sql function is more flexible, however, it has trouble when working with geodata. The easiest options is therefore to use geoalchemy , the database wraper used in geopandas (extension of sqlalchemy , which is a database wrapper for pandas ). First, we need to create the insert statement. The example here uses a modification for handeling duplicite elements. meta = sqlalchemy.MetaData() # create a collection for geoalchemy database # objects table = geoalchemy2.Table( '<TABLE NAME>', meta, autoload_with=<SQL ALCHEMY CONNECTION>) insert_statement = sqlalchemy.dialects.postgresql.insert(table).on_conflict_do_nothing() In the above example, we create a geoalchemy representation of a table and then we use this representation to create a customized insert statement (the on_conflict_do_nothing is the speciality here.). Note that we use a speciatl PostgreSQL insert statement instead of the standard SQLAlchemy insert statement. Second, we need to prepare the data as a list of dictionary entries: list_to_insert = [ {'id': 0, 'geom': <GEOM>, ...}, {'id': 0, 'geom': <GEOM>, ...}, .... ] Note that the geometry in the geodataframe is in the shapely format. Therefore, we need to convert it to string using the geoalchemy from_shape function: geoalchemy2.shape.from_shape(<GEOMETRY>, srid=<SRID>) Finally, we can execute the query using an sqlalchemy connection: sqlalchemy_connection.execute(insert_statement, list_to_insert)","title":"Customized Insertion: geoalchemy"},{"location":"Programming/Python/Plotly%20Manual/","text":"In plotly, we have two options: plot quickly with plotly express full control using graph objects Note that these options can hardly be mixed. For example, we cannot use plotly express to create a figure and then add a subplot to it. Similarly, we cannot use the make_subplots function to create a figure and then add a plotly express plot to it. In general, it is easier to use plotly express, so we should use it if we are not affected by its limitations. The plotly express cannot create custom subplots . However, automatic \"facet\" subplots (same plot divided between multiple plots using some data attribute) are possible. z-order of the traces. For example, we need to first plot a trace using graph objects. Then it is much easier to plot the rest of the traces using graph objects as well. Plotly Express \u00b6 documentation Plotly express modul is loaded as: import plotly.express as px Common Parameters For All Types of Plots \u00b6 data_frame : the dataframe to use. Mandatory, first positional parameter. x : the name of the column to use as x axis. Mandatory, second positional parameter. color : the name of the column to use as color. facet_col : the name of the column to use as facet column. facet_row : the name of the column to use as facet row. color_discrete_sequence : the list of colors in hexadecimal format to use for the color column. If the number of colors is less than the number of categories, the colors are reused. If the number of colors is greater, the colors are truncated. title : the title of the plot. hover_data : the list of columns to show in the hover tooltip. Axes columns are shown automatically. text : text labels for the data points. Automatic color assignment \u00b6 If we use the color parameter of a graph, plotly express plots a trace for each color value and assigns a color to the trace. To customize this color, we can use two parameters: color_discrete_sequence : list of the colors to be used color_discrete_map : dictionary mapping the color values to the colors. With the color_discrete_sequence parameter, plotly express iterates through the list of colors and assigns the colors to the color values in the order they appear in the data. If the number of colors is less than the number of color values, the colors are reused. If the number of colors is greater, the colors are truncated. Therefore, this parameter is useful only if: the number of colors is equal to the number of color values the number of colors is greater than the number of color values and we use categorical colors, so the truncation does not matter. The color_discrete_map parameter is more flexible. We can manually assign the colors to the color values, to use the color scale optimally. Histogram \u00b6 documentation Note that the Plotly histogram is only good for simple cases of small size . See below for more details. Plotly express has histogram function for creating histograms. The basic syntax is: px.histogram(<dataframe>, <xcol name>) The y is then the number of occurences of each value in the x column. Important parameters: nbins : number of bins. Plotly Histogram Limitations \u00b6 Plotly histogram is only good for simple cases of small size. This is because it first stores all data points in JSON and only computes the bins on the javascript size. As a result, the function is slow and the size of the Jupyter notebook cell can be enormous (hundreds of MBs). For more complex figures, it is better to generate the histogram manually (using numpy or pandas) and then plot it using the px.bar function. Bar Chart \u00b6 documentation reference For bar charts, we use the px.bar function. The basic syntax is: px.bar(<dataframe>, <xcol name>, <y col name>) Important parameters: barmode : how to combine the bars in case of multiple traces. Can be group (default), stack (default in facet plots ), relative or overlay . bargap : the gap between bars. bar_groupgap : the gap between the bars from the same group (only for barmode = group ). Unfortunately, there is no way how to represent missing values in the bar chart (they appear as y = 0 ). To mark the missing values, we can use annotations. Why bar charts with a lot of records appear transparent? \u00b6 If the number of records is large, the bar chart may appear transparent. This is because each bar has a border, which has a brighter color. To prevent this effect, we have to remove the border: fig.update_traces(marker_line_width=0) Numerical vs categorical color \u00b6 The values in the color column are interpreted as numerical (continuous) if the column is numeric and as categorical if the column is of any other type. Even if the color column contains only integers, it is still interpreted as numerical with all consequences (color bar instead of categorical colors, color_discrete_sequence parameter is ignored, etc.). To force the categorical interpretation, we can convert the column to a string. Example: px.bar(df, x=\"x\", y=\"y\", color=df[\"color\"].astype(str)) Scatter Plot \u00b6 documentation For scatter plots, we use the scatter function. The basic syntax is: px.scatter(<dataframe>, <xcol name>, <y col name>) Important parameters: Line Chart \u00b6 documentation reference For line charts in plotly express, we use the px.line function. The basic syntax is: px.line(<dataframe>, <xcol name>, <y col name>) Automatic Subplots: Facet plots \u00b6 Facet plots can be created using the same plot function as for normal plotly express plots and supplying the facet_col and/or facet_row parameters. Example: fig = px.histogram(df, x=\"column\", facet_row=\"<col 1>\", facet_col=\"<col 2>\") Here, the figure will be devided into subplotts. Each row will share the <column 1> values, and each column will share the <column 2> values. The number of rows and columns will be determined automatically as the number of unique values in <column 1> and <column 2> , respectively. Independent axes between rows and columns \u00b6 It can happen that each row or column should have its own x or y axis due to a different scale. We can accomplish this by calling the update_xaxes and update_yaxes functions on the figure. Example: fig.update_xaxes(matches=None) fig.update_yaxes(matches=None) Sharing the axes titles between rows and columns \u00b6 Unfortunately, the axes titles cannot be easily shared between rows and columns. The only way is configure the axis text manually. Example: # remove y axis titles except for the first subplot for i in range(1, 4): fig.update_yaxes(title_text='', row=1, col=i) # set the text of the first y axis fig.update_yaxes(title_text=\"Comp. time relative to IH\", row=1, col=1) 3D Scatter Plot \u00b6 documentation For 3D scatter plots, we use the scatter_3d function. The basic syntax is: fig = px.scatter_3d(<dataframe>, <xcol name>, <y col name>, <z col name>) Plotly Graph Objects \u00b6 documentation If the plotly express is not enough, we can use the graph objects. We can either add the graph objects to a plotly express figure or create a graph objects figure from scratch. Most of the time, we will use the first option, as using plotly express is easier. We need to use the second option only for complex figures, for example: facet plots with more than one metric plots with custom traces behind the plotly express traces To make the figure from scratch, we can use the make_subplots function from the plotly.subplots module. Example: from plotly.subplots import make_subplots fig = make_subplots(rows=2, cols=2) Important parameters: rows , cols : the number of rows and columns in the figure shared_xaxes , shared_yaxes : configures the axes sharing. Possible values: False (default) : each subplot has its own axes True : only one axis per row (for shared_xaxes ) or column (for shared_yaxes ). row or col : equivalent to True , applicable only to shared_xaxes or shared_yaxes , respectively. all : all subplots share the same axes horizontal_spacing , vertical_spacing : the spacing between the subplots in relative units, values are in the range [0, 1] . The default is 0.2 for both, which means that the spacing is 20% of the figure width/height. subplot_titles : the titles of the subplots. Common Parameters For All Types of Plots \u00b6 Legend \u00b6 The name in the legend is determined by the name parameter of the trace. To share the legend between multiple traces, the following steps are needed: set the name parameter of all traces to the same value set the showlegend parameter of one trace to True and to False for all other traces Bar Chart \u00b6 The basic syntax is: go.Bar(x, y, ...) Some more complicated examples are in the documentation . Stacked or grouped bars \u00b6 Unlike plotly express, the graph objects do not have the color parameter to set the column to use for determining the group to which the bar belongs. There are two options how to create stacked or grouped bars: create a trace for each group manually and add them all to the figure crete the figure with plotly express and then extract the traces from the figure python for trace in occ_fig.data: fig.add_trace(trace, row=1, col=i + 1) Also, to set the bar mode, we need to use the update_layout as the go.Bar function does not have the barmode parameter. Example: fig.update_layout(barmode=\"stack\") Line, Scatter and Shape Plots \u00b6 line plot documentation Line plots, scatter plots and shapes, all of that can be created by the go.Scatter function. Example: go.Scatter(x, y, ...) Important parameters: mode : the mode of the line. Can be: lines , markers , lines+markers , text , lines+text , markers+text , lines+markers+text . The default is lines+markers if there are less than 20 data points and lines otherwise. line : dictionary containing the line parameters. The most important parameters are: color : the color of the line width : the width of the line Shapes \u00b6 documentation The shapes can be created by supplying the coordinates of the shape to the go.Scatter function and setting the fill parameter to \"toself\" . Create subplots \u00b6 documentation and examples To create a figure with multiple subplots, we use the make_subplots function. Example: fig = make_subplots(rows=2, cols=2, start_cell=\"bottom-left\") fig.add_trace(go.Scatter(x=[1, 2, 3], y=[4, 5, 6]), row=1, col=1) ... fig.show() Importanta parameters: shared_xaxes and shared_yaxes : if True , the subplots will share the same x and y axes. x_title and y_title : the titles of the x and y axes. horizontal_spacing and vertical_spacing : the spacing between the subplots. 3D plots \u00b6 Common Parameters \u00b6 Setting color of the plots with continuous color scale \u00b6 If the plot type is continuous (e.g., surface plot, cone plot), we cannot set a color for the whole trace as a continuous color scale is used. However, we can set the colorscale using the colorscale parameter. Example: fig.add_trace(go.Surface(z=z, colorscale=\"Viridis\")) The color scale can be also set manually as a list of colors. This way, we can overcome the limitation of the continuous color scale and set the color of the whole trace. Example: fig.add_trace(go.Surface(z=z, colorscale=[[0, \"red\"], [1, \"red\"]])) 3D Scatter Plot \u00b6 documentation The 3D scatter plot is created using the go.Scatter3d function. Example: go.Scatter3d(x, y, z, ...) Surface Plots \u00b6 There are multiple types of plots that may be considered as surface plots in plotly: 3D surface ( go.Surface ): a surface plot defined by a grid of x, y, and a 2D array of z values of the shape (|x|, |y|). The surface is plotted for all combinations of x and y, which makes this function suitable only for cases where the surface is defined for all combinations of x and y. 3D mesh ( go.Mesh3d ): a surface plot defined by vertices (points) and faces (connections between the points). The connections are defined by triangles, which can be either defined manually or computed automatically using a triangulation algorithm. Tri-surface ( figure_factory.create_trisurf ): a surface plot created by a triangulation of the data points. It accepts triangles in a single argument. I have to further investigate, how it differs from the 3D mesh plot. 3D Surface Plot \u00b6 documentation The 3D surface plot is created using the go.Surface function. 3D Mesh Plot \u00b6 documentation The 3D mesh plot is created using the go.Mesh3d function. Example: x = [1, 1, 1] y = [1, 2, 1] z = [0, 0, 1] fig.add_trace(go.Mesh3d(x=x, y=y, z=z)) This way, the triangulation is used to compute the connections between the points. If we want to use a different triangulation, we can use the i , j and k parameters. These parameters define the indices (in the source data used for x , y , and z ) of the vertices that form the triangles. For example, i[0] , j[0] and k[0] define the vertices of the first triangle. Example code: x = [1, 1, 1] y = [1, 2, 1] z = [0, 0, 1] i = [0] j = [1] k = [2] fig.add_trace(go.Mesh3d(x=x, y=y, z=z, i=i, j=j, k=k)) Note that here, the i , j and k parameters are redundant, as there is only one possible triangulation of the three points. However, for more complex surfaces, there can be multiple valid triangulations, and the i , j and k parameters can be used to select the triangulation manually. Cone Plots \u00b6 documentation reference Customizing the Figure \u00b6 The figure object can be custommized in many ways. Size and margins \u00b6 The size and margins can be set using the figure.update_layout function. The specific parameters are: width : the width of the figure in pixels height : the height of the figure in pixels autosize : needs to be False if we want to set the width and height manually margin : dictionary containing the margins ( l , r , t , b ) and one another property: pad . All properties are in pixels. l , r , t , b : distance between the plot and the figure border. Note that titles are not included in the plot, so we have make space for the titles if we set margins manually. pad : distance between the plotting area (i.e., the plotting coordinates for data points) and the axis lines. Most of the time, this should be set to 0 (default) Unfortunately, there is no way how to set the margins automatically to fit all content like titles, annotations, etc. documentation reference Customize axes \u00b6 For customizing the axes, we can use the figure.update_xaxes and figure.update_yaxes functions. By default, the functions will update all axes. To update only a specific axis, we can use the row and col parameters. axis reference The range of the axis is determined automatically as the range of the data plus some margin. If we want any other range, we need to set it manually using the range parameter, e.g.: range=[0, 1] . Unfortunately, there is no way how to automatically set the range to match the data range exactly Another thing we usually want to customize are the ticks. Important tick parameters are: dtick : the distance between the ticks tickvals : the exact values of the ticks. This overrides the dtick parameter. ticks : the position of the ticks. Can be outside , inside or \"\" (no ticks, default). ticklen : the length of the ticks in pixels tickformat : the format of the tick labels. Depending on the axis datatype, we can use number formats (e.g., \".2f\" for two decimal places), datetime formats (e.g., \"%Y-%m-%d\" for dates) or scientific notation (e.g., \"e\" for scientific notation). for percentage, we can use \".0%\" for integer percentage and \".1%\" for one decimal place. Note that this way, the % sign is added automatically to each tick label. If we do not want this, we can either set the text manually using the ticktext parameter, or multiply the data by 100. tickangle : the angle of the tick labels in degrees Other important parameters are: title_text : the title of the axis. linecolor : the color of the axis line gridcolor : the color of the grid lines mirror : if True , the axis line will be mirrored to the other side of the plot Drawing borders using axes \u00b6 The borders can be drawn using the axes. To draw a black border around the plot, we can use the following code: fig.update_xaxes(linecolor=\"black\", mirror=True) fig.update_yaxes(linecolor=\"black\", mirror=True) Customizing datetime axes \u00b6 Unfortunately, we cannot treat the datetime axes as expected, i.e., using the datetime objects. For example, to set the tick interval, we cannot use the datetime.timedelta object. Instead, we need to use the number of milliseconds. Example: fig.update_xaxes(dtick=1000 * 60 * 60 * 24 * 7) # one week interval Tick markers and shared axes \u00b6 By default, only tick labels are shared between subplots with shared axes. To share the tick markers, we need to hide them manually and add them to the first subplot. Example: fig.update_yaxes(showticklabels=False) fig.update_yaxes(showticklabels=True, row=1, col=1) 3d axes \u00b6 documentation Unfortunately, the customization of the 3d axes works differently than for the 2d axes. The update_xaxes and update_yaxes functions do not work for the 3d axes. Instead, we need to use the update_layout function with the scene parameter. Example: fig.update_layout(scene=dict( xaxis=dict( title=\"x\", titlefont_size=16, tickfont_size=14, ), yaxis=dict( title=\"y\", titlefont_size=16, tickfont_size=14, ), zaxis=dict( title=\"z\", titlefont_size=16, tickfont_size=14, ), )) One thing that is not possible to customize idn 3D is the position of the axis . The x and y axes are always in the bottom, while the z axis is always on the left. Change position of the axis title \u00b6 Unfortunately, there is no way how to change the position of the axis title. The solution is to hide the title and add a new annotation with the title text. Example: fig.update_xaxes(title_text=\"\") fig.add_annotation( text=\"Comp. time relative to IH\", xref=\"paper\", yref=\"paper\", x=-0.09, y=0.5, showarrow=False, font=dict( size=14, ), textangle=270, ) Legend \u00b6 documentation reference The legend can be styled using the figure.update_layout function. The most important parameters are: legend_title : the title of the legend legend : dictionary containing many parameters orientation : h or v for horizontal or vertical legend x , y : the position of the in normalized coordinates of the whole plot. 'xref', yref : the coordinate system of the x and y coordinates. Can be \"container\" : the whole plot \"paper\" (default): the plotting area xanchor , yanchor : the position of the legend box relative to the x and y coordinates title : if string, the title of the legend (equivalent to the legend_title parameter). If dictionary, multiple legend title parameters can be set. bordercolor : the color of the legend border borderwidth : the width of the legend border Unfortunately, there is no way how to customize the padding between the legend items and the legend box . Also, it is not possible to set the legend border to have rounded corners . Hide legend \u00b6 To hide the legend, we can use the showlegend parameter. Example: fig.update_layout(showlegend=False) Legend items order \u00b6 The order of the legend items is determined by the order of the traces in the figure. However, we can change the order using the legend_traceorder parameter. Example: fig.update_layout(legend_traceorder=\"reversed\") Legend position \u00b6 The coordinates of the legend are normalized with respect to each axis. By default, the legend x and y coordinates are set so that the legend is outside the plot, in the top right corner. If the legend is positioned inside the plot, the plot expands to the whole width, but if the legend is positioned outside the plot, the plot width is smaller to leave space for the legend. Legend items text \u00b6 The legend item text is determined by the name parameter of the trace. Therefore, to customize the legend item text, we need to set the name parameter of the trace. For normal single trace functions, this is simple: fig.add_trace(go.Scatter(x=x, y=y, name=\"Custom name\")) However, it can be complicated for plotly express functions that plot multiple traces at once, as these determine the name parameter automatically from data. For example, when we use the color parameter, the name parameter is set to the color value. To overcome this, we have two options: set the name parameter manually for each trace after the figure is created change the data so that the name parameter is set automatically to the desired value. The first approach is usually preferable as we do not mix the data and appearance. To change the name parameter, we can use the update_traces function: for trace in fig.data: trace.name = process_name(trace.name) Figure Annotations \u00b6 documentation For adding annotations to the whole Figure, we can use the add_annotation function. Important parameters: x , y : the x and y coordinates of the annotation text : the text of the annotation xref , yref : the coordinate system of the x and y coordinates. Can be \"paper\" or \"data\" . showarrow : if True , an arrow will be added to the annotation textangle : the angle of the text in degrees, bgcolor : the background color of the annotation By default the annotation is meant to annotate the data. Therefore, the x and y coordinates use the coordinate system of the data (x and y axes). To align the annotation with respect to the whole figure, we need to set the xref and yref parameters to \"paper\" . In this case, the x and y coordinates are in the range [0, 1] and the origin is the bottom left corner of the figure. Annotations in facet plots \u00b6 When using facet plots, the annotations are added to the whole figure. Therefore, the x and y coordinates are in the range [0, 1] and the origin is the bottom left corner of the figure. To align the annotation with respect to the subplot, we need to set the xref and yref parameters to the x and y axes of the subplot. Example: fig.add_annotation(x=0.5, y=0.5, text=\"Title\", xref=\"x5\", yref=\"y5\", showarrow=False) Unfortunately, there is no way how to set the xref and yref parameters automatically . Therefore, we need to compute them manually for each annotation. Markers \u00b6 documentation To style the markers, we can use the update_traces function. Example: fig.update_traces(marker=dict(size=10, line=dict(width=2, color='DarkSlateGrey'))) Marker parameters: size : the size of the marker in pixels line : dictionary containing border parameters width : the width of the border in pixels. The default is 0. This setting is ignored for 3d scatter plots, where the border width is always 1 (see github issue ) color : the color of the border color : the color of the marker. To make the marker transparent, it is best to use the rgba format. Example: rgba(255, 0, 0, 0.5) for a red marker with 50% transparency. Adding a marker to a hard-coded location \u00b6 To add a marker to a hard-coded location, we can add it as a new trace. Note that we can add new traces even to a figure created using plotly express. Lines \u00b6 Lines can be styled using the line parameter of the plotting functions or using the update_traces function. Important parameters: color : the color of the line width : the width of the line in pixels dash : the dash pattern. Can be \"solid\" (default) \"dot\" : dense dashed line \"dash\" : sparse dashed line ... Title \u00b6 documentation The title can be set using the plotly express functions or when creating the graph objects figure. To update the text or to customize the title layout, we can use the update_layout function with the title object parameter. Example: fig.update_layout( title={ 'x': 0.5, 'xanchor': 'center', 'y': 0.85, 'yanchor': 'top' } ) Important parameters: x , y : the x and y coordinates of the title. The origin is the bottom left corner of the figure. xanchor , yanchor : the position of the title relative to the x and y coordinates. Can be left , center or right for xanchor and top , middle or bottom for yanchor . Subplot titles \u00b6 Automatic subplot titles (facet plots) \u00b6 When using facet plots, the subplot titles are generated automatically as <facet column name>=<facet column value> . Usually, we want to remove the column name and keep only the value. To do this, we can use the for_each_annotation function. Example: fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1])) Individual subplot titles \u00b6 To set the title of an individual subplot, we can use the subplot_titles parameter of the make_subplots function. Z-Order of the Traces \u00b6 The z-order of the traces cannot be configured. Instead, the traces are drawn in the order they are added to the figure. This simple rule has an exception: the webGL traces are always drawn on top of the standard traces. Because all plotly express traces are WebGL traces, they are drawn on top of the graph objects traces added later if those are not WebGL. To overcome this limitation, we have two options: not combine plotly express and graph objects traces and convert everything to graph objects for figures with custom traces use the webGL versions of the graph objects traces. Example: python fig.add_trace(go.Scattergl(...)) Other Layout Parameters (background, borders, etc.) \u00b6 background color: plot_bgcolor border: borders are best drawn by showing and mirroring the axes (see the axis section above). Text \u00b6 documentation Subscript and superscript \u00b6 To add a subscript or superscript to a text, we can use HTML tags. Example: fig.add_annotation(x=0.5, y=0.5, text=\"Title<sub>subscript</sub>\", xref=\"x5\", yref=\"y5\", showarrow=False) Bold and italic \u00b6 To add bold or italic text, we can use the HTML tags <b> and <i> . Example: fig.add_annotation(x=0.5, y=0.5, text=\"Title<b>bold</b><i>italic</i>\", xref=\"x5\", yref=\"y5\", showarrow=False) Math symbols \u00b6 To add math symbols, we can use the LaTeX syntax. Example: fig.add_annotation(x=0.5, y=0.5, text=r\"Title$ \\alpha $ and $ \\beta $\", xref=\"x5\", yref=\"y5\", showarrow=False) Exporting the Figure \u00b6 documentation The static export is handeled using the figure's write_image function. Example: fig.write_image(\"figure.png\") The output format is determined by the extension. The margins of the figure should be set for the figure itself, not for the export. Export hangs out \u00b6 It can be cause by kaleido. The solution si to install an older version, specifically 0.1.0.post1 . https://community.plotly.com/t/static-image-export-hangs-using-kaleido/61519/4 Colors \u00b6 documentation For colors in plotly, we can use the color scales supplied by plotly express. These are: px.colors.sequential : sequential color scales px.colors.diverging : diverging color scales px.colors.qualitative : qualitative color scales","title":"Plotly Manual"},{"location":"Programming/Python/Plotly%20Manual/#plotly-express","text":"documentation Plotly express modul is loaded as: import plotly.express as px","title":"Plotly Express"},{"location":"Programming/Python/Plotly%20Manual/#common-parameters-for-all-types-of-plots","text":"data_frame : the dataframe to use. Mandatory, first positional parameter. x : the name of the column to use as x axis. Mandatory, second positional parameter. color : the name of the column to use as color. facet_col : the name of the column to use as facet column. facet_row : the name of the column to use as facet row. color_discrete_sequence : the list of colors in hexadecimal format to use for the color column. If the number of colors is less than the number of categories, the colors are reused. If the number of colors is greater, the colors are truncated. title : the title of the plot. hover_data : the list of columns to show in the hover tooltip. Axes columns are shown automatically. text : text labels for the data points.","title":"Common Parameters For All Types of Plots"},{"location":"Programming/Python/Plotly%20Manual/#automatic-color-assignment","text":"If we use the color parameter of a graph, plotly express plots a trace for each color value and assigns a color to the trace. To customize this color, we can use two parameters: color_discrete_sequence : list of the colors to be used color_discrete_map : dictionary mapping the color values to the colors. With the color_discrete_sequence parameter, plotly express iterates through the list of colors and assigns the colors to the color values in the order they appear in the data. If the number of colors is less than the number of color values, the colors are reused. If the number of colors is greater, the colors are truncated. Therefore, this parameter is useful only if: the number of colors is equal to the number of color values the number of colors is greater than the number of color values and we use categorical colors, so the truncation does not matter. The color_discrete_map parameter is more flexible. We can manually assign the colors to the color values, to use the color scale optimally.","title":"Automatic color assignment"},{"location":"Programming/Python/Plotly%20Manual/#histogram","text":"documentation Note that the Plotly histogram is only good for simple cases of small size . See below for more details. Plotly express has histogram function for creating histograms. The basic syntax is: px.histogram(<dataframe>, <xcol name>) The y is then the number of occurences of each value in the x column. Important parameters: nbins : number of bins.","title":"Histogram"},{"location":"Programming/Python/Plotly%20Manual/#plotly-histogram-limitations","text":"Plotly histogram is only good for simple cases of small size. This is because it first stores all data points in JSON and only computes the bins on the javascript size. As a result, the function is slow and the size of the Jupyter notebook cell can be enormous (hundreds of MBs). For more complex figures, it is better to generate the histogram manually (using numpy or pandas) and then plot it using the px.bar function.","title":"Plotly Histogram Limitations"},{"location":"Programming/Python/Plotly%20Manual/#bar-chart","text":"documentation reference For bar charts, we use the px.bar function. The basic syntax is: px.bar(<dataframe>, <xcol name>, <y col name>) Important parameters: barmode : how to combine the bars in case of multiple traces. Can be group (default), stack (default in facet plots ), relative or overlay . bargap : the gap between bars. bar_groupgap : the gap between the bars from the same group (only for barmode = group ). Unfortunately, there is no way how to represent missing values in the bar chart (they appear as y = 0 ). To mark the missing values, we can use annotations.","title":"Bar Chart"},{"location":"Programming/Python/Plotly%20Manual/#why-bar-charts-with-a-lot-of-records-appear-transparent","text":"If the number of records is large, the bar chart may appear transparent. This is because each bar has a border, which has a brighter color. To prevent this effect, we have to remove the border: fig.update_traces(marker_line_width=0)","title":"Why bar charts with a lot of records appear transparent?"},{"location":"Programming/Python/Plotly%20Manual/#numerical-vs-categorical-color","text":"The values in the color column are interpreted as numerical (continuous) if the column is numeric and as categorical if the column is of any other type. Even if the color column contains only integers, it is still interpreted as numerical with all consequences (color bar instead of categorical colors, color_discrete_sequence parameter is ignored, etc.). To force the categorical interpretation, we can convert the column to a string. Example: px.bar(df, x=\"x\", y=\"y\", color=df[\"color\"].astype(str))","title":"Numerical vs categorical color"},{"location":"Programming/Python/Plotly%20Manual/#scatter-plot","text":"documentation For scatter plots, we use the scatter function. The basic syntax is: px.scatter(<dataframe>, <xcol name>, <y col name>) Important parameters:","title":"Scatter Plot"},{"location":"Programming/Python/Plotly%20Manual/#line-chart","text":"documentation reference For line charts in plotly express, we use the px.line function. The basic syntax is: px.line(<dataframe>, <xcol name>, <y col name>)","title":"Line Chart"},{"location":"Programming/Python/Plotly%20Manual/#automatic-subplots-facet-plots","text":"Facet plots can be created using the same plot function as for normal plotly express plots and supplying the facet_col and/or facet_row parameters. Example: fig = px.histogram(df, x=\"column\", facet_row=\"<col 1>\", facet_col=\"<col 2>\") Here, the figure will be devided into subplotts. Each row will share the <column 1> values, and each column will share the <column 2> values. The number of rows and columns will be determined automatically as the number of unique values in <column 1> and <column 2> , respectively.","title":"Automatic Subplots: Facet plots"},{"location":"Programming/Python/Plotly%20Manual/#independent-axes-between-rows-and-columns","text":"It can happen that each row or column should have its own x or y axis due to a different scale. We can accomplish this by calling the update_xaxes and update_yaxes functions on the figure. Example: fig.update_xaxes(matches=None) fig.update_yaxes(matches=None)","title":"Independent axes between rows and columns"},{"location":"Programming/Python/Plotly%20Manual/#sharing-the-axes-titles-between-rows-and-columns","text":"Unfortunately, the axes titles cannot be easily shared between rows and columns. The only way is configure the axis text manually. Example: # remove y axis titles except for the first subplot for i in range(1, 4): fig.update_yaxes(title_text='', row=1, col=i) # set the text of the first y axis fig.update_yaxes(title_text=\"Comp. time relative to IH\", row=1, col=1)","title":"Sharing the axes titles between rows and columns"},{"location":"Programming/Python/Plotly%20Manual/#3d-scatter-plot","text":"documentation For 3D scatter plots, we use the scatter_3d function. The basic syntax is: fig = px.scatter_3d(<dataframe>, <xcol name>, <y col name>, <z col name>)","title":"3D Scatter Plot"},{"location":"Programming/Python/Plotly%20Manual/#plotly-graph-objects","text":"documentation If the plotly express is not enough, we can use the graph objects. We can either add the graph objects to a plotly express figure or create a graph objects figure from scratch. Most of the time, we will use the first option, as using plotly express is easier. We need to use the second option only for complex figures, for example: facet plots with more than one metric plots with custom traces behind the plotly express traces To make the figure from scratch, we can use the make_subplots function from the plotly.subplots module. Example: from plotly.subplots import make_subplots fig = make_subplots(rows=2, cols=2) Important parameters: rows , cols : the number of rows and columns in the figure shared_xaxes , shared_yaxes : configures the axes sharing. Possible values: False (default) : each subplot has its own axes True : only one axis per row (for shared_xaxes ) or column (for shared_yaxes ). row or col : equivalent to True , applicable only to shared_xaxes or shared_yaxes , respectively. all : all subplots share the same axes horizontal_spacing , vertical_spacing : the spacing between the subplots in relative units, values are in the range [0, 1] . The default is 0.2 for both, which means that the spacing is 20% of the figure width/height. subplot_titles : the titles of the subplots.","title":"Plotly Graph Objects"},{"location":"Programming/Python/Plotly%20Manual/#common-parameters-for-all-types-of-plots_1","text":"","title":"Common Parameters For All Types of Plots"},{"location":"Programming/Python/Plotly%20Manual/#legend","text":"The name in the legend is determined by the name parameter of the trace. To share the legend between multiple traces, the following steps are needed: set the name parameter of all traces to the same value set the showlegend parameter of one trace to True and to False for all other traces","title":"Legend"},{"location":"Programming/Python/Plotly%20Manual/#bar-chart_1","text":"The basic syntax is: go.Bar(x, y, ...) Some more complicated examples are in the documentation .","title":"Bar Chart"},{"location":"Programming/Python/Plotly%20Manual/#stacked-or-grouped-bars","text":"Unlike plotly express, the graph objects do not have the color parameter to set the column to use for determining the group to which the bar belongs. There are two options how to create stacked or grouped bars: create a trace for each group manually and add them all to the figure crete the figure with plotly express and then extract the traces from the figure python for trace in occ_fig.data: fig.add_trace(trace, row=1, col=i + 1) Also, to set the bar mode, we need to use the update_layout as the go.Bar function does not have the barmode parameter. Example: fig.update_layout(barmode=\"stack\")","title":"Stacked or grouped bars"},{"location":"Programming/Python/Plotly%20Manual/#line-scatter-and-shape-plots","text":"line plot documentation Line plots, scatter plots and shapes, all of that can be created by the go.Scatter function. Example: go.Scatter(x, y, ...) Important parameters: mode : the mode of the line. Can be: lines , markers , lines+markers , text , lines+text , markers+text , lines+markers+text . The default is lines+markers if there are less than 20 data points and lines otherwise. line : dictionary containing the line parameters. The most important parameters are: color : the color of the line width : the width of the line","title":"Line, Scatter and Shape Plots"},{"location":"Programming/Python/Plotly%20Manual/#shapes","text":"documentation The shapes can be created by supplying the coordinates of the shape to the go.Scatter function and setting the fill parameter to \"toself\" .","title":"Shapes"},{"location":"Programming/Python/Plotly%20Manual/#create-subplots","text":"documentation and examples To create a figure with multiple subplots, we use the make_subplots function. Example: fig = make_subplots(rows=2, cols=2, start_cell=\"bottom-left\") fig.add_trace(go.Scatter(x=[1, 2, 3], y=[4, 5, 6]), row=1, col=1) ... fig.show() Importanta parameters: shared_xaxes and shared_yaxes : if True , the subplots will share the same x and y axes. x_title and y_title : the titles of the x and y axes. horizontal_spacing and vertical_spacing : the spacing between the subplots.","title":"Create subplots"},{"location":"Programming/Python/Plotly%20Manual/#3d-plots","text":"","title":"3D plots"},{"location":"Programming/Python/Plotly%20Manual/#common-parameters","text":"","title":"Common Parameters"},{"location":"Programming/Python/Plotly%20Manual/#setting-color-of-the-plots-with-continuous-color-scale","text":"If the plot type is continuous (e.g., surface plot, cone plot), we cannot set a color for the whole trace as a continuous color scale is used. However, we can set the colorscale using the colorscale parameter. Example: fig.add_trace(go.Surface(z=z, colorscale=\"Viridis\")) The color scale can be also set manually as a list of colors. This way, we can overcome the limitation of the continuous color scale and set the color of the whole trace. Example: fig.add_trace(go.Surface(z=z, colorscale=[[0, \"red\"], [1, \"red\"]]))","title":"Setting color of the plots with continuous color scale"},{"location":"Programming/Python/Plotly%20Manual/#3d-scatter-plot_1","text":"documentation The 3D scatter plot is created using the go.Scatter3d function. Example: go.Scatter3d(x, y, z, ...)","title":"3D Scatter Plot"},{"location":"Programming/Python/Plotly%20Manual/#surface-plots","text":"There are multiple types of plots that may be considered as surface plots in plotly: 3D surface ( go.Surface ): a surface plot defined by a grid of x, y, and a 2D array of z values of the shape (|x|, |y|). The surface is plotted for all combinations of x and y, which makes this function suitable only for cases where the surface is defined for all combinations of x and y. 3D mesh ( go.Mesh3d ): a surface plot defined by vertices (points) and faces (connections between the points). The connections are defined by triangles, which can be either defined manually or computed automatically using a triangulation algorithm. Tri-surface ( figure_factory.create_trisurf ): a surface plot created by a triangulation of the data points. It accepts triangles in a single argument. I have to further investigate, how it differs from the 3D mesh plot.","title":"Surface Plots"},{"location":"Programming/Python/Plotly%20Manual/#3d-surface-plot","text":"documentation The 3D surface plot is created using the go.Surface function.","title":"3D Surface Plot"},{"location":"Programming/Python/Plotly%20Manual/#3d-mesh-plot","text":"documentation The 3D mesh plot is created using the go.Mesh3d function. Example: x = [1, 1, 1] y = [1, 2, 1] z = [0, 0, 1] fig.add_trace(go.Mesh3d(x=x, y=y, z=z)) This way, the triangulation is used to compute the connections between the points. If we want to use a different triangulation, we can use the i , j and k parameters. These parameters define the indices (in the source data used for x , y , and z ) of the vertices that form the triangles. For example, i[0] , j[0] and k[0] define the vertices of the first triangle. Example code: x = [1, 1, 1] y = [1, 2, 1] z = [0, 0, 1] i = [0] j = [1] k = [2] fig.add_trace(go.Mesh3d(x=x, y=y, z=z, i=i, j=j, k=k)) Note that here, the i , j and k parameters are redundant, as there is only one possible triangulation of the three points. However, for more complex surfaces, there can be multiple valid triangulations, and the i , j and k parameters can be used to select the triangulation manually.","title":"3D Mesh Plot"},{"location":"Programming/Python/Plotly%20Manual/#cone-plots","text":"documentation reference","title":"Cone Plots"},{"location":"Programming/Python/Plotly%20Manual/#customizing-the-figure","text":"The figure object can be custommized in many ways.","title":"Customizing the Figure"},{"location":"Programming/Python/Plotly%20Manual/#size-and-margins","text":"The size and margins can be set using the figure.update_layout function. The specific parameters are: width : the width of the figure in pixels height : the height of the figure in pixels autosize : needs to be False if we want to set the width and height manually margin : dictionary containing the margins ( l , r , t , b ) and one another property: pad . All properties are in pixels. l , r , t , b : distance between the plot and the figure border. Note that titles are not included in the plot, so we have make space for the titles if we set margins manually. pad : distance between the plotting area (i.e., the plotting coordinates for data points) and the axis lines. Most of the time, this should be set to 0 (default) Unfortunately, there is no way how to set the margins automatically to fit all content like titles, annotations, etc. documentation reference","title":"Size and margins"},{"location":"Programming/Python/Plotly%20Manual/#customize-axes","text":"For customizing the axes, we can use the figure.update_xaxes and figure.update_yaxes functions. By default, the functions will update all axes. To update only a specific axis, we can use the row and col parameters. axis reference The range of the axis is determined automatically as the range of the data plus some margin. If we want any other range, we need to set it manually using the range parameter, e.g.: range=[0, 1] . Unfortunately, there is no way how to automatically set the range to match the data range exactly Another thing we usually want to customize are the ticks. Important tick parameters are: dtick : the distance between the ticks tickvals : the exact values of the ticks. This overrides the dtick parameter. ticks : the position of the ticks. Can be outside , inside or \"\" (no ticks, default). ticklen : the length of the ticks in pixels tickformat : the format of the tick labels. Depending on the axis datatype, we can use number formats (e.g., \".2f\" for two decimal places), datetime formats (e.g., \"%Y-%m-%d\" for dates) or scientific notation (e.g., \"e\" for scientific notation). for percentage, we can use \".0%\" for integer percentage and \".1%\" for one decimal place. Note that this way, the % sign is added automatically to each tick label. If we do not want this, we can either set the text manually using the ticktext parameter, or multiply the data by 100. tickangle : the angle of the tick labels in degrees Other important parameters are: title_text : the title of the axis. linecolor : the color of the axis line gridcolor : the color of the grid lines mirror : if True , the axis line will be mirrored to the other side of the plot","title":"Customize axes"},{"location":"Programming/Python/Plotly%20Manual/#drawing-borders-using-axes","text":"The borders can be drawn using the axes. To draw a black border around the plot, we can use the following code: fig.update_xaxes(linecolor=\"black\", mirror=True) fig.update_yaxes(linecolor=\"black\", mirror=True)","title":"Drawing borders using axes"},{"location":"Programming/Python/Plotly%20Manual/#customizing-datetime-axes","text":"Unfortunately, we cannot treat the datetime axes as expected, i.e., using the datetime objects. For example, to set the tick interval, we cannot use the datetime.timedelta object. Instead, we need to use the number of milliseconds. Example: fig.update_xaxes(dtick=1000 * 60 * 60 * 24 * 7) # one week interval","title":"Customizing datetime axes"},{"location":"Programming/Python/Plotly%20Manual/#tick-markers-and-shared-axes","text":"By default, only tick labels are shared between subplots with shared axes. To share the tick markers, we need to hide them manually and add them to the first subplot. Example: fig.update_yaxes(showticklabels=False) fig.update_yaxes(showticklabels=True, row=1, col=1)","title":"Tick markers and shared axes"},{"location":"Programming/Python/Plotly%20Manual/#3d-axes","text":"documentation Unfortunately, the customization of the 3d axes works differently than for the 2d axes. The update_xaxes and update_yaxes functions do not work for the 3d axes. Instead, we need to use the update_layout function with the scene parameter. Example: fig.update_layout(scene=dict( xaxis=dict( title=\"x\", titlefont_size=16, tickfont_size=14, ), yaxis=dict( title=\"y\", titlefont_size=16, tickfont_size=14, ), zaxis=dict( title=\"z\", titlefont_size=16, tickfont_size=14, ), )) One thing that is not possible to customize idn 3D is the position of the axis . The x and y axes are always in the bottom, while the z axis is always on the left.","title":"3d axes"},{"location":"Programming/Python/Plotly%20Manual/#change-position-of-the-axis-title","text":"Unfortunately, there is no way how to change the position of the axis title. The solution is to hide the title and add a new annotation with the title text. Example: fig.update_xaxes(title_text=\"\") fig.add_annotation( text=\"Comp. time relative to IH\", xref=\"paper\", yref=\"paper\", x=-0.09, y=0.5, showarrow=False, font=dict( size=14, ), textangle=270, )","title":"Change position of the axis title"},{"location":"Programming/Python/Plotly%20Manual/#legend_1","text":"documentation reference The legend can be styled using the figure.update_layout function. The most important parameters are: legend_title : the title of the legend legend : dictionary containing many parameters orientation : h or v for horizontal or vertical legend x , y : the position of the in normalized coordinates of the whole plot. 'xref', yref : the coordinate system of the x and y coordinates. Can be \"container\" : the whole plot \"paper\" (default): the plotting area xanchor , yanchor : the position of the legend box relative to the x and y coordinates title : if string, the title of the legend (equivalent to the legend_title parameter). If dictionary, multiple legend title parameters can be set. bordercolor : the color of the legend border borderwidth : the width of the legend border Unfortunately, there is no way how to customize the padding between the legend items and the legend box . Also, it is not possible to set the legend border to have rounded corners .","title":"Legend"},{"location":"Programming/Python/Plotly%20Manual/#hide-legend","text":"To hide the legend, we can use the showlegend parameter. Example: fig.update_layout(showlegend=False)","title":"Hide legend"},{"location":"Programming/Python/Plotly%20Manual/#legend-items-order","text":"The order of the legend items is determined by the order of the traces in the figure. However, we can change the order using the legend_traceorder parameter. Example: fig.update_layout(legend_traceorder=\"reversed\")","title":"Legend items order"},{"location":"Programming/Python/Plotly%20Manual/#legend-position","text":"The coordinates of the legend are normalized with respect to each axis. By default, the legend x and y coordinates are set so that the legend is outside the plot, in the top right corner. If the legend is positioned inside the plot, the plot expands to the whole width, but if the legend is positioned outside the plot, the plot width is smaller to leave space for the legend.","title":"Legend position"},{"location":"Programming/Python/Plotly%20Manual/#legend-items-text","text":"The legend item text is determined by the name parameter of the trace. Therefore, to customize the legend item text, we need to set the name parameter of the trace. For normal single trace functions, this is simple: fig.add_trace(go.Scatter(x=x, y=y, name=\"Custom name\")) However, it can be complicated for plotly express functions that plot multiple traces at once, as these determine the name parameter automatically from data. For example, when we use the color parameter, the name parameter is set to the color value. To overcome this, we have two options: set the name parameter manually for each trace after the figure is created change the data so that the name parameter is set automatically to the desired value. The first approach is usually preferable as we do not mix the data and appearance. To change the name parameter, we can use the update_traces function: for trace in fig.data: trace.name = process_name(trace.name)","title":"Legend items text"},{"location":"Programming/Python/Plotly%20Manual/#figure-annotations","text":"documentation For adding annotations to the whole Figure, we can use the add_annotation function. Important parameters: x , y : the x and y coordinates of the annotation text : the text of the annotation xref , yref : the coordinate system of the x and y coordinates. Can be \"paper\" or \"data\" . showarrow : if True , an arrow will be added to the annotation textangle : the angle of the text in degrees, bgcolor : the background color of the annotation By default the annotation is meant to annotate the data. Therefore, the x and y coordinates use the coordinate system of the data (x and y axes). To align the annotation with respect to the whole figure, we need to set the xref and yref parameters to \"paper\" . In this case, the x and y coordinates are in the range [0, 1] and the origin is the bottom left corner of the figure.","title":"Figure Annotations"},{"location":"Programming/Python/Plotly%20Manual/#annotations-in-facet-plots","text":"When using facet plots, the annotations are added to the whole figure. Therefore, the x and y coordinates are in the range [0, 1] and the origin is the bottom left corner of the figure. To align the annotation with respect to the subplot, we need to set the xref and yref parameters to the x and y axes of the subplot. Example: fig.add_annotation(x=0.5, y=0.5, text=\"Title\", xref=\"x5\", yref=\"y5\", showarrow=False) Unfortunately, there is no way how to set the xref and yref parameters automatically . Therefore, we need to compute them manually for each annotation.","title":"Annotations in facet plots"},{"location":"Programming/Python/Plotly%20Manual/#markers","text":"documentation To style the markers, we can use the update_traces function. Example: fig.update_traces(marker=dict(size=10, line=dict(width=2, color='DarkSlateGrey'))) Marker parameters: size : the size of the marker in pixels line : dictionary containing border parameters width : the width of the border in pixels. The default is 0. This setting is ignored for 3d scatter plots, where the border width is always 1 (see github issue ) color : the color of the border color : the color of the marker. To make the marker transparent, it is best to use the rgba format. Example: rgba(255, 0, 0, 0.5) for a red marker with 50% transparency.","title":"Markers"},{"location":"Programming/Python/Plotly%20Manual/#adding-a-marker-to-a-hard-coded-location","text":"To add a marker to a hard-coded location, we can add it as a new trace. Note that we can add new traces even to a figure created using plotly express.","title":"Adding a marker to a hard-coded location"},{"location":"Programming/Python/Plotly%20Manual/#lines","text":"Lines can be styled using the line parameter of the plotting functions or using the update_traces function. Important parameters: color : the color of the line width : the width of the line in pixels dash : the dash pattern. Can be \"solid\" (default) \"dot\" : dense dashed line \"dash\" : sparse dashed line ...","title":"Lines"},{"location":"Programming/Python/Plotly%20Manual/#title","text":"documentation The title can be set using the plotly express functions or when creating the graph objects figure. To update the text or to customize the title layout, we can use the update_layout function with the title object parameter. Example: fig.update_layout( title={ 'x': 0.5, 'xanchor': 'center', 'y': 0.85, 'yanchor': 'top' } ) Important parameters: x , y : the x and y coordinates of the title. The origin is the bottom left corner of the figure. xanchor , yanchor : the position of the title relative to the x and y coordinates. Can be left , center or right for xanchor and top , middle or bottom for yanchor .","title":"Title"},{"location":"Programming/Python/Plotly%20Manual/#subplot-titles","text":"","title":"Subplot titles"},{"location":"Programming/Python/Plotly%20Manual/#automatic-subplot-titles-facet-plots","text":"When using facet plots, the subplot titles are generated automatically as <facet column name>=<facet column value> . Usually, we want to remove the column name and keep only the value. To do this, we can use the for_each_annotation function. Example: fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))","title":"Automatic subplot titles (facet plots)"},{"location":"Programming/Python/Plotly%20Manual/#individual-subplot-titles","text":"To set the title of an individual subplot, we can use the subplot_titles parameter of the make_subplots function.","title":"Individual subplot titles"},{"location":"Programming/Python/Plotly%20Manual/#z-order-of-the-traces","text":"The z-order of the traces cannot be configured. Instead, the traces are drawn in the order they are added to the figure. This simple rule has an exception: the webGL traces are always drawn on top of the standard traces. Because all plotly express traces are WebGL traces, they are drawn on top of the graph objects traces added later if those are not WebGL. To overcome this limitation, we have two options: not combine plotly express and graph objects traces and convert everything to graph objects for figures with custom traces use the webGL versions of the graph objects traces. Example: python fig.add_trace(go.Scattergl(...))","title":"Z-Order of the Traces"},{"location":"Programming/Python/Plotly%20Manual/#other-layout-parameters-background-borders-etc","text":"background color: plot_bgcolor border: borders are best drawn by showing and mirroring the axes (see the axis section above).","title":"Other Layout Parameters (background, borders, etc.)"},{"location":"Programming/Python/Plotly%20Manual/#text","text":"documentation","title":"Text"},{"location":"Programming/Python/Plotly%20Manual/#subscript-and-superscript","text":"To add a subscript or superscript to a text, we can use HTML tags. Example: fig.add_annotation(x=0.5, y=0.5, text=\"Title<sub>subscript</sub>\", xref=\"x5\", yref=\"y5\", showarrow=False)","title":"Subscript and superscript"},{"location":"Programming/Python/Plotly%20Manual/#bold-and-italic","text":"To add bold or italic text, we can use the HTML tags <b> and <i> . Example: fig.add_annotation(x=0.5, y=0.5, text=\"Title<b>bold</b><i>italic</i>\", xref=\"x5\", yref=\"y5\", showarrow=False)","title":"Bold and italic"},{"location":"Programming/Python/Plotly%20Manual/#math-symbols","text":"To add math symbols, we can use the LaTeX syntax. Example: fig.add_annotation(x=0.5, y=0.5, text=r\"Title$ \\alpha $ and $ \\beta $\", xref=\"x5\", yref=\"y5\", showarrow=False)","title":"Math symbols"},{"location":"Programming/Python/Plotly%20Manual/#exporting-the-figure","text":"documentation The static export is handeled using the figure's write_image function. Example: fig.write_image(\"figure.png\") The output format is determined by the extension. The margins of the figure should be set for the figure itself, not for the export.","title":"Exporting the Figure"},{"location":"Programming/Python/Plotly%20Manual/#export-hangs-out","text":"It can be cause by kaleido. The solution si to install an older version, specifically 0.1.0.post1 . https://community.plotly.com/t/static-image-export-hangs-using-kaleido/61519/4","title":"Export hangs out"},{"location":"Programming/Python/Plotly%20Manual/#colors","text":"documentation For colors in plotly, we can use the color scales supplied by plotly express. These are: px.colors.sequential : sequential color scales px.colors.diverging : diverging color scales px.colors.qualitative : qualitative color scales","title":"Colors"},{"location":"Programming/Python/Python%20Debugging/","text":"Pycharm has a build in debugger, however, there are some tricky problems described below. Breaking on Exception \u00b6 Breaking on exception is one of the most important debugger tools. However, there are some problems with Pycharm exception debugging. Breake on Termination vs on Raise \u00b6 By default, the program breakes on termination (unhandled exception). This is usually a correct configuration. However, in jupyter, all exceptions are caught to not break the jupyter itself. Therefore, in jupyter, all exceptions are ignored by the debugger if the breakpoins are set to break on termination. To break on exceptions in jupyter, we have to breake on raise. By this setting, however, we stop even on expected/handeled exceptions, stoping potentially on hundereds breakpoints in library code. Another issue is with the setting itself. To propagate the change between breaking on raise/termination, we have to deactivate and then activate again the exception breakpoints, otherwise, the setting is ignored.","title":"Python Debugging"},{"location":"Programming/Python/Python%20Debugging/#breaking-on-exception","text":"Breaking on exception is one of the most important debugger tools. However, there are some problems with Pycharm exception debugging.","title":"Breaking on Exception"},{"location":"Programming/Python/Python%20Debugging/#breake-on-termination-vs-on-raise","text":"By default, the program breakes on termination (unhandled exception). This is usually a correct configuration. However, in jupyter, all exceptions are caught to not break the jupyter itself. Therefore, in jupyter, all exceptions are ignored by the debugger if the breakpoins are set to break on termination. To break on exceptions in jupyter, we have to breake on raise. By this setting, however, we stop even on expected/handeled exceptions, stoping potentially on hundereds breakpoints in library code. Another issue is with the setting itself. To propagate the change between breaking on raise/termination, we have to deactivate and then activate again the exception breakpoints, otherwise, the setting is ignored.","title":"Breake on Termination vs on Raise"},{"location":"Programming/Python/Python%20Manual/","text":"Basic Data types \u00b6 Numbers \u00b6 Python has the following numeric types: int - integer float - floating point number The int type is unlimited, i.e., it can represent any integer number. The float type is limited by the machine precision, i.e., it can represent only a finite number of real numbers. Check whether a float number is integer \u00b6 To check whether a float number is integer, we can use the is_integer function: Check whether a number is NaN \u00b6 To check whether a number is NaN, we can use the math.isnan function or the numpyp.isnan function: Strings \u00b6 Strings in Python can be enclosed in single or double quotes (equivalent). The triple quotes can be used for multiline strings. String formatting \u00b6 The string formatting can be done in several ways: using the f prefix to string literal: f'{<VAR>}' using the format method: '{}'.format(<VAR>) Each variable can be formatted for that, Python has a string formatting mini language . The format is specified after the : character (e.g., f'{47:4}' set the width of the number 47 to 4 characters). Most of the format specifiers have default values, so we can omit them (e.g., f'{47:4}' is equivalent to f'{47:4d}' ). The following are the most common options: To use the character { and } in the string, we have to escape them using double braces: {{ and }} . String methods \u00b6 capitalize : capitalize the first letter of the string lower : convert the string to lowercase upper : convert the string to uppercase strip : remove leading and trailing whitespaces lstrip : remove leading whitespaces rstrip : remove trailing whitespaces Checking the type \u00b6 To check the exact type: if type(<VAR>) is <TYPE>: # e.g. if type(o) is str: To check the type in the polymorphic way, including the subtypes: if isinstance(<VAR>, <TYPE>): # e.g. if isinstance(o, str): Conditions and boolean context \u00b6 Comparison operators \u00b6 Python uses the standard set of comparison operators ( == , != , < , > , <= , >= ). They are functionally similar to C++ operators: they can be overloaded and the semantic meaning of == is equality, not identity (in contrast to Java). Automatic conversion to bool \u00b6 Unlike in other languages, any expression can be used in boolean context in python, as there are rules how to convert any type to bool . The following statement is valid, foor example: s = 'hello' if s: print(s) The code above prints 'hello', as the variable s evaluates to True . Any object in Python evaluates to True , with exeption of: False None numerically zero values (e.g., 0 , 0.0 ) standard library types that are empty (e.g., empty string, list , dict ) The automatic conversion to bool in boolean context has some couner intuitive consequences. The following conditions are not equal: s = 'hello' if s: # s evaluates to True if s == True: # the result of s == True is False, then False evaluete to False Functions \u00b6 Argument unpacking \u00b6 if we need to conditionaly execute function with a different set of parameters (supposed the function has optional/default parameters), we can avoid multiple function calls inside the branching tree by using argument unpacking. Suppose we have a function with three optional parameters: a , b , c . If we skip only last n parameters, we can use a list for parameters and unpack it using * : def call_me(a, b, c): ... l = ['param A', True] call_me(*l) # calls the function with a = 'param A' and b = True If we need to skip some parameters in the middle, we have to use a dict and unpack it using ** : d = {'c': 142} call_me(**d) # calls the function with c = 142 # String formatting To format python strings we can use the format function of the string or the equivalen fstring: ```Python a = 'world' message = \"Hello {} world\".format(a) message = f\"Hello {a}\" # equivalent If we need to a special formatting for a variable, we can specify it behind : as we can see in the following example that padds the number from left: uid = '47' message = \"Hello user {:0>4d}\".format(a) # prints \"Hello user 0047\" message = f\"Hello {a:0>4d}\" # equivalent More formating optios can be found in the Python string formatting cookbook . Classes \u00b6 Official Manual Classes in Python are defined using the class keyword: class MyClass: ... Unlike in other languages, we only declare the function members, other members are declared in the constructor or even later. Constructor \u00b6 The constructor is a special function named __init__ . Usually, non-function members are declared in the constructor: class MyClass: def __init__(self, a, b): self.a = a self.b = b self.c = 0 self.d = None Constructor overloading \u00b6 Python does not support function overloading, including the constructor. That is unfortunate as default arguments are less powerfull mechanism. For other functions, we can supplement overloading using a function with a different name. However, for the constructor, we need to use a different approach. The most clean way is to use a class method as a constructor. Example: class MyClass: def __init__(self, a, b = 0): self.a = a self.b = b self.c = 0 self.d = None @classmethod def from_a(cls, b): return cls(0, b) Built-in data structures and generators \u00b6 Python has several built-in data structures, most notably list , tuple , dict , and set . These are less efficient then comparable structures in other languages, but they are very convenient to use. Also, there is a special generator type. It does not store the data it is only a convinient way how to access data generated by some function. Generator \u00b6 Python wiki Generators are mostly used in the iteration, we can iterte them the same way as lists. To get the first item of the generator, we can use the next function: g = (x for x in range(10)) first = next(g) # 0 To create a generator function (a function that returns a generator), we can use the yield keyword. The following function returns a generator that yields the numbers 1, 2, and 3: def gen(): yield 1 yield 2 yield 3 The length of the generator is not known in advance, to get the length, we have to iterate the generator first, for example using len(list(<generator>)) Dictionary \u00b6 Official Manual Disctionaries are initialized using curly braces ( {} ) and the : operator: d = { 'key1': 'value1', 'key2': 'value2', ... } Two dictionaries can be merged using the | operator: d3 = d1 | d2 Comprehensions \u00b6 In addition to literals, Python has a convinient way of creating basic data structures: the comprehensions. The basic syntax is: <struct var> = <op. brace> <member var expr.> for <member var> in <iterable><cl. brace> As for literals, we use square braces ( [] ) for lists, curly braces ( {} ) for sets, and curly braces with colons for dictionaries. In contrast, we get a generator expression when using round braces ( () ), not a tuple. We can also use the if keyword to filter the elements: a = [it for it in range(10) if it % 2 == 0] # [0, 2, 4, 6, 8] Sorting \u00b6 Official Manual For sorting, you can use the sorted function. Instead of using comparators, Python has a different concept of key functions for custom sorting. The key function is a function that is applied to each element before sorting. For any expected object, the key function should return a value that can be compared. Complex sorting using tuples \u00b6 If we need to apply some complex sorting, we can use tuples as the key function return value. The tuples have comparison operator defined, the implementation is as follows: elements are compared one by one on first non-equal element, the comparison result is returned This way, we can implement a complex sorting that would normaly require several conditions by storing the condition results in the tuple. Slices \u00b6 Many Python data structures support slicing: selecting a subset of elements. The syntax is: <object>[<start>:<end>:<step>] The start and end are inclusive. The step is optional and defaults to 1. The start is also optional and defaults to 0. Instead of omitting the start and end , we can use the None keyword: a = [1, 2, 3, 4, 5] a[None:3] # [1, 2, 3] Sometimes, it is not possible to use the slice syntax: when we need to use a variable for the step or, when the object use the slice syntax for something else, e.g., for selecting columns in a Pandas dataframe. In such cases, we can use the slice object: a[0:10:2] s = slice(0, 10, 2) a[s] # equivalent Here, the parameters can be ommited as well. We can select everything by using slice(None) , which is equivalent to slice(None, None, None) . Named tuples \u00b6 Apart from the standard tuple, Python has a named tuple class that can be created using the collections.namedtuple function. In named tuple, each member has a name and can be accessed using the dot operator: from collections import namedtuple Point = namedtuple('Point', ['x', 'y']) p = Point(1, 2) print(p.x) # 1 Exceptions \u00b6 documentation Syntax: try: <code that can raise exception> except <ERROR TYPE> as <ERROR VAR>: <ERROR HANDELING> Date and time \u00b6 Python documentation The base object for date and time is datetime datetime construction \u00b6 The datetime object can be directly constructed from the parts: from daterime import datetime d = datetime(2022, 12, 20, 22, 30, 0) # 2022-12-20 22:30:00 The time part can be ommited. We can load the datetime from string using the strptime function: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') For all possible time formats, check the strftime cheatsheet Accessing the parts of datetime \u00b6 The datetime object has the following attributes: year month day hour minute second We can also query the day of the week using the weekday() method. The day of the week is represented as an integer, where Monday is 0 and Sunday is 6. Intervals \u00b6 There is also a dedicated object for time interval named timedelta . It can be constructed from parts (seconds to days), all parts are optional. We can obtain a timedelta by substracting a datetime from another datetime : d1 = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') d2 = datetime.strptime('2022-05-20 18:30', '%Y-%m-%d %H:%M') interval = d2 - d1 # 30 minutes We can also add or substract a timedelta object from the datetime object: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') interval = timedelta(hours=1) d2 = d + interval # '2022-05-20 19:00' Converting to Unix timestamp \u00b6 To convert a datetime object to unix timestamp, we can use the timestamp method. It returns the number of seconds since the epoch (1.1.1970 00:00:00). Note however, that the timestamp is computed based on the datetime object's timezone, or your local timezone if the datetime object has no timezone information. Filesystem \u00b6 There are three ways commonlz used to work with filesystem in Python: manipulate paths as strings os.path pathlib The folowing code compares both approaches for path concatenation: # string path concatenation a = \"C:/workspace\" b = \"project/file.txt\" c = f\"{a}/{b}\" # os.path concatenation a = \"C:/workspace\" b = \"project/file.txt\" c = os.path.join(a, b) # pathlib concatentation a = Path(\"C:/workspace\") b = Path(\"project/file.txt\") c = a / b As the pathlib is the most modern approach, we will use it in the following examples. Appart from pathlib documentation, there is also a cheat sheet available on github . Path editing \u00b6 Computing relative path \u00b6 To prevent misetakes, it is better to compute relative paths beteen directories than to hard-code them. Fortunately, there are methods we can use for that. If the desired relative path is a child of the start path, we can simply use the relative_to method of the Path object: a = Path(\"C:/workspace\") b = Path(\"C:/workspace/project/file.txt\") rel = b.relative_to(a) # rel = 'project/file.txt' However, if we need to go back in the filetree, we need a more sophisticated method from os.path : a = Path(\"C:/Users\") b = Path(\"C:/workspace/project/file.txt\") rel = os.path.relpath(a, b) # rel = '../Workspaces/project/file.txt' Get parent directory \u00b6 We can use the parent property of the Path object: p = Path(\"C:/workspace/project/file.txt\") parent = p.parent # 'C:\\\\workspace\\\\project' Absolute and canonical path \u00b6 We can use the absolute method of the Path object to get the absolute path. To get the canonical path, we can use the resolve method. Splitting paths and working with path parts \u00b6 To read the file extension , we can use the suffix property of the Path object. The property returns the extension with the dot . To change the extension , we can use the with_suffix method: p = Path(\"C:/workspace/project/file.txt\") p = p.with_suffix('.csv') # 'C:\\\\workspace\\\\project\\\\file.csv' To remove the extension , just use the with_suffix method with an empty string. We can split the path into parts using the parts property: p = Path(\"C:/workspace/project/file.txt\") parts = p.parts # ('C:\\\\', 'workspace', 'project', 'file.txt') To find the index of some specific part, we can use the index method: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 Later, we can use the index to manipulate the path: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 p = Path(*p.parts[:index]) # 'C:\\\\workspace' Changing path separators \u00b6 To change the path separators to forward slashes, we can use the as_posix and method: p = Path(r\"C:\\workspace\\project\\file.txt\") p = p.as_posix() # 'C:/workspace/project/file.txt' Working directory \u00b6 os.getcwd() - get the current working directory os.chdir(<path>) - set the current working directory Iterating over files \u00b6 The pathlib module provides a convenient way to iterate over files in a directory. The particular methods are: iterdir - iterate all files and directories in a directory glob - iterate over files in a single directory, using a filter rglob - iterate over files in a directory and all its subdirectories, using a filter In general, the files will be sorted alphabetically. Single directory iteration \u00b6 Using pathlib, we can iterate over files using a filter with the glob method: p = Path(\"C:/workspace/project\") for filepath in p.glob('*.txt') # iterate over all txt files in the project directory The old way is to use the os.listdir method: p = Path(\"C:/workspace/project\") for filename in os.listdir(p): if filename.endswith('.txt'): filepath = p / filename Recursive iteration \u00b6 Using pathlib, we can iterate over files using a filter with the rglob method: p = Path(\"C:/workspace/project\") for filepath in p.rglob('*.txt') # iterate over all txt files in the project directory and all its subdirectories The old way is to use the os.walk method: p = Path(\"C:/workspace/project\") for root, dirs, files in os.walk(p): for filename in files: if filename.endswith('.txt'): filepath = Path(root) / filename Iterate only directories/files \u00b6 There is no specific filter for files/directories, but we can use the is_file or is_dir method to filter out directories: p = Path(\"C:/workspace/project\") for filepath in p.glob('*'): if filepath.is_file(): # do something Use more complex filters \u00b6 Unfortunately, the glob and rglob methods do not support more complex filters (like regex). However, we can easily apply the regex filter manually: p = Path(\"C:/workspace/project\") for filepath in p.glob('*'): if not re.match(r'^config.yaml$', filepath.name): # do something Get the path to the current script \u00b6 Path(__file__).resolve().parent Checking write permissions for a directory \u00b6 Unfortunatelly, most of the methods for checking write permissions are not reliable outside Unix systems. The most reliable way is to try to create a file in the directory: p = Path(\"C:/workspace/project\") try: with open(p / 'test.txt', 'w') as f: pass p.unlink() return True except PermissionError: return False except: raise # re-raise the exception Other methods like os.access or using tempfile module are not reliable on Windows (see e.g.: https://github.com/python/cpython/issues/66305). Deleting files and directories \u00b6 To delete a file, we can use the unlink method of the Path object: p = Path(\"C:/workspace/project/file.txt\") p.unlink() for deleting directories, we can use the rmdir method: p = Path(\"C:/workspace/project\") p.rmdir() However, the rmdir method can delete only empty directories. To delete a directory with content, we can use the shutil module: p = Path(\"C:/workspace/project\") shutil.rmtree(p) Deleting Windows read-only files (i.e. Access Denied error) \u00b6 On Windows, all the delete methods can fail because lot of files and directories are read-only. This is not a problem for most application, but it breaks Python delete methods. One way to solve this is to handle the error and change the attribute in the habdler. Example for shutil: import os import stat import shutil p = Path(\"C:/workspace/project\") shutil.rmtree(p, onerror=lambda func, path, _: (os.chmod(path, stat.S_IWRITE), func(path))) I/O \u00b6 CSV \u00b6 Official Manual The csv module provides a Python interface for working with CSV files. The basic usage is: import csv with open('file.csv', 'r') as f: reader = csv.reader(f) for row in reader: # do something Reader parameters: delimiter - the delimiter character HDF5 \u00b6 HDF5 is a binary file format for storing large amounts of data. The h5py module provides a Python interface for working with HDF5 files. An example of reading a dataset from an HDF5 file on SO Command line arguments \u00b6 The sys module provides access to the command line arguments. They are stored in the argv list with the first element being the name of the script. Logging \u00b6 Official Manual A simple logging configuration: import logging logging.basicConfig( level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s', handlers=[ logging.FileHandler(\"log.txt\"), logging.StreamHandler() ] ) The logging itself is then done using the logging module methods: logging.info(\"message\") logging.warning(\"message %s\", \"with parameter\") Type hints \u00b6 Official Manual Type hints are a useful way to help the IDE and other tools to understand the code so that they can provide better support (autocompletion, type checking, refactoring, etc.). The type hints are not enforced at runtime, so they do not affect the performance of the code. We can specify the type of a variable using the : operator: a: int = 1 Apart from the basic types, we can also use the typing module to specify more complex types: from typing import List, Dict, Tuple, Set, Optional, Union, Any a: List[int] = [1, 2, 3] We can also specify the type of a function argument and return value: def foo(a: int, b: str) -> List[int]: return [a, b] Type hints in loops \u00b6 The type of the loop variable is usually inferred by IDE from the type of the iterable. However, this sometimes fails, e.g., for zip objects. In such cases, we need to specify the type of the loop variable. However, we cannot use the : directly in the loop, but instead, we have to declare the variable before the loop : for a: int in ... # error a: int for a in ... # ok Numpy \u00b6 Initialization \u00b6 We can create the new array as: zero-filled: np.zeros(<shape>, <dtype>) ones-filled: np.ones(<shape>, <dtype>) empty: np.empty(<shape>, <dtype>) filled with a constant: np.full(<shape>, <value>, <dtype>) Sorting \u00b6 for sorting, we use the sort function. There is no way how to set the sorting order, we have to use a trick for that: a = np.array([1, 2, 3, 4, 5]) a[::-1].sort() # sort in reverse order Export to CSV \u00b6 To export the numpy array to CSV, we can use the savetxt function: np.savetxt('file.csv', a, delimiter=',') By default, the function saves values in the mathematical float format, even if the values are integers. To save the values as integers, we can use the fmt parameter: np.savetxt('file.csv', a, delimiter=',', fmt='%i') Usefull array properties: \u00b6 size : number of array items unlike len, it counts all items in the mutli-dimensional array itemsize : memory (bytes) needed to store one item in the array nbytes : array size in bytes. Should be equal to size * itemsize . Usefull functions \u00b6 Regular expressions \u00b6 In Python, the regex patterns are not compiled by default. Therefore we can use strings to store them. The basic syntax for regex search is: result = re.search(<pattern>, <string>) if result: # pattern matches group = result.group(<group index>)) # print the first group The 0th group is the whole match, as usual. Lambda functions \u00b6 Lambda functions in python have the following syntax: lambda <input parameters>: <return value> Example: f = lambda x: x**2 Only a single expression can be used in the lambda function, so we need standard functions for more complex logic (temporary variables, loops, etc.). Decorators \u00b6 Decorators are a special type of function that can be used to modify other functions. When we write an annotation with the name of a function above another function, the annotated function is decorated . It means that when we call the annotated function, a wrapper function is called instead. The wrapper function is the function returned by the decorater : the function with the same name as the annotation. If we want to also keep the original function functionality, we have to pass the function to the decorator and call it inside the wrapper function. In the following example, we create a dummy decorator that keeps the original function functionality: Example: def decorator(func): def wrapper(): result = func() return result return wrapper @decorator def my_func(): # do something return result Decorator with arguments \u00b6 If the original function has arguments, we have to pass them to the wrapper function. Example: def decorator(func): def wrapper(param_1, param_2): result = func(param_1, param_2) return result return wrapper @decorator def my_func(param_1, param_2): # do something return result Jupyter \u00b6 Memory \u00b6 Most of the time, when the memory allocated by the notebook is larger than expected, it is caused by some library objects (plots, tables...]). However sometimes, it can be forgotten user objects. To list all user objects, from the largest: # These are the usual ipython objects, including this one you are creating ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars'] # Get a sorted list of the objects and their sizes sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True) Plotting \u00b6 There are several libraries for plotting in Python. The most common are: matplotlib plotly In the table below, we can see a comparison of the most common plotting libraries: | Functionality | Matplotlib | Plotly | | --- | --- | --- | | real 3D plots | no | yes | | detail legend styling (padding, round corners...) | yes | no | Matplotlib \u00b6 Official Manual Saving figures \u00b6 To save a figure, we can use the savefig function. The savefig function has to be called before the show function, otherwise the figure will be empty . Docstrings \u00b6 For documenting Python code, we use docstrings, special comments soroudned by three quotation marks: \"\"\" docstring \"\"\" Unlike in other languages, there are multiple styles for docstring content. Progress bars \u00b6 For displaying progress bars, we can use the tqdm library. It is very simple to use: from tqdm import tqdm for i in tqdm(range(100)): ... Important parameters: desc : description of the progress bar PostgreSQL \u00b6 When working with PostgreSQL databases, we usually use either the psycopg2 adapter or, the sqlalchemy . psycopg2 \u00b6 documentation To connect to a database: con = psycopg2.connect(<connection string>) After running this code a new session is created in the database, this session is handeled by the con object. The operation to the database is then done as follows: create a cursor object which represents a database transaction Python cur = con.cursor() execute any number of SQL commands Python cur.execute(<sql>) commit the transaction Python con.commit() SQLAlchemy \u00b6 Connection documentation SQLAlchemy works with engine objects that represent the application's connection to the database. The engine object is created using the create_engine function: from sqlalchemy import create_engine engine = create_engine('postgresql://user:password@localhost:5432/dbname') A simple SELECT query can be executed using using the following code: with engine.connect() as conneciton: result = conneciton.execute(\"SELECT * FROM table\") ... With modifying statements, the situation is more complicated as SQLAlchemy uses transactions by default. Therefore we need to commit the transaction. There are two ways how to do that: using the commit method of the connection object Python with engine.connect() as conneciton: conneciton.execute(\"INSERT INTO table VALUES (1, 2, 3)\") conneciton.commit() creating a new block for the transaction using the begin method of the connection object Python with engine.connect() as conneciton: with conneciton.begin(): conneciton.execute(\"INSERT INTO table VALUES (1, 2, 3)\") this option has also its shortcut: the begin method of the engine object Python with engine.begin() as conneciton: conneciton.execute(\"INSERT INTO table VALUES (1, 2, 3)\") Note that the old execute method of the engine object is not available anymore in newer versions of SQLAlchemy. Executing statements without transaction \u00b6 By default, sqlalchemy executes sql statements in a transaction. However, some statements (e.g., CREATE DATABASE ) cannot be executed in a transaction. To execute such statements, we have to use the execution_options method: with sqlalchemy_engine.connect() as conn: conn.execution_options(isolation_level=\"AUTOCOMMIT\") conn.execute(\"<sql>\") conn.commit() Executing multiple statements at once \u00b6 To execute multiple statements at once, for example when executing a script, it is best to use the execute method of the psycopg2 connection object. Moreover, to safely handle errors, it is best to catch the exceptions and manually rollback the transaction in case of an error: conn = psycopg2.connect(<connection string>) cursor = conn.cursor() try: cursor.execute(<sql>) conn.commit() except Exception as e: conn.rollback() raise e finally: cursor.close() conn.close() Working with GIS \u00b6 When working with gis data, we usually change the pandas library for its GIS extension called geopandas . For more, see the pandas manual. Geocoding \u00b6 For geocoding, we can use the Geocoder library. Complex data structures \u00b6 KDTree \u00b6 documentation KDTree can be found in the scipy library. Geometry \u00b6 There are various libraries for working with geometry in Python: scipy.spatial : for basic geometry operations shapely geopandas : for gis data","title":"Python Manual"},{"location":"Programming/Python/Python%20Manual/#basic-data-types","text":"","title":"Basic Data types"},{"location":"Programming/Python/Python%20Manual/#numbers","text":"Python has the following numeric types: int - integer float - floating point number The int type is unlimited, i.e., it can represent any integer number. The float type is limited by the machine precision, i.e., it can represent only a finite number of real numbers.","title":"Numbers"},{"location":"Programming/Python/Python%20Manual/#check-whether-a-float-number-is-integer","text":"To check whether a float number is integer, we can use the is_integer function:","title":"Check whether a float number is integer"},{"location":"Programming/Python/Python%20Manual/#check-whether-a-number-is-nan","text":"To check whether a number is NaN, we can use the math.isnan function or the numpyp.isnan function:","title":"Check whether a number is NaN"},{"location":"Programming/Python/Python%20Manual/#strings","text":"Strings in Python can be enclosed in single or double quotes (equivalent). The triple quotes can be used for multiline strings.","title":"Strings"},{"location":"Programming/Python/Python%20Manual/#string-formatting","text":"The string formatting can be done in several ways: using the f prefix to string literal: f'{<VAR>}' using the format method: '{}'.format(<VAR>) Each variable can be formatted for that, Python has a string formatting mini language . The format is specified after the : character (e.g., f'{47:4}' set the width of the number 47 to 4 characters). Most of the format specifiers have default values, so we can omit them (e.g., f'{47:4}' is equivalent to f'{47:4d}' ). The following are the most common options: To use the character { and } in the string, we have to escape them using double braces: {{ and }} .","title":"String formatting"},{"location":"Programming/Python/Python%20Manual/#string-methods","text":"capitalize : capitalize the first letter of the string lower : convert the string to lowercase upper : convert the string to uppercase strip : remove leading and trailing whitespaces lstrip : remove leading whitespaces rstrip : remove trailing whitespaces","title":"String methods"},{"location":"Programming/Python/Python%20Manual/#checking-the-type","text":"To check the exact type: if type(<VAR>) is <TYPE>: # e.g. if type(o) is str: To check the type in the polymorphic way, including the subtypes: if isinstance(<VAR>, <TYPE>): # e.g. if isinstance(o, str):","title":"Checking the type"},{"location":"Programming/Python/Python%20Manual/#conditions-and-boolean-context","text":"","title":"Conditions and boolean context"},{"location":"Programming/Python/Python%20Manual/#comparison-operators","text":"Python uses the standard set of comparison operators ( == , != , < , > , <= , >= ). They are functionally similar to C++ operators: they can be overloaded and the semantic meaning of == is equality, not identity (in contrast to Java).","title":"Comparison operators"},{"location":"Programming/Python/Python%20Manual/#automatic-conversion-to-bool","text":"Unlike in other languages, any expression can be used in boolean context in python, as there are rules how to convert any type to bool . The following statement is valid, foor example: s = 'hello' if s: print(s) The code above prints 'hello', as the variable s evaluates to True . Any object in Python evaluates to True , with exeption of: False None numerically zero values (e.g., 0 , 0.0 ) standard library types that are empty (e.g., empty string, list , dict ) The automatic conversion to bool in boolean context has some couner intuitive consequences. The following conditions are not equal: s = 'hello' if s: # s evaluates to True if s == True: # the result of s == True is False, then False evaluete to False","title":"Automatic conversion to bool"},{"location":"Programming/Python/Python%20Manual/#functions","text":"","title":"Functions"},{"location":"Programming/Python/Python%20Manual/#argument-unpacking","text":"if we need to conditionaly execute function with a different set of parameters (supposed the function has optional/default parameters), we can avoid multiple function calls inside the branching tree by using argument unpacking. Suppose we have a function with three optional parameters: a , b , c . If we skip only last n parameters, we can use a list for parameters and unpack it using * : def call_me(a, b, c): ... l = ['param A', True] call_me(*l) # calls the function with a = 'param A' and b = True If we need to skip some parameters in the middle, we have to use a dict and unpack it using ** : d = {'c': 142} call_me(**d) # calls the function with c = 142 # String formatting To format python strings we can use the format function of the string or the equivalen fstring: ```Python a = 'world' message = \"Hello {} world\".format(a) message = f\"Hello {a}\" # equivalent If we need to a special formatting for a variable, we can specify it behind : as we can see in the following example that padds the number from left: uid = '47' message = \"Hello user {:0>4d}\".format(a) # prints \"Hello user 0047\" message = f\"Hello {a:0>4d}\" # equivalent More formating optios can be found in the Python string formatting cookbook .","title":"Argument unpacking"},{"location":"Programming/Python/Python%20Manual/#classes","text":"Official Manual Classes in Python are defined using the class keyword: class MyClass: ... Unlike in other languages, we only declare the function members, other members are declared in the constructor or even later.","title":"Classes"},{"location":"Programming/Python/Python%20Manual/#constructor","text":"The constructor is a special function named __init__ . Usually, non-function members are declared in the constructor: class MyClass: def __init__(self, a, b): self.a = a self.b = b self.c = 0 self.d = None","title":"Constructor"},{"location":"Programming/Python/Python%20Manual/#constructor-overloading","text":"Python does not support function overloading, including the constructor. That is unfortunate as default arguments are less powerfull mechanism. For other functions, we can supplement overloading using a function with a different name. However, for the constructor, we need to use a different approach. The most clean way is to use a class method as a constructor. Example: class MyClass: def __init__(self, a, b = 0): self.a = a self.b = b self.c = 0 self.d = None @classmethod def from_a(cls, b): return cls(0, b)","title":"Constructor overloading"},{"location":"Programming/Python/Python%20Manual/#built-in-data-structures-and-generators","text":"Python has several built-in data structures, most notably list , tuple , dict , and set . These are less efficient then comparable structures in other languages, but they are very convenient to use. Also, there is a special generator type. It does not store the data it is only a convinient way how to access data generated by some function.","title":"Built-in data structures and generators"},{"location":"Programming/Python/Python%20Manual/#generator","text":"Python wiki Generators are mostly used in the iteration, we can iterte them the same way as lists. To get the first item of the generator, we can use the next function: g = (x for x in range(10)) first = next(g) # 0 To create a generator function (a function that returns a generator), we can use the yield keyword. The following function returns a generator that yields the numbers 1, 2, and 3: def gen(): yield 1 yield 2 yield 3 The length of the generator is not known in advance, to get the length, we have to iterate the generator first, for example using len(list(<generator>))","title":"Generator"},{"location":"Programming/Python/Python%20Manual/#dictionary","text":"Official Manual Disctionaries are initialized using curly braces ( {} ) and the : operator: d = { 'key1': 'value1', 'key2': 'value2', ... } Two dictionaries can be merged using the | operator: d3 = d1 | d2","title":"Dictionary"},{"location":"Programming/Python/Python%20Manual/#comprehensions","text":"In addition to literals, Python has a convinient way of creating basic data structures: the comprehensions. The basic syntax is: <struct var> = <op. brace> <member var expr.> for <member var> in <iterable><cl. brace> As for literals, we use square braces ( [] ) for lists, curly braces ( {} ) for sets, and curly braces with colons for dictionaries. In contrast, we get a generator expression when using round braces ( () ), not a tuple. We can also use the if keyword to filter the elements: a = [it for it in range(10) if it % 2 == 0] # [0, 2, 4, 6, 8]","title":"Comprehensions"},{"location":"Programming/Python/Python%20Manual/#sorting","text":"Official Manual For sorting, you can use the sorted function. Instead of using comparators, Python has a different concept of key functions for custom sorting. The key function is a function that is applied to each element before sorting. For any expected object, the key function should return a value that can be compared.","title":"Sorting"},{"location":"Programming/Python/Python%20Manual/#complex-sorting-using-tuples","text":"If we need to apply some complex sorting, we can use tuples as the key function return value. The tuples have comparison operator defined, the implementation is as follows: elements are compared one by one on first non-equal element, the comparison result is returned This way, we can implement a complex sorting that would normaly require several conditions by storing the condition results in the tuple.","title":"Complex sorting using tuples"},{"location":"Programming/Python/Python%20Manual/#slices","text":"Many Python data structures support slicing: selecting a subset of elements. The syntax is: <object>[<start>:<end>:<step>] The start and end are inclusive. The step is optional and defaults to 1. The start is also optional and defaults to 0. Instead of omitting the start and end , we can use the None keyword: a = [1, 2, 3, 4, 5] a[None:3] # [1, 2, 3] Sometimes, it is not possible to use the slice syntax: when we need to use a variable for the step or, when the object use the slice syntax for something else, e.g., for selecting columns in a Pandas dataframe. In such cases, we can use the slice object: a[0:10:2] s = slice(0, 10, 2) a[s] # equivalent Here, the parameters can be ommited as well. We can select everything by using slice(None) , which is equivalent to slice(None, None, None) .","title":"Slices"},{"location":"Programming/Python/Python%20Manual/#named-tuples","text":"Apart from the standard tuple, Python has a named tuple class that can be created using the collections.namedtuple function. In named tuple, each member has a name and can be accessed using the dot operator: from collections import namedtuple Point = namedtuple('Point', ['x', 'y']) p = Point(1, 2) print(p.x) # 1","title":"Named tuples"},{"location":"Programming/Python/Python%20Manual/#exceptions","text":"documentation Syntax: try: <code that can raise exception> except <ERROR TYPE> as <ERROR VAR>: <ERROR HANDELING>","title":"Exceptions"},{"location":"Programming/Python/Python%20Manual/#date-and-time","text":"Python documentation The base object for date and time is datetime","title":"Date and time"},{"location":"Programming/Python/Python%20Manual/#datetime-construction","text":"The datetime object can be directly constructed from the parts: from daterime import datetime d = datetime(2022, 12, 20, 22, 30, 0) # 2022-12-20 22:30:00 The time part can be ommited. We can load the datetime from string using the strptime function: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') For all possible time formats, check the strftime cheatsheet","title":"datetime construction"},{"location":"Programming/Python/Python%20Manual/#accessing-the-parts-of-datetime","text":"The datetime object has the following attributes: year month day hour minute second We can also query the day of the week using the weekday() method. The day of the week is represented as an integer, where Monday is 0 and Sunday is 6.","title":"Accessing the parts of datetime"},{"location":"Programming/Python/Python%20Manual/#intervals","text":"There is also a dedicated object for time interval named timedelta . It can be constructed from parts (seconds to days), all parts are optional. We can obtain a timedelta by substracting a datetime from another datetime : d1 = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') d2 = datetime.strptime('2022-05-20 18:30', '%Y-%m-%d %H:%M') interval = d2 - d1 # 30 minutes We can also add or substract a timedelta object from the datetime object: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') interval = timedelta(hours=1) d2 = d + interval # '2022-05-20 19:00'","title":"Intervals"},{"location":"Programming/Python/Python%20Manual/#converting-to-unix-timestamp","text":"To convert a datetime object to unix timestamp, we can use the timestamp method. It returns the number of seconds since the epoch (1.1.1970 00:00:00). Note however, that the timestamp is computed based on the datetime object's timezone, or your local timezone if the datetime object has no timezone information.","title":"Converting to Unix timestamp"},{"location":"Programming/Python/Python%20Manual/#filesystem","text":"There are three ways commonlz used to work with filesystem in Python: manipulate paths as strings os.path pathlib The folowing code compares both approaches for path concatenation: # string path concatenation a = \"C:/workspace\" b = \"project/file.txt\" c = f\"{a}/{b}\" # os.path concatenation a = \"C:/workspace\" b = \"project/file.txt\" c = os.path.join(a, b) # pathlib concatentation a = Path(\"C:/workspace\") b = Path(\"project/file.txt\") c = a / b As the pathlib is the most modern approach, we will use it in the following examples. Appart from pathlib documentation, there is also a cheat sheet available on github .","title":"Filesystem"},{"location":"Programming/Python/Python%20Manual/#path-editing","text":"","title":"Path editing"},{"location":"Programming/Python/Python%20Manual/#computing-relative-path","text":"To prevent misetakes, it is better to compute relative paths beteen directories than to hard-code them. Fortunately, there are methods we can use for that. If the desired relative path is a child of the start path, we can simply use the relative_to method of the Path object: a = Path(\"C:/workspace\") b = Path(\"C:/workspace/project/file.txt\") rel = b.relative_to(a) # rel = 'project/file.txt' However, if we need to go back in the filetree, we need a more sophisticated method from os.path : a = Path(\"C:/Users\") b = Path(\"C:/workspace/project/file.txt\") rel = os.path.relpath(a, b) # rel = '../Workspaces/project/file.txt'","title":"Computing relative path"},{"location":"Programming/Python/Python%20Manual/#get-parent-directory","text":"We can use the parent property of the Path object: p = Path(\"C:/workspace/project/file.txt\") parent = p.parent # 'C:\\\\workspace\\\\project'","title":"Get parent directory"},{"location":"Programming/Python/Python%20Manual/#absolute-and-canonical-path","text":"We can use the absolute method of the Path object to get the absolute path. To get the canonical path, we can use the resolve method.","title":"Absolute and canonical path"},{"location":"Programming/Python/Python%20Manual/#splitting-paths-and-working-with-path-parts","text":"To read the file extension , we can use the suffix property of the Path object. The property returns the extension with the dot . To change the extension , we can use the with_suffix method: p = Path(\"C:/workspace/project/file.txt\") p = p.with_suffix('.csv') # 'C:\\\\workspace\\\\project\\\\file.csv' To remove the extension , just use the with_suffix method with an empty string. We can split the path into parts using the parts property: p = Path(\"C:/workspace/project/file.txt\") parts = p.parts # ('C:\\\\', 'workspace', 'project', 'file.txt') To find the index of some specific part, we can use the index method: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 Later, we can use the index to manipulate the path: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 p = Path(*p.parts[:index]) # 'C:\\\\workspace'","title":"Splitting paths and working with path parts"},{"location":"Programming/Python/Python%20Manual/#changing-path-separators","text":"To change the path separators to forward slashes, we can use the as_posix and method: p = Path(r\"C:\\workspace\\project\\file.txt\") p = p.as_posix() # 'C:/workspace/project/file.txt'","title":"Changing path separators"},{"location":"Programming/Python/Python%20Manual/#working-directory","text":"os.getcwd() - get the current working directory os.chdir(<path>) - set the current working directory","title":"Working directory"},{"location":"Programming/Python/Python%20Manual/#iterating-over-files","text":"The pathlib module provides a convenient way to iterate over files in a directory. The particular methods are: iterdir - iterate all files and directories in a directory glob - iterate over files in a single directory, using a filter rglob - iterate over files in a directory and all its subdirectories, using a filter In general, the files will be sorted alphabetically.","title":"Iterating over files"},{"location":"Programming/Python/Python%20Manual/#single-directory-iteration","text":"Using pathlib, we can iterate over files using a filter with the glob method: p = Path(\"C:/workspace/project\") for filepath in p.glob('*.txt') # iterate over all txt files in the project directory The old way is to use the os.listdir method: p = Path(\"C:/workspace/project\") for filename in os.listdir(p): if filename.endswith('.txt'): filepath = p / filename","title":"Single directory iteration"},{"location":"Programming/Python/Python%20Manual/#recursive-iteration","text":"Using pathlib, we can iterate over files using a filter with the rglob method: p = Path(\"C:/workspace/project\") for filepath in p.rglob('*.txt') # iterate over all txt files in the project directory and all its subdirectories The old way is to use the os.walk method: p = Path(\"C:/workspace/project\") for root, dirs, files in os.walk(p): for filename in files: if filename.endswith('.txt'): filepath = Path(root) / filename","title":"Recursive iteration"},{"location":"Programming/Python/Python%20Manual/#iterate-only-directoriesfiles","text":"There is no specific filter for files/directories, but we can use the is_file or is_dir method to filter out directories: p = Path(\"C:/workspace/project\") for filepath in p.glob('*'): if filepath.is_file(): # do something","title":"Iterate only directories/files"},{"location":"Programming/Python/Python%20Manual/#use-more-complex-filters","text":"Unfortunately, the glob and rglob methods do not support more complex filters (like regex). However, we can easily apply the regex filter manually: p = Path(\"C:/workspace/project\") for filepath in p.glob('*'): if not re.match(r'^config.yaml$', filepath.name): # do something","title":"Use more complex filters"},{"location":"Programming/Python/Python%20Manual/#get-the-path-to-the-current-script","text":"Path(__file__).resolve().parent","title":"Get the path to the current script"},{"location":"Programming/Python/Python%20Manual/#checking-write-permissions-for-a-directory","text":"Unfortunatelly, most of the methods for checking write permissions are not reliable outside Unix systems. The most reliable way is to try to create a file in the directory: p = Path(\"C:/workspace/project\") try: with open(p / 'test.txt', 'w') as f: pass p.unlink() return True except PermissionError: return False except: raise # re-raise the exception Other methods like os.access or using tempfile module are not reliable on Windows (see e.g.: https://github.com/python/cpython/issues/66305).","title":"Checking write permissions for a directory"},{"location":"Programming/Python/Python%20Manual/#deleting-files-and-directories","text":"To delete a file, we can use the unlink method of the Path object: p = Path(\"C:/workspace/project/file.txt\") p.unlink() for deleting directories, we can use the rmdir method: p = Path(\"C:/workspace/project\") p.rmdir() However, the rmdir method can delete only empty directories. To delete a directory with content, we can use the shutil module: p = Path(\"C:/workspace/project\") shutil.rmtree(p)","title":"Deleting files and directories"},{"location":"Programming/Python/Python%20Manual/#deleting-windows-read-only-files-ie-access-denied-error","text":"On Windows, all the delete methods can fail because lot of files and directories are read-only. This is not a problem for most application, but it breaks Python delete methods. One way to solve this is to handle the error and change the attribute in the habdler. Example for shutil: import os import stat import shutil p = Path(\"C:/workspace/project\") shutil.rmtree(p, onerror=lambda func, path, _: (os.chmod(path, stat.S_IWRITE), func(path)))","title":"Deleting Windows read-only files (i.e. Access Denied error)"},{"location":"Programming/Python/Python%20Manual/#io","text":"","title":"I/O"},{"location":"Programming/Python/Python%20Manual/#csv","text":"Official Manual The csv module provides a Python interface for working with CSV files. The basic usage is: import csv with open('file.csv', 'r') as f: reader = csv.reader(f) for row in reader: # do something Reader parameters: delimiter - the delimiter character","title":"CSV"},{"location":"Programming/Python/Python%20Manual/#hdf5","text":"HDF5 is a binary file format for storing large amounts of data. The h5py module provides a Python interface for working with HDF5 files. An example of reading a dataset from an HDF5 file on SO","title":"HDF5"},{"location":"Programming/Python/Python%20Manual/#command-line-arguments","text":"The sys module provides access to the command line arguments. They are stored in the argv list with the first element being the name of the script.","title":"Command line arguments"},{"location":"Programming/Python/Python%20Manual/#logging","text":"Official Manual A simple logging configuration: import logging logging.basicConfig( level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s', handlers=[ logging.FileHandler(\"log.txt\"), logging.StreamHandler() ] ) The logging itself is then done using the logging module methods: logging.info(\"message\") logging.warning(\"message %s\", \"with parameter\")","title":"Logging"},{"location":"Programming/Python/Python%20Manual/#type-hints","text":"Official Manual Type hints are a useful way to help the IDE and other tools to understand the code so that they can provide better support (autocompletion, type checking, refactoring, etc.). The type hints are not enforced at runtime, so they do not affect the performance of the code. We can specify the type of a variable using the : operator: a: int = 1 Apart from the basic types, we can also use the typing module to specify more complex types: from typing import List, Dict, Tuple, Set, Optional, Union, Any a: List[int] = [1, 2, 3] We can also specify the type of a function argument and return value: def foo(a: int, b: str) -> List[int]: return [a, b]","title":"Type hints"},{"location":"Programming/Python/Python%20Manual/#type-hints-in-loops","text":"The type of the loop variable is usually inferred by IDE from the type of the iterable. However, this sometimes fails, e.g., for zip objects. In such cases, we need to specify the type of the loop variable. However, we cannot use the : directly in the loop, but instead, we have to declare the variable before the loop : for a: int in ... # error a: int for a in ... # ok","title":"Type hints in loops"},{"location":"Programming/Python/Python%20Manual/#numpy","text":"","title":"Numpy"},{"location":"Programming/Python/Python%20Manual/#initialization","text":"We can create the new array as: zero-filled: np.zeros(<shape>, <dtype>) ones-filled: np.ones(<shape>, <dtype>) empty: np.empty(<shape>, <dtype>) filled with a constant: np.full(<shape>, <value>, <dtype>)","title":"Initialization"},{"location":"Programming/Python/Python%20Manual/#sorting_1","text":"for sorting, we use the sort function. There is no way how to set the sorting order, we have to use a trick for that: a = np.array([1, 2, 3, 4, 5]) a[::-1].sort() # sort in reverse order","title":"Sorting"},{"location":"Programming/Python/Python%20Manual/#export-to-csv","text":"To export the numpy array to CSV, we can use the savetxt function: np.savetxt('file.csv', a, delimiter=',') By default, the function saves values in the mathematical float format, even if the values are integers. To save the values as integers, we can use the fmt parameter: np.savetxt('file.csv', a, delimiter=',', fmt='%i')","title":"Export to CSV"},{"location":"Programming/Python/Python%20Manual/#usefull-array-properties","text":"size : number of array items unlike len, it counts all items in the mutli-dimensional array itemsize : memory (bytes) needed to store one item in the array nbytes : array size in bytes. Should be equal to size * itemsize .","title":"Usefull array properties:"},{"location":"Programming/Python/Python%20Manual/#usefull-functions","text":"","title":"Usefull functions"},{"location":"Programming/Python/Python%20Manual/#regular-expressions","text":"In Python, the regex patterns are not compiled by default. Therefore we can use strings to store them. The basic syntax for regex search is: result = re.search(<pattern>, <string>) if result: # pattern matches group = result.group(<group index>)) # print the first group The 0th group is the whole match, as usual.","title":"Regular expressions"},{"location":"Programming/Python/Python%20Manual/#lambda-functions","text":"Lambda functions in python have the following syntax: lambda <input parameters>: <return value> Example: f = lambda x: x**2 Only a single expression can be used in the lambda function, so we need standard functions for more complex logic (temporary variables, loops, etc.).","title":"Lambda functions"},{"location":"Programming/Python/Python%20Manual/#decorators","text":"Decorators are a special type of function that can be used to modify other functions. When we write an annotation with the name of a function above another function, the annotated function is decorated . It means that when we call the annotated function, a wrapper function is called instead. The wrapper function is the function returned by the decorater : the function with the same name as the annotation. If we want to also keep the original function functionality, we have to pass the function to the decorator and call it inside the wrapper function. In the following example, we create a dummy decorator that keeps the original function functionality: Example: def decorator(func): def wrapper(): result = func() return result return wrapper @decorator def my_func(): # do something return result","title":"Decorators"},{"location":"Programming/Python/Python%20Manual/#decorator-with-arguments","text":"If the original function has arguments, we have to pass them to the wrapper function. Example: def decorator(func): def wrapper(param_1, param_2): result = func(param_1, param_2) return result return wrapper @decorator def my_func(param_1, param_2): # do something return result","title":"Decorator with arguments"},{"location":"Programming/Python/Python%20Manual/#jupyter","text":"","title":"Jupyter"},{"location":"Programming/Python/Python%20Manual/#memory","text":"Most of the time, when the memory allocated by the notebook is larger than expected, it is caused by some library objects (plots, tables...]). However sometimes, it can be forgotten user objects. To list all user objects, from the largest: # These are the usual ipython objects, including this one you are creating ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars'] # Get a sorted list of the objects and their sizes sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)","title":"Memory"},{"location":"Programming/Python/Python%20Manual/#plotting","text":"There are several libraries for plotting in Python. The most common are: matplotlib plotly In the table below, we can see a comparison of the most common plotting libraries: | Functionality | Matplotlib | Plotly | | --- | --- | --- | | real 3D plots | no | yes | | detail legend styling (padding, round corners...) | yes | no |","title":"Plotting"},{"location":"Programming/Python/Python%20Manual/#matplotlib","text":"Official Manual","title":"Matplotlib"},{"location":"Programming/Python/Python%20Manual/#saving-figures","text":"To save a figure, we can use the savefig function. The savefig function has to be called before the show function, otherwise the figure will be empty .","title":"Saving figures"},{"location":"Programming/Python/Python%20Manual/#docstrings","text":"For documenting Python code, we use docstrings, special comments soroudned by three quotation marks: \"\"\" docstring \"\"\" Unlike in other languages, there are multiple styles for docstring content.","title":"Docstrings"},{"location":"Programming/Python/Python%20Manual/#progress-bars","text":"For displaying progress bars, we can use the tqdm library. It is very simple to use: from tqdm import tqdm for i in tqdm(range(100)): ... Important parameters: desc : description of the progress bar","title":"Progress bars"},{"location":"Programming/Python/Python%20Manual/#postgresql","text":"When working with PostgreSQL databases, we usually use either the psycopg2 adapter or, the sqlalchemy .","title":"PostgreSQL"},{"location":"Programming/Python/Python%20Manual/#psycopg2","text":"documentation To connect to a database: con = psycopg2.connect(<connection string>) After running this code a new session is created in the database, this session is handeled by the con object. The operation to the database is then done as follows: create a cursor object which represents a database transaction Python cur = con.cursor() execute any number of SQL commands Python cur.execute(<sql>) commit the transaction Python con.commit()","title":"psycopg2"},{"location":"Programming/Python/Python%20Manual/#sqlalchemy","text":"Connection documentation SQLAlchemy works with engine objects that represent the application's connection to the database. The engine object is created using the create_engine function: from sqlalchemy import create_engine engine = create_engine('postgresql://user:password@localhost:5432/dbname') A simple SELECT query can be executed using using the following code: with engine.connect() as conneciton: result = conneciton.execute(\"SELECT * FROM table\") ... With modifying statements, the situation is more complicated as SQLAlchemy uses transactions by default. Therefore we need to commit the transaction. There are two ways how to do that: using the commit method of the connection object Python with engine.connect() as conneciton: conneciton.execute(\"INSERT INTO table VALUES (1, 2, 3)\") conneciton.commit() creating a new block for the transaction using the begin method of the connection object Python with engine.connect() as conneciton: with conneciton.begin(): conneciton.execute(\"INSERT INTO table VALUES (1, 2, 3)\") this option has also its shortcut: the begin method of the engine object Python with engine.begin() as conneciton: conneciton.execute(\"INSERT INTO table VALUES (1, 2, 3)\") Note that the old execute method of the engine object is not available anymore in newer versions of SQLAlchemy.","title":"SQLAlchemy"},{"location":"Programming/Python/Python%20Manual/#executing-statements-without-transaction","text":"By default, sqlalchemy executes sql statements in a transaction. However, some statements (e.g., CREATE DATABASE ) cannot be executed in a transaction. To execute such statements, we have to use the execution_options method: with sqlalchemy_engine.connect() as conn: conn.execution_options(isolation_level=\"AUTOCOMMIT\") conn.execute(\"<sql>\") conn.commit()","title":"Executing statements without transaction"},{"location":"Programming/Python/Python%20Manual/#executing-multiple-statements-at-once","text":"To execute multiple statements at once, for example when executing a script, it is best to use the execute method of the psycopg2 connection object. Moreover, to safely handle errors, it is best to catch the exceptions and manually rollback the transaction in case of an error: conn = psycopg2.connect(<connection string>) cursor = conn.cursor() try: cursor.execute(<sql>) conn.commit() except Exception as e: conn.rollback() raise e finally: cursor.close() conn.close()","title":"Executing multiple statements at once"},{"location":"Programming/Python/Python%20Manual/#working-with-gis","text":"When working with gis data, we usually change the pandas library for its GIS extension called geopandas . For more, see the pandas manual.","title":"Working with GIS"},{"location":"Programming/Python/Python%20Manual/#geocoding","text":"For geocoding, we can use the Geocoder library.","title":"Geocoding"},{"location":"Programming/Python/Python%20Manual/#complex-data-structures","text":"","title":"Complex data structures"},{"location":"Programming/Python/Python%20Manual/#kdtree","text":"documentation KDTree can be found in the scipy library.","title":"KDTree"},{"location":"Programming/Python/Python%20Manual/#geometry","text":"There are various libraries for working with geometry in Python: scipy.spatial : for basic geometry operations shapely geopandas : for gis data","title":"Geometry"},{"location":"Programming/Python/Python%20Workflow/","text":"Dev Stack \u00b6 I use the following stack: the latest Python, 64 bit pip as the package manager Pycharm IDE pytest test suite Visual Studio for deugging native code Python \u00b6 Python should be installed from the official web page , not using any package manager. Steps: Dowload the 64-bit installer Run the installer, choose advanced install Include the GUI tools (Tkinter, TK) Includ *.py launcher, but only if there is no newer python version installed. If this is checked and there is a newer vesrsion of Python installed, the setup will fail. Include documentation Check download debug symbols to enable native code debugging The source code for python can be inspected on GitHub Command line \u00b6 We execute python scripts from the command line as: python <path to the .py file> . Parameters: -m executes a module as a script, e.g. python -m venv . This is useful for executing scripts whithout knowing the path to the script. Pip \u00b6 Installing Packages \u00b6 Normal packages are installed using: pip install <package name> . However, if a package uses a C/C++ backend and does not contain the compiled wheel on PyPy, this approach will fail on Windows. Instead, you have to download the wheel from the Chris Gohlke page and install it: pip install <path to wheel> . Also, you have to install the dependencies mentioned on that page. Uninstalling packages \u00b6 To uninstall a package, use pip uninstall <package name> . There is no way how to uninstall more packages using some wildcard. To uninstall more packages efficiently (not one by one): create a file with the list of all installed packages: pip freeze > packages.txt edit the file and remove all packages you want to keep uninstall all packages from the file: pip uninstall -r packages.txt -y Troubleshooting package installation \u00b6 If the installation fails, check the following: if you installed the package by name, check for the wheel on the Chris Golthke page. if you installed the package from a wheel, check the notes/requirement info on Chris Golthke page Check the log. Specifically, no building should appear there whatsoever. If a build starts, it means that some dependency that should be installed as a prebuild wheel is missing. Possible reasons: you forget to install the dependency, go back to step 2 the dependency version does not correspond with the version required by the package you are installing. Check the log for the required version. Upgrading pip \u00b6 To upgrade pip, use python -m pip install --upgrade pip . Sometimes, this command end ith an error. There can be specific solutions to this, but what always seems to fix the pip is the get-pip script . Download the script and run it using python get-pip.py . Local packages \u00b6 A useful method how to develop and test packages is to have them installed locally. This way, each change in the source code is immediately reflected in the package and also in the code that uses the package. To install a package locally, use pip install -e <path to the package> . Note that the package needs to be properly initialized first, i.e.: at least one __init__.py file in the package root (optionally others in subpackages) a setup.py file in the parent directory of the package root Pycharm \u00b6 Configuration \u00b6 Settings synchronization \u00b6 Same as in IDEA: Log in into JetBrains Toolbox or to the App Click on the gear icon on the top-right and choose Sync Check all categories and click on pull settings from the cloud Do Not Run scripts in Python Console by Default \u00b6 Run configuration select box -> Edit Configurations... -> Edit configuration templates -> Python -> uncheck the Run with Python Console Enable Progress Bars in output console \u00b6 Run configuration select box -> Edit Configurations... -> Select the configuration -> check the Emulate terminal in output console Project Configuration \u00b6 configure the correct test suite in File -> Settings -> Tools -> Python Integrated Tools -> testing Known problems & solutions \u00b6 Non deterministic output in the run window \u00b6 Problem: It can happen that the output printing/logging can be reordered randomly (not matching the order of calls in the source, neither the system console output). Solution: Edit Configurations... -> select configuration for the script -> check Emulate terminal in output console . Pycharm does not recognize a locally installed package \u00b6 It can happen that a locally installed package ( -e ) is not recognized by Pycharm. This can be solved by adding the path to the package to the interpreter paths: File -> Settings -> Project: <project name> -> Python Interpreter Click on the arrow next to the interpreter name and choose Show All... Click on the desired interpreter and click on the filetree icon on the top of the window Add the path to the package to the list of paths Jupyter \u00b6 Jupyter can be used both in Pycharm and in a web browser. Jupyter in Pycharm \u00b6 The key for the effectivity of Jupyter in Pycharm is using the command mode . To enter the command mode, press Esc . To enter the edit mode, pres enter. Command mode shortcuts \u00b6 m : change cell type to markdown Ctrl + C : copy cell Ctrl + V : paste cell Ctrl + Shift + Up : move cell up Ctrl + Shift + Down : move cell down Text mode shortcuts: Ctrl + Shift + - : split cell on cursor position Web Browser Configuration \u00b6 Install Extension Manager with basic extensions \u00b6 Best use the official installation guide . The extensions then can be toggled on in the Nbextensions tab in the jupyter homepage. Be sure to unselect the disable configuration for nbextensions without explicit compatibility (they may break your notebook environment, but can be useful to show for nbextension development) checkbox, otherwise, all extensions will be disabled. Debugging \u00b6 Pycharm contains a good debuger for python code. However, it cannot step into most standard library functions, as those are native, implemented in C/C++. For that, we need mixed python/native debugging. Testing \u00b6 To run pytest, simply go to the folder and run pytest . Arguments: -x : stop on first failure Mixed Python-native debugging \u00b6 In theory, there are two ways how to debug python native code: use a tool that can step from python to C++ (only Visual Studio offers this possibility AFAIK) use two debuggers, start with a python debugger, and attach a native debugger to the python process. This way, the debuggers can be independent. However, one needs to put a breakpoint in the native code, and for that, we need to know the location of the code that will be executed from python (non-trivial for standard library) Python-native debugger in Visual Studio \u00b6 First check and install the requirements: Python 3.5-3.9, including debugging symbols Python 3.10 is not compatible yet in Visual Studio, a Python development component needs to be installed, including Python native development tools Then, configure the Visual studio: Go to Tools -> Options -> Python -> Debugging and check: Enable debugging of the Python standard library . Finally, create a new Python project (either clean, or from existing code) and configure it: use the interpreter directly, do not create a virtual environment enable native code debugging: Go to project properties -> Debug Check enable native code debugging use the -i flag to always see the debug console, even if the program ends without breaking Go to project properties -> Debug Add the -i flag to the Interpreter Arguments field Known issues \u00b6 The reference assemblies for .NETFramework,Version=v4.7.2 were not found. -> Install this component using the visual studio installer Other Sources \u00b6 Microsoft official documentation Python tools for Visual Studio GitHub page Releasing libraries to PyPi \u00b6 Steps \u00b6 add license headers: https://github.com/johann-petrak/licenseheaders check that setup.py contains all requirements - pipreqs release update the min version in dependencies Release \u00b6 raise version run sdist in Pycharm: Tools -> Run setup.py task -> sdist upload to pypi: twine upload dist/*","title":"Python Workflow"},{"location":"Programming/Python/Python%20Workflow/#dev-stack","text":"I use the following stack: the latest Python, 64 bit pip as the package manager Pycharm IDE pytest test suite Visual Studio for deugging native code","title":"Dev Stack"},{"location":"Programming/Python/Python%20Workflow/#python","text":"Python should be installed from the official web page , not using any package manager. Steps: Dowload the 64-bit installer Run the installer, choose advanced install Include the GUI tools (Tkinter, TK) Includ *.py launcher, but only if there is no newer python version installed. If this is checked and there is a newer vesrsion of Python installed, the setup will fail. Include documentation Check download debug symbols to enable native code debugging The source code for python can be inspected on GitHub","title":"Python"},{"location":"Programming/Python/Python%20Workflow/#command-line","text":"We execute python scripts from the command line as: python <path to the .py file> . Parameters: -m executes a module as a script, e.g. python -m venv . This is useful for executing scripts whithout knowing the path to the script.","title":"Command line"},{"location":"Programming/Python/Python%20Workflow/#pip","text":"","title":"Pip"},{"location":"Programming/Python/Python%20Workflow/#installing-packages","text":"Normal packages are installed using: pip install <package name> . However, if a package uses a C/C++ backend and does not contain the compiled wheel on PyPy, this approach will fail on Windows. Instead, you have to download the wheel from the Chris Gohlke page and install it: pip install <path to wheel> . Also, you have to install the dependencies mentioned on that page.","title":"Installing Packages"},{"location":"Programming/Python/Python%20Workflow/#uninstalling-packages","text":"To uninstall a package, use pip uninstall <package name> . There is no way how to uninstall more packages using some wildcard. To uninstall more packages efficiently (not one by one): create a file with the list of all installed packages: pip freeze > packages.txt edit the file and remove all packages you want to keep uninstall all packages from the file: pip uninstall -r packages.txt -y","title":"Uninstalling packages"},{"location":"Programming/Python/Python%20Workflow/#troubleshooting-package-installation","text":"If the installation fails, check the following: if you installed the package by name, check for the wheel on the Chris Golthke page. if you installed the package from a wheel, check the notes/requirement info on Chris Golthke page Check the log. Specifically, no building should appear there whatsoever. If a build starts, it means that some dependency that should be installed as a prebuild wheel is missing. Possible reasons: you forget to install the dependency, go back to step 2 the dependency version does not correspond with the version required by the package you are installing. Check the log for the required version.","title":"Troubleshooting package installation"},{"location":"Programming/Python/Python%20Workflow/#upgrading-pip","text":"To upgrade pip, use python -m pip install --upgrade pip . Sometimes, this command end ith an error. There can be specific solutions to this, but what always seems to fix the pip is the get-pip script . Download the script and run it using python get-pip.py .","title":"Upgrading pip"},{"location":"Programming/Python/Python%20Workflow/#local-packages","text":"A useful method how to develop and test packages is to have them installed locally. This way, each change in the source code is immediately reflected in the package and also in the code that uses the package. To install a package locally, use pip install -e <path to the package> . Note that the package needs to be properly initialized first, i.e.: at least one __init__.py file in the package root (optionally others in subpackages) a setup.py file in the parent directory of the package root","title":"Local packages"},{"location":"Programming/Python/Python%20Workflow/#pycharm","text":"","title":"Pycharm"},{"location":"Programming/Python/Python%20Workflow/#configuration","text":"","title":"Configuration"},{"location":"Programming/Python/Python%20Workflow/#settings-synchronization","text":"Same as in IDEA: Log in into JetBrains Toolbox or to the App Click on the gear icon on the top-right and choose Sync Check all categories and click on pull settings from the cloud","title":"Settings synchronization"},{"location":"Programming/Python/Python%20Workflow/#do-not-run-scripts-in-python-console-by-default","text":"Run configuration select box -> Edit Configurations... -> Edit configuration templates -> Python -> uncheck the Run with Python Console","title":"Do Not Run scripts in Python Console by Default"},{"location":"Programming/Python/Python%20Workflow/#enable-progress-bars-in-output-console","text":"Run configuration select box -> Edit Configurations... -> Select the configuration -> check the Emulate terminal in output console","title":"Enable Progress Bars in output console"},{"location":"Programming/Python/Python%20Workflow/#project-configuration","text":"configure the correct test suite in File -> Settings -> Tools -> Python Integrated Tools -> testing","title":"Project Configuration"},{"location":"Programming/Python/Python%20Workflow/#known-problems-solutions","text":"","title":"Known problems &amp; solutions"},{"location":"Programming/Python/Python%20Workflow/#non-deterministic-output-in-the-run-window","text":"Problem: It can happen that the output printing/logging can be reordered randomly (not matching the order of calls in the source, neither the system console output). Solution: Edit Configurations... -> select configuration for the script -> check Emulate terminal in output console .","title":"Non deterministic output in the run window"},{"location":"Programming/Python/Python%20Workflow/#pycharm-does-not-recognize-a-locally-installed-package","text":"It can happen that a locally installed package ( -e ) is not recognized by Pycharm. This can be solved by adding the path to the package to the interpreter paths: File -> Settings -> Project: <project name> -> Python Interpreter Click on the arrow next to the interpreter name and choose Show All... Click on the desired interpreter and click on the filetree icon on the top of the window Add the path to the package to the list of paths","title":"Pycharm does not recognize a locally installed package"},{"location":"Programming/Python/Python%20Workflow/#jupyter","text":"Jupyter can be used both in Pycharm and in a web browser.","title":"Jupyter"},{"location":"Programming/Python/Python%20Workflow/#jupyter-in-pycharm","text":"The key for the effectivity of Jupyter in Pycharm is using the command mode . To enter the command mode, press Esc . To enter the edit mode, pres enter.","title":"Jupyter in Pycharm"},{"location":"Programming/Python/Python%20Workflow/#command-mode-shortcuts","text":"m : change cell type to markdown Ctrl + C : copy cell Ctrl + V : paste cell Ctrl + Shift + Up : move cell up Ctrl + Shift + Down : move cell down Text mode shortcuts: Ctrl + Shift + - : split cell on cursor position","title":"Command mode shortcuts"},{"location":"Programming/Python/Python%20Workflow/#web-browser-configuration","text":"","title":"Web Browser Configuration"},{"location":"Programming/Python/Python%20Workflow/#install-extension-manager-with-basic-extensions","text":"Best use the official installation guide . The extensions then can be toggled on in the Nbextensions tab in the jupyter homepage. Be sure to unselect the disable configuration for nbextensions without explicit compatibility (they may break your notebook environment, but can be useful to show for nbextension development) checkbox, otherwise, all extensions will be disabled.","title":"Install Extension Manager with basic extensions"},{"location":"Programming/Python/Python%20Workflow/#debugging","text":"Pycharm contains a good debuger for python code. However, it cannot step into most standard library functions, as those are native, implemented in C/C++. For that, we need mixed python/native debugging.","title":"Debugging"},{"location":"Programming/Python/Python%20Workflow/#testing","text":"To run pytest, simply go to the folder and run pytest . Arguments: -x : stop on first failure","title":"Testing"},{"location":"Programming/Python/Python%20Workflow/#mixed-python-native-debugging","text":"In theory, there are two ways how to debug python native code: use a tool that can step from python to C++ (only Visual Studio offers this possibility AFAIK) use two debuggers, start with a python debugger, and attach a native debugger to the python process. This way, the debuggers can be independent. However, one needs to put a breakpoint in the native code, and for that, we need to know the location of the code that will be executed from python (non-trivial for standard library)","title":"Mixed Python-native debugging"},{"location":"Programming/Python/Python%20Workflow/#python-native-debugger-in-visual-studio","text":"First check and install the requirements: Python 3.5-3.9, including debugging symbols Python 3.10 is not compatible yet in Visual Studio, a Python development component needs to be installed, including Python native development tools Then, configure the Visual studio: Go to Tools -> Options -> Python -> Debugging and check: Enable debugging of the Python standard library . Finally, create a new Python project (either clean, or from existing code) and configure it: use the interpreter directly, do not create a virtual environment enable native code debugging: Go to project properties -> Debug Check enable native code debugging use the -i flag to always see the debug console, even if the program ends without breaking Go to project properties -> Debug Add the -i flag to the Interpreter Arguments field","title":"Python-native debugger in Visual Studio"},{"location":"Programming/Python/Python%20Workflow/#known-issues","text":"The reference assemblies for .NETFramework,Version=v4.7.2 were not found. -> Install this component using the visual studio installer","title":"Known issues"},{"location":"Programming/Python/Python%20Workflow/#other-sources","text":"Microsoft official documentation Python tools for Visual Studio GitHub page","title":"Other Sources"},{"location":"Programming/Python/Python%20Workflow/#releasing-libraries-to-pypi","text":"","title":"Releasing libraries to PyPi"},{"location":"Programming/Python/Python%20Workflow/#steps","text":"add license headers: https://github.com/johann-petrak/licenseheaders check that setup.py contains all requirements - pipreqs release update the min version in dependencies","title":"Steps"},{"location":"Programming/Python/Python%20Workflow/#release","text":"raise version run sdist in Pycharm: Tools -> Run setup.py task -> sdist upload to pypi: twine upload dist/*","title":"Release"},{"location":"Programming/Web/CSS%20Manual/","text":"Layout options \u00b6 Grid Layout \u00b6 For complex pages. See the tutorial . Flex Layout \u00b6 For simpler pages. See the tutorial . Note that setting max_width: 100% for child elements of flex items does not work frequenly, so it's better to specify max_witdth ( SO ). Oldschool Layout \u00b6 Oldschool layout use floats. Very Oldschool Layout \u00b6 With tables... Practical Selectors \u00b6 Last Child of a Specific Parent \u00b6 #parent > :last-child Written with StackEdit .","title":"CSS Manual"},{"location":"Programming/Web/CSS%20Manual/#layout-options","text":"","title":"Layout options"},{"location":"Programming/Web/CSS%20Manual/#grid-layout","text":"For complex pages. See the tutorial .","title":"Grid Layout"},{"location":"Programming/Web/CSS%20Manual/#flex-layout","text":"For simpler pages. See the tutorial . Note that setting max_width: 100% for child elements of flex items does not work frequenly, so it's better to specify max_witdth ( SO ).","title":"Flex Layout"},{"location":"Programming/Web/CSS%20Manual/#oldschool-layout","text":"Oldschool layout use floats.","title":"Oldschool Layout"},{"location":"Programming/Web/CSS%20Manual/#very-oldschool-layout","text":"With tables...","title":"Very Oldschool Layout"},{"location":"Programming/Web/CSS%20Manual/#practical-selectors","text":"","title":"Practical Selectors"},{"location":"Programming/Web/CSS%20Manual/#last-child-of-a-specific-parent","text":"#parent > :last-child Written with StackEdit .","title":"Last Child of a Specific Parent"},{"location":"Programming/Web/Google%20API%20and%20Apps%20Script/","text":"Comments \u00b6 API reference Anchored Comments \u00b6 According to my experiments and SO . It is not possible to add achore comments to google docs. Also, it is not possible to get any readable anchors from existing comments. The only way to extract comments with anchors is to export the document as Microsoft Word. Written with StackEdit .","title":"Google API and Apps Script"},{"location":"Programming/Web/Google%20API%20and%20Apps%20Script/#comments","text":"API reference","title":"Comments"},{"location":"Programming/Web/Google%20API%20and%20Apps%20Script/#anchored-comments","text":"According to my experiments and SO . It is not possible to add achore comments to google docs. Also, it is not possible to get any readable anchors from existing comments. The only way to extract comments with anchors is to export the document as Microsoft Word. Written with StackEdit .","title":"Anchored Comments"},{"location":"Programming/Web/JavaScript/","text":"","title":"JavaScript"},{"location":"Programming/Web/Jekyll/","text":"Local testing \u00b6 Install Ruby Download dependencies using bundle command Run bundle exec jekyll serve to start local server Directory structure \u00b6 Jekyll has a predefined directory structure. The important directories are: _posts : Contains all the posts. The file name should be in the format YYYY-MM-DD-title.md . _pages : Contains all the pages. The file name should be in the format title.md . _config.yml : Contains the configuration of the site. Configuration \u00b6 The configuration can be set in _config.yml file. Impotant options are: Markdown \u00b6 Code blocks \u00b6 Markdown code blocks are supported by Jekyll. The syntax is: ```language code \\``` Note that for Jekyll, the language name is case sensitive . For example, java is correct, but Java is not. This is in contrast to GitHub markdown, where the language name is case insensitive. Plugins \u00b6 Jekyll functionality can be extended using plugins. Plugins are Ruby gems, which should be added to the Gemfile and also to the _config.yml file. Note that GitHub Pages supports only a limited number of plugins . The list of supported plugins can be found here .","title":"Jekyll"},{"location":"Programming/Web/Jekyll/#local-testing","text":"Install Ruby Download dependencies using bundle command Run bundle exec jekyll serve to start local server","title":"Local testing"},{"location":"Programming/Web/Jekyll/#directory-structure","text":"Jekyll has a predefined directory structure. The important directories are: _posts : Contains all the posts. The file name should be in the format YYYY-MM-DD-title.md . _pages : Contains all the pages. The file name should be in the format title.md . _config.yml : Contains the configuration of the site.","title":"Directory structure"},{"location":"Programming/Web/Jekyll/#configuration","text":"The configuration can be set in _config.yml file. Impotant options are:","title":"Configuration"},{"location":"Programming/Web/Jekyll/#markdown","text":"","title":"Markdown"},{"location":"Programming/Web/Jekyll/#code-blocks","text":"Markdown code blocks are supported by Jekyll. The syntax is: ```language code \\``` Note that for Jekyll, the language name is case sensitive . For example, java is correct, but Java is not. This is in contrast to GitHub markdown, where the language name is case insensitive.","title":"Code blocks"},{"location":"Programming/Web/Jekyll/#plugins","text":"Jekyll functionality can be extended using plugins. Plugins are Ruby gems, which should be added to the Gemfile and also to the _config.yml file. Note that GitHub Pages supports only a limited number of plugins . The list of supported plugins can be found here .","title":"Plugins"},{"location":"Programming/Web/css/","text":"Selectors \u00b6 basic selectors are: element : e.g., p selects all paragraphs class : e.g., .intro selects all elements with class intro id : e.g., #first selects the element with id first (ids are unique) These basic selectors can be combined to form complex selectors the combination operators are: inside ( <space> ): e.g., div p selects all paragraphs inside a div adjacent ( + ): e.g., div + p selects the first paragraph after a div child ( > ): e.g., div > p selects all paragraphs that are direct children of a div parent ( < ): e.g., div < p selects all divs that are direct parents of a paragraph contains ( :has() ) : e.g., div:has(p) selects all divs that contain a paragraph","title":"css"},{"location":"Programming/Web/css/#selectors","text":"basic selectors are: element : e.g., p selects all paragraphs class : e.g., .intro selects all elements with class intro id : e.g., #first selects the element with id first (ids are unique) These basic selectors can be combined to form complex selectors the combination operators are: inside ( <space> ): e.g., div p selects all paragraphs inside a div adjacent ( + ): e.g., div + p selects the first paragraph after a div child ( > ): e.g., div > p selects all paragraphs that are direct children of a div parent ( < ): e.g., div < p selects all divs that are direct parents of a paragraph contains ( :has() ) : e.g., div:has(p) selects all divs that contain a paragraph","title":"Selectors"},{"location":"Programming/Web/htaccess/","text":"kakpsatweb","title":"htaccess"},{"location":"Programming/Web/mkdocs/","text":"https://www.mkdocs.org/ update steps: generate navigation: `python .\\generate_nav.py generate site: mkdocs build deploy: mkdocs gh-deploy Known limitations \u00b6 MkDocs use the Python-Markdown parser which does not follow the CommonMark specification. Therefore, some markdown elements may not work as expected: lists has to be separated by an empty line, otherwise they will be rendered as simple text.","title":"mkdocs"},{"location":"Programming/Web/mkdocs/#known-limitations","text":"MkDocs use the Python-Markdown parser which does not follow the CommonMark specification. Therefore, some markdown elements may not work as expected: lists has to be separated by an empty line, otherwise they will be rendered as simple text.","title":"Known limitations"},{"location":"Ubuntu/Linux%20Manual/","text":"Usefull Comands \u00b6 Check Ubuntu Version \u00b6 lsb_release -a Printing used ports \u00b6 sudo lsof -i -P Checking GLIBC version \u00b6 ldd --version Exit codes \u00b6 Exit code of the last command: $? Common exit codes for C++ programs Show path to executable \u00b6 Use the which command: which <executable> Unpack file \u00b6 Foe unpacking, you can use the tar -f <file> command. The most used options are: x : extract Environment Variables \u00b6 The environment variables are introduced with the export command: export <variable>=<value> without export, the variable is just a local shell variable: <variable>=<value> # local variable We will demonstrate the work with environment variables on the PATH example. If you have a program in a custom location, adding it to $PATH permanently and be able to run it under all circumstances is not an easy task on linux. Standard procedure is to add a system variable: Create a dedicated .sh file in /etc/profile.d. for your configuration (config for each app should be stored in a separate file). the file should contain: export PATH=$PATH:YOURPATH exit nano and save the file: ctrl+x and y logout and login again to load the newly added varibales for WSL close the console and reopen it, it is not necessary to restart the WSL ( click here for detailed description ) Enable Variable with sudo \u00b6 To enable the variable even if you use sudo , you need to edit sudo config using sudo visudo and: exclude PATH from variable being reset when running sudo : Defaults env_keep += \"PATH\" disable the safe path mechanism for sudo , i.e., comment the line: Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin logout and login again to load the new added varibales Enable the Variable Outside Bash \u00b6 If you need the variable outside bash, the above-mentioned approach won\u00e2\u20ac\u2122t work. Currently, I do not know about any general solution for variables. The solution below, unfortunately, work only for PAM shells (see this SO answer why). Add the variable to /etc/environment . Note that it is not a script file, so you can use only simple variable assignments. Enable Variable on a Device Without Root Access \u00b6 Without root access, we can only edit the user config files. put the necessary config into: ~/.bash_profile if it already exists or to ~/.profile Note that the .profile file is ignored when the .bash_profile file exists. Remove Windows $PATH from WSL \u00b6 By default, the PATH environment variable from Windows is inluded in the PATH variable in Ubuntu running in WSL. This can lead to conflicts, when the same executables are used in both systems (e.g., vcpkg, cmake). To turn of the Windows PATH inclusion: open /etc/wsl.conf add the fllowing code: [interop] appendWindowsPath = false restart WSL File System \u00b6 copy file \u00b6 The cp command is used to copy files: cp <source> <destination> . The most used options are: -r , -R : copy recursively -v : verbose -f : force -p : preserve permissions and timestamps -a : same as -p -R plus some other options For more sophisticated copying, use rsync : rsync <source> <destination> . The most used options are: -h : human readable -a : archive mode, equivalent to -rlptgoD --progress : show progress Remove file \u00b6 The rm command is used to remove files. The most used options are: -r , -R : remove recursively To remove all files in a directory, you can use Access rights \u00b6 The Linux access rights use the same system for files and folders. The access rights are divided into three groups, from left to right: owner group other Each group has three possible access rights: r : read w : write x : execute Important aspects: to access a directory, the user has to have the x right on the directory. to access a file, the user has to have the x right on all folders in the path to the file. Compute directory size \u00b6 To compute the size of a directory, use the du command: du <path> . The most used options are: -h : human readable -s : summarize Find files \u00b6 To find files, use the find command: find <where> <options> <params> . The most used options are: -name : find by name. This option should be followed by a file name pattern. -path : find by path. This option should be followed by a path pattern. Network \u00b6 netstat \u00b6 The netstat command is the basic command to monitor the networ. It displays the TCP connections. It is available both on Linux and on Windows, although the interface differs. Important parameters: -n : do not translate IP and ports into human readable names -a : show all connections. Without this, some connections can be skipped. Bash \u00b6 General Remarks \u00b6 It's important to use Linux newlines, otherwise, bash scripts will fail with unexpected character error Empty constructs are not allowed, i.e, empty function or loop results in an error brackets needs spaces around them, otherwise, there will be a syntax error space around = , which is typical in other languages, is not allowed in bash bash does not support any data structures, only arrays Variables \u00b6 Variables can be defined as: var=value Note that there must not be any spaces around = . We can also declare variables with a specific type, so that no other type can be assigned to it using the declare command: declare -i var # integer declare -a var # array declare -r var # read only To access the value of a variable, we use $ : echo $var List all variables \u00b6 To list all variables, we can use the declare command: declare However, this command also lists the functions. To list only the variables, we can use: declare -p which also prints the type and attributes of the variables. Operations on variables \u00b6 There are many operations on variables, the most important are: ${#<variable>} : length of the variable ${<variable>%%<pattern>} : remove the longest suffix matching the pattern ${<variable>##<pattern>} : remove the longest prefix matching the pattern Working with I/O \u00b6 Output Forwarding \u00b6 Output forwarding is a process of redirecting the output of a command to an input of another command. The operator for that is the pipe | . The syntax is: <command 1> | <command 2> Note that the content of the pipe cannot be examined, the process on the right hand side consume it. Therefore, it is not possible to simply branch on pipe content while use it in the subsequent process. Output Redirection \u00b6 Output redirection is a process of redirecting the output (stdout, stderr,..) from the console to a file. The syntax is: <command> <operator> <file> The possible operators and their effects are listed in the table (full explanation on SO ) below: Operator Stdout Stderr Mode (in file) > file console overwrite >> file console append &> file file overwrite &>> file file append 2> console file overwrite 2>> console file append \\| tee both console overwrite \\| tee -a both console append \\|& tee both both overwrite \\|& tee -a both both append tee is actully a command, not an operator. It is used as follows: <command> | tee <file> We can also use tee to forward the output of a command to multiple commands: <command 1> | tee >(<command 2>) | <command 3> This forward <command 1> to both <command 2> and <command 3> . Use bash variables as input \u00b6 Bash variables can be used as input of a command. Syntax: <command> <<< $<variable>` Command Substitution \u00b6 When we need to use an output of a command instead of a constant or variable, we have to use command substtution: `<command>` # or equivalently $(<command>) e.g.: echo resut is: `cut -d, -f 7` # or equivalently echo resut is: $(cut -d, -f 7) Bash Script Arguments \u00b6 We refer the arguments of a bash script as $0 - the name of the script $1..$n - the arguments of the script $@ - all the arguments of the script Sometimes, it is useful to throw away processed arguments. This can be done using the shift command shift <n> , where <n> is the number of arguments to be thrown away (default is 1). The remaining arguments are then shifted to the left, i.e., $2 becomes $1 and so on. Conditions \u00b6 In general, condition in bash has the following syntax: if <condition> then <command> else <command> fi The condition can have several formats: plain command : the condition is true if the command returns 0 bash if grep -q \"$text\" $file then ... fi [ <condition> ] or test <condition> : The standard POSIX test construct. Now only suitable if we want to run the script outside bash. bash if [ $var = 1 ] then ... fi [[ <condition> ]] : The extended test construct. This is the recommended way of writing conditions, due to several practical features (e.g., no need to quote variables, regex support, logical operators, etc.). bash if [[ $var = 1 ]] then ... fi (( <condition> )) : The arithmetic test construct. This is used for arithmetic conditions. bash if (( $var == 1 )) then ... fi Note that if we want to use some arbitrary value (e. g. the return value of a command), or comparisons in the condition (similar to programming languages), we have to use one of the test constructs. Mind the spaces around the braces! String comparison \u00b6 Strings can be compared using the standard = operator or the == operator. If we use the [ ] construct, we have to quote the variables, otherwise, the script will fail on empty strings or strings containing spaces: if [ \"$var\" = \"string\" ] then ... fi # or equivalently if [[ $var = \"string\" ]] then ... fi Loops \u00b6 while condition do <command1> <command2> ... done Forward to loop \u00b6 We can forward an input into while loop using | as usuall. Additionally, it is possible to read from file directly by adding < to the end like this: while condition do <command1> <command2> ... done < <input> The same goes for the output, i.e., we can forward th outut of a loop with | . Strings literals \u00b6 String literals can be easily defined as: str=\"string literal\" # or equivalently str='string literal' If we use double quotes, the variables are expanded, e.g., echo \"Hello $USER\" will print Hello <username> . The problem arises when we want to use double quotes in the string literal containing variables, e.g., normal string \"quotted string\" $myvar . In this case, we have to use quite cumbersome syntax: a = \"normal string \"\\\"\"quotted string\"\\\" # or equivalently a = \"normal string \"'\"'\"quotted string\"'\"' Multiline string literals \u00b6 There is no dedicated syntax for multiline string literals. However, we can use the here document syntax: <target> << <delimiter> <content> <delimiter> For example, to store the command in a variable, we can use: db_sql = $(cat << SQL CREATE DATABASE test_$name OWNER $name; grant all privileges on database test_$name to $name; SQL) Note that the <delimiter> must be at the beginning of the line, otherwise, it will not work. Functions \u00b6 Functions are defined as: function_name() { <command 1> <command 2> ... } For access the arguments of the function, we use the same syntax as for the script arguments (e.g., $1 for the first argument). We can create local variables in the function using the local keyword: function_name() { local var1=\"value\" ... } Reading from command line \u00b6 To read from command line, we can use the read command. The syntax is: read <variable> where <variable> is the name of the variable to store the input. Important parameters: -p <prompt> : prints the <prompt> before reading the input -s : do not echo the input (usefull for passwords) Bash Script Header Content \u00b6 Usually, here are some special lines at the beginning of the bash script. First, we can specify the interpreter to be used: #!/bin/bash Then we can set the script to exit on error: set -e Calling a script from another script in the same directory \u00b6 To call a script b from script a in the same directory, it is not wise to use the relative path, it is evaluated from the current directory, not from the directory of the script. To be sure that we can call script a from anywhere, we need to change the working directory to the directory of the scripts first: cd \"$(dirname \"$0\")\" ./B.sh Root Access \u00b6 Some commands require root privilages. The basic way how to run a command as root, we can use the sudo command: sudo <command> A password of the current user is required to run the command. Also, the user has have the right to run sudo . The password is usually required only once per some time period (e.g., 15 minutes). For some operations (e.g., browsing folders requiring root access), we have to run multiple commands as root. In this case, we have to switch the shell user to root. Resources: SO Changing shell user \u00b6 To change the shell user, we can use the su command: su <username> If we omit the username, the root user is used. Note that the password of the target user is required . To change the shell user without the password of the target user, we can use the sudo command: sudo su <username> This way, the password of the current user is required. The su command can be used the same way as the sudo command, i.e., we can pass the command: su <username> -c \"<command>\" To exit the shell user, we can use the exit command. Changing shell user in a script \u00b6 In a script, we cannot change the shell user for the remaining commands in the script. If we do that, the script will execute a new shell, and the remaining commands will be executed in the old shell. To execute commands as a different user, we have several options. Every option pass the commands as a parameter to the change user command (e.g., su -c \"<command>\" ). The options are: for each command, create a new change user command create a single change user command and pass all the commands as a single parameter (e.g., with the heredoc syntax) move commands to a separate script and execute the script as a different user Managing packages with apt and dpkg \u00b6 To update the list of possible updates: sudo apt update To perform the update : sudo apt upgrade To list installed packages: apt list --installed We can filter the list using the grep command. To find the install location of a package: dpkg -L <package> Unfortunately, it is not possible to easily search for the user who installed the package. To search for a package: apt-cache search <package> We can limit the search to check only names of the packages using the --names-only parameter. To remove a package: sudo apt remove <package> Installing non-stable package versions \u00b6 On Linux, the stable package versions are usually outdated, sometimes years behind the current version. To install the newer version, we have usually a few options: upgrade the system : if we use an old version of the system, we can check whether the newer version is available that includes the newer package version. For more, see the Upgrade section. install from source : We can manually build the package from the source and install it. See the C++ Workflow for more. install package from an alternative repository : We can add an alternative repository to the system and install the package from there. The first two options are covered in different part of this manual. Here, we focus on the third option. To use an alternative repository, we have to a) add the repository to the system, which is a one time task, and b) install the specific package from the repository. To add a repository , we have to: add the repository to the /etc/apt/sources.list (or to a separate file in the /etc/apt/sources.list.d/ directory). Each repository should has the line that should be added to the file on its website. sudo apt update to update the list of available packages To install a package from the repository : sudo apt install -t <repository> <package> Some useful repositories \u00b6 debian backports : the repository with the newer versions of the packages for the stable Debian version Changing default package repositories \u00b6 If the downolad speed is not satisfactory, we can change the repositories. To find the fastest repository from the list of nearby repositories, run: curl -s http://mirrors.ubuntu.com/mirrors.txt | xargs -n1 -I {} sh -c 'echo `curl -r 0-10240000 -s -w %{speed_download} -o /dev/null {}/ls-lR.gz` {}' | sort -g -r The number in the leftmost column indicates the bandwidth in bytes (larger number is better). To change the repositories to the best mirror, we need to replace the mirror in etc/apt/source.list . We can do it manually, however, to prevent the mistakes, it is better to use a dedicated python script: apt-mirror-updater . Steps: install the python script: sudo pip install apt-mirror-updater backup the old file: sudo cp sources.list sources.list.bak change the mirror with the script: apt-mirror-updater -c <mirror URL> Note that the apt-mirror-updater script can also measure the bandwidth, however, the result does not seem to be reliable. String Processing \u00b6 String filtering with grep \u00b6 The grep command is used to filter lines containing a pattern. The syntax is: grep <pattern> <file> The pattern can be a simple string, or a regex. The most used options are: -v : invert the match, i.e., print only lines not matching the pattern -e : use multiple patterns (e.g., -e <pattern 1> -e <pattern 2> ) -i : ignore case Word count with wc \u00b6 The wc command counts words, lines, and characters. What is counted is determined by the parameters: -w : words -l : lines -c : characters String mofification with sed \u00b6 Sed is a multi purpose command for string modification. It can search and replace in string. The syntax is folowing: sed s/<search>/<replace>/[<occurance>] Example: s/$/,3/ Any regex can be used as <search> . Some characters (e.g. | ) must be escaped with backslash and used together with the -E parameter. This replace the nd of the line with string \",3\" . Note that there is a slash at the end, despite we use the default option for occurance. Also, it can delete lines containing string using: sed /<pattern>/d cut \u00b6 Cut is a useful command for data with delimiters. Usage: cut -f <columns> Where columns are splited by comma, e.g., 2,4,7 . If we need to specify delimiters, we use the -d parameter: cut -d, -f 1,5 AWK \u00b6 AWK is a powerful tool for text processing. It is a programming language, so it can be used for more complex tasks than sed or cut . The basic syntax is: awk '<pattern> {<action>}' Where <pattern> is a regex and <action> is a command. The <action> is executed only if the line matches the <pattern> . Pattern \u00b6 In the awk , / is used as a delimiter of the regex pattern. Action \u00b6 The <action> can be a sequence of commands, separated by ; . We can use column values by using special column variables: $0 : the whole line $1 : the first column ... Trim string \u00b6 <command with string output> | xargs Processes \u00b6 for checking all processes , we can use htop to get a path to executable of a process by PID, we can use pwdx <PID> for checking a specific process, we can use ps to kill a process, we can use kill to kill a process by name, we can use pkill to get information about a process selected by name, we can use pgrep pkill \u00b6 The pkill command kills a process by name. The syntax is: pkill <process name> important parameters: -f : match the whole command line, not only the process name -9 : force kill Process Info \u00b6 Users \u00b6 The users are listed in /etc/passwd . The file contains one line per user, each line has the following format: <username>:<password>:<user ID>:<group ID>:<GECOS>:<home folder>:<shell> The password is typically stored in /etc/shadow and is represented by x . GECOS is some kind of a comment storing arbitrary information about the user. Adding a user \u00b6 To add a user, we can use either the useradd binary directly, or the adduser wrapper script. Here we describe the adduser script. The basic syntax is adduser <username> . Important parameters: --gecos \"<GECOS>\" : supply the content of the GECOS field. If skipped, the command will ask for the GECOS content interactively. --shell <shell> : The shell to be used by the user. If skipped, the default shell is used. This can be used to create a user without a shell by setting the shell to /usr/sbin/nologin . Note that the adduser needs to be run as root. Otherwise it will fail with bash: adduser: command not found . User Groups \u00b6 An important aspect of user management in Linux is the user groups. For example, by belonging to the sudo group, the user can execute commands with sudo . The groups are listed in /etc/group . The file contains one line per group, each line has the following format: <group name>:<password>:<group ID>:<user list> To see the groups of a user, we can use the groups command (no arguments needed). To manipulate groups and users, we need a root access. To add a user to a group, we can use the usermod command: usermod -a -G <group name> <username> To remove a user from a group, we can use the same command: usermod -G <group list> <username> where <group list> is a comma separated list of groups the user should belong to. File ownership \u00b6 Each file has a pair of owners: the user owner and the group owner. These ownerships are important, as file permissions in Linux are usually set for a triplet of: owner : the user owner group : the group owner other : all other users To change the owner of a file, we can use the chown command. The syntax is: chown <user>:<group> <file> We can skip the :<group> part, in which case the group is not changed. The chown command can also be used to change the owner of a directory. In this case, the -R parameter is used to change the owner recursively. Disable access to shell for a user \u00b6 To disable access to shell for a user, we have to configure his/her shell to /usr/sbin/nologin or similar. For new users, we can use the --shell parameter of the adduser command. For existing users, we can use the usermod command: usermod --shell /usr/sbin/nologin <username> Note that for some ssh client implementations, it is necessary to connect to a shell by default, otherwise, the connection is terminated immediately after login. In this case, the user has to connect to the server with the -N parameter, which tells the client not to execute any command after login. Services and systemd \u00b6 The systemd is a system and service manager for Linux. It is used to manage services, devices, and other aspects of the system. The main command to manage services is systemctl . The general syntax is: sudo systemctl <action> <service name> The most used actions are: start : start the service stop : stop the service restart : restart the service status : get the status of the service reload : reload the configuration of the service Get the status of a service \u00b6 To get the status of a service, we can use the status action: sudo systemctl status <service name> The statuses can be: active (running) : the service is running active (exited) : the service has finished active (waiting) : the service is not running, but it is waiting for some event TODO: add more statuses Listing services \u00b6 To list all services, we can use one of the following commands: list-units to list all units ever run on the server or list-units-files to list all units, including the ones that have never been run Frequently used software \u00b6 Installing Java \u00b6 Oracle JDK \u00b6 Go to the download page, the link to the dowload page for current version of java is on the main JDK page . Click on the debian package, accept the license, and download it. If installing on system without GUI, copy now (after accepting the license) the target link and dowload the debian package with wget : wget --header \"Cookie: oraclelicense=accept-securebackup-cookie\" <COPIED LINK> . More info on SO . Install the package with sudo apt-get install <PATH TO DOWNLOADED .deb> if there is a problem with the isntallation, check the file integritiy with: sha256 <PATH TO DOWNLOADED .deb> . It should match with the checksums refered on the download page. If not cheksums do not match, go back to download step. In case there is another version of Java alreadz install, we need to overwrite it using the update-alternatives command: sudo update-alternatives --install /usr/bin/java java <PATH TO JAVA> <PRIORITY> . Example: sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk-16/bin/java 2 To check the priorities, call update-alternatives --query java . The newly installed JDK should have the highest priority. Python \u00b6 Python has to be executed with python3 by default, instead of python . GCC \u00b6 GCC is typically installed by default and itts minor versions are updated with the system updates. However, if we need a major version update, we have to install it manually as a new package: sudo apt install gcc-<version> This way, the new version is installed alongside the old version. To switch to the new version, we have to use the update-alternatives command: sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-<version> <priority> The <priority> is a number that determines the priority of the version. The version with the highest priority is used. To check the priorities, we can use the update-alternatives --query gcc command. Note that these steps only updates the C compiler . To affect the C++ compiler as well, we have to repeat the steps with the g++ command: sudo apt install g++-<version> sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-<version> <priority> Other usefull commands \u00b6 Selecting lines from file \u00b6 The head and tail commands are for that, printing the top and bottom 10 lines, respectively. Skip the header \u00b6 tail -n +2 Print lines from to \u00b6 tail -n +<from line> | head -n <number of lines> Progress bar \u00b6 the progress bar can be printed using the pv command. pv <file> | <other comands> # or <other comands> | pv | <other comands> Free disk space \u00b6 df -h piping parameters using xargs \u00b6 The xargs command transfers the output of one command into call of another command with the output of the first command as parameters of the second command. This is usefull when the second command does not accept the output of the first command as input, but accepts the output as parameters. Example: ls | xargs rm # remove all files in the current directory Upgrade \u00b6 For a simple system upgrade, we can use the following steps: First run the update of the current version . Then optionaly backup the WSL run sudo do-release-upgrade However, this only work if there is a LTS version available. If not, the last step will fail. In this case, we can use the do-release-upgrade -d command, which upgrades to the latest version, regardless of the LTS status. Upgrade to other version than the latest \u00b6 When upgrading to a version other than the latest, we can try the following steps: perform steps 1 and 2 from the normal upgrade open the /etc/update-manager/release-upgrades file and set the Prompt parameter to normal or lts (depending on the desired version) backup the /etc/apt/sources.list file change the sources to the new version: e.g., run sudo sed -i 's/<current version name>/<new version name>/g' /etc/apt/sources.list run a normal upgrade: sudo apt update && sudo apt upgrade finalizing the upgrade: sudo apt dist-upgrade WSL backup \u00b6 check the WSL distro name: wsl -l -v shutdown WSL: wsl --shutdown backup the distro: wsl --export <disto name> <backup folder path>/<backup name>.tar vim \u00b6 Vim is a console text editor. It is a modal editor, i.e., it has different modes for different operations. The most important modes are: normal mode : for navigation and file manipulation insert mode : for text editing visual mode : for text selection Normal Mode \u00b6 In normal mode, we can: navigate the file using arrow keys or hjkl (left, down, up, right) enter global commands using : (e.g., :q for quit) edit file content using special commands (e.g., dd for delete line) Global Commands \u00b6 :q : quit :w : save :wq : save and quit :q! : quit without saving File Editing Commands \u00b6 dd : delete line Insert Mode \u00b6 Insert mode is the normal text mode we know from other editors. To enter insert mode, press i . To exit insert mode, press esc . Visual Mode \u00b6 Visual mode is used for text selection. To enter visual mode, press v . To exit visual mode, press esc . Copy and paste \u00b6 Vim has its own clipboard for copy-pasting (yank, ...). However, this cannot be used to copy text outside of vim, nor to paste text from outside vim. To copy text to the system clipboard, we can: select the text using mouse or keyboard press enter to copy the text to the clipboard To paste text from the system clipboard, we press Ctrl + Shift + v .","title":"Linux Manual"},{"location":"Ubuntu/Linux%20Manual/#usefull-comands","text":"","title":"Usefull Comands"},{"location":"Ubuntu/Linux%20Manual/#check-ubuntu-version","text":"lsb_release -a","title":"Check Ubuntu Version"},{"location":"Ubuntu/Linux%20Manual/#printing-used-ports","text":"sudo lsof -i -P","title":"Printing used ports"},{"location":"Ubuntu/Linux%20Manual/#checking-glibc-version","text":"ldd --version","title":"Checking GLIBC version"},{"location":"Ubuntu/Linux%20Manual/#exit-codes","text":"Exit code of the last command: $? Common exit codes for C++ programs","title":"Exit codes"},{"location":"Ubuntu/Linux%20Manual/#show-path-to-executable","text":"Use the which command: which <executable>","title":"Show path to executable"},{"location":"Ubuntu/Linux%20Manual/#unpack-file","text":"Foe unpacking, you can use the tar -f <file> command. The most used options are: x : extract","title":"Unpack file"},{"location":"Ubuntu/Linux%20Manual/#environment-variables","text":"The environment variables are introduced with the export command: export <variable>=<value> without export, the variable is just a local shell variable: <variable>=<value> # local variable We will demonstrate the work with environment variables on the PATH example. If you have a program in a custom location, adding it to $PATH permanently and be able to run it under all circumstances is not an easy task on linux. Standard procedure is to add a system variable: Create a dedicated .sh file in /etc/profile.d. for your configuration (config for each app should be stored in a separate file). the file should contain: export PATH=$PATH:YOURPATH exit nano and save the file: ctrl+x and y logout and login again to load the newly added varibales for WSL close the console and reopen it, it is not necessary to restart the WSL ( click here for detailed description )","title":"Environment Variables"},{"location":"Ubuntu/Linux%20Manual/#enable-variable-with-sudo","text":"To enable the variable even if you use sudo , you need to edit sudo config using sudo visudo and: exclude PATH from variable being reset when running sudo : Defaults env_keep += \"PATH\" disable the safe path mechanism for sudo , i.e., comment the line: Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin logout and login again to load the new added varibales","title":"Enable Variable with sudo"},{"location":"Ubuntu/Linux%20Manual/#enable-the-variable-outside-bash","text":"If you need the variable outside bash, the above-mentioned approach won\u00e2\u20ac\u2122t work. Currently, I do not know about any general solution for variables. The solution below, unfortunately, work only for PAM shells (see this SO answer why). Add the variable to /etc/environment . Note that it is not a script file, so you can use only simple variable assignments.","title":"Enable the Variable Outside Bash"},{"location":"Ubuntu/Linux%20Manual/#enable-variable-on-a-device-without-root-access","text":"Without root access, we can only edit the user config files. put the necessary config into: ~/.bash_profile if it already exists or to ~/.profile Note that the .profile file is ignored when the .bash_profile file exists.","title":"Enable Variable on a Device Without Root Access"},{"location":"Ubuntu/Linux%20Manual/#remove-windows-path-from-wsl","text":"By default, the PATH environment variable from Windows is inluded in the PATH variable in Ubuntu running in WSL. This can lead to conflicts, when the same executables are used in both systems (e.g., vcpkg, cmake). To turn of the Windows PATH inclusion: open /etc/wsl.conf add the fllowing code: [interop] appendWindowsPath = false restart WSL","title":"Remove Windows $PATH from WSL"},{"location":"Ubuntu/Linux%20Manual/#file-system","text":"","title":"File System"},{"location":"Ubuntu/Linux%20Manual/#copy-file","text":"The cp command is used to copy files: cp <source> <destination> . The most used options are: -r , -R : copy recursively -v : verbose -f : force -p : preserve permissions and timestamps -a : same as -p -R plus some other options For more sophisticated copying, use rsync : rsync <source> <destination> . The most used options are: -h : human readable -a : archive mode, equivalent to -rlptgoD --progress : show progress","title":"copy file"},{"location":"Ubuntu/Linux%20Manual/#remove-file","text":"The rm command is used to remove files. The most used options are: -r , -R : remove recursively To remove all files in a directory, you can use","title":"Remove file"},{"location":"Ubuntu/Linux%20Manual/#access-rights","text":"The Linux access rights use the same system for files and folders. The access rights are divided into three groups, from left to right: owner group other Each group has three possible access rights: r : read w : write x : execute Important aspects: to access a directory, the user has to have the x right on the directory. to access a file, the user has to have the x right on all folders in the path to the file.","title":"Access rights"},{"location":"Ubuntu/Linux%20Manual/#compute-directory-size","text":"To compute the size of a directory, use the du command: du <path> . The most used options are: -h : human readable -s : summarize","title":"Compute directory size"},{"location":"Ubuntu/Linux%20Manual/#find-files","text":"To find files, use the find command: find <where> <options> <params> . The most used options are: -name : find by name. This option should be followed by a file name pattern. -path : find by path. This option should be followed by a path pattern.","title":"Find files"},{"location":"Ubuntu/Linux%20Manual/#network","text":"","title":"Network"},{"location":"Ubuntu/Linux%20Manual/#netstat","text":"The netstat command is the basic command to monitor the networ. It displays the TCP connections. It is available both on Linux and on Windows, although the interface differs. Important parameters: -n : do not translate IP and ports into human readable names -a : show all connections. Without this, some connections can be skipped.","title":"netstat"},{"location":"Ubuntu/Linux%20Manual/#bash","text":"","title":"Bash"},{"location":"Ubuntu/Linux%20Manual/#general-remarks","text":"It's important to use Linux newlines, otherwise, bash scripts will fail with unexpected character error Empty constructs are not allowed, i.e, empty function or loop results in an error brackets needs spaces around them, otherwise, there will be a syntax error space around = , which is typical in other languages, is not allowed in bash bash does not support any data structures, only arrays","title":"General Remarks"},{"location":"Ubuntu/Linux%20Manual/#variables","text":"Variables can be defined as: var=value Note that there must not be any spaces around = . We can also declare variables with a specific type, so that no other type can be assigned to it using the declare command: declare -i var # integer declare -a var # array declare -r var # read only To access the value of a variable, we use $ : echo $var","title":"Variables"},{"location":"Ubuntu/Linux%20Manual/#list-all-variables","text":"To list all variables, we can use the declare command: declare However, this command also lists the functions. To list only the variables, we can use: declare -p which also prints the type and attributes of the variables.","title":"List all variables"},{"location":"Ubuntu/Linux%20Manual/#operations-on-variables","text":"There are many operations on variables, the most important are: ${#<variable>} : length of the variable ${<variable>%%<pattern>} : remove the longest suffix matching the pattern ${<variable>##<pattern>} : remove the longest prefix matching the pattern","title":"Operations on variables"},{"location":"Ubuntu/Linux%20Manual/#working-with-io","text":"","title":"Working with I/O"},{"location":"Ubuntu/Linux%20Manual/#output-forwarding","text":"Output forwarding is a process of redirecting the output of a command to an input of another command. The operator for that is the pipe | . The syntax is: <command 1> | <command 2> Note that the content of the pipe cannot be examined, the process on the right hand side consume it. Therefore, it is not possible to simply branch on pipe content while use it in the subsequent process.","title":"Output Forwarding"},{"location":"Ubuntu/Linux%20Manual/#output-redirection","text":"Output redirection is a process of redirecting the output (stdout, stderr,..) from the console to a file. The syntax is: <command> <operator> <file> The possible operators and their effects are listed in the table (full explanation on SO ) below: Operator Stdout Stderr Mode (in file) > file console overwrite >> file console append &> file file overwrite &>> file file append 2> console file overwrite 2>> console file append \\| tee both console overwrite \\| tee -a both console append \\|& tee both both overwrite \\|& tee -a both both append tee is actully a command, not an operator. It is used as follows: <command> | tee <file> We can also use tee to forward the output of a command to multiple commands: <command 1> | tee >(<command 2>) | <command 3> This forward <command 1> to both <command 2> and <command 3> .","title":"Output Redirection"},{"location":"Ubuntu/Linux%20Manual/#use-bash-variables-as-input","text":"Bash variables can be used as input of a command. Syntax: <command> <<< $<variable>`","title":"Use bash variables as input"},{"location":"Ubuntu/Linux%20Manual/#command-substitution","text":"When we need to use an output of a command instead of a constant or variable, we have to use command substtution: `<command>` # or equivalently $(<command>) e.g.: echo resut is: `cut -d, -f 7` # or equivalently echo resut is: $(cut -d, -f 7)","title":"Command Substitution"},{"location":"Ubuntu/Linux%20Manual/#bash-script-arguments","text":"We refer the arguments of a bash script as $0 - the name of the script $1..$n - the arguments of the script $@ - all the arguments of the script Sometimes, it is useful to throw away processed arguments. This can be done using the shift command shift <n> , where <n> is the number of arguments to be thrown away (default is 1). The remaining arguments are then shifted to the left, i.e., $2 becomes $1 and so on.","title":"Bash Script Arguments"},{"location":"Ubuntu/Linux%20Manual/#conditions","text":"In general, condition in bash has the following syntax: if <condition> then <command> else <command> fi The condition can have several formats: plain command : the condition is true if the command returns 0 bash if grep -q \"$text\" $file then ... fi [ <condition> ] or test <condition> : The standard POSIX test construct. Now only suitable if we want to run the script outside bash. bash if [ $var = 1 ] then ... fi [[ <condition> ]] : The extended test construct. This is the recommended way of writing conditions, due to several practical features (e.g., no need to quote variables, regex support, logical operators, etc.). bash if [[ $var = 1 ]] then ... fi (( <condition> )) : The arithmetic test construct. This is used for arithmetic conditions. bash if (( $var == 1 )) then ... fi Note that if we want to use some arbitrary value (e. g. the return value of a command), or comparisons in the condition (similar to programming languages), we have to use one of the test constructs. Mind the spaces around the braces!","title":"Conditions"},{"location":"Ubuntu/Linux%20Manual/#string-comparison","text":"Strings can be compared using the standard = operator or the == operator. If we use the [ ] construct, we have to quote the variables, otherwise, the script will fail on empty strings or strings containing spaces: if [ \"$var\" = \"string\" ] then ... fi # or equivalently if [[ $var = \"string\" ]] then ... fi","title":"String comparison"},{"location":"Ubuntu/Linux%20Manual/#loops","text":"while condition do <command1> <command2> ... done","title":"Loops"},{"location":"Ubuntu/Linux%20Manual/#forward-to-loop","text":"We can forward an input into while loop using | as usuall. Additionally, it is possible to read from file directly by adding < to the end like this: while condition do <command1> <command2> ... done < <input> The same goes for the output, i.e., we can forward th outut of a loop with | .","title":"Forward to loop"},{"location":"Ubuntu/Linux%20Manual/#strings-literals","text":"String literals can be easily defined as: str=\"string literal\" # or equivalently str='string literal' If we use double quotes, the variables are expanded, e.g., echo \"Hello $USER\" will print Hello <username> . The problem arises when we want to use double quotes in the string literal containing variables, e.g., normal string \"quotted string\" $myvar . In this case, we have to use quite cumbersome syntax: a = \"normal string \"\\\"\"quotted string\"\\\" # or equivalently a = \"normal string \"'\"'\"quotted string\"'\"'","title":"Strings literals"},{"location":"Ubuntu/Linux%20Manual/#multiline-string-literals","text":"There is no dedicated syntax for multiline string literals. However, we can use the here document syntax: <target> << <delimiter> <content> <delimiter> For example, to store the command in a variable, we can use: db_sql = $(cat << SQL CREATE DATABASE test_$name OWNER $name; grant all privileges on database test_$name to $name; SQL) Note that the <delimiter> must be at the beginning of the line, otherwise, it will not work.","title":"Multiline string literals"},{"location":"Ubuntu/Linux%20Manual/#functions","text":"Functions are defined as: function_name() { <command 1> <command 2> ... } For access the arguments of the function, we use the same syntax as for the script arguments (e.g., $1 for the first argument). We can create local variables in the function using the local keyword: function_name() { local var1=\"value\" ... }","title":"Functions"},{"location":"Ubuntu/Linux%20Manual/#reading-from-command-line","text":"To read from command line, we can use the read command. The syntax is: read <variable> where <variable> is the name of the variable to store the input. Important parameters: -p <prompt> : prints the <prompt> before reading the input -s : do not echo the input (usefull for passwords)","title":"Reading from command line"},{"location":"Ubuntu/Linux%20Manual/#bash-script-header-content","text":"Usually, here are some special lines at the beginning of the bash script. First, we can specify the interpreter to be used: #!/bin/bash Then we can set the script to exit on error: set -e","title":"Bash Script Header Content"},{"location":"Ubuntu/Linux%20Manual/#calling-a-script-from-another-script-in-the-same-directory","text":"To call a script b from script a in the same directory, it is not wise to use the relative path, it is evaluated from the current directory, not from the directory of the script. To be sure that we can call script a from anywhere, we need to change the working directory to the directory of the scripts first: cd \"$(dirname \"$0\")\" ./B.sh","title":"Calling a script from another script in the same directory"},{"location":"Ubuntu/Linux%20Manual/#root-access","text":"Some commands require root privilages. The basic way how to run a command as root, we can use the sudo command: sudo <command> A password of the current user is required to run the command. Also, the user has have the right to run sudo . The password is usually required only once per some time period (e.g., 15 minutes). For some operations (e.g., browsing folders requiring root access), we have to run multiple commands as root. In this case, we have to switch the shell user to root. Resources: SO","title":"Root Access"},{"location":"Ubuntu/Linux%20Manual/#changing-shell-user","text":"To change the shell user, we can use the su command: su <username> If we omit the username, the root user is used. Note that the password of the target user is required . To change the shell user without the password of the target user, we can use the sudo command: sudo su <username> This way, the password of the current user is required. The su command can be used the same way as the sudo command, i.e., we can pass the command: su <username> -c \"<command>\" To exit the shell user, we can use the exit command.","title":"Changing shell user"},{"location":"Ubuntu/Linux%20Manual/#changing-shell-user-in-a-script","text":"In a script, we cannot change the shell user for the remaining commands in the script. If we do that, the script will execute a new shell, and the remaining commands will be executed in the old shell. To execute commands as a different user, we have several options. Every option pass the commands as a parameter to the change user command (e.g., su -c \"<command>\" ). The options are: for each command, create a new change user command create a single change user command and pass all the commands as a single parameter (e.g., with the heredoc syntax) move commands to a separate script and execute the script as a different user","title":"Changing shell user in a script"},{"location":"Ubuntu/Linux%20Manual/#managing-packages-with-apt-and-dpkg","text":"To update the list of possible updates: sudo apt update To perform the update : sudo apt upgrade To list installed packages: apt list --installed We can filter the list using the grep command. To find the install location of a package: dpkg -L <package> Unfortunately, it is not possible to easily search for the user who installed the package. To search for a package: apt-cache search <package> We can limit the search to check only names of the packages using the --names-only parameter. To remove a package: sudo apt remove <package>","title":"Managing packages with apt and dpkg"},{"location":"Ubuntu/Linux%20Manual/#installing-non-stable-package-versions","text":"On Linux, the stable package versions are usually outdated, sometimes years behind the current version. To install the newer version, we have usually a few options: upgrade the system : if we use an old version of the system, we can check whether the newer version is available that includes the newer package version. For more, see the Upgrade section. install from source : We can manually build the package from the source and install it. See the C++ Workflow for more. install package from an alternative repository : We can add an alternative repository to the system and install the package from there. The first two options are covered in different part of this manual. Here, we focus on the third option. To use an alternative repository, we have to a) add the repository to the system, which is a one time task, and b) install the specific package from the repository. To add a repository , we have to: add the repository to the /etc/apt/sources.list (or to a separate file in the /etc/apt/sources.list.d/ directory). Each repository should has the line that should be added to the file on its website. sudo apt update to update the list of available packages To install a package from the repository : sudo apt install -t <repository> <package>","title":"Installing non-stable package versions"},{"location":"Ubuntu/Linux%20Manual/#some-useful-repositories","text":"debian backports : the repository with the newer versions of the packages for the stable Debian version","title":"Some useful repositories"},{"location":"Ubuntu/Linux%20Manual/#changing-default-package-repositories","text":"If the downolad speed is not satisfactory, we can change the repositories. To find the fastest repository from the list of nearby repositories, run: curl -s http://mirrors.ubuntu.com/mirrors.txt | xargs -n1 -I {} sh -c 'echo `curl -r 0-10240000 -s -w %{speed_download} -o /dev/null {}/ls-lR.gz` {}' | sort -g -r The number in the leftmost column indicates the bandwidth in bytes (larger number is better). To change the repositories to the best mirror, we need to replace the mirror in etc/apt/source.list . We can do it manually, however, to prevent the mistakes, it is better to use a dedicated python script: apt-mirror-updater . Steps: install the python script: sudo pip install apt-mirror-updater backup the old file: sudo cp sources.list sources.list.bak change the mirror with the script: apt-mirror-updater -c <mirror URL> Note that the apt-mirror-updater script can also measure the bandwidth, however, the result does not seem to be reliable.","title":"Changing default package repositories"},{"location":"Ubuntu/Linux%20Manual/#string-processing","text":"","title":"String Processing"},{"location":"Ubuntu/Linux%20Manual/#string-filtering-with-grep","text":"The grep command is used to filter lines containing a pattern. The syntax is: grep <pattern> <file> The pattern can be a simple string, or a regex. The most used options are: -v : invert the match, i.e., print only lines not matching the pattern -e : use multiple patterns (e.g., -e <pattern 1> -e <pattern 2> ) -i : ignore case","title":"String filtering with grep"},{"location":"Ubuntu/Linux%20Manual/#word-count-with-wc","text":"The wc command counts words, lines, and characters. What is counted is determined by the parameters: -w : words -l : lines -c : characters","title":"Word count with wc"},{"location":"Ubuntu/Linux%20Manual/#string-mofification-with-sed","text":"Sed is a multi purpose command for string modification. It can search and replace in string. The syntax is folowing: sed s/<search>/<replace>/[<occurance>] Example: s/$/,3/ Any regex can be used as <search> . Some characters (e.g. | ) must be escaped with backslash and used together with the -E parameter. This replace the nd of the line with string \",3\" . Note that there is a slash at the end, despite we use the default option for occurance. Also, it can delete lines containing string using: sed /<pattern>/d","title":"String mofification with sed"},{"location":"Ubuntu/Linux%20Manual/#cut","text":"Cut is a useful command for data with delimiters. Usage: cut -f <columns> Where columns are splited by comma, e.g., 2,4,7 . If we need to specify delimiters, we use the -d parameter: cut -d, -f 1,5","title":"cut"},{"location":"Ubuntu/Linux%20Manual/#awk","text":"AWK is a powerful tool for text processing. It is a programming language, so it can be used for more complex tasks than sed or cut . The basic syntax is: awk '<pattern> {<action>}' Where <pattern> is a regex and <action> is a command. The <action> is executed only if the line matches the <pattern> .","title":"AWK"},{"location":"Ubuntu/Linux%20Manual/#pattern","text":"In the awk , / is used as a delimiter of the regex pattern.","title":"Pattern"},{"location":"Ubuntu/Linux%20Manual/#action","text":"The <action> can be a sequence of commands, separated by ; . We can use column values by using special column variables: $0 : the whole line $1 : the first column ...","title":"Action"},{"location":"Ubuntu/Linux%20Manual/#trim-string","text":"<command with string output> | xargs","title":"Trim string"},{"location":"Ubuntu/Linux%20Manual/#processes","text":"for checking all processes , we can use htop to get a path to executable of a process by PID, we can use pwdx <PID> for checking a specific process, we can use ps to kill a process, we can use kill to kill a process by name, we can use pkill to get information about a process selected by name, we can use pgrep","title":"Processes"},{"location":"Ubuntu/Linux%20Manual/#pkill","text":"The pkill command kills a process by name. The syntax is: pkill <process name> important parameters: -f : match the whole command line, not only the process name -9 : force kill","title":"pkill"},{"location":"Ubuntu/Linux%20Manual/#process-info","text":"","title":"Process Info"},{"location":"Ubuntu/Linux%20Manual/#users","text":"The users are listed in /etc/passwd . The file contains one line per user, each line has the following format: <username>:<password>:<user ID>:<group ID>:<GECOS>:<home folder>:<shell> The password is typically stored in /etc/shadow and is represented by x . GECOS is some kind of a comment storing arbitrary information about the user.","title":"Users"},{"location":"Ubuntu/Linux%20Manual/#adding-a-user","text":"To add a user, we can use either the useradd binary directly, or the adduser wrapper script. Here we describe the adduser script. The basic syntax is adduser <username> . Important parameters: --gecos \"<GECOS>\" : supply the content of the GECOS field. If skipped, the command will ask for the GECOS content interactively. --shell <shell> : The shell to be used by the user. If skipped, the default shell is used. This can be used to create a user without a shell by setting the shell to /usr/sbin/nologin . Note that the adduser needs to be run as root. Otherwise it will fail with bash: adduser: command not found .","title":"Adding a user"},{"location":"Ubuntu/Linux%20Manual/#user-groups","text":"An important aspect of user management in Linux is the user groups. For example, by belonging to the sudo group, the user can execute commands with sudo . The groups are listed in /etc/group . The file contains one line per group, each line has the following format: <group name>:<password>:<group ID>:<user list> To see the groups of a user, we can use the groups command (no arguments needed). To manipulate groups and users, we need a root access. To add a user to a group, we can use the usermod command: usermod -a -G <group name> <username> To remove a user from a group, we can use the same command: usermod -G <group list> <username> where <group list> is a comma separated list of groups the user should belong to.","title":"User Groups"},{"location":"Ubuntu/Linux%20Manual/#file-ownership","text":"Each file has a pair of owners: the user owner and the group owner. These ownerships are important, as file permissions in Linux are usually set for a triplet of: owner : the user owner group : the group owner other : all other users To change the owner of a file, we can use the chown command. The syntax is: chown <user>:<group> <file> We can skip the :<group> part, in which case the group is not changed. The chown command can also be used to change the owner of a directory. In this case, the -R parameter is used to change the owner recursively.","title":"File ownership"},{"location":"Ubuntu/Linux%20Manual/#disable-access-to-shell-for-a-user","text":"To disable access to shell for a user, we have to configure his/her shell to /usr/sbin/nologin or similar. For new users, we can use the --shell parameter of the adduser command. For existing users, we can use the usermod command: usermod --shell /usr/sbin/nologin <username> Note that for some ssh client implementations, it is necessary to connect to a shell by default, otherwise, the connection is terminated immediately after login. In this case, the user has to connect to the server with the -N parameter, which tells the client not to execute any command after login.","title":"Disable access to shell for a user"},{"location":"Ubuntu/Linux%20Manual/#services-and-systemd","text":"The systemd is a system and service manager for Linux. It is used to manage services, devices, and other aspects of the system. The main command to manage services is systemctl . The general syntax is: sudo systemctl <action> <service name> The most used actions are: start : start the service stop : stop the service restart : restart the service status : get the status of the service reload : reload the configuration of the service","title":"Services and systemd"},{"location":"Ubuntu/Linux%20Manual/#get-the-status-of-a-service","text":"To get the status of a service, we can use the status action: sudo systemctl status <service name> The statuses can be: active (running) : the service is running active (exited) : the service has finished active (waiting) : the service is not running, but it is waiting for some event TODO: add more statuses","title":"Get the status of a service"},{"location":"Ubuntu/Linux%20Manual/#listing-services","text":"To list all services, we can use one of the following commands: list-units to list all units ever run on the server or list-units-files to list all units, including the ones that have never been run","title":"Listing services"},{"location":"Ubuntu/Linux%20Manual/#frequently-used-software","text":"","title":"Frequently used software"},{"location":"Ubuntu/Linux%20Manual/#installing-java","text":"","title":"Installing Java"},{"location":"Ubuntu/Linux%20Manual/#oracle-jdk","text":"Go to the download page, the link to the dowload page for current version of java is on the main JDK page . Click on the debian package, accept the license, and download it. If installing on system without GUI, copy now (after accepting the license) the target link and dowload the debian package with wget : wget --header \"Cookie: oraclelicense=accept-securebackup-cookie\" <COPIED LINK> . More info on SO . Install the package with sudo apt-get install <PATH TO DOWNLOADED .deb> if there is a problem with the isntallation, check the file integritiy with: sha256 <PATH TO DOWNLOADED .deb> . It should match with the checksums refered on the download page. If not cheksums do not match, go back to download step. In case there is another version of Java alreadz install, we need to overwrite it using the update-alternatives command: sudo update-alternatives --install /usr/bin/java java <PATH TO JAVA> <PRIORITY> . Example: sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk-16/bin/java 2 To check the priorities, call update-alternatives --query java . The newly installed JDK should have the highest priority.","title":"Oracle JDK"},{"location":"Ubuntu/Linux%20Manual/#python","text":"Python has to be executed with python3 by default, instead of python .","title":"Python"},{"location":"Ubuntu/Linux%20Manual/#gcc","text":"GCC is typically installed by default and itts minor versions are updated with the system updates. However, if we need a major version update, we have to install it manually as a new package: sudo apt install gcc-<version> This way, the new version is installed alongside the old version. To switch to the new version, we have to use the update-alternatives command: sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-<version> <priority> The <priority> is a number that determines the priority of the version. The version with the highest priority is used. To check the priorities, we can use the update-alternatives --query gcc command. Note that these steps only updates the C compiler . To affect the C++ compiler as well, we have to repeat the steps with the g++ command: sudo apt install g++-<version> sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-<version> <priority>","title":"GCC"},{"location":"Ubuntu/Linux%20Manual/#other-usefull-commands","text":"","title":"Other usefull commands"},{"location":"Ubuntu/Linux%20Manual/#selecting-lines-from-file","text":"The head and tail commands are for that, printing the top and bottom 10 lines, respectively.","title":"Selecting lines from file"},{"location":"Ubuntu/Linux%20Manual/#skip-the-header","text":"tail -n +2","title":"Skip the header"},{"location":"Ubuntu/Linux%20Manual/#print-lines-from-to","text":"tail -n +<from line> | head -n <number of lines>","title":"Print lines from to"},{"location":"Ubuntu/Linux%20Manual/#progress-bar","text":"the progress bar can be printed using the pv command. pv <file> | <other comands> # or <other comands> | pv | <other comands>","title":"Progress bar"},{"location":"Ubuntu/Linux%20Manual/#free-disk-space","text":"df -h","title":"Free disk space"},{"location":"Ubuntu/Linux%20Manual/#piping-parameters-using-xargs","text":"The xargs command transfers the output of one command into call of another command with the output of the first command as parameters of the second command. This is usefull when the second command does not accept the output of the first command as input, but accepts the output as parameters. Example: ls | xargs rm # remove all files in the current directory","title":"piping parameters using xargs"},{"location":"Ubuntu/Linux%20Manual/#upgrade","text":"For a simple system upgrade, we can use the following steps: First run the update of the current version . Then optionaly backup the WSL run sudo do-release-upgrade However, this only work if there is a LTS version available. If not, the last step will fail. In this case, we can use the do-release-upgrade -d command, which upgrades to the latest version, regardless of the LTS status.","title":"Upgrade"},{"location":"Ubuntu/Linux%20Manual/#upgrade-to-other-version-than-the-latest","text":"When upgrading to a version other than the latest, we can try the following steps: perform steps 1 and 2 from the normal upgrade open the /etc/update-manager/release-upgrades file and set the Prompt parameter to normal or lts (depending on the desired version) backup the /etc/apt/sources.list file change the sources to the new version: e.g., run sudo sed -i 's/<current version name>/<new version name>/g' /etc/apt/sources.list run a normal upgrade: sudo apt update && sudo apt upgrade finalizing the upgrade: sudo apt dist-upgrade","title":"Upgrade to other version than the latest"},{"location":"Ubuntu/Linux%20Manual/#wsl-backup","text":"check the WSL distro name: wsl -l -v shutdown WSL: wsl --shutdown backup the distro: wsl --export <disto name> <backup folder path>/<backup name>.tar","title":"WSL backup"},{"location":"Ubuntu/Linux%20Manual/#vim","text":"Vim is a console text editor. It is a modal editor, i.e., it has different modes for different operations. The most important modes are: normal mode : for navigation and file manipulation insert mode : for text editing visual mode : for text selection","title":"vim"},{"location":"Ubuntu/Linux%20Manual/#normal-mode","text":"In normal mode, we can: navigate the file using arrow keys or hjkl (left, down, up, right) enter global commands using : (e.g., :q for quit) edit file content using special commands (e.g., dd for delete line)","title":"Normal Mode"},{"location":"Ubuntu/Linux%20Manual/#global-commands","text":":q : quit :w : save :wq : save and quit :q! : quit without saving","title":"Global Commands"},{"location":"Ubuntu/Linux%20Manual/#file-editing-commands","text":"dd : delete line","title":"File Editing Commands"},{"location":"Ubuntu/Linux%20Manual/#insert-mode","text":"Insert mode is the normal text mode we know from other editors. To enter insert mode, press i . To exit insert mode, press esc .","title":"Insert Mode"},{"location":"Ubuntu/Linux%20Manual/#visual-mode","text":"Visual mode is used for text selection. To enter visual mode, press v . To exit visual mode, press esc .","title":"Visual Mode"},{"location":"Ubuntu/Linux%20Manual/#copy-and-paste","text":"Vim has its own clipboard for copy-pasting (yank, ...). However, this cannot be used to copy text outside of vim, nor to paste text from outside vim. To copy text to the system clipboard, we can: select the text using mouse or keyboard press enter to copy the text to the clipboard To paste text from the system clipboard, we press Ctrl + Shift + v .","title":"Copy and paste"},{"location":"Windows/Excel%20Manual/","text":"Move a column: hold Shift and drag the column to the new location","title":"Excel Manual"},{"location":"Windows/PowerPoint/","text":"Slide design \u00b6 Slide design should not be set separately for each slide but globaly for the whole presentation. This can be done by configuring the theme in the slide master view: View > Master Views > Slide Master . On the left, we can select a type of the slide to edit, while on the right, we edit the style for that style type. On the left, the top slide, larger than the others is the master slide . It represents the default design for the whole theme. Shareing the theme \u00b6 The theme can be exported in the slide master view by clicking: Edit theme > Save current theme . The theme can be imported in the slide master view by clicking: Edit theme > Browse for themes . Edit the master slide \u00b6 The master slide is the default design for the whole theme. We cannot add any content to it (it makes no sense, as this would be added to all slides.) However, we can set up some properties of predefined basic elements here: Title : The title of the slide. Text : The text for different levels of indentation Date Slide number : page numbering. Footer Note that the configuration here applies to all slides. Therefore, if we want to have for example date only on the first slide, we should not configure the date here, but on the first slide itself. To select which of the elements should be configured for all slides, click on Master Layout -> Master Layout and select the elements to be configured. Edit the layout slide \u00b6 Symbols \u00b6 arrow: write ==> and press space. Tables \u00b6 Tables are treated specially in PowerPoint. They can be created using the Insert > Table menu. To edit the table style, we have to select it and go to the special Table Design tab. To edit the table layaut, add rows or columns, we have to select the table and go to the Layout tab. Insert table from csv \u00b6 Unfortunately, csv text cannot be pasted directly into a PowerPoint table. However, there is a workaround: Copy the csv text into Excel. Select the cells in Excel and copy them. Paste in the powerpoint slide. This way a new table is created. Text formatting \u00b6 Subscript and superscript \u00b6 First, select the text to adjust, then: subscript: Ctrl + = superscript: Ctrl + Shift + +","title":"PowerPoint"},{"location":"Windows/PowerPoint/#slide-design","text":"Slide design should not be set separately for each slide but globaly for the whole presentation. This can be done by configuring the theme in the slide master view: View > Master Views > Slide Master . On the left, we can select a type of the slide to edit, while on the right, we edit the style for that style type. On the left, the top slide, larger than the others is the master slide . It represents the default design for the whole theme.","title":"Slide design"},{"location":"Windows/PowerPoint/#shareing-the-theme","text":"The theme can be exported in the slide master view by clicking: Edit theme > Save current theme . The theme can be imported in the slide master view by clicking: Edit theme > Browse for themes .","title":"Shareing the theme"},{"location":"Windows/PowerPoint/#edit-the-master-slide","text":"The master slide is the default design for the whole theme. We cannot add any content to it (it makes no sense, as this would be added to all slides.) However, we can set up some properties of predefined basic elements here: Title : The title of the slide. Text : The text for different levels of indentation Date Slide number : page numbering. Footer Note that the configuration here applies to all slides. Therefore, if we want to have for example date only on the first slide, we should not configure the date here, but on the first slide itself. To select which of the elements should be configured for all slides, click on Master Layout -> Master Layout and select the elements to be configured.","title":"Edit the master slide"},{"location":"Windows/PowerPoint/#edit-the-layout-slide","text":"","title":"Edit the layout slide"},{"location":"Windows/PowerPoint/#symbols","text":"arrow: write ==> and press space.","title":"Symbols"},{"location":"Windows/PowerPoint/#tables","text":"Tables are treated specially in PowerPoint. They can be created using the Insert > Table menu. To edit the table style, we have to select it and go to the special Table Design tab. To edit the table layaut, add rows or columns, we have to select the table and go to the Layout tab.","title":"Tables"},{"location":"Windows/PowerPoint/#insert-table-from-csv","text":"Unfortunately, csv text cannot be pasted directly into a PowerPoint table. However, there is a workaround: Copy the csv text into Excel. Select the cells in Excel and copy them. Paste in the powerpoint slide. This way a new table is created.","title":"Insert table from csv"},{"location":"Windows/PowerPoint/#text-formatting","text":"","title":"Text formatting"},{"location":"Windows/PowerPoint/#subscript-and-superscript","text":"First, select the text to adjust, then: subscript: Ctrl + = superscript: Ctrl + Shift + +","title":"Subscript and superscript"},{"location":"Windows/Powershell%20Manual/","text":"Introduction \u00b6 PowerShell is the new command line interface for Windows that replaces the old command prompt. It is superior in almost every aspect so it is recommended to use it instead of the old command prompt. The PowerShell script files have the .ps1 extension. In addition to system commands, PowerShell can also execute native PowerShell commands called cmdlets . Script Blocks \u00b6 documentation A basic unit of execution in PowerShell is a script block. A script block can be: a piece of code enclosed in curly braces {} . a function a script A script block can have parameters. The parameters are defined using the param keyword. Example: $myFunction = { param($param1, $param2) # do something } # or in a function function MyFunction { param($param1, $param2) # do something } # or in a script param($param1, $param2) Parameter blocks \u00b6 The parameter block defines the parameters of the script block. By default ( param() ), only the build in parameters are available. In the parameter block, individual parameters are divided by commas. Example: $myFunction = { param($param1, $param2) # do something } Parameters can be typed. Example: $myFunction = { param([int]$param1, [string]$param2, [switch]$param3) # do something } Parameters can be mandatory. Example: $myFunction = { param( [Parameter(Mandatory)][int]$param1, [Parameter(Mandatory)][string]$param2, [switch]$param3 ) # do something } We can also validate the parameters. Example: $myFunction = { param( [Parameter(Mandatory)][ValidateRange(0, 100)][int]$param1, [Parameter(Mandatory)][ValidateSet(\"a\", \"b\", \"c\")][string]$param2, [Parameter(Mandatory)][ValidateScript({$_ -eq \"a\" -or $_ -eq \"b\"})][string]$param3 ) # do something } Also with custom error message: $myFunction = { param( [Parameter(Mandatory)][ValidateScript({ Test-Path $_ }, ErrorMessage=\"Path does not exists: {0}\")][string]$path ) # do something } The advanced usage of parameters is described in the documentation . Important Aspects \u00b6 New PowerShell \u00b6 The PowerShell Integrated in Windows is version 5. You can recognize it by the iconical blue background color. This old version has some important limitations (e. g. it cannot pass arguments containing arguments with spaces ). Therefore, it is best to install the new PowerShell first. Quick Edit / Insert Mode \u00b6 PowerShell enables copy/pase of commands. The downside is that every time you click inside PowerShell, the execution (if PowerShell is currently executing somethig) stops. To resume the execution, hit enter . Arguments starting with - and containing . \u00b6 If a program argument starts with - , and contains . it needs to be wrapped by ' . Otherwise, the argument will be split on the dot. Example: mvn exec:java -Dexec.mainClass=com.example.MyExample -Dfile.encoding=UTF-8 In Powershell, this needs to be converted to: mvn exec:java '-Dexec.mainClass=com.example.MyExample' '-Dfile.encoding=UTF-8' The problem may arise if the argument or its part needs to be quotted as well. Then: for first level of quoting, use \" (double quotes) for second level of quoting, use '' (two single quotes) Example: mvn exec:exec '-Dexec.executable=\"java\"' '-Dexec.args=\"-Xmx30g -Djava.library.path=''C:\\Program Files\\HDF_Group\\HDF5\\1.14.3\\bin'' -classpath %classpath cz.cvut.fel.aic.simod.OnDemandVehiclesSimulation\"' Escaping \" and ' in Arguments \u00b6 Double quotes \" contained in arguments can be preserved by escaping with backslash: \\\" . Example for that can be passing an argument list to some executable: 'args=\\\"arg1 arg2\\\"' Single quotes ' are esceped by duble single quote: '' . Example can be passing a list of args, where some of them contains space: 'args=\\\"''arg1 with space'' arg2\\\"' No Output for EXE file \u00b6 Some errors are unfortunatelly not reported by powershell (e.g. missing dll ). The solution is to run such program in cmd, which reports the error. Command execution \u00b6 Normal commands are executed by just typing them. However, if the command contains a space, wrapping it in quotes does not work. In this case, the & operator has to be used. Example: & \"C:\\Program Files\\Java\\jdk1.8.0_181\\bin\\java.exe\" -version Variables \u00b6 Variables are defined by the $ sign. Example: $myVar = \"Hello, World!\" To print the variable, just type its name. Example: $myVar Environment variables \u00b6 They are accessed by the $env: prefix. Example: $env:PATH Operations on Variables \u00b6 The variables can be used in expressions. Example: $myVar = 5 $myVar + 3 String Operations \u00b6 Strings can be concatenated using the + operator. Examples: $myVar = \"Hello, \" + \"World!\" # append to a path: $env:PATH += \";C:\\Program Files\\Java\\jdk1.8.0_181\\bin\" # prepend to a path: $env:PATH = \"C:\\Program Files\\Java\\jdk1.8.0_181\\bin;\" + $env:PATH Operators \u00b6 Comparison and String Operators \u00b6 documentation Equality operators: -eq : equal -ne : not equal -gt : greater than -lt : less than -ge : greater or equal -le : less or equal Matching operators: -match : match Logical Operators \u00b6 documentation -and : logical and -or : logical or -not : logical not Control Structures \u00b6 Conditions \u00b6 if \u00b6 The if statement is used for conditional execution. The syntax is: if ($condition) { # do something } elseif ($anotherCondition) { # do something else } else { # do something else } The if structure is also available as a cmdlet If . Example: If ($condition) { \"True\" } Else { \"False\" } The If cmdlet is also available as an alias if and ? . Loops \u00b6 foreach \u00b6 The foreach cycle iterates over a collection. The syntax is: foreach ($item in $collection) { # do something with $item } The foreach structure is also available as a cmdlet ForEach-Object . In this case, we access the current item using the $_ variable. Example: Get-ChildItem | ForEach-Object { $_.Name } The above command lists the names of all files in the current directory. The alias for the ForEach-Object cmdlet is foreach and % . Inputs and Outputs \u00b6 Inputs \u00b6 To read a file, use the Get-Content cmdlet. Example: Get-Content \"C:\\Users\\user\\file.txt\" Outputs \u00b6 There are many output streams in PowerShell. We can use: Write-Output : for standard output Write-Error : for standard error Write-Warning : for warnings Write-Verbose : for verbose output Write-Debug : for debug output Write-Information : for information output Write-Host : for writing directly to the console. No output stream is used. By default, only the standard and error output streams are displayed. To display the other streams, we have several options: manually set the $VerbosePreference , $DebugPreference , $WarningPreference , $InformationPreference variables to Continue (default is SilentlyContinue ), or make the function or script we are running an Advanced Function or Script and use the -Verbose , -Debug , -WarningAction , -InformationAction parameters Pipes and Redirection \u00b6 pipe documentation redirection documentation Output forwading is done using | (pipe) operator, just like in Linux. For redirecting the output to a file, there are the following operators: > : redirect to file, overwrite if exists >> : redirect to file, append if exists >&1 : redirect to standard output stream When using any of these operators, by default, the standard output stream is redirected. If we want to redirect the standard error stream, we have to prepend 2 to the operator. Example: dir > out.txt # redirect standard output stream to out.txt dir 2> err.txt # redirect standard error stream to err.txt If we want both see the output and redirect it to a file, we can use the Tee-Object command which is the equivalent of the tee command from Linux In new PowerShell, we have even more options: 3> : redirect Warning stream 4> : redirect Verbose stream 5> : redirect Debug stream 6> : redirect Information stream *> : redirect all streams String Manipulation \u00b6 Replace \u00b6 For replacing a substring in a string, we have two options: Replace method -replace operator The Replace method replaces all occurences of a string with another string. The syntax is: $myString.Replace(\"oldString\", \"newString\") The -replace operator uses regular expressions for replacing. The syntax is: $myString -replace \"pattern\", \"newString\" Multiple replacements can be done using chained -replace operators. Example: $myString -replace \"pattern1\", \"newString1\" -replace \"pattern2\", \"newString2\" Match \u00b6 To test if a string matches a regular expression, use the Match method. The syntax is: $myString -match \"pattern\" Select-String \u00b6 The Select-String is the grep equivalent for PowerShell. The alias for the comand is sls . Parameters: -Content <before>[, <after>] : Select also <before> lines before the matched line and <after> lines after the matched line -Pattern : If we want to use a regular expression for searching, not a plain string Selecting the matched string \u00b6 If we use the Select-String with the -Pattern parameter, the matching lines are returned with the matching string highlighted. If we want to get only the matching string, we have to access the Matches.Value property for each line. Example: Select-String -Pattern \"pattern\" | ForEach-Object { $_.Matches.Value } Select-Object \u00b6 The Select-Object selects the specified properties of an object. Example: Get-Process | Select-Object -Property Name, Id Arrays \u00b6 To creante an array, use the @() operator: a = @() To add an element to an array, use the += operator: a += 1 . The arrays can be iterated over using the foreach loop. Network \u00b6 netstat \u00b6 The netstat is the basic network monitoring program. It displays TCP connections. It originated on Linux, so the usage and parameters are described in the Linux manual. Below, we discuss the differencies in the command interface and behavior of the Windows version of the command. Winndows manual . Lot of kubernetes entries in the log \u00b6 By default, the netstat command translates IPs into names. Unfortunatelly, on Windows, it also uses the information from the hosts file ( C:\\Windows\\System32\\drivers\\etc\\hosts ). This is a problem, because some services, like Docker, can use the hosts file to redirect some adddress to localhost. Therefore, at the end, all localhost entries are named kubernetes. Solution options: use the -n parameter for netstat or remove the redirection to localhost in the hosts file Display executable for connection \u00b6 To display executable for all connection, just use the -b swith. For filtering out only some, you have to use the -Context parameter in the Select-String command, as the executable is printed one line below the connection: netstat -b | sls <search pattern> -Context 0,1 System Information \u00b6 CIM documentation WMI classes There are several interfaces for getting system information in PowerShell: Using the Common Information Model (CIM) Using the Windows Management Instrumentation (WMI) By reading from the registry CIM and WMI \u00b6 Because the CIM and WMI interfaces are very similar, we will discuss them together. The main difference is that the CIM is the newer interface, which is more powerful and more user-friendly. The CIM is also cross-platform, while the WMI is Windows-only. The advantage of the CIM and WMI interfaces is that they are clear and object-oriented. The information can be queried using database-like operations. The disadvantage is that they are slow. The main command for getting system information is: Get-CimInstance : for CIM Get-WmiObject : for WMI For both commands, we need to specify the class of the object we want to get using the -ClassName (CIM) or -Class (WMI) parameter. The classes are the same for both interfaces. Typical classes are: Win32_ComputerSystem : information about the computer Win32_OperatingSystem : information about the operating system Win32_InstalledWin32Program : information about installed programs Win32_Product : information about programs installed using the MSI installer contains more information than Win32_InstalledWin32Program Windows Registry \u00b6 To access the Windows registry, we use the same commands that are used for file system: Get-ItemProperty : to get the value of a registry key Get-ChilItem to get a list of child keys Advanced Script blocks \u00b6 Any script block can be made an advanced script block. Advanced script blocks are run similarly to the compiled cmdlets. To make a script block an advanced script block, we have to use the [CmdletBinding()] attribute at the beginning of the script block. Example: function MyFunction { [CmdletBinding()] # do something } The advanced functions and scripts have the following features: parameters are not available as $args (have to be parsed using the param block) builtin parameters like -Verbose , -Debug , -WarningAction , -InformationAction are parsed automatically support for pipeline input Elevation \u00b6 Some commands may require administrator privileges. Therefore, it is wise to check if the script is running with administrator privileges so that the execution is not interrupted. To check if the script is running with administrator privileges, use the following code: $myWindowsID=[Security.Principal.WindowsIdentity]::GetCurrent() $myWindowsPrincipal=new-object Security.Principal.WindowsPrincipal($myWindowsID) if (!$myWindowsPrincipal.IsInRole([Security.Principal.WindowsBuiltInRole]::Administrator)) { Write-Host \"This script requires elevated privileges. Please run it as an administrator.\" exit } Another solution may be to restart the script with administrator privileges. However, this has several limitations: when the script is run with administrator privileges a new terminal window is opened parameters are not passed automatically to the new script. We can manually pass them using the -ArgumentList parameter. However, this is problematic for non-string parameters, especially in an advanced script block. To restart the script with administrator privileges, use the following code: $argString = $args -join ' ' Start-Process \"pwsh\" -Verb RunAs -ArgumentList \"-noexit -File `\"$PSCommandPath`\" $argString\" exit Usefull Commands \u00b6 File Encoding Conversion \u00b6 gc [INPUT PATH] | Out-File -en [ENCODING] [OUTPUT PATH] example: gc \"C:\\AIC data\\Roadmaptools big data test/map-filtered.osm\" | Out-File -en utf8 \"C:\\AIC data\\Roadmaptools big data test/map-filtered-utf8.osm\" NOTE: it is not very fast :) Delete all files with a certain extension \u00b6 ls *.extension -Recurse | foreach {rm $_} to try it, add the -WhatIf parameter to rm Batch rename \u00b6 Example dir . | % { $newName = $_.Name -replace '^DSC_0(.*)', 'DSC_1$1'; rename-item -newname $newName -literalPath $_.Fullname -whatif} Count Lines in large file \u00b6 switch -File FILE { default { ++$count } } Get Help \u00b6 Mirosoft documentation To get help about a command, use the Get-Help (alias man ) command. Example: Get-Help Get-ChildItem If the output is the list of articles, it means that there is no help for the command. Translate alias to command \u00b6 To translate an alias to a command, use the Get-Alias command. Example: Get-Alias ls # returns Get-ChildItem","title":"Powershell Manual"},{"location":"Windows/Powershell%20Manual/#introduction","text":"PowerShell is the new command line interface for Windows that replaces the old command prompt. It is superior in almost every aspect so it is recommended to use it instead of the old command prompt. The PowerShell script files have the .ps1 extension. In addition to system commands, PowerShell can also execute native PowerShell commands called cmdlets .","title":"Introduction"},{"location":"Windows/Powershell%20Manual/#script-blocks","text":"documentation A basic unit of execution in PowerShell is a script block. A script block can be: a piece of code enclosed in curly braces {} . a function a script A script block can have parameters. The parameters are defined using the param keyword. Example: $myFunction = { param($param1, $param2) # do something } # or in a function function MyFunction { param($param1, $param2) # do something } # or in a script param($param1, $param2)","title":"Script Blocks"},{"location":"Windows/Powershell%20Manual/#parameter-blocks","text":"The parameter block defines the parameters of the script block. By default ( param() ), only the build in parameters are available. In the parameter block, individual parameters are divided by commas. Example: $myFunction = { param($param1, $param2) # do something } Parameters can be typed. Example: $myFunction = { param([int]$param1, [string]$param2, [switch]$param3) # do something } Parameters can be mandatory. Example: $myFunction = { param( [Parameter(Mandatory)][int]$param1, [Parameter(Mandatory)][string]$param2, [switch]$param3 ) # do something } We can also validate the parameters. Example: $myFunction = { param( [Parameter(Mandatory)][ValidateRange(0, 100)][int]$param1, [Parameter(Mandatory)][ValidateSet(\"a\", \"b\", \"c\")][string]$param2, [Parameter(Mandatory)][ValidateScript({$_ -eq \"a\" -or $_ -eq \"b\"})][string]$param3 ) # do something } Also with custom error message: $myFunction = { param( [Parameter(Mandatory)][ValidateScript({ Test-Path $_ }, ErrorMessage=\"Path does not exists: {0}\")][string]$path ) # do something } The advanced usage of parameters is described in the documentation .","title":"Parameter blocks"},{"location":"Windows/Powershell%20Manual/#important-aspects","text":"","title":"Important Aspects"},{"location":"Windows/Powershell%20Manual/#new-powershell","text":"The PowerShell Integrated in Windows is version 5. You can recognize it by the iconical blue background color. This old version has some important limitations (e. g. it cannot pass arguments containing arguments with spaces ). Therefore, it is best to install the new PowerShell first.","title":"New PowerShell"},{"location":"Windows/Powershell%20Manual/#quick-edit-insert-mode","text":"PowerShell enables copy/pase of commands. The downside is that every time you click inside PowerShell, the execution (if PowerShell is currently executing somethig) stops. To resume the execution, hit enter .","title":"Quick Edit / Insert Mode"},{"location":"Windows/Powershell%20Manual/#arguments-starting-with-and-containing","text":"If a program argument starts with - , and contains . it needs to be wrapped by ' . Otherwise, the argument will be split on the dot. Example: mvn exec:java -Dexec.mainClass=com.example.MyExample -Dfile.encoding=UTF-8 In Powershell, this needs to be converted to: mvn exec:java '-Dexec.mainClass=com.example.MyExample' '-Dfile.encoding=UTF-8' The problem may arise if the argument or its part needs to be quotted as well. Then: for first level of quoting, use \" (double quotes) for second level of quoting, use '' (two single quotes) Example: mvn exec:exec '-Dexec.executable=\"java\"' '-Dexec.args=\"-Xmx30g -Djava.library.path=''C:\\Program Files\\HDF_Group\\HDF5\\1.14.3\\bin'' -classpath %classpath cz.cvut.fel.aic.simod.OnDemandVehiclesSimulation\"'","title":"Arguments starting with - and containing ."},{"location":"Windows/Powershell%20Manual/#escaping-and-in-arguments","text":"Double quotes \" contained in arguments can be preserved by escaping with backslash: \\\" . Example for that can be passing an argument list to some executable: 'args=\\\"arg1 arg2\\\"' Single quotes ' are esceped by duble single quote: '' . Example can be passing a list of args, where some of them contains space: 'args=\\\"''arg1 with space'' arg2\\\"'","title":"Escaping \" and ' in Arguments"},{"location":"Windows/Powershell%20Manual/#no-output-for-exe-file","text":"Some errors are unfortunatelly not reported by powershell (e.g. missing dll ). The solution is to run such program in cmd, which reports the error.","title":"No Output for EXE file"},{"location":"Windows/Powershell%20Manual/#command-execution","text":"Normal commands are executed by just typing them. However, if the command contains a space, wrapping it in quotes does not work. In this case, the & operator has to be used. Example: & \"C:\\Program Files\\Java\\jdk1.8.0_181\\bin\\java.exe\" -version","title":"Command execution"},{"location":"Windows/Powershell%20Manual/#variables","text":"Variables are defined by the $ sign. Example: $myVar = \"Hello, World!\" To print the variable, just type its name. Example: $myVar","title":"Variables"},{"location":"Windows/Powershell%20Manual/#environment-variables","text":"They are accessed by the $env: prefix. Example: $env:PATH","title":"Environment variables"},{"location":"Windows/Powershell%20Manual/#operations-on-variables","text":"The variables can be used in expressions. Example: $myVar = 5 $myVar + 3","title":"Operations on Variables"},{"location":"Windows/Powershell%20Manual/#string-operations","text":"Strings can be concatenated using the + operator. Examples: $myVar = \"Hello, \" + \"World!\" # append to a path: $env:PATH += \";C:\\Program Files\\Java\\jdk1.8.0_181\\bin\" # prepend to a path: $env:PATH = \"C:\\Program Files\\Java\\jdk1.8.0_181\\bin;\" + $env:PATH","title":"String Operations"},{"location":"Windows/Powershell%20Manual/#operators","text":"","title":"Operators"},{"location":"Windows/Powershell%20Manual/#comparison-and-string-operators","text":"documentation Equality operators: -eq : equal -ne : not equal -gt : greater than -lt : less than -ge : greater or equal -le : less or equal Matching operators: -match : match","title":"Comparison and String Operators"},{"location":"Windows/Powershell%20Manual/#logical-operators","text":"documentation -and : logical and -or : logical or -not : logical not","title":"Logical Operators"},{"location":"Windows/Powershell%20Manual/#control-structures","text":"","title":"Control Structures"},{"location":"Windows/Powershell%20Manual/#conditions","text":"","title":"Conditions"},{"location":"Windows/Powershell%20Manual/#if","text":"The if statement is used for conditional execution. The syntax is: if ($condition) { # do something } elseif ($anotherCondition) { # do something else } else { # do something else } The if structure is also available as a cmdlet If . Example: If ($condition) { \"True\" } Else { \"False\" } The If cmdlet is also available as an alias if and ? .","title":"if"},{"location":"Windows/Powershell%20Manual/#loops","text":"","title":"Loops"},{"location":"Windows/Powershell%20Manual/#foreach","text":"The foreach cycle iterates over a collection. The syntax is: foreach ($item in $collection) { # do something with $item } The foreach structure is also available as a cmdlet ForEach-Object . In this case, we access the current item using the $_ variable. Example: Get-ChildItem | ForEach-Object { $_.Name } The above command lists the names of all files in the current directory. The alias for the ForEach-Object cmdlet is foreach and % .","title":"foreach"},{"location":"Windows/Powershell%20Manual/#inputs-and-outputs","text":"","title":"Inputs and Outputs"},{"location":"Windows/Powershell%20Manual/#inputs","text":"To read a file, use the Get-Content cmdlet. Example: Get-Content \"C:\\Users\\user\\file.txt\"","title":"Inputs"},{"location":"Windows/Powershell%20Manual/#outputs","text":"There are many output streams in PowerShell. We can use: Write-Output : for standard output Write-Error : for standard error Write-Warning : for warnings Write-Verbose : for verbose output Write-Debug : for debug output Write-Information : for information output Write-Host : for writing directly to the console. No output stream is used. By default, only the standard and error output streams are displayed. To display the other streams, we have several options: manually set the $VerbosePreference , $DebugPreference , $WarningPreference , $InformationPreference variables to Continue (default is SilentlyContinue ), or make the function or script we are running an Advanced Function or Script and use the -Verbose , -Debug , -WarningAction , -InformationAction parameters","title":"Outputs"},{"location":"Windows/Powershell%20Manual/#pipes-and-redirection","text":"pipe documentation redirection documentation Output forwading is done using | (pipe) operator, just like in Linux. For redirecting the output to a file, there are the following operators: > : redirect to file, overwrite if exists >> : redirect to file, append if exists >&1 : redirect to standard output stream When using any of these operators, by default, the standard output stream is redirected. If we want to redirect the standard error stream, we have to prepend 2 to the operator. Example: dir > out.txt # redirect standard output stream to out.txt dir 2> err.txt # redirect standard error stream to err.txt If we want both see the output and redirect it to a file, we can use the Tee-Object command which is the equivalent of the tee command from Linux In new PowerShell, we have even more options: 3> : redirect Warning stream 4> : redirect Verbose stream 5> : redirect Debug stream 6> : redirect Information stream *> : redirect all streams","title":"Pipes and Redirection"},{"location":"Windows/Powershell%20Manual/#string-manipulation","text":"","title":"String Manipulation"},{"location":"Windows/Powershell%20Manual/#replace","text":"For replacing a substring in a string, we have two options: Replace method -replace operator The Replace method replaces all occurences of a string with another string. The syntax is: $myString.Replace(\"oldString\", \"newString\") The -replace operator uses regular expressions for replacing. The syntax is: $myString -replace \"pattern\", \"newString\" Multiple replacements can be done using chained -replace operators. Example: $myString -replace \"pattern1\", \"newString1\" -replace \"pattern2\", \"newString2\"","title":"Replace"},{"location":"Windows/Powershell%20Manual/#match","text":"To test if a string matches a regular expression, use the Match method. The syntax is: $myString -match \"pattern\"","title":"Match"},{"location":"Windows/Powershell%20Manual/#select-string","text":"The Select-String is the grep equivalent for PowerShell. The alias for the comand is sls . Parameters: -Content <before>[, <after>] : Select also <before> lines before the matched line and <after> lines after the matched line -Pattern : If we want to use a regular expression for searching, not a plain string","title":"Select-String"},{"location":"Windows/Powershell%20Manual/#selecting-the-matched-string","text":"If we use the Select-String with the -Pattern parameter, the matching lines are returned with the matching string highlighted. If we want to get only the matching string, we have to access the Matches.Value property for each line. Example: Select-String -Pattern \"pattern\" | ForEach-Object { $_.Matches.Value }","title":"Selecting the matched string"},{"location":"Windows/Powershell%20Manual/#select-object","text":"The Select-Object selects the specified properties of an object. Example: Get-Process | Select-Object -Property Name, Id","title":"Select-Object"},{"location":"Windows/Powershell%20Manual/#arrays","text":"To creante an array, use the @() operator: a = @() To add an element to an array, use the += operator: a += 1 . The arrays can be iterated over using the foreach loop.","title":"Arrays"},{"location":"Windows/Powershell%20Manual/#network","text":"","title":"Network"},{"location":"Windows/Powershell%20Manual/#netstat","text":"The netstat is the basic network monitoring program. It displays TCP connections. It originated on Linux, so the usage and parameters are described in the Linux manual. Below, we discuss the differencies in the command interface and behavior of the Windows version of the command. Winndows manual .","title":"netstat"},{"location":"Windows/Powershell%20Manual/#lot-of-kubernetes-entries-in-the-log","text":"By default, the netstat command translates IPs into names. Unfortunatelly, on Windows, it also uses the information from the hosts file ( C:\\Windows\\System32\\drivers\\etc\\hosts ). This is a problem, because some services, like Docker, can use the hosts file to redirect some adddress to localhost. Therefore, at the end, all localhost entries are named kubernetes. Solution options: use the -n parameter for netstat or remove the redirection to localhost in the hosts file","title":"Lot of kubernetes entries in the log"},{"location":"Windows/Powershell%20Manual/#display-executable-for-connection","text":"To display executable for all connection, just use the -b swith. For filtering out only some, you have to use the -Context parameter in the Select-String command, as the executable is printed one line below the connection: netstat -b | sls <search pattern> -Context 0,1","title":"Display executable for connection"},{"location":"Windows/Powershell%20Manual/#system-information","text":"CIM documentation WMI classes There are several interfaces for getting system information in PowerShell: Using the Common Information Model (CIM) Using the Windows Management Instrumentation (WMI) By reading from the registry","title":"System Information"},{"location":"Windows/Powershell%20Manual/#cim-and-wmi","text":"Because the CIM and WMI interfaces are very similar, we will discuss them together. The main difference is that the CIM is the newer interface, which is more powerful and more user-friendly. The CIM is also cross-platform, while the WMI is Windows-only. The advantage of the CIM and WMI interfaces is that they are clear and object-oriented. The information can be queried using database-like operations. The disadvantage is that they are slow. The main command for getting system information is: Get-CimInstance : for CIM Get-WmiObject : for WMI For both commands, we need to specify the class of the object we want to get using the -ClassName (CIM) or -Class (WMI) parameter. The classes are the same for both interfaces. Typical classes are: Win32_ComputerSystem : information about the computer Win32_OperatingSystem : information about the operating system Win32_InstalledWin32Program : information about installed programs Win32_Product : information about programs installed using the MSI installer contains more information than Win32_InstalledWin32Program","title":"CIM and WMI"},{"location":"Windows/Powershell%20Manual/#windows-registry","text":"To access the Windows registry, we use the same commands that are used for file system: Get-ItemProperty : to get the value of a registry key Get-ChilItem to get a list of child keys","title":"Windows Registry"},{"location":"Windows/Powershell%20Manual/#advanced-script-blocks","text":"Any script block can be made an advanced script block. Advanced script blocks are run similarly to the compiled cmdlets. To make a script block an advanced script block, we have to use the [CmdletBinding()] attribute at the beginning of the script block. Example: function MyFunction { [CmdletBinding()] # do something } The advanced functions and scripts have the following features: parameters are not available as $args (have to be parsed using the param block) builtin parameters like -Verbose , -Debug , -WarningAction , -InformationAction are parsed automatically support for pipeline input","title":"Advanced Script blocks"},{"location":"Windows/Powershell%20Manual/#elevation","text":"Some commands may require administrator privileges. Therefore, it is wise to check if the script is running with administrator privileges so that the execution is not interrupted. To check if the script is running with administrator privileges, use the following code: $myWindowsID=[Security.Principal.WindowsIdentity]::GetCurrent() $myWindowsPrincipal=new-object Security.Principal.WindowsPrincipal($myWindowsID) if (!$myWindowsPrincipal.IsInRole([Security.Principal.WindowsBuiltInRole]::Administrator)) { Write-Host \"This script requires elevated privileges. Please run it as an administrator.\" exit } Another solution may be to restart the script with administrator privileges. However, this has several limitations: when the script is run with administrator privileges a new terminal window is opened parameters are not passed automatically to the new script. We can manually pass them using the -ArgumentList parameter. However, this is problematic for non-string parameters, especially in an advanced script block. To restart the script with administrator privileges, use the following code: $argString = $args -join ' ' Start-Process \"pwsh\" -Verb RunAs -ArgumentList \"-noexit -File `\"$PSCommandPath`\" $argString\" exit","title":"Elevation"},{"location":"Windows/Powershell%20Manual/#usefull-commands","text":"","title":"Usefull Commands"},{"location":"Windows/Powershell%20Manual/#file-encoding-conversion","text":"gc [INPUT PATH] | Out-File -en [ENCODING] [OUTPUT PATH] example: gc \"C:\\AIC data\\Roadmaptools big data test/map-filtered.osm\" | Out-File -en utf8 \"C:\\AIC data\\Roadmaptools big data test/map-filtered-utf8.osm\" NOTE: it is not very fast :)","title":"File Encoding Conversion"},{"location":"Windows/Powershell%20Manual/#delete-all-files-with-a-certain-extension","text":"ls *.extension -Recurse | foreach {rm $_} to try it, add the -WhatIf parameter to rm","title":"Delete all files with a certain extension"},{"location":"Windows/Powershell%20Manual/#batch-rename","text":"Example dir . | % { $newName = $_.Name -replace '^DSC_0(.*)', 'DSC_1$1'; rename-item -newname $newName -literalPath $_.Fullname -whatif}","title":"Batch rename"},{"location":"Windows/Powershell%20Manual/#count-lines-in-large-file","text":"switch -File FILE { default { ++$count } }","title":"Count Lines in large file"},{"location":"Windows/Powershell%20Manual/#get-help","text":"Mirosoft documentation To get help about a command, use the Get-Help (alias man ) command. Example: Get-Help Get-ChildItem If the output is the list of articles, it means that there is no help for the command.","title":"Get Help"},{"location":"Windows/Powershell%20Manual/#translate-alias-to-command","text":"To translate an alias to a command, use the Get-Alias command. Example: Get-Alias ls # returns Get-ChildItem","title":"Translate alias to command"},{"location":"Windows/VSCode/","text":"Settings \u00b6 Language specific settings \u00b6 Almost all settings can be set specifically to some language. To do that: in settings, next to the filter box, click on the filter icon and select language select the language find the setting either manually or by adding more filters change the setting To be sure that the setting is applied only to the selected language, look at the panel under the search box. Instead of User , Workspace , there should be User[<language>] , Workspace[<language>] .","title":"VSCode"},{"location":"Windows/VSCode/#settings","text":"","title":"Settings"},{"location":"Windows/VSCode/#language-specific-settings","text":"Almost all settings can be set specifically to some language. To do that: in settings, next to the filter box, click on the filter icon and select language select the language find the setting either manually or by adding more filters change the setting To be sure that the setting is applied only to the selected language, look at the panel under the search box. Instead of User , Workspace , there should be User[<language>] , Workspace[<language>] .","title":"Language specific settings"},{"location":"Windows/Windows%20Manual/","text":"Terminals and Shells \u00b6 There are two main terminal applications in Windows: Windows Console Host : an old terminal application present in Windows for years, and Windows Terminal : a new terminal application that is still in development. To get the best experience similar to Linux, it is recommended to use the Windows Terminal. It is available in the Microsoft Store . The simplest shell is the command prompt ( cmd.exe ). However, due to its limited functionality, it is better to use PowerShell . For PowerShell solutions/guides, check the PoweShell manual . Run a non-default terminal application \u00b6 It is not an easy task to manually run a shell in a non-default terminal application. To do it, we need to execute the terminal application executable: Windows Terminal: wt.exe Windows Console Host: conhost.exe Windows Terminal \u00b6 The Windows Terminal is a new terminal application with many great features: multiple tabs smart text selection To configure Windows Terminal, click the v button in the top bar and select Settings . There are two kinds of settings: Global settings sorted in categories in the left panel Profile settings for each profile. The profiles are listed in the left panel. After selecting a profile, the settings are displayed in the right panel. Nonbasic settings are sorted in categories under the Additonal settings header. Windows terminal pass stdout to text editor instead displaying it \u00b6 This can happen if the output is too long or wide for the terminal. If a command outputs a text in Windows console host but passes it to a text editor in Windows Terminal, the possible cause is the font size. The default size in Windows Terminal is 12, which is much larger than the 16 in Windows Console Host (default). To fix it, change the font size in Windows Terminal to 10 which corresponds to the 16 in Windows Console Host. Keyboard Shortcuts \u00b6 Alt + Shift : change input language Win + Space : change keyboard input method Wireless Network \u00b6 Problem: Can't connect to this network \u00b6 Solution: Forget the connection and connect to the network manually Connect to a Network Manually \u00b6 Control Panel -> Network and Internet -> Network and Sharing Center Set up a new connection or network Manually connect to a wireless network Fill the credentials: Network name: SSID Security type: depends, try WPA2 personal Security key: password Click next Close the dialog Click the wifi icon in the taskbar and connect to the network There are various usefull comands. For most of the commands, you need to open PowerShell as admin. Various Commands Related to the Wifi \u00b6 Show All Network Profiles \u00b6 This command show network configurations stored on the device. netsh wlan show profile Various Wifi Reports in HTML \u00b6 netsh wlan show wlanreport Changing the input method \u00b6 It is possible to let the system have a different input method for each app. It is not possible however, to remember the input method (after app/OS restart). Troubleshooting \u00b6 Nothing happens after clicking on the input method in the taskbar (windows 10) \u00b6 restrat the computer :) Bluetooth \u00b6 Troiubleshooting \u00b6 Cannot connect to the device \u00b6 Try to remove the device and pair it with the PC again If it does not help, proceeed to the next section (even if the pairing is successfull) Cannot pair with the device \u00b6 Turn off the device and unplug it from the electricity/remove batteries. Then plug it back after ~10 seconds, power it of, and try to pair with it again. Bluetooth Command Line Tools \u00b6 https://bluetoothinstaller.com/bluetooth-command-line-tools Bluetooth Command Line Tools is a set off tools that enables command line interaction with blootooth services. Basic usage: discover and list available devices: btdiscovery -s Filesytem \u00b6 Standard folder structure \u00b6 In Windows, the standard folder structure is completely different for system and user instalation. Details are listed below, but the main difference is that the system instalations are stored in a single root folder for each application (similarly to Android), while the user instalations' files are distributed among multiple folders, depending on the type of the file (similarly to Linux). User home folder \u00b6 The user home folder is located in C:\\Users\\<username> by default. It is aliased as %userprofile% . System instalation folders \u00b6 If an application is installed for all users, all its files are usually installed in a single folder per application. The location of the folder depends on the type of the application: C:\\Program Files : 64-bit applications C:\\Program Files (x86) : 32-bit applications If the application needs to store some data, they are usually stored in the C:\\ProgramData (aliased as %programdata% ) folder. User instalation folders \u00b6 User instalations are stored in multiple folders, depending on the type of the file. All these folders are located in the user's home folder, which is C:\\Users\\<username> by default. The folders are: ~\\AppData\\Local : Program data and sometimes also executables ~\\AppData\\Local\\Promgrams : program files and executables ~\\AppData\\LocalLow : ~\\AppData\\Roaming (aliased as %appdata% ): Start Menu folder \u00b6 The user specific shortcuts are stored in: %appdata%\\Microsoft\\Windows\\Start Menu\\Programs . The system wide shortcuts are stored in: %programdata%\\Microsoft\\Windows\\Start Menu\\Programs . Read Only Files and Folders \u00b6 An ancient form of file protection on Windows is the read only flag that can be set on files and folders. It is not a real protection, as it can be easily removed by the user, but it can be used to prevent accidental changes. Most of the programs can ignore this flag and work with the file anyway. However, some programs (e.g. Python) can have problems with it. Sugarsync \u00b6 Quick orientation in the desktop app: for file changes, check left menu -> Activity for deleted files, check left menu -> Deleted Items Solving sync problems \u00b6 check if the file is updated in cloud using web browser if not, check the activity log on the computer with the updated file if the change is not in the log, a simple hack can help: copy the file outside SugarSync folder and back. Useful Commands \u00b6 Get Motherboard Info \u00b6 wmic baseboard get product,Manufacturer,version,serialnumber Copy multiple files/dirs \u00b6 robocopy is the best command for that. Usefull params: /e : Copy subdirectories, including empty ones /b : Copy in the backup mode, so that even files with a different owner can be copied /xc : Excludes changed files. /xn : Excludes newer files. /xo : Excludes older files. /r:<n> : Specifies the number of retries on failed copies. The default value of n is 1,000,000 (one million retries). /w:<n> : Specifies the wait time between retries, in seconds. The default value of n is 30 (wait time 30 seconds). User info \u00b6 Information about users can be obtained with the Get-LocalUser command. By default, the command lists all users. Some useful params: -Name : Specifies the user account names of the users to get. -SID : Specifies the security identifier (SID) of the users to get. Installation \u00b6 Windows 11 \u00b6 Windows 11 can be installed only as an update of Windows 10. Windows 10 \u00b6 Can be installed from bootable USB created by a tool downloaded from the official Miccosoft website. Single image for all Windows versions, a particular version is choosen based on the license key. Steps: Download the install tool from Microsoft Create a bootable USB Start the installation Fill in the licence key we couldn\u2019t create a partition or locate an existing one \u00b6 Ensure that the boot priority of the drive where the Windows should be installed is right behind the installation USB priority. Configuration \u00b6 Right Click Menu \u00b6 Unfortunatelly, the right click menu is not directly configurable in Windows. Usually, the actions are enabled by the application installation (sometimes, this can be disabled in the installation process), and can only be removed by editing the registry or uninstalling the application. Below, we list instructions for each specific action. Share with Skype \u00b6 in an elevated PowerShell, run: PowerShell REG ADD \"HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Shell Extensions\\Blocked\" /v \"{776DBC8D-7347-478C-8D71-791E12EF49D8}\" /d Skype restart the Explorer PowerToys modules \u00b6 These can be removed by deactivating the specific modules in the PowerToys settings. Edit with Notepad \u00b6 Just Uninstall the Notepad. Yes, it can be done. Scan with Microsoft Defender \u00b6 The following commands removes four entries from the registry that are related to this icon: REG DELETE \"HKEY_CLASSES_ROOT\\*\\shellex\\ContextMenuHandlers\\EPP\" REG DELETE \"HKEY_CLASSES_ROOT\\CLSID\\{09A47860-11B0-4DA5-AFA5-26D86198A780}\" REG DELETE \"HKEY_CLASSES_ROOT\\Directory\\shellex\\ContextMenuHandlers\\EPP\" REG DELETE \"HKEY_CLASSES_ROOT\\Drive\\shellex\\ContextMenuHandlers\\EPP\" Source Translate with Deepl \u00b6 Haven't found a way to remove it yet. Even uninstalling the Deepl does not help. Diskpart \u00b6 Diskpart is a useful command line tool for work with disks, partitions, etc. Find out wheteher a disk is MBR or GPT \u00b6 Open Command Promp from the Windows 10 USB \u00b6 Insert the USB stick Wait till the first installation screen shift + F10 Firewall \u00b6 Generate firewall logs \u00b6 Go to Windows firewall and select properties on the right At the top, choose the profile corresponding to the current network profile In the logging section, click to customizze set both DROP and ACCEPT to yes Do not forgot to turn of the logging after the investigation! SSH \u00b6 For ssh, we can use the standard ssh commannd available in Windows (check Linux manual for more info). If the command is not available, it can be installed in Apps & Features -> Optional features . One thing that differs from Linux is that the Windows ssh does not support the <addres>:<port> syntax. To specify the port, it is necessary to use the -p parameter For more features, we can use other programs KiTTY for credentials storage, automatic reconection, etc. WinSCP for file manipulation KiTTY \u00b6 It is best to use the portable version, so that nothing is stored in the Windows registry. Configurtation: copy the PuTTY credentials : .\\kitty_portable-0.76.1.3.exe -convert-dir auto reconnect: Connection -> auto reconnect on connection failure and auto reconnect on system wakeup WinSCP \u00b6 WinSCP is a graphical tool for file manipulation. Ii can be used both for local and remote files, and it supports various protocols (FTP, SFTP, SCP, WebDAV, etc.). Adding a new connection \u00b6 There is a simple New Site button on the left, which opens a straightforward dialog. The only complicated thing can be the SSH key. To add it, click on the Advanced button and go to the SSH -> Authentication tab. There, we can select the private key file. Bookmarks \u00b6 To add bookmarks, go to Local / Remote -> Add Path to Bookmarks or press Ctrl + B . To open a bookmark, go to Local / Remote -> Go To -> Open Drirectory/bookmark or press Ctrl + O . SSH key agent \u00b6 To enable the ssh-agent on Windows, one extra step is needed: we need to start the ssh-agent service. To do that, open the services manager and start the OpenSSH Authentication Agent service. Git \u00b6 Most of the git functionality is the same as in Linux, so check the Linux manual for more info. However, there are some important differences mostly resulting from the fact that Git is not a native Windows application, but it runs in MinGW. Git on Windows and SSH \u00b6 As the git on Windows runs in MinGW, it does not use the Windows SSH command. That can be problematic if we want to debug the SSH connection using the env variable or configuring an ssh key agent. To force git to use the Windows SSH, we need to set the sshCommand config variable to the path to the Windows SSH: git config --global core.sshCommand C:/Windows/System32/OpenSSH/ssh.exe Dual Boot \u00b6 Make Windows Work after Linux Uninstall if the Bootloader is Grub \u00b6 Get rid of the Grub bootloader Set the Windows bootloader as the primary partition it is the small partition at the beginning of the main disk Problems \u00b6 Folder Sharing Problems \u00b6 Note that updated Windows 10 disabled anonymous sharing , so password protected sharing has to be turned on. To login, use the credentials for the computer with the shared folder . Below is a list of possible problems, together with solutions. The user name or password is incorrect \u00b6 Check whether the computer can be seen in the network. If not, resolve this issue first. quick check by running net view <IP address> Check that you are using the right username. You need to use the username and password of the computer you are connecting to . Check that the user name is correct by running net user on the target computer Check that the folder is shared with you in: right click on the folder -> Properties -> Sharing -> Advanced Sharing... -> Permisions . Note that your full name can be there instead of your username, which is OK. Check that you are using the right password. You have to use the password associated with your microsoft account. Note that it can differ from the password (PIN) you are using to log in to the computer! check it on the command line: net use * \\\\<IP address>\\<drive letter>$ /use:<username> <password> Folder right and ownership cannot be read \u00b6 Try to clear the windows filecache (CCcleaner or restart) Computer does not see itself in Network section in File Explorer \u00b6 Solution to this problem is to restart the service called Function Discovery Resource Publication . Either restart it in Computer Management -> Services, or by: net stop FDResPub net start FDResPub PC wakes up or cannot enter sleep \u00b6 1 Find the source \u00b6 Using the Event viwer open the event viewer go to windows logs -> system In case of wake up inspect the logs when the wake up happened and search for the Information log with the message \"The system has returned from a low power state.\" There is a wake up source in the end of the log message. If the soure is Unknown go to the next section In case of not entering sleep Search for the any kernel power event If there is an event stating: The system is entering connected standby , it means that the modern fake sleep is present in the system, replacing the real sleep mode. Using command line (admin): 1. Try powercfg -lastwake 2. If the results are not know, try to call powercfg -devicequery wake_armed to get the list of devices that can wake the computer 2 Solve the problem \u00b6 Device waken up by network adapter \u00b6 Open device manager and search for the specific network adapter right click -> Properties -> Power Management Check Only allow a magic packet to wake up the computer The real sleep mode is not available on the system \u00b6 If this is the case, use the hibernate mode instead. To add it to the start menu: go to Control panel -> Hardware and sound -> Power options click on the left panel to Choose what the power buttons does click on Change settings that are currently unavailable check the hibernate checkbox below Camera problem \u00b6 Symptoms: the screen is blank, black, single color, in all apps and there are no problems reported in device manager Cause: it can be caused by some external cameras (now disconnected) that are still selected in the apps using the camera. Go Solution: Go to the app setting and select the correct camera Phone app cannot see the connected cell phone \u00b6 It can be due to the fucked up Windows N edition. Just install the normal edition. vmmem process uses a lot of CPU \u00b6 This process represents all virtual systems. One cultprit is therefore WSL. Try to shutdown the WSL using wsl --shutdown","title":"Windows Manual"},{"location":"Windows/Windows%20Manual/#terminals-and-shells","text":"There are two main terminal applications in Windows: Windows Console Host : an old terminal application present in Windows for years, and Windows Terminal : a new terminal application that is still in development. To get the best experience similar to Linux, it is recommended to use the Windows Terminal. It is available in the Microsoft Store . The simplest shell is the command prompt ( cmd.exe ). However, due to its limited functionality, it is better to use PowerShell . For PowerShell solutions/guides, check the PoweShell manual .","title":"Terminals and Shells"},{"location":"Windows/Windows%20Manual/#run-a-non-default-terminal-application","text":"It is not an easy task to manually run a shell in a non-default terminal application. To do it, we need to execute the terminal application executable: Windows Terminal: wt.exe Windows Console Host: conhost.exe","title":"Run a non-default terminal application"},{"location":"Windows/Windows%20Manual/#windows-terminal","text":"The Windows Terminal is a new terminal application with many great features: multiple tabs smart text selection To configure Windows Terminal, click the v button in the top bar and select Settings . There are two kinds of settings: Global settings sorted in categories in the left panel Profile settings for each profile. The profiles are listed in the left panel. After selecting a profile, the settings are displayed in the right panel. Nonbasic settings are sorted in categories under the Additonal settings header.","title":"Windows Terminal"},{"location":"Windows/Windows%20Manual/#windows-terminal-pass-stdout-to-text-editor-instead-displaying-it","text":"This can happen if the output is too long or wide for the terminal. If a command outputs a text in Windows console host but passes it to a text editor in Windows Terminal, the possible cause is the font size. The default size in Windows Terminal is 12, which is much larger than the 16 in Windows Console Host (default). To fix it, change the font size in Windows Terminal to 10 which corresponds to the 16 in Windows Console Host.","title":"Windows terminal pass stdout to text editor instead displaying it"},{"location":"Windows/Windows%20Manual/#keyboard-shortcuts","text":"Alt + Shift : change input language Win + Space : change keyboard input method","title":"Keyboard Shortcuts"},{"location":"Windows/Windows%20Manual/#wireless-network","text":"","title":"Wireless Network"},{"location":"Windows/Windows%20Manual/#problem-cant-connect-to-this-network","text":"Solution: Forget the connection and connect to the network manually","title":"Problem: Can't connect to this network"},{"location":"Windows/Windows%20Manual/#connect-to-a-network-manually","text":"Control Panel -> Network and Internet -> Network and Sharing Center Set up a new connection or network Manually connect to a wireless network Fill the credentials: Network name: SSID Security type: depends, try WPA2 personal Security key: password Click next Close the dialog Click the wifi icon in the taskbar and connect to the network There are various usefull comands. For most of the commands, you need to open PowerShell as admin.","title":"Connect to a Network Manually"},{"location":"Windows/Windows%20Manual/#various-commands-related-to-the-wifi","text":"","title":"Various Commands Related to the Wifi"},{"location":"Windows/Windows%20Manual/#show-all-network-profiles","text":"This command show network configurations stored on the device. netsh wlan show profile","title":"Show All Network Profiles"},{"location":"Windows/Windows%20Manual/#various-wifi-reports-in-html","text":"netsh wlan show wlanreport","title":"Various Wifi Reports in HTML"},{"location":"Windows/Windows%20Manual/#changing-the-input-method","text":"It is possible to let the system have a different input method for each app. It is not possible however, to remember the input method (after app/OS restart).","title":"Changing the input method"},{"location":"Windows/Windows%20Manual/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Windows/Windows%20Manual/#nothing-happens-after-clicking-on-the-input-method-in-the-taskbar-windows-10","text":"restrat the computer :)","title":"Nothing happens after clicking on the input method in the taskbar (windows 10)"},{"location":"Windows/Windows%20Manual/#bluetooth","text":"","title":"Bluetooth"},{"location":"Windows/Windows%20Manual/#troiubleshooting","text":"","title":"Troiubleshooting"},{"location":"Windows/Windows%20Manual/#cannot-connect-to-the-device","text":"Try to remove the device and pair it with the PC again If it does not help, proceeed to the next section (even if the pairing is successfull)","title":"Cannot connect to the device"},{"location":"Windows/Windows%20Manual/#cannot-pair-with-the-device","text":"Turn off the device and unplug it from the electricity/remove batteries. Then plug it back after ~10 seconds, power it of, and try to pair with it again.","title":"Cannot pair with the device"},{"location":"Windows/Windows%20Manual/#bluetooth-command-line-tools","text":"https://bluetoothinstaller.com/bluetooth-command-line-tools Bluetooth Command Line Tools is a set off tools that enables command line interaction with blootooth services. Basic usage: discover and list available devices: btdiscovery -s","title":"Bluetooth Command Line Tools"},{"location":"Windows/Windows%20Manual/#filesytem","text":"","title":"Filesytem"},{"location":"Windows/Windows%20Manual/#standard-folder-structure","text":"In Windows, the standard folder structure is completely different for system and user instalation. Details are listed below, but the main difference is that the system instalations are stored in a single root folder for each application (similarly to Android), while the user instalations' files are distributed among multiple folders, depending on the type of the file (similarly to Linux).","title":"Standard folder structure"},{"location":"Windows/Windows%20Manual/#user-home-folder","text":"The user home folder is located in C:\\Users\\<username> by default. It is aliased as %userprofile% .","title":"User home folder"},{"location":"Windows/Windows%20Manual/#system-instalation-folders","text":"If an application is installed for all users, all its files are usually installed in a single folder per application. The location of the folder depends on the type of the application: C:\\Program Files : 64-bit applications C:\\Program Files (x86) : 32-bit applications If the application needs to store some data, they are usually stored in the C:\\ProgramData (aliased as %programdata% ) folder.","title":"System instalation folders"},{"location":"Windows/Windows%20Manual/#user-instalation-folders","text":"User instalations are stored in multiple folders, depending on the type of the file. All these folders are located in the user's home folder, which is C:\\Users\\<username> by default. The folders are: ~\\AppData\\Local : Program data and sometimes also executables ~\\AppData\\Local\\Promgrams : program files and executables ~\\AppData\\LocalLow : ~\\AppData\\Roaming (aliased as %appdata% ):","title":"User instalation folders"},{"location":"Windows/Windows%20Manual/#start-menu-folder","text":"The user specific shortcuts are stored in: %appdata%\\Microsoft\\Windows\\Start Menu\\Programs . The system wide shortcuts are stored in: %programdata%\\Microsoft\\Windows\\Start Menu\\Programs .","title":"Start Menu folder"},{"location":"Windows/Windows%20Manual/#read-only-files-and-folders","text":"An ancient form of file protection on Windows is the read only flag that can be set on files and folders. It is not a real protection, as it can be easily removed by the user, but it can be used to prevent accidental changes. Most of the programs can ignore this flag and work with the file anyway. However, some programs (e.g. Python) can have problems with it.","title":"Read Only Files and Folders"},{"location":"Windows/Windows%20Manual/#sugarsync","text":"Quick orientation in the desktop app: for file changes, check left menu -> Activity for deleted files, check left menu -> Deleted Items","title":"Sugarsync"},{"location":"Windows/Windows%20Manual/#solving-sync-problems","text":"check if the file is updated in cloud using web browser if not, check the activity log on the computer with the updated file if the change is not in the log, a simple hack can help: copy the file outside SugarSync folder and back.","title":"Solving sync problems"},{"location":"Windows/Windows%20Manual/#useful-commands","text":"","title":"Useful Commands"},{"location":"Windows/Windows%20Manual/#get-motherboard-info","text":"wmic baseboard get product,Manufacturer,version,serialnumber","title":"Get Motherboard Info"},{"location":"Windows/Windows%20Manual/#copy-multiple-filesdirs","text":"robocopy is the best command for that. Usefull params: /e : Copy subdirectories, including empty ones /b : Copy in the backup mode, so that even files with a different owner can be copied /xc : Excludes changed files. /xn : Excludes newer files. /xo : Excludes older files. /r:<n> : Specifies the number of retries on failed copies. The default value of n is 1,000,000 (one million retries). /w:<n> : Specifies the wait time between retries, in seconds. The default value of n is 30 (wait time 30 seconds).","title":"Copy multiple files/dirs"},{"location":"Windows/Windows%20Manual/#user-info","text":"Information about users can be obtained with the Get-LocalUser command. By default, the command lists all users. Some useful params: -Name : Specifies the user account names of the users to get. -SID : Specifies the security identifier (SID) of the users to get.","title":"User info"},{"location":"Windows/Windows%20Manual/#installation","text":"","title":"Installation"},{"location":"Windows/Windows%20Manual/#windows-11","text":"Windows 11 can be installed only as an update of Windows 10.","title":"Windows 11"},{"location":"Windows/Windows%20Manual/#windows-10","text":"Can be installed from bootable USB created by a tool downloaded from the official Miccosoft website. Single image for all Windows versions, a particular version is choosen based on the license key. Steps: Download the install tool from Microsoft Create a bootable USB Start the installation Fill in the licence key","title":"Windows 10"},{"location":"Windows/Windows%20Manual/#we-couldnt-create-a-partition-or-locate-an-existing-one","text":"Ensure that the boot priority of the drive where the Windows should be installed is right behind the installation USB priority.","title":"we couldn\u2019t create a partition or locate an existing one"},{"location":"Windows/Windows%20Manual/#configuration","text":"","title":"Configuration"},{"location":"Windows/Windows%20Manual/#right-click-menu","text":"Unfortunatelly, the right click menu is not directly configurable in Windows. Usually, the actions are enabled by the application installation (sometimes, this can be disabled in the installation process), and can only be removed by editing the registry or uninstalling the application. Below, we list instructions for each specific action.","title":"Right Click Menu"},{"location":"Windows/Windows%20Manual/#share-with-skype","text":"in an elevated PowerShell, run: PowerShell REG ADD \"HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Shell Extensions\\Blocked\" /v \"{776DBC8D-7347-478C-8D71-791E12EF49D8}\" /d Skype restart the Explorer","title":"Share with Skype"},{"location":"Windows/Windows%20Manual/#powertoys-modules","text":"These can be removed by deactivating the specific modules in the PowerToys settings.","title":"PowerToys modules"},{"location":"Windows/Windows%20Manual/#edit-with-notepad","text":"Just Uninstall the Notepad. Yes, it can be done.","title":"Edit with Notepad"},{"location":"Windows/Windows%20Manual/#scan-with-microsoft-defender","text":"The following commands removes four entries from the registry that are related to this icon: REG DELETE \"HKEY_CLASSES_ROOT\\*\\shellex\\ContextMenuHandlers\\EPP\" REG DELETE \"HKEY_CLASSES_ROOT\\CLSID\\{09A47860-11B0-4DA5-AFA5-26D86198A780}\" REG DELETE \"HKEY_CLASSES_ROOT\\Directory\\shellex\\ContextMenuHandlers\\EPP\" REG DELETE \"HKEY_CLASSES_ROOT\\Drive\\shellex\\ContextMenuHandlers\\EPP\" Source","title":"Scan with Microsoft Defender"},{"location":"Windows/Windows%20Manual/#translate-with-deepl","text":"Haven't found a way to remove it yet. Even uninstalling the Deepl does not help.","title":"Translate with Deepl"},{"location":"Windows/Windows%20Manual/#diskpart","text":"Diskpart is a useful command line tool for work with disks, partitions, etc.","title":"Diskpart"},{"location":"Windows/Windows%20Manual/#find-out-wheteher-a-disk-is-mbr-or-gpt","text":"","title":"Find out wheteher a disk is MBR or GPT"},{"location":"Windows/Windows%20Manual/#open-command-promp-from-the-windows-10-usb","text":"Insert the USB stick Wait till the first installation screen shift + F10","title":"Open Command Promp from the Windows 10 USB"},{"location":"Windows/Windows%20Manual/#firewall","text":"","title":"Firewall"},{"location":"Windows/Windows%20Manual/#generate-firewall-logs","text":"Go to Windows firewall and select properties on the right At the top, choose the profile corresponding to the current network profile In the logging section, click to customizze set both DROP and ACCEPT to yes Do not forgot to turn of the logging after the investigation!","title":"Generate firewall logs"},{"location":"Windows/Windows%20Manual/#ssh","text":"For ssh, we can use the standard ssh commannd available in Windows (check Linux manual for more info). If the command is not available, it can be installed in Apps & Features -> Optional features . One thing that differs from Linux is that the Windows ssh does not support the <addres>:<port> syntax. To specify the port, it is necessary to use the -p parameter For more features, we can use other programs KiTTY for credentials storage, automatic reconection, etc. WinSCP for file manipulation","title":"SSH"},{"location":"Windows/Windows%20Manual/#kitty","text":"It is best to use the portable version, so that nothing is stored in the Windows registry. Configurtation: copy the PuTTY credentials : .\\kitty_portable-0.76.1.3.exe -convert-dir auto reconnect: Connection -> auto reconnect on connection failure and auto reconnect on system wakeup","title":"KiTTY"},{"location":"Windows/Windows%20Manual/#winscp","text":"WinSCP is a graphical tool for file manipulation. Ii can be used both for local and remote files, and it supports various protocols (FTP, SFTP, SCP, WebDAV, etc.).","title":"WinSCP"},{"location":"Windows/Windows%20Manual/#adding-a-new-connection","text":"There is a simple New Site button on the left, which opens a straightforward dialog. The only complicated thing can be the SSH key. To add it, click on the Advanced button and go to the SSH -> Authentication tab. There, we can select the private key file.","title":"Adding a new connection"},{"location":"Windows/Windows%20Manual/#bookmarks","text":"To add bookmarks, go to Local / Remote -> Add Path to Bookmarks or press Ctrl + B . To open a bookmark, go to Local / Remote -> Go To -> Open Drirectory/bookmark or press Ctrl + O .","title":"Bookmarks"},{"location":"Windows/Windows%20Manual/#ssh-key-agent","text":"To enable the ssh-agent on Windows, one extra step is needed: we need to start the ssh-agent service. To do that, open the services manager and start the OpenSSH Authentication Agent service.","title":"SSH key agent"},{"location":"Windows/Windows%20Manual/#git","text":"Most of the git functionality is the same as in Linux, so check the Linux manual for more info. However, there are some important differences mostly resulting from the fact that Git is not a native Windows application, but it runs in MinGW.","title":"Git"},{"location":"Windows/Windows%20Manual/#git-on-windows-and-ssh","text":"As the git on Windows runs in MinGW, it does not use the Windows SSH command. That can be problematic if we want to debug the SSH connection using the env variable or configuring an ssh key agent. To force git to use the Windows SSH, we need to set the sshCommand config variable to the path to the Windows SSH: git config --global core.sshCommand C:/Windows/System32/OpenSSH/ssh.exe","title":"Git on Windows and SSH"},{"location":"Windows/Windows%20Manual/#dual-boot","text":"","title":"Dual Boot"},{"location":"Windows/Windows%20Manual/#make-windows-work-after-linux-uninstall-if-the-bootloader-is-grub","text":"Get rid of the Grub bootloader Set the Windows bootloader as the primary partition it is the small partition at the beginning of the main disk","title":"Make Windows Work after Linux Uninstall if the Bootloader is Grub"},{"location":"Windows/Windows%20Manual/#problems","text":"","title":"Problems"},{"location":"Windows/Windows%20Manual/#folder-sharing-problems","text":"Note that updated Windows 10 disabled anonymous sharing , so password protected sharing has to be turned on. To login, use the credentials for the computer with the shared folder . Below is a list of possible problems, together with solutions.","title":"Folder Sharing Problems"},{"location":"Windows/Windows%20Manual/#the-user-name-or-password-is-incorrect","text":"Check whether the computer can be seen in the network. If not, resolve this issue first. quick check by running net view <IP address> Check that you are using the right username. You need to use the username and password of the computer you are connecting to . Check that the user name is correct by running net user on the target computer Check that the folder is shared with you in: right click on the folder -> Properties -> Sharing -> Advanced Sharing... -> Permisions . Note that your full name can be there instead of your username, which is OK. Check that you are using the right password. You have to use the password associated with your microsoft account. Note that it can differ from the password (PIN) you are using to log in to the computer! check it on the command line: net use * \\\\<IP address>\\<drive letter>$ /use:<username> <password>","title":"The user name or password is incorrect"},{"location":"Windows/Windows%20Manual/#folder-right-and-ownership-cannot-be-read","text":"Try to clear the windows filecache (CCcleaner or restart)","title":"Folder right and ownership cannot be read"},{"location":"Windows/Windows%20Manual/#computer-does-not-see-itself-in-network-section-in-file-explorer","text":"Solution to this problem is to restart the service called Function Discovery Resource Publication . Either restart it in Computer Management -> Services, or by: net stop FDResPub net start FDResPub","title":"Computer does not see itself in Network section in File Explorer"},{"location":"Windows/Windows%20Manual/#pc-wakes-up-or-cannot-enter-sleep","text":"","title":"PC wakes up or cannot enter sleep"},{"location":"Windows/Windows%20Manual/#1-find-the-source","text":"Using the Event viwer open the event viewer go to windows logs -> system In case of wake up inspect the logs when the wake up happened and search for the Information log with the message \"The system has returned from a low power state.\" There is a wake up source in the end of the log message. If the soure is Unknown go to the next section In case of not entering sleep Search for the any kernel power event If there is an event stating: The system is entering connected standby , it means that the modern fake sleep is present in the system, replacing the real sleep mode. Using command line (admin): 1. Try powercfg -lastwake 2. If the results are not know, try to call powercfg -devicequery wake_armed to get the list of devices that can wake the computer","title":"1 Find the source"},{"location":"Windows/Windows%20Manual/#2-solve-the-problem","text":"","title":"2 Solve the problem"},{"location":"Windows/Windows%20Manual/#device-waken-up-by-network-adapter","text":"Open device manager and search for the specific network adapter right click -> Properties -> Power Management Check Only allow a magic packet to wake up the computer","title":"Device waken up by network adapter"},{"location":"Windows/Windows%20Manual/#the-real-sleep-mode-is-not-available-on-the-system","text":"If this is the case, use the hibernate mode instead. To add it to the start menu: go to Control panel -> Hardware and sound -> Power options click on the left panel to Choose what the power buttons does click on Change settings that are currently unavailable check the hibernate checkbox below","title":"The real sleep mode is not available on the system"},{"location":"Windows/Windows%20Manual/#camera-problem","text":"Symptoms: the screen is blank, black, single color, in all apps and there are no problems reported in device manager Cause: it can be caused by some external cameras (now disconnected) that are still selected in the apps using the camera. Go Solution: Go to the app setting and select the correct camera","title":"Camera problem"},{"location":"Windows/Windows%20Manual/#phone-app-cannot-see-the-connected-cell-phone","text":"It can be due to the fucked up Windows N edition. Just install the normal edition.","title":"Phone app cannot see the connected cell phone"},{"location":"Windows/Windows%20Manual/#vmmem-process-uses-a-lot-of-cpu","text":"This process represents all virtual systems. One cultprit is therefore WSL. Try to shutdown the WSL using wsl --shutdown","title":"vmmem process uses a lot of CPU"}]}