{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the manuals pages!","title":"index"},{"location":"FFMpeg/","text":"First download binaries for Windows . Converting a Video to Gif Image \u00b6 Example: ffmpeg -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=512:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -ss 00:00:03 -to 00:00:06 simod_showcase.gif To change the speed, we can use an -itsscale inut option: ffmpeg -itsscale 0.2 -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=838:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -ss 00:00:00 -to 00:00:15 simod_showcase.gif Detailed description on SE Written with StackEdit .","title":"FFMpeg"},{"location":"FFMpeg/#converting-a-video-to-gif-image","text":"Example: ffmpeg -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=512:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -ss 00:00:03 -to 00:00:06 simod_showcase.gif To change the speed, we can use an -itsscale inut option: ffmpeg -itsscale 0.2 -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=838:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -ss 00:00:00 -to 00:00:15 simod_showcase.gif Detailed description on SE Written with StackEdit .","title":"Converting a Video to Gif Image"},{"location":"Overpass%20Manual/","text":"Sources \u00b6 wiki/Overpass QL Strucutre \u00b6 Every statement ents with ; . Sets \u00b6 Almost all statements works with sets of data. Unless specified, we work with a default set ( _ ). To wrrito to a specific set, we can use ->. operator: <statement>->.<set> writes the result of the <statement> to the . The default set can be ommited: <statement> is equal to <statement>->._ . out statement \u00b6 All queries should contain an out statement that determines the output format. - out is used for data only request - out geom returns data with all elements associated with their geometry. Note that while the output format can be specified, we cannot filter the output (e.g., we cannot filter the ralation members ). Area specification \u00b6 We select an area by name as: area[\"name\"=\"Letkov\"]; And then we used it as: node(area); If we need more areas in a query, we can store them in variable: area[\"name\"=\"Letkov\"]->.let; And: node(area.let); Important note: If an area needs to be used repetadly, it has to be named, as the value of area ( area._ ) is replaced by the output of any statement. Filtering \u00b6 filters are specified in brackets: rel[\"admin_level\"=8]; It is also possible to use a regex filtering, we just neeed to replace = with ~ : rel[\"admin_level\"~\".*\"]; Selecting Multiple Data Sets \u00b6 Implicitely, all filters are aplied to a default dataset called _ and also written to it. Therefore, we cannot do: rel[\"admin_level\"~\".*\"]; node; because we are basically selecting nodes from a set of relations. Solutions: 1) Union statement 2) Named sets Union Utatement \u00b6 Union statement is surounded by parantheses. We can achieve the intended behaviour by: ( rel[\"admin_level\"~\".*\"]; node; ); Select Area Boundary \u00b6 Sometimes, it is usefull to check the boundary of the named area. However, the administrative level differ for various areas cities, countries, so the admin_level relation cannot be used for that. Luckilly, there is an option to use the pivot filter. Example: area[\"name\"=\"Praha\"]; node(pivot); out geom; Discover the full name of an area \u00b6 If we want to know the full name of the area while the above query return multiple results, we can do that in openstreet map: 1. Move the map to see the area 2. Click the button with cusor and question mark to select the exploration tool 3. Click inside the area 4. Scroll down to area relations 5. Click on the proper region 6. The name property is what we are looking for Filter areas with duplicite names \u00b6 Sometimes, even the full name of the area is not specific enough. In that case, we can use two approaches: - select the area by the area relation id - specify the area by the higher level area (state, country) Select area by ID \u00b6 select the requested area copy the id and add 3 600 000 000 to it (start with 36 and fill zerose till there are 10 digits in total) replace area[\"name\"=\"<NAME>] with area(<ID>) . Note that round brackets are used instead of square brackets! Specify area with higher level area \u00b6 In this exaple, we select the Coo County, Illinois. area[name=\"Illinois\"]; rel[name=\"Cook County\"](area); map_to_area; node(pivot); out geom; more info Get historical data \u00b6 To get historical data, prepend the query with a date statement. Example: [date:\"2016-01-01T00:00:00Z\"]; area[name=\"City of New York\"]; node(pivot); out geom;","title":"Overpass Manual"},{"location":"Overpass%20Manual/#sources","text":"wiki/Overpass QL","title":"Sources"},{"location":"Overpass%20Manual/#strucutre","text":"Every statement ents with ; .","title":"Strucutre"},{"location":"Overpass%20Manual/#sets","text":"Almost all statements works with sets of data. Unless specified, we work with a default set ( _ ). To wrrito to a specific set, we can use ->. operator: <statement>->.<set> writes the result of the <statement> to the . The default set can be ommited: <statement> is equal to <statement>->._ .","title":"Sets"},{"location":"Overpass%20Manual/#out-statement","text":"All queries should contain an out statement that determines the output format. - out is used for data only request - out geom returns data with all elements associated with their geometry. Note that while the output format can be specified, we cannot filter the output (e.g., we cannot filter the ralation members ).","title":"out statement"},{"location":"Overpass%20Manual/#area-specification","text":"We select an area by name as: area[\"name\"=\"Letkov\"]; And then we used it as: node(area); If we need more areas in a query, we can store them in variable: area[\"name\"=\"Letkov\"]->.let; And: node(area.let); Important note: If an area needs to be used repetadly, it has to be named, as the value of area ( area._ ) is replaced by the output of any statement.","title":"Area specification"},{"location":"Overpass%20Manual/#filtering","text":"filters are specified in brackets: rel[\"admin_level\"=8]; It is also possible to use a regex filtering, we just neeed to replace = with ~ : rel[\"admin_level\"~\".*\"];","title":"Filtering"},{"location":"Overpass%20Manual/#selecting-multiple-data-sets","text":"Implicitely, all filters are aplied to a default dataset called _ and also written to it. Therefore, we cannot do: rel[\"admin_level\"~\".*\"]; node; because we are basically selecting nodes from a set of relations. Solutions: 1) Union statement 2) Named sets","title":"Selecting Multiple Data Sets"},{"location":"Overpass%20Manual/#union-utatement","text":"Union statement is surounded by parantheses. We can achieve the intended behaviour by: ( rel[\"admin_level\"~\".*\"]; node; );","title":"Union Utatement"},{"location":"Overpass%20Manual/#select-area-boundary","text":"Sometimes, it is usefull to check the boundary of the named area. However, the administrative level differ for various areas cities, countries, so the admin_level relation cannot be used for that. Luckilly, there is an option to use the pivot filter. Example: area[\"name\"=\"Praha\"]; node(pivot); out geom;","title":"Select Area Boundary"},{"location":"Overpass%20Manual/#discover-the-full-name-of-an-area","text":"If we want to know the full name of the area while the above query return multiple results, we can do that in openstreet map: 1. Move the map to see the area 2. Click the button with cusor and question mark to select the exploration tool 3. Click inside the area 4. Scroll down to area relations 5. Click on the proper region 6. The name property is what we are looking for","title":"Discover the full name of an area"},{"location":"Overpass%20Manual/#filter-areas-with-duplicite-names","text":"Sometimes, even the full name of the area is not specific enough. In that case, we can use two approaches: - select the area by the area relation id - specify the area by the higher level area (state, country)","title":"Filter areas with duplicite names"},{"location":"Overpass%20Manual/#select-area-by-id","text":"select the requested area copy the id and add 3 600 000 000 to it (start with 36 and fill zerose till there are 10 digits in total) replace area[\"name\"=\"<NAME>] with area(<ID>) . Note that round brackets are used instead of square brackets!","title":"Select area by ID"},{"location":"Overpass%20Manual/#specify-area-with-higher-level-area","text":"In this exaple, we select the Coo County, Illinois. area[name=\"Illinois\"]; rel[name=\"Cook County\"](area); map_to_area; node(pivot); out geom; more info","title":"Specify area with higher level area"},{"location":"Overpass%20Manual/#get-historical-data","text":"To get historical data, prepend the query with a date statement. Example: [date:\"2016-01-01T00:00:00Z\"]; area[name=\"City of New York\"]; node(pivot); out geom;","title":"Get historical data"},{"location":"PowerPoint/","text":"Slide design \u00b6 Slide design should not be set separately for each slide but globaly for the whole presentation. This can be done by configuring the theme in the slide master view: View > Master Views > Slide Master . On the left, we can select a type of the slide to edit, while on the right, we edit the style for that style type. On the left, the top slide, larger than the others is the master slide . It represents the default design for the whole theme. Shareing the theme \u00b6 The theme can be exported in the slide master view by clicking: Edit theme > Save current theme . The theme can be imported in the slide master view by clicking: Edit theme > Browse for themes . Edit the master slide \u00b6 The master slide is the default design for the whole theme. We cannot add any content to it (it makes no sense, as this would be added to all slides.) However, we can set up some properties of predefined basic elements here: - Title : The title of the slide. - Text : The text for different levels of indentation - Date - Slide number : page numbering. - Footer Note that the configuration here applies to all slides. Therefore, if we want to have for example date only on the first slide, we should not configure the date here, but on the first slide itself. To select which of the elements should be configured for all slides, click on Master Layout -> Master Layout and select the elements to be configured. Edit the layout slide \u00b6 Symbols \u00b6 arrow: write ==> and press space. Tables \u00b6 Tables are treated specially in PowerPoint. They can be created using the Insert > Table menu. To edit the table style, we have to select it and go to the special Table Design tab. To edit the table layaut, add rows or columns, we have to select the table and go to the Layout tab. Insert table from csv \u00b6 Unfortunately, csv text cannot be pasted directly into a PowerPoint table. However, there is a workaround: 1. Copy the csv text into Excel. 2. Select the cells in Excel and copy them. 3. Paste in the powerpoint slide. This way a new table is created. Text formatting \u00b6 Subscript and superscript \u00b6 First, select the text to adjust, then: - subscript: Ctrl + = - superscript: Ctrl + Shift + +","title":"PowerPoint"},{"location":"PowerPoint/#slide-design","text":"Slide design should not be set separately for each slide but globaly for the whole presentation. This can be done by configuring the theme in the slide master view: View > Master Views > Slide Master . On the left, we can select a type of the slide to edit, while on the right, we edit the style for that style type. On the left, the top slide, larger than the others is the master slide . It represents the default design for the whole theme.","title":"Slide design"},{"location":"PowerPoint/#shareing-the-theme","text":"The theme can be exported in the slide master view by clicking: Edit theme > Save current theme . The theme can be imported in the slide master view by clicking: Edit theme > Browse for themes .","title":"Shareing the theme"},{"location":"PowerPoint/#edit-the-master-slide","text":"The master slide is the default design for the whole theme. We cannot add any content to it (it makes no sense, as this would be added to all slides.) However, we can set up some properties of predefined basic elements here: - Title : The title of the slide. - Text : The text for different levels of indentation - Date - Slide number : page numbering. - Footer Note that the configuration here applies to all slides. Therefore, if we want to have for example date only on the first slide, we should not configure the date here, but on the first slide itself. To select which of the elements should be configured for all slides, click on Master Layout -> Master Layout and select the elements to be configured.","title":"Edit the master slide"},{"location":"PowerPoint/#edit-the-layout-slide","text":"","title":"Edit the layout slide"},{"location":"PowerPoint/#symbols","text":"arrow: write ==> and press space.","title":"Symbols"},{"location":"PowerPoint/#tables","text":"Tables are treated specially in PowerPoint. They can be created using the Insert > Table menu. To edit the table style, we have to select it and go to the special Table Design tab. To edit the table layaut, add rows or columns, we have to select the table and go to the Layout tab.","title":"Tables"},{"location":"PowerPoint/#insert-table-from-csv","text":"Unfortunately, csv text cannot be pasted directly into a PowerPoint table. However, there is a workaround: 1. Copy the csv text into Excel. 2. Select the cells in Excel and copy them. 3. Paste in the powerpoint slide. This way a new table is created.","title":"Insert table from csv"},{"location":"PowerPoint/#text-formatting","text":"","title":"Text formatting"},{"location":"PowerPoint/#subscript-and-superscript","text":"First, select the text to adjust, then: - subscript: Ctrl + = - superscript: Ctrl + Shift + +","title":"Subscript and superscript"},{"location":"QGIS/","text":"Adding layers \u00b6 Layer -> Add Layer Count the number of features in area \u00b6 select the right layer on the top panel, third row, select the tool Select Features by area or single click select features Open the attribute table for layer. In the heder, there should be the number of selected features Postgis Layers \u00b6 Invalid layer \u00b6 The layer can be marked as invalid because of a missing or invalid index column. Each Postgis layer needs an id column consisting of unique integer values .","title":"QGIS"},{"location":"QGIS/#adding-layers","text":"Layer -> Add Layer","title":"Adding layers"},{"location":"QGIS/#count-the-number-of-features-in-area","text":"select the right layer on the top panel, third row, select the tool Select Features by area or single click select features Open the attribute table for layer. In the heder, there should be the number of selected features","title":"Count the number of features in area"},{"location":"QGIS/#postgis-layers","text":"","title":"Postgis Layers"},{"location":"QGIS/#invalid-layer","text":"The layer can be marked as invalid because of a missing or invalid index column. Each Postgis layer needs an id column consisting of unique integer values .","title":"Invalid layer"},{"location":"Cloud%20computing/Lmod/","text":"Lmod is a module system that simplify the package management on the Linux cloud computing clusters. It is used on the RCI cluster. home user guide RCi cluster documentation module ( ml ) command \u00b6 The module performs different operations based on its first argument. The default operation (if the first/main argument is omited) is load . - load <module name> : loads the module. Specific version can be used by adding / and the version number. If the version is not specified, the latest version is loaded. - list : lists all loaded modules","title":"Lmod"},{"location":"Cloud%20computing/Lmod/#module-ml-command","text":"The module performs different operations based on its first argument. The default operation (if the first/main argument is omited) is load . - load <module name> : loads the module. Specific version can be used by adding / and the version number. If the version is not specified, the latest version is loaded. - list : lists all loaded modules","title":"module (ml) command"},{"location":"Cloud%20computing/RCI%20cluster/","text":"General \u00b6 To get access to the RCI cluster: https://docs.google.com/forms/d/e/1FAIpQLSewws_V6-D567fkp6QZmr0GQlkzQrEoB6QquAgQkZu8so818Q/viewform Official instructions: https://login.rci.cvut.cz/wiki/how_to_start Usual command Usage \u00b6 You can watch your jobs with squeue -u username . You test/debug your program when running it with srun command you usually don\u2019t have to allocate resources when testing To start an interactive shell, run: srun -p cpufast --pty bash -i Set your main script file (sh) executable via chmod +x <filename> command Test your script in console Cancel the job: scancel <JOB ID> You run your job with sbatch command with allocated resources Example sbatch: sbatch --mem=30G -t 60 -n30 -o /home/fiedlda1/Amodsim/log/ih.log /home/fiedlda1/Amodsim/rci_launchers/ih.sh How to clone projects \u00b6 Usually, you need to clone some of the SUM projects to start working on the RCI cluster. To do that: 1. Copy your key to ~/.ssh/ 2. Set file permissions to your ssh key safely 3. Modify ~/.ssh/config IdentityFile to point to your key 4. Clone your project Specifics for java projects \u00b6 Clone project on RCI cluster Download binary maven from: http://maven.apache.org/download.cgi and export it to your home folder on the RCI cluster. Prepare your bash script Add #!/bin/bash to the first line Change the environment variable PATH for your maven location with this command: PATH=$PATH:/home/$USER/apache-maven-3.6.1/bin/ Load all required software via ml command, definitely ml Java Build, compile, run your project via mvn commands Set your file executable via chmod +x filename command Example run command: mvn exec:exec -Dexec.executable=java -Dexec.args=\u2019-classpath %classpath -Xmx30g cz.cvut.fel.aic.amodsim.OnDemandVehiclesSimulation /home/kholkolg/amod-to-agentpolis/local_config_files/olga_vga_RCI.cfg\u2019 -Dfile.encoding=UTF-8 Example - Bash script for amodsim project: Run your script with srun, sbatch etc. commands, I recommend first use srun, to check everything is set up ok and then use sbatch command, because if computational nodes are busy, your job will be added to the queue and you can do other work. Specifics for python projects \u00b6 First load the appropriate version of python, e. G.: ml Python/3.6.6-foss-2018b You can\u2019t just install the packages with sudo , you have to install them to the user space instead: Run pip install --user packagename Specifics for C++ projects \u00b6 Workflow options \u00b6 As linux binaries are usually not portable. They are not compatible with older linux versions due to the infamous glibc incompatibility. There are three solutions to this problem: Method Setup Program Upgrade Compile the code on the RCI Setup the compilation on RCI. Copy the source code to RCI and recompile after every change Use a Singularity container learn with singularity, create the container Generate new container and copy it to the RCI Build a compatible binary using a modified toolchain learn with a toolchain generator, configure and generate the right toolchain Copy the updated binary Building on RCI \u00b6 In general the workflow is the same as on a local machine. The difference is that we do not have root access, so for all needed tools, we have to either load them via ml command, or, if not available, install them in the user space. Typically, we need to load: - git: ml git - GCC: ml GCC - CMake: ml CMake Specific for projects with gurobi \u00b6 Load Gurobi with ml Gurobi or ml Gurobi/8.1.1-foss-2018b-Python-3.6.6 for a specific version Be aware that this operation can reload other packages Gurobi and Java \u00b6 It is necessary to install Gurobi to maven: mvn install:install-file -Dfile=/mnt/appl/software/Gurobi/9.0.3-GCCcore-8.3.0-Python-3.7.4/lib/gurobi.jar -DgroupId=com.gurobi -DartifactId=gurobi -Dversion=1.0 -Dpackaging=jar Gurobi and C++ \u00b6 As RCI use Linux as OS, we need to compile the Gurobi C++ libs with the same compiler as the one we use for compilation of our code (see C++ Workflow for more details). Note that this is necessary even if the Gurobi seems to be compiled with the same copiler we use for compilation . Unlike in Linux installation we controll, we cannot build the C++ lib in the Gurobi installation folder. To make the Linking work, foloow these steps: 1. copy the src dir from the RCI Gurobi module located at mnt/appl/software/Gurobi/<desired version> to our home 2. run make located in src/build 3. copy the libgurobi_c++.a to the lib subfolder of your project 4. configure the searching for the C++ lib in FindGUROVI.cmake file: find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED ) if the CMake cache is already generated, delete it. Generate the CMake cache and build the project Commands \u00b6 For command description, see Slurm manual. Main params - --mem=<required memory> - <required memory> is in megabytes by default, for gigabytes, we need to add G - -t <time> - Time in minutes Task Explanations \u00b6 srun --pty bash -i - --pty runs the first task and close output and error stream for everything except the first task - bash : what we want to run - -i Input setting, here followed by no param indicating that the input stream is closed osed","title":"RCI cluster"},{"location":"Cloud%20computing/RCI%20cluster/#general","text":"To get access to the RCI cluster: https://docs.google.com/forms/d/e/1FAIpQLSewws_V6-D567fkp6QZmr0GQlkzQrEoB6QquAgQkZu8so818Q/viewform Official instructions: https://login.rci.cvut.cz/wiki/how_to_start","title":"General"},{"location":"Cloud%20computing/RCI%20cluster/#usual-command-usage","text":"You can watch your jobs with squeue -u username . You test/debug your program when running it with srun command you usually don\u2019t have to allocate resources when testing To start an interactive shell, run: srun -p cpufast --pty bash -i Set your main script file (sh) executable via chmod +x <filename> command Test your script in console Cancel the job: scancel <JOB ID> You run your job with sbatch command with allocated resources Example sbatch: sbatch --mem=30G -t 60 -n30 -o /home/fiedlda1/Amodsim/log/ih.log /home/fiedlda1/Amodsim/rci_launchers/ih.sh","title":"Usual command Usage"},{"location":"Cloud%20computing/RCI%20cluster/#how-to-clone-projects","text":"Usually, you need to clone some of the SUM projects to start working on the RCI cluster. To do that: 1. Copy your key to ~/.ssh/ 2. Set file permissions to your ssh key safely 3. Modify ~/.ssh/config IdentityFile to point to your key 4. Clone your project","title":"How to clone projects"},{"location":"Cloud%20computing/RCI%20cluster/#specifics-for-java-projects","text":"Clone project on RCI cluster Download binary maven from: http://maven.apache.org/download.cgi and export it to your home folder on the RCI cluster. Prepare your bash script Add #!/bin/bash to the first line Change the environment variable PATH for your maven location with this command: PATH=$PATH:/home/$USER/apache-maven-3.6.1/bin/ Load all required software via ml command, definitely ml Java Build, compile, run your project via mvn commands Set your file executable via chmod +x filename command Example run command: mvn exec:exec -Dexec.executable=java -Dexec.args=\u2019-classpath %classpath -Xmx30g cz.cvut.fel.aic.amodsim.OnDemandVehiclesSimulation /home/kholkolg/amod-to-agentpolis/local_config_files/olga_vga_RCI.cfg\u2019 -Dfile.encoding=UTF-8 Example - Bash script for amodsim project: Run your script with srun, sbatch etc. commands, I recommend first use srun, to check everything is set up ok and then use sbatch command, because if computational nodes are busy, your job will be added to the queue and you can do other work.","title":"Specifics for java projects"},{"location":"Cloud%20computing/RCI%20cluster/#specifics-for-python-projects","text":"First load the appropriate version of python, e. G.: ml Python/3.6.6-foss-2018b You can\u2019t just install the packages with sudo , you have to install them to the user space instead: Run pip install --user packagename","title":"Specifics for python projects"},{"location":"Cloud%20computing/RCI%20cluster/#specifics-for-c-projects","text":"","title":"Specifics for C++ projects"},{"location":"Cloud%20computing/RCI%20cluster/#workflow-options","text":"As linux binaries are usually not portable. They are not compatible with older linux versions due to the infamous glibc incompatibility. There are three solutions to this problem: Method Setup Program Upgrade Compile the code on the RCI Setup the compilation on RCI. Copy the source code to RCI and recompile after every change Use a Singularity container learn with singularity, create the container Generate new container and copy it to the RCI Build a compatible binary using a modified toolchain learn with a toolchain generator, configure and generate the right toolchain Copy the updated binary","title":"Workflow options"},{"location":"Cloud%20computing/RCI%20cluster/#building-on-rci","text":"In general the workflow is the same as on a local machine. The difference is that we do not have root access, so for all needed tools, we have to either load them via ml command, or, if not available, install them in the user space. Typically, we need to load: - git: ml git - GCC: ml GCC - CMake: ml CMake","title":"Building on RCI"},{"location":"Cloud%20computing/RCI%20cluster/#specific-for-projects-with-gurobi","text":"Load Gurobi with ml Gurobi or ml Gurobi/8.1.1-foss-2018b-Python-3.6.6 for a specific version Be aware that this operation can reload other packages","title":"Specific for projects with gurobi"},{"location":"Cloud%20computing/RCI%20cluster/#gurobi-and-java","text":"It is necessary to install Gurobi to maven: mvn install:install-file -Dfile=/mnt/appl/software/Gurobi/9.0.3-GCCcore-8.3.0-Python-3.7.4/lib/gurobi.jar -DgroupId=com.gurobi -DartifactId=gurobi -Dversion=1.0 -Dpackaging=jar","title":"Gurobi and Java"},{"location":"Cloud%20computing/RCI%20cluster/#gurobi-and-c","text":"As RCI use Linux as OS, we need to compile the Gurobi C++ libs with the same compiler as the one we use for compilation of our code (see C++ Workflow for more details). Note that this is necessary even if the Gurobi seems to be compiled with the same copiler we use for compilation . Unlike in Linux installation we controll, we cannot build the C++ lib in the Gurobi installation folder. To make the Linking work, foloow these steps: 1. copy the src dir from the RCI Gurobi module located at mnt/appl/software/Gurobi/<desired version> to our home 2. run make located in src/build 3. copy the libgurobi_c++.a to the lib subfolder of your project 4. configure the searching for the C++ lib in FindGUROVI.cmake file: find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED ) if the CMake cache is already generated, delete it. Generate the CMake cache and build the project","title":"Gurobi and C++"},{"location":"Cloud%20computing/RCI%20cluster/#commands","text":"For command description, see Slurm manual. Main params - --mem=<required memory> - <required memory> is in megabytes by default, for gigabytes, we need to add G - -t <time> - Time in minutes","title":"Commands"},{"location":"Cloud%20computing/RCI%20cluster/#task-explanations","text":"srun --pty bash -i - --pty runs the first task and close output and error stream for everything except the first task - bash : what we want to run - -i Input setting, here followed by no param indicating that the input stream is closed osed","title":"Task Explanations"},{"location":"Cloud%20computing/Slurm/","text":"Commands \u00b6 srun \u00b6 Run the task in the current shell in blocking mode, i.e., the console will be blocked till the task finishes. This command is only useful if we expect that the resources will be available immediatelly and the task will finish quickly. Otherwise, we should use sbatch . Params: - --pty runs the in terminal mode. Output and error streams are closed for everything except the first task. - -i Input setting. If followed by no param, it indicates that the input stream is closed. sbatch \u00b6 Request the execution of a task, with the required resources specified as sbatch parameters. The plain call with all resources defaulted is: sbatch <bash script> Note that the <bash script> here realy needs to be a bash script, it cannot be an arbitrary command or executable. Important parameters: - -n, --ntasks : maximum number of tasks/threads that will be allocated by the job - default is one task per node - -N, --nodes : number of allocated nodes. - default: minimum nodes that are needed to allocate resources according to other parameters (e.g., --ntasks , --mem ). - --mem maximum memory that will be allocated by the job. The suffix G stands for gigabytes, by default, it uses megabytes. Example: --mem=40G . - -t, --time : time limit. possible formats are <minutes> , <minutes:seconds> , <hours:minutes:seconds> , <days-hours> , <days-hours:minutes> , and <days-hours:minutes:seconds> . - default: partition time limit - -p , --partition= : partition name - -o , --output= : job's output file name. The default name is slurm-<JOB ID>.out squeue \u00b6 --me filter just my jobs -u <username> filter just a specific user --start print the expected start time and the nodes planed to run the task -w --nodelist filter jobs running (but not planned) on specific nodes. The format for nodelist is <name>[<range>] , e.g., n[05-06] . sinfo \u00b6 Prints information about the computer cluster. scancel \u00b6 The scancel command cancels the execution of a job specified by the ID (firsta argument). To instead cancel jobs by name, use the --name option. Note however, that full name has to be specified and no wildcards are allowed . To cancel all jobs with a certain name, we have to mess with various linux commands instead: squeue --me | awk '/smod_cha/ {print $1}' | xargs scancel sacctmgr \u00b6 For viewing and modifying Slurm account information. The most important command for users is show (or list , which is equivalent). Baset on the parameter, it shows different information: - show associations : associations between users and accounts - show qos : quality of service: limits and priorities for each group-queue combination Determining why the job was killed \u00b6 Usually, the error message is at the end of the output file. Message meaning: - Detected 1 oom_kill event in ... : oom stands for out of memory. The job was killed because it exceeded the memory limit.","title":"Slurm"},{"location":"Cloud%20computing/Slurm/#commands","text":"","title":"Commands"},{"location":"Cloud%20computing/Slurm/#srun","text":"Run the task in the current shell in blocking mode, i.e., the console will be blocked till the task finishes. This command is only useful if we expect that the resources will be available immediatelly and the task will finish quickly. Otherwise, we should use sbatch . Params: - --pty runs the in terminal mode. Output and error streams are closed for everything except the first task. - -i Input setting. If followed by no param, it indicates that the input stream is closed.","title":"srun"},{"location":"Cloud%20computing/Slurm/#sbatch","text":"Request the execution of a task, with the required resources specified as sbatch parameters. The plain call with all resources defaulted is: sbatch <bash script> Note that the <bash script> here realy needs to be a bash script, it cannot be an arbitrary command or executable. Important parameters: - -n, --ntasks : maximum number of tasks/threads that will be allocated by the job - default is one task per node - -N, --nodes : number of allocated nodes. - default: minimum nodes that are needed to allocate resources according to other parameters (e.g., --ntasks , --mem ). - --mem maximum memory that will be allocated by the job. The suffix G stands for gigabytes, by default, it uses megabytes. Example: --mem=40G . - -t, --time : time limit. possible formats are <minutes> , <minutes:seconds> , <hours:minutes:seconds> , <days-hours> , <days-hours:minutes> , and <days-hours:minutes:seconds> . - default: partition time limit - -p , --partition= : partition name - -o , --output= : job's output file name. The default name is slurm-<JOB ID>.out","title":"sbatch"},{"location":"Cloud%20computing/Slurm/#squeue","text":"--me filter just my jobs -u <username> filter just a specific user --start print the expected start time and the nodes planed to run the task -w --nodelist filter jobs running (but not planned) on specific nodes. The format for nodelist is <name>[<range>] , e.g., n[05-06] .","title":"squeue"},{"location":"Cloud%20computing/Slurm/#sinfo","text":"Prints information about the computer cluster.","title":"sinfo"},{"location":"Cloud%20computing/Slurm/#scancel","text":"The scancel command cancels the execution of a job specified by the ID (firsta argument). To instead cancel jobs by name, use the --name option. Note however, that full name has to be specified and no wildcards are allowed . To cancel all jobs with a certain name, we have to mess with various linux commands instead: squeue --me | awk '/smod_cha/ {print $1}' | xargs scancel","title":"scancel"},{"location":"Cloud%20computing/Slurm/#sacctmgr","text":"For viewing and modifying Slurm account information. The most important command for users is show (or list , which is equivalent). Baset on the parameter, it shows different information: - show associations : associations between users and accounts - show qos : quality of service: limits and priorities for each group-queue combination","title":"sacctmgr"},{"location":"Cloud%20computing/Slurm/#determining-why-the-job-was-killed","text":"Usually, the error message is at the end of the output file. Message meaning: - Detected 1 oom_kill event in ... : oom stands for out of memory. The job was killed because it exceeded the memory limit.","title":"Determining why the job was killed"},{"location":"LaTeX/LaTeX%20workflow/","text":"So far we have three LaTeX toolchains tah has proven to work well: - Overleaf : Cloud tool which is stable and very good for collaboration. - Texmaker + MiKTex : Traditioanl desktop setup. - VSCode + Latex Workshop + Tinytex : Modern desktop setup. The main advantage is that VSCode has the best Copilot support from all the editors, which is a huge time saver. VSCode + Latex Workshop + MikTeX/Tinytex \u00b6 Installation \u00b6 The installation of VSCode and Latex Workshop (VSCode extension) is straightforward, so we cover only the installation of MikTeX/ Tinytex here. MikTeX \u00b6 The installation of MikTeX is straightforward. After the installation, do not forget to add the MikTeX bin dir to PATH . MikTeX installs all the required packages on the fly, so there is no need to install them manually. The only thing that we need to do manually is to install Perl which is needed for the latexmk tool. There are two Perl distributions for Windows: ActivePerl and Strawberry Perl . This LaTeX toolchain has been only tested with Strawberry Perl. The installation of Strawberry Perl is straightforward: there is an executable installer, and the PATH variable is set automatically. Tinytex \u00b6 Official installation guide Install Tinytex using the shell script for the respective OS. The links to the scripts are in the installation guide. Add the executable path of Tinytex to the PATH variable. Installing additional packages \u00b6 Unlike MiKTex, Tinytex does not install required packages on the fly. Instead, it only shows an error in the log. To install a missing package, run the following command: tlmgr install <package name> Latex Workshop Configuration and Usage \u00b6 wiki Syncing between PDF and source \u00b6 To jump from PDF to source, use the binding configured in: Settings > Latex Workshop > View > Pdf > Internal > SyncTeX: Keybinding . To jump from source to PDF, use the binding configured in: Keyboad Shortcuts > Latex Workshop: SynTeX from cursor . Unfortunately, mouse cannot be used here due to VSCode limitations .","title":"LaTeX workflow"},{"location":"LaTeX/LaTeX%20workflow/#vscode-latex-workshop-miktextinytex","text":"","title":"VSCode + Latex Workshop + MikTeX/Tinytex"},{"location":"LaTeX/LaTeX%20workflow/#installation","text":"The installation of VSCode and Latex Workshop (VSCode extension) is straightforward, so we cover only the installation of MikTeX/ Tinytex here.","title":"Installation"},{"location":"LaTeX/LaTeX%20workflow/#miktex","text":"The installation of MikTeX is straightforward. After the installation, do not forget to add the MikTeX bin dir to PATH . MikTeX installs all the required packages on the fly, so there is no need to install them manually. The only thing that we need to do manually is to install Perl which is needed for the latexmk tool. There are two Perl distributions for Windows: ActivePerl and Strawberry Perl . This LaTeX toolchain has been only tested with Strawberry Perl. The installation of Strawberry Perl is straightforward: there is an executable installer, and the PATH variable is set automatically.","title":"MikTeX"},{"location":"LaTeX/LaTeX%20workflow/#tinytex","text":"Official installation guide Install Tinytex using the shell script for the respective OS. The links to the scripts are in the installation guide. Add the executable path of Tinytex to the PATH variable.","title":"Tinytex"},{"location":"LaTeX/LaTeX%20workflow/#installing-additional-packages","text":"Unlike MiKTex, Tinytex does not install required packages on the fly. Instead, it only shows an error in the log. To install a missing package, run the following command: tlmgr install <package name>","title":"Installing additional packages"},{"location":"LaTeX/LaTeX%20workflow/#latex-workshop-configuration-and-usage","text":"wiki","title":"Latex Workshop Configuration and Usage"},{"location":"LaTeX/LaTeX%20workflow/#syncing-between-pdf-and-source","text":"To jump from PDF to source, use the binding configured in: Settings > Latex Workshop > View > Pdf > Internal > SyncTeX: Keybinding . To jump from source to PDF, use the binding configured in: Keyboad Shortcuts > Latex Workshop: SynTeX from cursor . Unfortunately, mouse cannot be used here due to VSCode limitations .","title":"Syncing between PDF and source"},{"location":"LaTeX/Latex%20manual/","text":"Document structure \u00b6 The document structure is well documented on wikibooks . The basic structure is: \\documentclass[<options>]{<class>} ... \\begin{document} ... \\end{document} Escape characters \u00b6 LaTeX uses man6y special characters which needs to be escaped. Unfortunatelly, there is no single escape character, instead, there are many. The following table lists the most common escape characters: Character Escape sequence [ {[} ] {]} Text formatting \u00b6 Subscript and superscript \u00b6 In math mode, the subscript and superscript are created using the _ and ^ characters. In text mode, we need to use a special commands: \\textsubscript and \\textsuperscript . Example: H\\textsubscript{2}O Floats \u00b6 The following environments are floats: - figure - table - algorithm Placement \u00b6 Any float takes the position as a first argument. The following positions are available: - h : here - t : top - b : bottom - p : special dedicated page per float - ! : ignore nice positioning and put it according to the float specifiers The placement algorithm then iterate pages starting from the page where the float is placed. For each page, it tries to find a place for the float according to the float specifiers (in the same order as they appear in the float position argument). In case of success, the procedure stops and place the float. If the procedure fails for all pages, the float is placed at the end. Sources: - LaTeX Wikibook - Overleaf Default placement \u00b6 The default placement differs between environments and also classes. For example for article class, the default placement for figure and table is tbp ( see SO ). Tables \u00b6 The float environment for tables is table . However, the rows and columns are wrapped in another environment. The default inner enviroment is tabular , however, there are many other packages that extends the functionality. In practice, there are currently three inner environments to consider: - tabular : the default environment that is sufficient for simple tables - tabulary : the environment that allows to create columns with automatic width. If the main or only issue of the table is that it needs to fit a specific width, this is the environment to use. - tblr : the tblr environment from the tabulararray package is the most up to date tabular environment that support many features. Also, it splits the table presentation from the table content, which can make generating tables from code easier. The only downside is that it does not support automatic column width . Column types \u00b6 The column types are specified in the argument of the tabular or equivalent environment. The following column types are available by default: - l : left aligned - c : centered - r : right aligned - p{width} : paragraph column with specified width Other column types can be provided by the inner environment package or by the user. Simple tables with tabular environment \u00b6 The usual way to create a table in the tabular environment is: \\begin{table}[h] \\centering % center the content of the table environment \\begin{tabular}{|c|c|} ... rows and columns \\end{tabular} \\caption{My table} \\label{tab:my_table} \\end{table} Columns with automatic width: tabulary \u00b6 By default, laTeX does not support automatic width for columns, i.e., sizing the columns by their content. To enable this feature, we can use the tabulary package, which provides the tabulary environment (which is a replacement for the tabular environment). The columns with automatic width are specified by the L , C , R column types. Note that the new column types can be combined with the standard column types. In that case, the standard columns will have width according to their content, and the rest of the space will be distributed among the new column types. Configure the space between columns \u00b6 In most packages, the space between columns is configured using the \\tabcolsep variable. Example: \\setlength{\\tabcolsep}{10pt} However, in the tblr environment, the space between columns is configured using the leftsep and rightsep keys. Example:, \\begin{tblr} { colspec={llllrr}, leftsep=2pt, rightsep=2pt } By default, the leftsep and rightsep are set to 6pt . Export google sheets to latex tables \u00b6 There is ann addon called LatexKit which can be used for that. Footnotes in tables \u00b6 In tables and table captions, the \\footnote command does not work correctly. Also, it is not desirable to have the footnote at the bottom of page, instead, we want the footnote to be at the bottom of the table. To achieve this, we use a special environment: - threeparttable : if we are using the tabular or tabulary environment - talltblr : if we are using the tblr environment Using the threeparttable \u00b6 The threeparttable environment is used as follows: \\begin{table}[h] \\centering \\caption{My table} \\label{tab:my_table} \\begin{threeparttable} \\begin{tabular}{|c|c|} one$^a$ & two$^b$ \\\\ ... other rows and columns \\end{tabular} \\begin{tablenotes} \\item $^a$footnote 1 \\item $^b$footnote 2 \\end{tablenotes} \\end{threeparttable} \\end{table} Using the talltblr \u00b6 The talltblr environment is used as follows: \\begin{table}[h] \\centering \\caption{My table} \\label{tab:my_table} \\begin{talltblr}[ label = none, note{a} = {footnote 1}, note{b} = {footnote 2} ]{ colspec={|c|c|}, } one\\TblrNote{a} & two\\TblrNote{b} \\\\ ... other rows and columns \\end{talltblr} \\end{table} Notice the label = none option. Without it, the table numbering is raised again, resulting in the table being numbered twice. Math \u00b6 wiki To use math, we need the amsmath package. The math commands only works in math mode which can be entered in one of the many math environments. If/else variants \u00b6 For that, we use the cases environment. Example: \\begin{equation} f(x) = \\begin{cases} 0 & \\quad \\text{if } x < 0 \\\\ 1 & \\quad \\text{if } x \\geq 0 \\end{cases} \\end{equation} Problem and similar environments \u00b6 wiki The environments for special math text blocks are not included in the amsmath package. Instead, we can define them manually using the \\newtheorem command. Example: \\newtheorem{problem}{Problem} \\begin{problem} This is a problem. \\end{problem} Footnotes \u00b6 The footnote is created using the \\footnote{} command. Override footnote numbering \u00b6 To override the footnote numbering (e.g. to repeat the same number twice), we can use the \\setcounter command. Example: \\setcounter{footnote}{1} # set the footnote counter to 1 Bibliography \u00b6 See more on SE . For bibliography management, whole toolchain is usually needed, including: - a tool that generates the bibliography file (e.g. Zotero, Mendeley, ...) - a latex package that cares about the citations style (e.g. biblatex, natbib, or default style) - the real bibliography processer that generates and sorts the bibliography (e.g. bibtex, biber, ...) However, not all combinations of theses tools are possible. For understanding the pipeline and the possible combinations, see the following figure: When choosing what package to use in latex, we have to take care that we: - have the bibliography file in the right format ( .bib for all pipelines, but the content differs) - have the style in the right format ( .bst for default or natbib, .bbx for biblatex) By default, we should use the biblatex - Biber pipeline. Howevver, there are some circumstances where we need to use bibtex, for example, if we need to use a style that is not available for biblatex (as there is no conversion tool ). The styles available for biblatex are listed on CTAN . Latex document configuration \u00b6 Biblatex styling \u00b6 Basic setup: \\usepackage[style=numeric]{biblatex} ... \\addbibresource{bibliography.bib} ... \\printbibliography The style parameter is optional. The styles available for biblatex are listed on CTAN . Handle overflowing URLs in bibliography \u00b6 Sometimes, the links overflow the bibliography. To fix this, we can use the following commands: \\setcounter{biburllcpenalty}{100} \\setcounter{biburlucpenalty}{100} \\setcounter{biburlnumpenalty}{100} \\biburlnumskip=0mu plus 1mu\\relax \\biburlucskip=0mu plus 1mu\\relax \\biburllcskip=0mu plus 1mu\\relax Default and natbib styling \u00b6 Basic setup: \\bibliographystyle{plain} ... \\bibliography{bibliography} Note that we do not have to use any package to use basic cite commands. Also note, that the \\bibliographystyle command is mandatory . Finally, we do not need to specify the extension of the bibliography file. Natbib \u00b6 The bibtex bibliography management system is quite old and does not support many features. To overcome this, we can use the natbib package: \\usepackage{natbib} Splitting the document into multiple files \u00b6 There are two ways to split the document into multiple files: - \\input{file} - \\include{file} The \\include is intended for chapters or other large parts of the document. It has the following properties: - it starts a new page before and after the included file - it does not allow nesting - there is a special command \\includeonly{file1,file2,...} which allows to include only the specified files. This is useful for large documents where we want to compile only a part of the document. Without this command we would need to search for the include command and comment it out. The \\input command is intended for smaller parts of the document. Contrary to the \\include command, there is no special behavior involved. Instead, the content of the file is simply pasted at the place of the \\input command. Speedup Techniques \u00b6 The compilation of large documents can be slow. There are several techniques to speed up the compilation: - split the document into multiple files and use \\includeonly to include only the relevant files - precompiling the preamble - using draft mode Precompiling the preamble \u00b6 The preamble is the part of the document before the \\begin{document} command. It contains the document configuration, packages, etc. Because the included packages are usually large, the compilation of the preamble can be slow. To speed up the compilation, we can precompile the preamble and use the precompiled preamble in the main document. This can be done using the mylatexformat package. The usage is as follows: At the beginning of the preamble, add the following comment: %&<format name> . This will tell the compiler to use the specified format. The <format name> can be arbitrary, but it is recommended to use the same name as the main document. To spare some preamble content from being precompiled (dynamic content), add a command \\endofdump after the content that should not be precompiled. run the following command: PowerShell pdflatex --ini -jobname=\"<format name>\" \"&pdflatex\" mylatexformat.ltx <format name>.tex Afther this, the compilation of the main document should be faster. For more information, see the package documentation or the SO question . Miscelaneous tasks \u00b6 Balancing columns in two-column documents \u00b6 To balance the columns at the end of the document, we can use the flushend package. Just add \\usepackage{flushend} to the preamble. Common problems \u00b6 Ugly font in pdf \u00b6 This can be cause by the missing vector fonts. If the vector fonts are missing, the bitmap fonts are used instead. 1. To check if this is the cause, zoom in on the pdf. If the text is blurry, the bitmap fonts are used. 1. To fix this, install the vector fonts. - On Windows, install the cm-super package through MikTeX.","title":"Latex manual"},{"location":"LaTeX/Latex%20manual/#document-structure","text":"The document structure is well documented on wikibooks . The basic structure is: \\documentclass[<options>]{<class>} ... \\begin{document} ... \\end{document}","title":"Document structure"},{"location":"LaTeX/Latex%20manual/#escape-characters","text":"LaTeX uses man6y special characters which needs to be escaped. Unfortunatelly, there is no single escape character, instead, there are many. The following table lists the most common escape characters: Character Escape sequence [ {[} ] {]}","title":"Escape characters"},{"location":"LaTeX/Latex%20manual/#text-formatting","text":"","title":"Text formatting"},{"location":"LaTeX/Latex%20manual/#subscript-and-superscript","text":"In math mode, the subscript and superscript are created using the _ and ^ characters. In text mode, we need to use a special commands: \\textsubscript and \\textsuperscript . Example: H\\textsubscript{2}O","title":"Subscript and superscript"},{"location":"LaTeX/Latex%20manual/#floats","text":"The following environments are floats: - figure - table - algorithm","title":"Floats"},{"location":"LaTeX/Latex%20manual/#placement","text":"Any float takes the position as a first argument. The following positions are available: - h : here - t : top - b : bottom - p : special dedicated page per float - ! : ignore nice positioning and put it according to the float specifiers The placement algorithm then iterate pages starting from the page where the float is placed. For each page, it tries to find a place for the float according to the float specifiers (in the same order as they appear in the float position argument). In case of success, the procedure stops and place the float. If the procedure fails for all pages, the float is placed at the end. Sources: - LaTeX Wikibook - Overleaf","title":"Placement"},{"location":"LaTeX/Latex%20manual/#default-placement","text":"The default placement differs between environments and also classes. For example for article class, the default placement for figure and table is tbp ( see SO ).","title":"Default placement"},{"location":"LaTeX/Latex%20manual/#tables","text":"The float environment for tables is table . However, the rows and columns are wrapped in another environment. The default inner enviroment is tabular , however, there are many other packages that extends the functionality. In practice, there are currently three inner environments to consider: - tabular : the default environment that is sufficient for simple tables - tabulary : the environment that allows to create columns with automatic width. If the main or only issue of the table is that it needs to fit a specific width, this is the environment to use. - tblr : the tblr environment from the tabulararray package is the most up to date tabular environment that support many features. Also, it splits the table presentation from the table content, which can make generating tables from code easier. The only downside is that it does not support automatic column width .","title":"Tables"},{"location":"LaTeX/Latex%20manual/#column-types","text":"The column types are specified in the argument of the tabular or equivalent environment. The following column types are available by default: - l : left aligned - c : centered - r : right aligned - p{width} : paragraph column with specified width Other column types can be provided by the inner environment package or by the user.","title":"Column types"},{"location":"LaTeX/Latex%20manual/#simple-tables-with-tabular-environment","text":"The usual way to create a table in the tabular environment is: \\begin{table}[h] \\centering % center the content of the table environment \\begin{tabular}{|c|c|} ... rows and columns \\end{tabular} \\caption{My table} \\label{tab:my_table} \\end{table}","title":"Simple tables with tabular environment"},{"location":"LaTeX/Latex%20manual/#columns-with-automatic-width-tabulary","text":"By default, laTeX does not support automatic width for columns, i.e., sizing the columns by their content. To enable this feature, we can use the tabulary package, which provides the tabulary environment (which is a replacement for the tabular environment). The columns with automatic width are specified by the L , C , R column types. Note that the new column types can be combined with the standard column types. In that case, the standard columns will have width according to their content, and the rest of the space will be distributed among the new column types.","title":"Columns with automatic width: tabulary"},{"location":"LaTeX/Latex%20manual/#configure-the-space-between-columns","text":"In most packages, the space between columns is configured using the \\tabcolsep variable. Example: \\setlength{\\tabcolsep}{10pt} However, in the tblr environment, the space between columns is configured using the leftsep and rightsep keys. Example:, \\begin{tblr} { colspec={llllrr}, leftsep=2pt, rightsep=2pt } By default, the leftsep and rightsep are set to 6pt .","title":"Configure the space between columns"},{"location":"LaTeX/Latex%20manual/#export-google-sheets-to-latex-tables","text":"There is ann addon called LatexKit which can be used for that.","title":"Export google sheets to latex tables"},{"location":"LaTeX/Latex%20manual/#footnotes-in-tables","text":"In tables and table captions, the \\footnote command does not work correctly. Also, it is not desirable to have the footnote at the bottom of page, instead, we want the footnote to be at the bottom of the table. To achieve this, we use a special environment: - threeparttable : if we are using the tabular or tabulary environment - talltblr : if we are using the tblr environment","title":"Footnotes in tables"},{"location":"LaTeX/Latex%20manual/#using-the-threeparttable","text":"The threeparttable environment is used as follows: \\begin{table}[h] \\centering \\caption{My table} \\label{tab:my_table} \\begin{threeparttable} \\begin{tabular}{|c|c|} one$^a$ & two$^b$ \\\\ ... other rows and columns \\end{tabular} \\begin{tablenotes} \\item $^a$footnote 1 \\item $^b$footnote 2 \\end{tablenotes} \\end{threeparttable} \\end{table}","title":"Using the threeparttable"},{"location":"LaTeX/Latex%20manual/#using-the-talltblr","text":"The talltblr environment is used as follows: \\begin{table}[h] \\centering \\caption{My table} \\label{tab:my_table} \\begin{talltblr}[ label = none, note{a} = {footnote 1}, note{b} = {footnote 2} ]{ colspec={|c|c|}, } one\\TblrNote{a} & two\\TblrNote{b} \\\\ ... other rows and columns \\end{talltblr} \\end{table} Notice the label = none option. Without it, the table numbering is raised again, resulting in the table being numbered twice.","title":"Using the talltblr"},{"location":"LaTeX/Latex%20manual/#math","text":"wiki To use math, we need the amsmath package. The math commands only works in math mode which can be entered in one of the many math environments.","title":"Math"},{"location":"LaTeX/Latex%20manual/#ifelse-variants","text":"For that, we use the cases environment. Example: \\begin{equation} f(x) = \\begin{cases} 0 & \\quad \\text{if } x < 0 \\\\ 1 & \\quad \\text{if } x \\geq 0 \\end{cases} \\end{equation}","title":"If/else variants"},{"location":"LaTeX/Latex%20manual/#problem-and-similar-environments","text":"wiki The environments for special math text blocks are not included in the amsmath package. Instead, we can define them manually using the \\newtheorem command. Example: \\newtheorem{problem}{Problem} \\begin{problem} This is a problem. \\end{problem}","title":"Problem and similar environments"},{"location":"LaTeX/Latex%20manual/#footnotes","text":"The footnote is created using the \\footnote{} command.","title":"Footnotes"},{"location":"LaTeX/Latex%20manual/#override-footnote-numbering","text":"To override the footnote numbering (e.g. to repeat the same number twice), we can use the \\setcounter command. Example: \\setcounter{footnote}{1} # set the footnote counter to 1","title":"Override footnote numbering"},{"location":"LaTeX/Latex%20manual/#bibliography","text":"See more on SE . For bibliography management, whole toolchain is usually needed, including: - a tool that generates the bibliography file (e.g. Zotero, Mendeley, ...) - a latex package that cares about the citations style (e.g. biblatex, natbib, or default style) - the real bibliography processer that generates and sorts the bibliography (e.g. bibtex, biber, ...) However, not all combinations of theses tools are possible. For understanding the pipeline and the possible combinations, see the following figure: When choosing what package to use in latex, we have to take care that we: - have the bibliography file in the right format ( .bib for all pipelines, but the content differs) - have the style in the right format ( .bst for default or natbib, .bbx for biblatex) By default, we should use the biblatex - Biber pipeline. Howevver, there are some circumstances where we need to use bibtex, for example, if we need to use a style that is not available for biblatex (as there is no conversion tool ). The styles available for biblatex are listed on CTAN .","title":"Bibliography"},{"location":"LaTeX/Latex%20manual/#latex-document-configuration","text":"","title":"Latex document configuration"},{"location":"LaTeX/Latex%20manual/#biblatex-styling","text":"Basic setup: \\usepackage[style=numeric]{biblatex} ... \\addbibresource{bibliography.bib} ... \\printbibliography The style parameter is optional. The styles available for biblatex are listed on CTAN .","title":"Biblatex styling"},{"location":"LaTeX/Latex%20manual/#handle-overflowing-urls-in-bibliography","text":"Sometimes, the links overflow the bibliography. To fix this, we can use the following commands: \\setcounter{biburllcpenalty}{100} \\setcounter{biburlucpenalty}{100} \\setcounter{biburlnumpenalty}{100} \\biburlnumskip=0mu plus 1mu\\relax \\biburlucskip=0mu plus 1mu\\relax \\biburllcskip=0mu plus 1mu\\relax","title":"Handle overflowing URLs in bibliography"},{"location":"LaTeX/Latex%20manual/#default-and-natbib-styling","text":"Basic setup: \\bibliographystyle{plain} ... \\bibliography{bibliography} Note that we do not have to use any package to use basic cite commands. Also note, that the \\bibliographystyle command is mandatory . Finally, we do not need to specify the extension of the bibliography file.","title":"Default and natbib styling"},{"location":"LaTeX/Latex%20manual/#natbib","text":"The bibtex bibliography management system is quite old and does not support many features. To overcome this, we can use the natbib package: \\usepackage{natbib}","title":"Natbib"},{"location":"LaTeX/Latex%20manual/#splitting-the-document-into-multiple-files","text":"There are two ways to split the document into multiple files: - \\input{file} - \\include{file} The \\include is intended for chapters or other large parts of the document. It has the following properties: - it starts a new page before and after the included file - it does not allow nesting - there is a special command \\includeonly{file1,file2,...} which allows to include only the specified files. This is useful for large documents where we want to compile only a part of the document. Without this command we would need to search for the include command and comment it out. The \\input command is intended for smaller parts of the document. Contrary to the \\include command, there is no special behavior involved. Instead, the content of the file is simply pasted at the place of the \\input command.","title":"Splitting the document into multiple files"},{"location":"LaTeX/Latex%20manual/#speedup-techniques","text":"The compilation of large documents can be slow. There are several techniques to speed up the compilation: - split the document into multiple files and use \\includeonly to include only the relevant files - precompiling the preamble - using draft mode","title":"Speedup Techniques"},{"location":"LaTeX/Latex%20manual/#precompiling-the-preamble","text":"The preamble is the part of the document before the \\begin{document} command. It contains the document configuration, packages, etc. Because the included packages are usually large, the compilation of the preamble can be slow. To speed up the compilation, we can precompile the preamble and use the precompiled preamble in the main document. This can be done using the mylatexformat package. The usage is as follows: At the beginning of the preamble, add the following comment: %&<format name> . This will tell the compiler to use the specified format. The <format name> can be arbitrary, but it is recommended to use the same name as the main document. To spare some preamble content from being precompiled (dynamic content), add a command \\endofdump after the content that should not be precompiled. run the following command: PowerShell pdflatex --ini -jobname=\"<format name>\" \"&pdflatex\" mylatexformat.ltx <format name>.tex Afther this, the compilation of the main document should be faster. For more information, see the package documentation or the SO question .","title":"Precompiling the preamble"},{"location":"LaTeX/Latex%20manual/#miscelaneous-tasks","text":"","title":"Miscelaneous tasks"},{"location":"LaTeX/Latex%20manual/#balancing-columns-in-two-column-documents","text":"To balance the columns at the end of the document, we can use the flushend package. Just add \\usepackage{flushend} to the preamble.","title":"Balancing columns in two-column documents"},{"location":"LaTeX/Latex%20manual/#common-problems","text":"","title":"Common problems"},{"location":"LaTeX/Latex%20manual/#ugly-font-in-pdf","text":"This can be cause by the missing vector fonts. If the vector fonts are missing, the bitmap fonts are used instead. 1. To check if this is the cause, zoom in on the pdf. If the text is blurry, the bitmap fonts are used. 1. To fix this, install the vector fonts. - On Windows, install the cm-super package through MikTeX.","title":"Ugly font in pdf"},{"location":"Programming/Common/","text":"Keymap \u00b6 Copy : Ctrl + C Cut : Ctrl + X Paste : Ctrl + V Toggle comment : Ctrl + Q Search in file : Ctrl + S Sellect all : Ctrl + A Format selection : Ctrl + F Format File : Ctrl + Shift + F Build : Ctrl + B Refactoring \u00b6 Rename : Ctrl + R Change signature : Ctrl + G Text transform : Ctrl + T + U : to upper case Surround with : Ctrl + W Testing private methods \u00b6 An urgent need to test privete method accompanied with a lack of knowledge of how to do it is a common problem. In almost all programming languages, the testing of private methods is obsturcted by the language itself, i.e., the test frameworks does not have a special access to private methods. In this section we disscuss the usuall solutions to this problem. These implementation is specific to a particular language, but the general ideas are the same. The possible approaches are: - Makeing the method public : Only recommended if the method should be exposed, i.e., its functionality is not limited to the class itself. - Move the method to a different class : Maybe, the method is correcly marked as private in the current context, but it can also be extracted to its own class, where it will become the main method of the class. This applies to methods that can be used in other contexts, or for methods contained in large classes. - Mark the method as internal and make it public : This is a strategy that can be always applied with minimum effort. Various ways how to signalize that the method is intended for internal use are: - Naming convention : The method name can start with an underscore, e.g., _my_method . - Documentation : The comments can contain a warning that the method is intended for internal use. - Namespace : The method can be placed in a namespace that signals that it is intended for internal use, e.g., internal::my_method . - Special access : We can use special language-dependant tools that can provide a special access to private methods: - in C++ the friend keyword can be used to grant access to a class to another class. - In Java, the @VisibleForTesting annotation can be used to mark a method as visible for testing. - In Python, the __test__ attribute can be used to mark a method as visible for testing. Finding Duplicates \u00b6 For finding duplicates, there are two possible approaches: - Using hash sets : iteratively checking if the current element is in the set of already seen elements and adding it to the set if not. - Sorting : sorting the collection and then for each element checking if the current element is the same as the previous one. Comparison: | Approach | Time complexity (worst case asymptothic)| Time complexity (average expected) | Space complexity | allocation complexity | | --- | --- | --- | --- | --- | | Sets | O(log n) (both contains and add) | O(1) (both contains and add) | O(n) | O(1) | | Sorting | O(n log n) (sorting) | O(n log n) (sorting) + O(n) (duplicates check) | 0 or O(n) if we need to left the source collection unsorted | 0 or O(1) in case of new collection |","title":"Common"},{"location":"Programming/Common/#keymap","text":"Copy : Ctrl + C Cut : Ctrl + X Paste : Ctrl + V Toggle comment : Ctrl + Q Search in file : Ctrl + S Sellect all : Ctrl + A Format selection : Ctrl + F Format File : Ctrl + Shift + F Build : Ctrl + B","title":"Keymap"},{"location":"Programming/Common/#refactoring","text":"Rename : Ctrl + R Change signature : Ctrl + G Text transform : Ctrl + T + U : to upper case Surround with : Ctrl + W","title":"Refactoring"},{"location":"Programming/Common/#testing-private-methods","text":"An urgent need to test privete method accompanied with a lack of knowledge of how to do it is a common problem. In almost all programming languages, the testing of private methods is obsturcted by the language itself, i.e., the test frameworks does not have a special access to private methods. In this section we disscuss the usuall solutions to this problem. These implementation is specific to a particular language, but the general ideas are the same. The possible approaches are: - Makeing the method public : Only recommended if the method should be exposed, i.e., its functionality is not limited to the class itself. - Move the method to a different class : Maybe, the method is correcly marked as private in the current context, but it can also be extracted to its own class, where it will become the main method of the class. This applies to methods that can be used in other contexts, or for methods contained in large classes. - Mark the method as internal and make it public : This is a strategy that can be always applied with minimum effort. Various ways how to signalize that the method is intended for internal use are: - Naming convention : The method name can start with an underscore, e.g., _my_method . - Documentation : The comments can contain a warning that the method is intended for internal use. - Namespace : The method can be placed in a namespace that signals that it is intended for internal use, e.g., internal::my_method . - Special access : We can use special language-dependant tools that can provide a special access to private methods: - in C++ the friend keyword can be used to grant access to a class to another class. - In Java, the @VisibleForTesting annotation can be used to mark a method as visible for testing. - In Python, the __test__ attribute can be used to mark a method as visible for testing.","title":"Testing private methods"},{"location":"Programming/Common/#finding-duplicates","text":"For finding duplicates, there are two possible approaches: - Using hash sets : iteratively checking if the current element is in the set of already seen elements and adding it to the set if not. - Sorting : sorting the collection and then for each element checking if the current element is the same as the previous one. Comparison: | Approach | Time complexity (worst case asymptothic)| Time complexity (average expected) | Space complexity | allocation complexity | | --- | --- | --- | --- | --- | | Sets | O(log n) (both contains and add) | O(1) (both contains and add) | O(n) | O(1) | | Sorting | O(n log n) (sorting) | O(n log n) (sorting) + O(n) (duplicates check) | 0 or O(n) if we need to left the source collection unsorted | 0 or O(1) in case of new collection |","title":"Finding Duplicates"},{"location":"Programming/Git/","text":"Revert local changes (uncommited): git checkout <pathspec> Rewrite remote with local changes without merging git push -f Go back to branch: git revert --no-commit 0766c053..HEAD Untrack files, but not delete them locally: \u00b6 git rm --cached <FILEPATH> usefull params: - -r : recursive - deletes content of directories - -n dry run Reset individual files \u00b6 To reset an individual file, call: git checkout <filepath> Wildcards \u00b6 Can be used in gitignore and also in some git commands. All described in the Manual . Usefull wildcards: - **/ in any directory - /** everything in a directory Removing files from all branches, local and remote \u00b6 Removing files from history can be done using multiple tools: 1. bfg repo cleaner is the simplest tool. It can only select files by filename or size, but that is sufficient in most cases. 2. git filter-repo is a more sophisticated successor to BFG. It can do almost anything possible. Nevertheless, it is less intuitive to operate, and it is harder to analyze its result. 3. The filter-branch command is the original tool for filtering git history. It is slow and problematic, so it should be used only if the above two tools are not available. No matter of the used tool, before you begin: 1. commit and push from all machines, 2. backup the repository Similarly, at the end: - It is important not to merge (branch or conflitcs) that originated before the cleanup on other machines. Otherwise, the deleted file can be reintroduced to the history. - Pull on other machines. - Add the file to gitignore so that it wont be commited again BFG \u00b6 With BFG, only a file with specific filename can be deleted . It is not possible to use exact file path. To remove file by its name: 1. remove the file locally 2. clone the whole repo again with the --mirror option 3. on the mirrored repo, run the cleaner: bfg --delete-files <FILENAME> 4. run the git commands that appears at the end of the bfg output 5. run git push Git filter-repo \u00b6 The git filter-repo can be installed using pip: pip install git-filter-repo . To remove file by its path: 1. run the command: git filter-repo --invert-paths --force --dry-run --path <PATH TO THE FILE TO BE REMOVED> 2. inspect the changes in .git/filter-repo directory: - Compare the files in KDiff3 - To skip the lines starting with original-oid , 1. go to the file selection dialog 2. click Configure 3. to the line-matching preprocessor command, add: sed 's/original-oid .*//' 3. add remote again: git remote add origin <REPO SSH ADDRESS> 4. force push the comman to the remote git push origin --force --all - If the push is rejected by the remote hook, the master branch is probably protected. It has to be unprotected first in the repository config. git filter-repo manual More information on github filter-branch \u00b6 git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch <FILE>' --prune-empty --tag-name-filter cat -- --all Merging \u00b6 Syntax: git merge <params> <commit> Merge can be aborted at any time by calling git merge --abort . This resets the state of the repository. Merging Moved Files \u00b6 Sometimes, it's necessary to tweak the command to help it locate the moved files. What can help: -X rename-threshold=25 : Changing the threshold is important for languages that changes the file content when moving it with automatic refactoring (especially important for Java files, which usually have tons of imports) -X ignore-space-change Revert merge: \u00b6 git revert -m 1 Note that when the merge is reverted, the changes cannot be merged again, because they predates the revert! Update \u00b6 On windows, run: git update-git-for-windows Repository Migration \u00b6 https://github.com/piceaTech/node-gitlab-2-github","title":"Git"},{"location":"Programming/Git/#untrack-files-but-not-delete-them-locally","text":"git rm --cached <FILEPATH> usefull params: - -r : recursive - deletes content of directories - -n dry run","title":"Untrack files, but not delete them locally:"},{"location":"Programming/Git/#reset-individual-files","text":"To reset an individual file, call: git checkout <filepath>","title":"Reset individual files"},{"location":"Programming/Git/#wildcards","text":"Can be used in gitignore and also in some git commands. All described in the Manual . Usefull wildcards: - **/ in any directory - /** everything in a directory","title":"Wildcards"},{"location":"Programming/Git/#removing-files-from-all-branches-local-and-remote","text":"Removing files from history can be done using multiple tools: 1. bfg repo cleaner is the simplest tool. It can only select files by filename or size, but that is sufficient in most cases. 2. git filter-repo is a more sophisticated successor to BFG. It can do almost anything possible. Nevertheless, it is less intuitive to operate, and it is harder to analyze its result. 3. The filter-branch command is the original tool for filtering git history. It is slow and problematic, so it should be used only if the above two tools are not available. No matter of the used tool, before you begin: 1. commit and push from all machines, 2. backup the repository Similarly, at the end: - It is important not to merge (branch or conflitcs) that originated before the cleanup on other machines. Otherwise, the deleted file can be reintroduced to the history. - Pull on other machines. - Add the file to gitignore so that it wont be commited again","title":"Removing files from all branches, local and remote"},{"location":"Programming/Git/#bfg","text":"With BFG, only a file with specific filename can be deleted . It is not possible to use exact file path. To remove file by its name: 1. remove the file locally 2. clone the whole repo again with the --mirror option 3. on the mirrored repo, run the cleaner: bfg --delete-files <FILENAME> 4. run the git commands that appears at the end of the bfg output 5. run git push","title":"BFG"},{"location":"Programming/Git/#git-filter-repo","text":"The git filter-repo can be installed using pip: pip install git-filter-repo . To remove file by its path: 1. run the command: git filter-repo --invert-paths --force --dry-run --path <PATH TO THE FILE TO BE REMOVED> 2. inspect the changes in .git/filter-repo directory: - Compare the files in KDiff3 - To skip the lines starting with original-oid , 1. go to the file selection dialog 2. click Configure 3. to the line-matching preprocessor command, add: sed 's/original-oid .*//' 3. add remote again: git remote add origin <REPO SSH ADDRESS> 4. force push the comman to the remote git push origin --force --all - If the push is rejected by the remote hook, the master branch is probably protected. It has to be unprotected first in the repository config. git filter-repo manual More information on github","title":"Git filter-repo"},{"location":"Programming/Git/#filter-branch","text":"git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch <FILE>' --prune-empty --tag-name-filter cat -- --all","title":"filter-branch"},{"location":"Programming/Git/#merging","text":"Syntax: git merge <params> <commit> Merge can be aborted at any time by calling git merge --abort . This resets the state of the repository.","title":"Merging"},{"location":"Programming/Git/#merging-moved-files","text":"Sometimes, it's necessary to tweak the command to help it locate the moved files. What can help: -X rename-threshold=25 : Changing the threshold is important for languages that changes the file content when moving it with automatic refactoring (especially important for Java files, which usually have tons of imports) -X ignore-space-change","title":"Merging Moved Files"},{"location":"Programming/Git/#revert-merge","text":"git revert -m 1 Note that when the merge is reverted, the changes cannot be merged again, because they predates the revert!","title":"Revert merge:"},{"location":"Programming/Git/#update","text":"On windows, run: git update-git-for-windows","title":"Update"},{"location":"Programming/Git/#repository-migration","text":"https://github.com/piceaTech/node-gitlab-2-github","title":"Repository Migration"},{"location":"Programming/Google%20API%20and%20Apps%20Script/","text":"Comments \u00b6 API reference Anchored Comments \u00b6 According to my experiments and SO . It is not possible to add achore comments to google docs. Also, it is not possible to get any readable anchors from existing comments. The only way to extract comments with anchors is to export the document as Microsoft Word. Written with StackEdit .","title":"Google API and Apps Script"},{"location":"Programming/Google%20API%20and%20Apps%20Script/#comments","text":"API reference","title":"Comments"},{"location":"Programming/Google%20API%20and%20Apps%20Script/#anchored-comments","text":"According to my experiments and SO . It is not possible to add achore comments to google docs. Also, it is not possible to get any readable anchors from existing comments. The only way to extract comments with anchors is to export the document as Microsoft Word. Written with StackEdit .","title":"Anchored Comments"},{"location":"Programming/Gurobi/","text":"Parallel Execution \u00b6 The gurobi solver solves a problem in parallel by default, trying multiple solution methods at the same time (see the official description ). It is also possible to run multiple problems in parallel ( source ), but each problem should be run in its own gurobi environment. Also, each environment should be configured to use only a single thread (e.g., in C++: env.set(GRB_IntParam_Threads, 1); ). The problem with this approach is that the CPU is usually not the bottleneck of the computation, the bottleneck is the memory ( source ). Therefore, solving multiple problems in parallel does not guarantee any speed up, it could be actually slower. The performance could be most likely improved when running the problems in parallel on multiple machines (not multiple cores of the same machine). Some advised to use MPI for that. Written with StackEdit .","title":"Gurobi"},{"location":"Programming/Gurobi/#parallel-execution","text":"The gurobi solver solves a problem in parallel by default, trying multiple solution methods at the same time (see the official description ). It is also possible to run multiple problems in parallel ( source ), but each problem should be run in its own gurobi environment. Also, each environment should be configured to use only a single thread (e.g., in C++: env.set(GRB_IntParam_Threads, 1); ). The problem with this approach is that the CPU is usually not the bottleneck of the computation, the bottleneck is the memory ( source ). Therefore, solving multiple problems in parallel does not guarantee any speed up, it could be actually slower. The performance could be most likely improved when running the problems in parallel on multiple machines (not multiple cores of the same machine). Some advised to use MPI for that. Written with StackEdit .","title":"Parallel Execution"},{"location":"Programming/JSON/","text":"Jackson \u00b6 Usefull Annotations \u00b6 @JsonIncludeProperties : Ignore all properties except listed @JsonProperty(\"my_name\") : Custom name of the JSON key @JsonIgnore : Ignore the json property below Wrapping the Obejct in Another JSON Object \u00b6 To do that, use these annotations above the class. @JsonTypeName(value = \"action\") @JsonTypeInfo(include=As.WRAPPER_OBJECT, use=Id.NAME) If you do not care about the name, you can skip the @JsonTypeName annotation. Written with StackEdit .","title":"JSON"},{"location":"Programming/JSON/#jackson","text":"","title":"Jackson"},{"location":"Programming/JSON/#usefull-annotations","text":"@JsonIncludeProperties : Ignore all properties except listed @JsonProperty(\"my_name\") : Custom name of the JSON key @JsonIgnore : Ignore the json property below","title":"Usefull Annotations"},{"location":"Programming/JSON/#wrapping-the-obejct-in-another-json-object","text":"To do that, use these annotations above the class. @JsonTypeName(value = \"action\") @JsonTypeInfo(include=As.WRAPPER_OBJECT, use=Id.NAME) If you do not care about the name, you can skip the @JsonTypeName annotation. Written with StackEdit .","title":"Wrapping the Obejct in Another JSON Object"},{"location":"Programming/Overpass%20Manual/","text":"Sources \u00b6 wiki/Overpass QL Strucutre \u00b6 Every statement ents with ; . Sets \u00b6 Almost all statements works with sets of data. Unless specified, we work with a default set ( _ ). To wrrito to a specific set, we can use ->. operator: <statement>->.<set> writes the result of the <statement> to the . The default set can be ommited: <statement> is equal to <statement>->._ . out statement \u00b6 All queries should contain an out statement that determines the output format. - out is used for data only request - out geom returns data with all elements associated with their geometry. Note that while the output format can be specified, we cannot filter the output (e.g., we cannot filter the ralation members ). Area specification \u00b6 We select an area by name as: area[\"name\"=\"Letkov\"]; And then we used it as: node(area); If we need more areas in a query, we can store them in variable: area[\"name\"=\"Letkov\"]->.let; And: node(area.let); Important note: If an area needs to be used repetadly, it has to be named, as the value of area ( area._ ) is replaced by the output of any statement. Filtering \u00b6 filters are specified in brackets: rel[\"admin_level\"=8]; It is also possible to use a regex filtering, we just neeed to replace = with ~ : rel[\"admin_level\"~\".*\"]; Selecting Multiple Data Sets \u00b6 Implicitely, all filters are aplied to a default dataset called _ and also written to it. Therefore, we cannot do: rel[\"admin_level\"~\".*\"]; node; because we are basically selecting nodes from a set of relations. Solutions: 1) Union statement 2) Named sets Union Utatement \u00b6 Union statement is surounded by parantheses. We can achieve the intended behaviour by: ( rel[\"admin_level\"~\".*\"]; node; ); Select Area Boundary \u00b6 Sometimes, it is usefull to check the boundary of the named area. However, the administrative level differ for various areas cities, countries, so the admin_level relation cannot be used for that. Luckilly, there is an option to use the pivot filter. Example: area[\"name\"=\"Praha\"]; node(pivot); out geom; Discover the full name of an area \u00b6 If we want to know the full name of the area while the above query return multiple results, we can do that in openstreet map: 1. Move the map to see the area 2. Click the button with cusor and question mark to select the exploration tool 3. Click inside the area 4. Scroll down to area relations 5. Click on the proper region 6. The name property is what we are looking for Filter areas with duplicite names \u00b6 Sometimes, even the full name of the area is not specific enough. In that case, we can use two approaches: - select the area by the area relation id - specify the area by the higher level area (state, country) Select area by ID \u00b6 select the requested area copy the id and add 3 600 000 000 to it (start with 36 and fill zerose till there are 10 digits in total) replace area[\"name\"=\"<NAME>] with area(<ID>) . Note that round brackets are used instead of square brackets! Specify area with higher level area \u00b6 In this exaple, we select the Coo County, Illinois. area[name=\"Illinois\"]; rel[name=\"Cook County\"](area); map_to_area; node(pivot); out geom; more info Get historical data \u00b6 To get historical data, prepend the query with a date statement. Example: [date:\"2016-01-01T00:00:00Z\"]; area[name=\"City of New York\"]; node(pivot); out geom;","title":"Overpass Manual"},{"location":"Programming/Overpass%20Manual/#sources","text":"wiki/Overpass QL","title":"Sources"},{"location":"Programming/Overpass%20Manual/#strucutre","text":"Every statement ents with ; .","title":"Strucutre"},{"location":"Programming/Overpass%20Manual/#sets","text":"Almost all statements works with sets of data. Unless specified, we work with a default set ( _ ). To wrrito to a specific set, we can use ->. operator: <statement>->.<set> writes the result of the <statement> to the . The default set can be ommited: <statement> is equal to <statement>->._ .","title":"Sets"},{"location":"Programming/Overpass%20Manual/#out-statement","text":"All queries should contain an out statement that determines the output format. - out is used for data only request - out geom returns data with all elements associated with their geometry. Note that while the output format can be specified, we cannot filter the output (e.g., we cannot filter the ralation members ).","title":"out statement"},{"location":"Programming/Overpass%20Manual/#area-specification","text":"We select an area by name as: area[\"name\"=\"Letkov\"]; And then we used it as: node(area); If we need more areas in a query, we can store them in variable: area[\"name\"=\"Letkov\"]->.let; And: node(area.let); Important note: If an area needs to be used repetadly, it has to be named, as the value of area ( area._ ) is replaced by the output of any statement.","title":"Area specification"},{"location":"Programming/Overpass%20Manual/#filtering","text":"filters are specified in brackets: rel[\"admin_level\"=8]; It is also possible to use a regex filtering, we just neeed to replace = with ~ : rel[\"admin_level\"~\".*\"];","title":"Filtering"},{"location":"Programming/Overpass%20Manual/#selecting-multiple-data-sets","text":"Implicitely, all filters are aplied to a default dataset called _ and also written to it. Therefore, we cannot do: rel[\"admin_level\"~\".*\"]; node; because we are basically selecting nodes from a set of relations. Solutions: 1) Union statement 2) Named sets","title":"Selecting Multiple Data Sets"},{"location":"Programming/Overpass%20Manual/#union-utatement","text":"Union statement is surounded by parantheses. We can achieve the intended behaviour by: ( rel[\"admin_level\"~\".*\"]; node; );","title":"Union Utatement"},{"location":"Programming/Overpass%20Manual/#select-area-boundary","text":"Sometimes, it is usefull to check the boundary of the named area. However, the administrative level differ for various areas cities, countries, so the admin_level relation cannot be used for that. Luckilly, there is an option to use the pivot filter. Example: area[\"name\"=\"Praha\"]; node(pivot); out geom;","title":"Select Area Boundary"},{"location":"Programming/Overpass%20Manual/#discover-the-full-name-of-an-area","text":"If we want to know the full name of the area while the above query return multiple results, we can do that in openstreet map: 1. Move the map to see the area 2. Click the button with cusor and question mark to select the exploration tool 3. Click inside the area 4. Scroll down to area relations 5. Click on the proper region 6. The name property is what we are looking for","title":"Discover the full name of an area"},{"location":"Programming/Overpass%20Manual/#filter-areas-with-duplicite-names","text":"Sometimes, even the full name of the area is not specific enough. In that case, we can use two approaches: - select the area by the area relation id - specify the area by the higher level area (state, country)","title":"Filter areas with duplicite names"},{"location":"Programming/Overpass%20Manual/#select-area-by-id","text":"select the requested area copy the id and add 3 600 000 000 to it (start with 36 and fill zerose till there are 10 digits in total) replace area[\"name\"=\"<NAME>] with area(<ID>) . Note that round brackets are used instead of square brackets!","title":"Select area by ID"},{"location":"Programming/Overpass%20Manual/#specify-area-with-higher-level-area","text":"In this exaple, we select the Coo County, Illinois. area[name=\"Illinois\"]; rel[name=\"Cook County\"](area); map_to_area; node(pivot); out geom; more info","title":"Specify area with higher level area"},{"location":"Programming/Overpass%20Manual/#get-historical-data","text":"To get historical data, prepend the query with a date statement. Example: [date:\"2016-01-01T00:00:00Z\"]; area[name=\"City of New York\"]; node(pivot); out geom;","title":"Get historical data"},{"location":"Programming/PostgreSQL%20Manual/","text":"Data types \u00b6 official documentation Date \u00b6 official documentation date : for dates time for time timestmp for both date and time interval Select a part of date/time/timestamp \u00b6 If we want just a part of a date, time, or timestamp, we can use the extract function. Example: SELECT extract(hour FROM <date column name>) FROM... Other parts can be extracted too. To extract day of week , we can use isodow (assigns 1 to Monday and 7 to Sunday). Auto incrementing columns \u00b6 In PostgreSQL, sequences are used for auto-incrementing columns. When you are creating a new db table or adding a new column, the process of creating a new sequence can be automated by choosing an identity or a serial column type. When updating an aexisting column, a manual intervention is required: 1. change the column to some numerical datatype 2. create the sequence: CREATE SEQUENCE <SEQUENCE NAME> OWNED BY <TABLE NAME>.<COLUMN NAME>; adjust the value of the sequence: SELECT setval(pg_get_serial_sequence('<TABLE NAME>', '<COLUMN NAME>'), max(<COLUMN NAME>)) FROM <TABLE NAME>; set the column to be incremented by the sequence: ALTER TABLE <TABLE NAME> ALTER COLUMN <COLUMN NAME> SET DEFAULT nextval('<SEQUENCE NAME>'); Strings \u00b6 There are many string function available, including the format function that works similarly to the C format function. For all functions, check the documentation . Arrays \u00b6 arrays array functions and operators arrays are declared as <type>[] , e.g., integer[] . The type can be any type, including composite types. To compute array length , use array_length(contracted_vertices, 1) where 1 stands for the first dimension. To cretea an array literal , we use single quatation and curly brackets: '{1, 2, 3}' . To check that some value match at least some member of the array, we use ANY : SELECT ... FROM tab WHERE tab.a = ANY(<array>) Working with the array members individualy \u00b6 For using the members of an array in the SELECT or JOIN , we have to first split the array using the unnest function. This function transforms the result set to a form where there is a separate row for each member of the array (a kind of inverse operation to group by). If we want to also keep the array index, we can use the WITH ORDINALITY expression, as shown in the manual or on SO . hstore \u00b6 A specific feature of PostgreSQL is the hstore column type. It enables to store structured data in a single column. It can be used to dump variables that we do not plan to utilize in the database (i.e., in the SELECT, JOIN statements) frequently. When we, exceptionally, want to access a variable from a hstore column, we can use the following syntax: SELECT <COLUMN NAME>-><VARIABLE NAME> AS ... Selecting rows for deletion based on data from another table \u00b6 If we want to delete rows from a table based on some condition on data from another table, we can use the DELETE statement with a USING clause. Example: DELETE FROM nodes_ways_speeds USING nodes_ways WHERE nodes_ways_speeds.to_node_ways_id = nodes_ways.id AND nodes_ways.area IN (5,6) Handeling duplicates in the INSERT statement \u00b6 To handle duplicates on INSERT , PostgreSQL provides the ON CONFLICT clause (see the INSERT documentation). The options are: - DO NOTHING : do nothing - DO UPDATE SET <column name> = <value> : update the column to the given value Random oredering \u00b6 To order the result set randomly, we can use the RANDOM() function in the ORDER BY clause: SELECT ... FROM ... ORDER BY RANDOM() Random ordering with a seed (Pseudo-random ordering) \u00b6 To receive a determinisic (repeatable) random ordering, we can use the setseed function: SELECT setseed(0.5); SELECT ... FROM ... ORDER BY RANDOM(); Note that we need two queries, one for setting the seed and one for the actual query. If we does not have an option to call arbitrary queries, we have to use UNION : SELECT col_1, ..., col_n FROM ( SELECT _, null AS col_1, ..., null AS col_n FROM setseed(0.5) UNION ALL SELECT null AS _, col_1, ..., col_n FROM ... OFFSET 1 ) ORDER BY RANDOM() The OFFSET 1 is used to skip the first row, which is the result of the setseed function. The union is the only way to guarantee that the seed will be set before the actual query. Other options, such as WITH or SELECT ... FROM (SELECT setseed(0.5)) do not guarantee the order of execution, and produce a different result for each call. Procedures and functions \u00b6 Calling a procedure \u00b6 to exectue a stored procedure, use\" CALL <procedure name>(<procedure arguments>) Unlike in programing languages, there is no implicit type cast of the program arguments, including literals. Therefore, we need to cast all parameters explicitely, as in the following example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments(target_area_id smallint) ... CALL compute_speeds_for_segments(1::smallint); Creating a procedure \u00b6 documentation The syntax is as follows: CREATE PROCEDURE <name> (<params>) LANGUAGE <language name> <procedure body> , while <procedure body> can be both a delimited string: AS $$ <sql statements> $$ OR an active SQL body (since PostgreSQL 14): BEGIN ATOMIC <sql statements> END There are some differences between those syntaxes (e.g., the second one works only for SQL and is evaluated/checked for validity at the time of creation), bot in most cases, they are interchangable. For more details, check the manual . Variables \u00b6 In PostgreSQL, all variables must be declared before assignmant in the DECLARE block which is before the sql body of the function/procedure/ DO . The syntax is: <variable name> <variable type>[ = <varianle value>]; Example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments() LANGUAGE plpgsql AS $$ DECLARE dataset_quality smallint = 1; BEGIN ... The, the value of a variable can be change using the classical assignment syntax: <variable name> = <varianle value>; Be carful to not use a variable name equal to the name of some of the columns used in the same context, which results in a name clash. Assigning a value to a variable using SQL \u00b6 There are two options how to assign a value to a variable using SQL: - using the INTO clause in the SELECT statement - using a SELECT statement as rvlaue of the assignment Example with INTO : SELECT name INTO customer_name FROM customers WHERE id = 1 EXAMPLE with SELECT as rvalue: customer_name = (SELECT name FROM customers WHERE id = 1) Functions \u00b6 Functions in PostgreSQL have a similar syntax to procedures. Unlike for procedures, we need to specify a return type for function, either as an OUT / INOUT parameter, or using the RETURNS clause. To return tabular data, we use TABLE return type: RETURNS TABLE(<param 1 name> <param 1 type>, ..., <param n name> <param n type>) To select the result of a function with the return type above, call: SELECT * FROM <function signature> RETURN NEXT and RETURN QUERY \u00b6 Sometimes, we need to do some cleanup after selecting the rows to be returned from function, or we need to build the result in a loop. In classical programming languages, we use variables for this purpose. In PG/plSQL, we can also use the RETURN NEXT and RETURN QUERY constructs. These constructs prepare the result, and does not return from the function . Instead, use an empty RETURN to return from the function. Example: RETURN QUERY SELECT ...; DROP TABLE target_ways; RETURN; Note that for these constructs, the return type needs to be a table or setof type. The RETURN QUERY cannot be used for returning a single value even if the query returns a single value. If we have a single value return type and need to do some postprocessing between selecting the value and returning from the function, we have to use a variable instead. Deciding the language \u00b6 For simple statements, we can use the SQL language. We need the PL/pgSQL if: - we need to use variables or control statements specific to PL/pgSQL - we need to use temporary tables, as the SQL language fails to recognize them if they are created inside the function/procedure Conditional filters based on the value of a variable or input parameter \u00b6 To add some filter to the WHERE or ON clause of a query based on the value of a variable or input parameter, we can use the following technique: 1. set the variable or input parameter to NULL if we do not want to apply the filter 2. in the filter test for disjunction of NULL or the filter condition Example: SELECT ... FROM ... WHERE param IS NULL OR some_column = param Temporary tables in PostgreSQL \u00b6 To use a result set efficiently in a function or procedure, we often use temporary tables. Unlike in other relational database systems, in PostgreSQl, the lifetime of a temporary table is bound to a session. Therefoe, if we call a function that creates a tempoary table multiple times in a single session, we encounter an error, because the table already exists. To tackle this problem, we need to delete all temporary tables manually. Luckily, there is a special DISCARD command that can be used to dtop all temporary tables at once: DISCARD TEMPORARY; DO command \u00b6 The DO command can be used to execude an anonymus code block in any of the languages suported by PostgreSQL. It behaves like a function with no parameters and no return value. Syntax: DO [LANGUAGE <lang name>] <code> The default language is plpgsql . Example: DO $$ BEGIN RAISE NOTICE 'Hello world'; END $$ Window functions \u00b6 PostgreSQL supports an extended syntax for window functions . We can use it for example to retrieve the value of a column that has maximum value in another column, as demonstrated in an SO answer . PostGis \u00b6 Geometry columns \u00b6 Postgis features can be utilized with geometry and geography column types. To add a new geometry column: ADD COLUMN <COLUUMN NAME> geometry(<GEOMETRY TYPE>, <SRID>) Spatial Indexing \u00b6 Documentation Analogously to standard SQL column indicis, there are spatial indices in PostGIS. The only difference is that we need to add the USING GIST at the end of the CREATE INDEX statement: CREATE INDEX nodes_geom_idx ON nodes USING GIST (geom); Converting between geometry types \u00b6 There are dedicated functions whcich we can use to convert between geometry types: - ST_Multi : converts geometries to their multi-variant, e.g., LineString to MultiLineString . Compute area surrounding geometries \u00b6 If we are ok with a convex envelope of the geometries, we can simply use the St_ConvexHull functon. Howeever, if we need the exact shape, We have to use the St_ConcaveHull function which computes the concave hull of a geometry. The St_ConcaveHull function takes an additional parameter param_pctconvex which determines how concave is the result: 0 means strictly concave, while 1 means convex hull. Note that while lowere values leads to more acurate results, the computation is much slower. There is also another parameter param_allow_holes which determines whether holes in the object are permited (default false). Split area to polygons based on points \u00b6 The split of spece into polygons based on a set of points is called Voronoi diagram . In PostGIS, we have a [ ST_Voronoi_Polygons](https://postgis.net/docs/ST_VoronoiPolygons.html) for that. To obtain a set of polygons from a set of points, it is necessary to 1. Aggregate the rows ( ST_Collect ) 2. Compute the polygon geometry ( ST_Voronoi_Polygons ) 3. Disaggregate the geometry into individual polygons ( ST_Dump`) Also, there is an important aspect of how far the polygons will reach outside of the points. By default, it enlarge the area determined by the points by about 50%. If we need a larger area, we can use the extend_to parameter. If we need a smaller area, however, we need to compute the intersection with this smaller area afterwards manually. Full example: SELECT st_intersection( (st_dump( st_voronoipolygons(st_collect(<GEOMETRY COLUMN>)) )).geom, (<select clause for area of desired voronoi polygons>) ) AS geom FROM ... If we need to join the polygons to the original points, we need to do it manually (e.g. by JOIN ... ON ST_Within(<POINT COLUMN>, <VORONOI POLYGONS GEO DUMP>) ). Other Useful Functions \u00b6 [ ST_Within ] ST_Within (A, B) if A is completly inside B ST_Intersects ST_Intersects(g1, g2) if g1 and g2 have at least one point in common. ST_Transform : ST_Transform(g, srid) transforms geometry g to a projection defined by the srid and returns the result as a geometry. ST_Buffer : ST_Buffer(g, radius) computes a geometry that is an extension of g by radius to all directions St_Collect aggregates data into single geometry. It is usually apllied to a geometry column in an SQL selection. ST_Union ST_Equals ST_MakeLine : make line between two points ST_SetSRID : sets the SRID of the geometry and returns the result as a new geometry. ST_MakePoint : creates a point geometry from the given coordinates. ST_Area : computes the area of a geometry. The units of the result are the same as the units of the SRID of the geometry (use UTM coordinate system for getting the area in square meters). PgRouting \u00b6 PgRouting is a PostgreSQL extension focused on graph/network manpulation. It contains functions for: - finding the strongly connected components: `pgr_strongComponents - graph contraction/simplification Finding strongly connected components \u00b6 The function pgr_strongComponents finds the strongly connected components of a graph. The only parameter of the script is a query that should return edge data in the folowing format: - id , - source , - target , - cost , - reverse_cost . The first three parameters are obvious. The cost parameter does not have any effect. You should provide a negative reverse_cost , othervise, the edge will be considered as bidirectional! PL/pgSQL \u00b6 PL/pgSQL is a procedural language available in PostgreSQL databases. It can be used inside: - functions - procedures - DO command Branching \u00b6 PL/pgSQL has the following branching: IF <condition> THEN ... [ELSEIF ... ] [ELSE ... ] END IF Logging \u00b6 Basic logging can be done using the RAISE command: RAISE NOTICE 'Some message'; We can add parameters by using the % placeholder: RAISE NOTICE 'Some message %', <variable or SQL command>; For more, see the documentation . Query diagnostics \u00b6 Various information about the last query can be obtained using the GET DIAGNOSTIC command. For example, the number of rows affected by the last query can be obtained using the ROW_COUNT parameter: GET DIAGNOSTIC <variable> = ROW_COUNT; The result is stored in the variable <variable> . Note that this constract is not available in the SQL queries but only in a PL/pgSQL block. For other diagnostic fields, see the documentation . psql \u00b6 psql is a basic command line utitiltyfor manipulating postgres database. To connect: psql -d <db name> Then the psql commands can be executed. To execute command immediatelly, use the -c parameter: psql -d <db name> -c \"<command>\" Do not forget to quote the command. Importing data from csv \u00b6 The prefered mathod depends on the characte of the data: - data exactly match the table in the database: use psql COPY command - data do not match the table, but they are small: 1. load the data with pandas 2. process the data as needed 3. use pandas.to_sql to upload the data - data do not match the table and they are large: 1. preprocess the data with bach commands 2. use psql COPY to upload the data - data do not match the table and they are large and dirty : use the file_fdw module: 1. create a table for SQL mapping with tolerant column types (e.g., text for problematic columns) 2. select from the mapping to the real table psql COPY command \u00b6 The COPY command can be used to copy the input into a database table. A subset of database column can be selected, but that is not true for the input, i.e, all input columns have to be used. If a subset of input columns needs to be used, or some columns requires processing, you need to perform some preprocessing. COPY manual Importing data from a shapefile \u00b6 There are multiple options: - shp2psql : simple tool that creates sql from a shapefile - easy start, almost no configuration - always imports all data from shapefile, cannot be configured to skip columns - included in the Postgres instalation - ogr2ogr - needs to be installed, part of GDAL - QGIS - The db manger can be used to export data from QGIS - data can be viewed before import - only suitable for creating new table, not for appending to an existing one Importing data from GeoJSON \u00b6 For a single geometry stored in a GeoJSON file, the function ST_GeomFromGeoJSON can be used. - just copy the geometry part of the file - change the geometry type to match the type in db - don't forget to surround the object with curly brackets For the whole document, the ogr2ogr tool can be used. Lost Password to the Postgres Server \u00b6 The password for the db superuser is stored in db postgres . In order to log there and change it, the whole authentification has to be turned off, and then we can proceed with changing the password. Steps: 1. find the pg_hba.conf file - usually located in C:\\Program Files\\PostgreSQL\\13\\data 2. backup the file and replace all occurances of scram-sha-256 in the file with trust 3. restart the posgreSQL service - in the Windows service management, there should be a service for postgresql running 4. change the password for the superuser 1. psql -U postgres 2. ALTER USER postgres WITH password 'yourpassword'; (do not forget the semicolon at the end!) 5. restore the pg_hba.conf file from backup 6. restart the postgreSQL service again 7. test if the new password works DataGrip \u00b6 Import Formats \u00b6 DataGrip can handle imports only from separator baset files (csv, tsv). View Geometry \u00b6 In the reuslt window/tab, click on the gear wheel -> Show GeoView . However, the geoviewer has a fixed WGS84 projection, so you have to project the result to this projection first . Create a spatial Index \u00b6 There is currently no GUI tool for that in DataGrip. Just add a normal index and modify the auto generated statement by changing <column> to USING GIST(<column>) at the end of the statement. Filter out store procedures \u00b6 right-click on routines click open table sort by type Creating functions and procedures \u00b6 There is no UI available currently, use Navicat or console Duplicate table \u00b6 Drag the table in the database explorer and drop it to the location you want it to copy to. Navicat \u00b6 Cannot connect to db \u00b6 Symptoms: - cant connect to db server: Could not connect - after editing the connection and trying to save it (ok button): connection is being used Try: 1. close navicat 2. open navicat, edit connection 1. click test connection 1. click ok, and start the connection by double click PgAdmin \u00b6 The best way to install the PgAdmin is to use the EDB PostgreSQL installer and uncheck the database installation during the installation configuration. This way, we also install useful tools like psql Diagrams \u00b6 To create diagram from an existing database: right click on the database -> Generate ERD Kill a hanging query \u00b6 To kill a hanging query, we need to complete two steps: 1. identify the query PID 1. kill the query To identify the PID of the problematic query, we can use tool such as pg_activity : sudo -u postgres pg_activity -U postgres To kill the query, run: SELECT pg_cancel_backend(<PID>) Troubleshooting \u00b6 If the db tools are unresponsive on certain tasks/queries, check if the table needed for those queries is not locke by some problematic query. Select PostgreSQL version \u00b6 SELECT version() Select PostGIS version \u00b6 SELECT PostGIS_version() Tried to send an out-of-range integer as a 2-byte value \u00b6 This error is caused by a too large number of values in the insert statement. The maximum index is a 2-byte number (max value: 32767). The solution is to split the insert statement into smaller bulks.","title":"PostgreSQL Manual"},{"location":"Programming/PostgreSQL%20Manual/#data-types","text":"official documentation","title":"Data types"},{"location":"Programming/PostgreSQL%20Manual/#date","text":"official documentation date : for dates time for time timestmp for both date and time interval","title":"Date"},{"location":"Programming/PostgreSQL%20Manual/#select-a-part-of-datetimetimestamp","text":"If we want just a part of a date, time, or timestamp, we can use the extract function. Example: SELECT extract(hour FROM <date column name>) FROM... Other parts can be extracted too. To extract day of week , we can use isodow (assigns 1 to Monday and 7 to Sunday).","title":"Select a part of date/time/timestamp"},{"location":"Programming/PostgreSQL%20Manual/#auto-incrementing-columns","text":"In PostgreSQL, sequences are used for auto-incrementing columns. When you are creating a new db table or adding a new column, the process of creating a new sequence can be automated by choosing an identity or a serial column type. When updating an aexisting column, a manual intervention is required: 1. change the column to some numerical datatype 2. create the sequence: CREATE SEQUENCE <SEQUENCE NAME> OWNED BY <TABLE NAME>.<COLUMN NAME>; adjust the value of the sequence: SELECT setval(pg_get_serial_sequence('<TABLE NAME>', '<COLUMN NAME>'), max(<COLUMN NAME>)) FROM <TABLE NAME>; set the column to be incremented by the sequence: ALTER TABLE <TABLE NAME> ALTER COLUMN <COLUMN NAME> SET DEFAULT nextval('<SEQUENCE NAME>');","title":"Auto incrementing columns"},{"location":"Programming/PostgreSQL%20Manual/#strings","text":"There are many string function available, including the format function that works similarly to the C format function. For all functions, check the documentation .","title":"Strings"},{"location":"Programming/PostgreSQL%20Manual/#arrays","text":"arrays array functions and operators arrays are declared as <type>[] , e.g., integer[] . The type can be any type, including composite types. To compute array length , use array_length(contracted_vertices, 1) where 1 stands for the first dimension. To cretea an array literal , we use single quatation and curly brackets: '{1, 2, 3}' . To check that some value match at least some member of the array, we use ANY : SELECT ... FROM tab WHERE tab.a = ANY(<array>)","title":"Arrays"},{"location":"Programming/PostgreSQL%20Manual/#working-with-the-array-members-individualy","text":"For using the members of an array in the SELECT or JOIN , we have to first split the array using the unnest function. This function transforms the result set to a form where there is a separate row for each member of the array (a kind of inverse operation to group by). If we want to also keep the array index, we can use the WITH ORDINALITY expression, as shown in the manual or on SO .","title":"Working with the array members individualy"},{"location":"Programming/PostgreSQL%20Manual/#hstore","text":"A specific feature of PostgreSQL is the hstore column type. It enables to store structured data in a single column. It can be used to dump variables that we do not plan to utilize in the database (i.e., in the SELECT, JOIN statements) frequently. When we, exceptionally, want to access a variable from a hstore column, we can use the following syntax: SELECT <COLUMN NAME>-><VARIABLE NAME> AS ...","title":"hstore"},{"location":"Programming/PostgreSQL%20Manual/#selecting-rows-for-deletion-based-on-data-from-another-table","text":"If we want to delete rows from a table based on some condition on data from another table, we can use the DELETE statement with a USING clause. Example: DELETE FROM nodes_ways_speeds USING nodes_ways WHERE nodes_ways_speeds.to_node_ways_id = nodes_ways.id AND nodes_ways.area IN (5,6)","title":"Selecting rows for deletion based on data from another table"},{"location":"Programming/PostgreSQL%20Manual/#handeling-duplicates-in-the-insert-statement","text":"To handle duplicates on INSERT , PostgreSQL provides the ON CONFLICT clause (see the INSERT documentation). The options are: - DO NOTHING : do nothing - DO UPDATE SET <column name> = <value> : update the column to the given value","title":"Handeling duplicates in the INSERT statement"},{"location":"Programming/PostgreSQL%20Manual/#random-oredering","text":"To order the result set randomly, we can use the RANDOM() function in the ORDER BY clause: SELECT ... FROM ... ORDER BY RANDOM()","title":"Random oredering"},{"location":"Programming/PostgreSQL%20Manual/#random-ordering-with-a-seed-pseudo-random-ordering","text":"To receive a determinisic (repeatable) random ordering, we can use the setseed function: SELECT setseed(0.5); SELECT ... FROM ... ORDER BY RANDOM(); Note that we need two queries, one for setting the seed and one for the actual query. If we does not have an option to call arbitrary queries, we have to use UNION : SELECT col_1, ..., col_n FROM ( SELECT _, null AS col_1, ..., null AS col_n FROM setseed(0.5) UNION ALL SELECT null AS _, col_1, ..., col_n FROM ... OFFSET 1 ) ORDER BY RANDOM() The OFFSET 1 is used to skip the first row, which is the result of the setseed function. The union is the only way to guarantee that the seed will be set before the actual query. Other options, such as WITH or SELECT ... FROM (SELECT setseed(0.5)) do not guarantee the order of execution, and produce a different result for each call.","title":"Random ordering with a seed (Pseudo-random ordering)"},{"location":"Programming/PostgreSQL%20Manual/#procedures-and-functions","text":"","title":"Procedures and functions"},{"location":"Programming/PostgreSQL%20Manual/#calling-a-procedure","text":"to exectue a stored procedure, use\" CALL <procedure name>(<procedure arguments>) Unlike in programing languages, there is no implicit type cast of the program arguments, including literals. Therefore, we need to cast all parameters explicitely, as in the following example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments(target_area_id smallint) ... CALL compute_speeds_for_segments(1::smallint);","title":"Calling a procedure"},{"location":"Programming/PostgreSQL%20Manual/#creating-a-procedure","text":"documentation The syntax is as follows: CREATE PROCEDURE <name> (<params>) LANGUAGE <language name> <procedure body> , while <procedure body> can be both a delimited string: AS $$ <sql statements> $$ OR an active SQL body (since PostgreSQL 14): BEGIN ATOMIC <sql statements> END There are some differences between those syntaxes (e.g., the second one works only for SQL and is evaluated/checked for validity at the time of creation), bot in most cases, they are interchangable. For more details, check the manual .","title":"Creating a procedure"},{"location":"Programming/PostgreSQL%20Manual/#variables","text":"In PostgreSQL, all variables must be declared before assignmant in the DECLARE block which is before the sql body of the function/procedure/ DO . The syntax is: <variable name> <variable type>[ = <varianle value>]; Example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments() LANGUAGE plpgsql AS $$ DECLARE dataset_quality smallint = 1; BEGIN ... The, the value of a variable can be change using the classical assignment syntax: <variable name> = <varianle value>; Be carful to not use a variable name equal to the name of some of the columns used in the same context, which results in a name clash.","title":"Variables"},{"location":"Programming/PostgreSQL%20Manual/#assigning-a-value-to-a-variable-using-sql","text":"There are two options how to assign a value to a variable using SQL: - using the INTO clause in the SELECT statement - using a SELECT statement as rvlaue of the assignment Example with INTO : SELECT name INTO customer_name FROM customers WHERE id = 1 EXAMPLE with SELECT as rvalue: customer_name = (SELECT name FROM customers WHERE id = 1)","title":"Assigning a value to a variable using SQL"},{"location":"Programming/PostgreSQL%20Manual/#functions","text":"Functions in PostgreSQL have a similar syntax to procedures. Unlike for procedures, we need to specify a return type for function, either as an OUT / INOUT parameter, or using the RETURNS clause. To return tabular data, we use TABLE return type: RETURNS TABLE(<param 1 name> <param 1 type>, ..., <param n name> <param n type>) To select the result of a function with the return type above, call: SELECT * FROM <function signature>","title":"Functions"},{"location":"Programming/PostgreSQL%20Manual/#return-next-and-return-query","text":"Sometimes, we need to do some cleanup after selecting the rows to be returned from function, or we need to build the result in a loop. In classical programming languages, we use variables for this purpose. In PG/plSQL, we can also use the RETURN NEXT and RETURN QUERY constructs. These constructs prepare the result, and does not return from the function . Instead, use an empty RETURN to return from the function. Example: RETURN QUERY SELECT ...; DROP TABLE target_ways; RETURN; Note that for these constructs, the return type needs to be a table or setof type. The RETURN QUERY cannot be used for returning a single value even if the query returns a single value. If we have a single value return type and need to do some postprocessing between selecting the value and returning from the function, we have to use a variable instead.","title":"RETURN NEXT and RETURN QUERY"},{"location":"Programming/PostgreSQL%20Manual/#deciding-the-language","text":"For simple statements, we can use the SQL language. We need the PL/pgSQL if: - we need to use variables or control statements specific to PL/pgSQL - we need to use temporary tables, as the SQL language fails to recognize them if they are created inside the function/procedure","title":"Deciding the language"},{"location":"Programming/PostgreSQL%20Manual/#conditional-filters-based-on-the-value-of-a-variable-or-input-parameter","text":"To add some filter to the WHERE or ON clause of a query based on the value of a variable or input parameter, we can use the following technique: 1. set the variable or input parameter to NULL if we do not want to apply the filter 2. in the filter test for disjunction of NULL or the filter condition Example: SELECT ... FROM ... WHERE param IS NULL OR some_column = param","title":"Conditional filters based on the value of a variable or input parameter"},{"location":"Programming/PostgreSQL%20Manual/#temporary-tables-in-postgresql","text":"To use a result set efficiently in a function or procedure, we often use temporary tables. Unlike in other relational database systems, in PostgreSQl, the lifetime of a temporary table is bound to a session. Therefoe, if we call a function that creates a tempoary table multiple times in a single session, we encounter an error, because the table already exists. To tackle this problem, we need to delete all temporary tables manually. Luckily, there is a special DISCARD command that can be used to dtop all temporary tables at once: DISCARD TEMPORARY;","title":"Temporary tables in PostgreSQL"},{"location":"Programming/PostgreSQL%20Manual/#do-command","text":"The DO command can be used to execude an anonymus code block in any of the languages suported by PostgreSQL. It behaves like a function with no parameters and no return value. Syntax: DO [LANGUAGE <lang name>] <code> The default language is plpgsql . Example: DO $$ BEGIN RAISE NOTICE 'Hello world'; END $$","title":"DO command"},{"location":"Programming/PostgreSQL%20Manual/#window-functions","text":"PostgreSQL supports an extended syntax for window functions . We can use it for example to retrieve the value of a column that has maximum value in another column, as demonstrated in an SO answer .","title":"Window functions"},{"location":"Programming/PostgreSQL%20Manual/#postgis","text":"","title":"PostGis"},{"location":"Programming/PostgreSQL%20Manual/#geometry-columns","text":"Postgis features can be utilized with geometry and geography column types. To add a new geometry column: ADD COLUMN <COLUUMN NAME> geometry(<GEOMETRY TYPE>, <SRID>)","title":"Geometry columns"},{"location":"Programming/PostgreSQL%20Manual/#spatial-indexing","text":"Documentation Analogously to standard SQL column indicis, there are spatial indices in PostGIS. The only difference is that we need to add the USING GIST at the end of the CREATE INDEX statement: CREATE INDEX nodes_geom_idx ON nodes USING GIST (geom);","title":"Spatial Indexing"},{"location":"Programming/PostgreSQL%20Manual/#converting-between-geometry-types","text":"There are dedicated functions whcich we can use to convert between geometry types: - ST_Multi : converts geometries to their multi-variant, e.g., LineString to MultiLineString .","title":"Converting between geometry types"},{"location":"Programming/PostgreSQL%20Manual/#compute-area-surrounding-geometries","text":"If we are ok with a convex envelope of the geometries, we can simply use the St_ConvexHull functon. Howeever, if we need the exact shape, We have to use the St_ConcaveHull function which computes the concave hull of a geometry. The St_ConcaveHull function takes an additional parameter param_pctconvex which determines how concave is the result: 0 means strictly concave, while 1 means convex hull. Note that while lowere values leads to more acurate results, the computation is much slower. There is also another parameter param_allow_holes which determines whether holes in the object are permited (default false).","title":"Compute area surrounding geometries"},{"location":"Programming/PostgreSQL%20Manual/#split-area-to-polygons-based-on-points","text":"The split of spece into polygons based on a set of points is called Voronoi diagram . In PostGIS, we have a [ ST_Voronoi_Polygons](https://postgis.net/docs/ST_VoronoiPolygons.html) for that. To obtain a set of polygons from a set of points, it is necessary to 1. Aggregate the rows ( ST_Collect ) 2. Compute the polygon geometry ( ST_Voronoi_Polygons ) 3. Disaggregate the geometry into individual polygons ( ST_Dump`) Also, there is an important aspect of how far the polygons will reach outside of the points. By default, it enlarge the area determined by the points by about 50%. If we need a larger area, we can use the extend_to parameter. If we need a smaller area, however, we need to compute the intersection with this smaller area afterwards manually. Full example: SELECT st_intersection( (st_dump( st_voronoipolygons(st_collect(<GEOMETRY COLUMN>)) )).geom, (<select clause for area of desired voronoi polygons>) ) AS geom FROM ... If we need to join the polygons to the original points, we need to do it manually (e.g. by JOIN ... ON ST_Within(<POINT COLUMN>, <VORONOI POLYGONS GEO DUMP>) ).","title":"Split area to polygons based on points"},{"location":"Programming/PostgreSQL%20Manual/#other-useful-functions","text":"[ ST_Within ] ST_Within (A, B) if A is completly inside B ST_Intersects ST_Intersects(g1, g2) if g1 and g2 have at least one point in common. ST_Transform : ST_Transform(g, srid) transforms geometry g to a projection defined by the srid and returns the result as a geometry. ST_Buffer : ST_Buffer(g, radius) computes a geometry that is an extension of g by radius to all directions St_Collect aggregates data into single geometry. It is usually apllied to a geometry column in an SQL selection. ST_Union ST_Equals ST_MakeLine : make line between two points ST_SetSRID : sets the SRID of the geometry and returns the result as a new geometry. ST_MakePoint : creates a point geometry from the given coordinates. ST_Area : computes the area of a geometry. The units of the result are the same as the units of the SRID of the geometry (use UTM coordinate system for getting the area in square meters).","title":"Other Useful Functions"},{"location":"Programming/PostgreSQL%20Manual/#pgrouting","text":"PgRouting is a PostgreSQL extension focused on graph/network manpulation. It contains functions for: - finding the strongly connected components: `pgr_strongComponents - graph contraction/simplification","title":"PgRouting"},{"location":"Programming/PostgreSQL%20Manual/#finding-strongly-connected-components","text":"The function pgr_strongComponents finds the strongly connected components of a graph. The only parameter of the script is a query that should return edge data in the folowing format: - id , - source , - target , - cost , - reverse_cost . The first three parameters are obvious. The cost parameter does not have any effect. You should provide a negative reverse_cost , othervise, the edge will be considered as bidirectional!","title":"Finding strongly connected components"},{"location":"Programming/PostgreSQL%20Manual/#plpgsql","text":"PL/pgSQL is a procedural language available in PostgreSQL databases. It can be used inside: - functions - procedures - DO command","title":"PL/pgSQL"},{"location":"Programming/PostgreSQL%20Manual/#branching","text":"PL/pgSQL has the following branching: IF <condition> THEN ... [ELSEIF ... ] [ELSE ... ] END IF","title":"Branching"},{"location":"Programming/PostgreSQL%20Manual/#logging","text":"Basic logging can be done using the RAISE command: RAISE NOTICE 'Some message'; We can add parameters by using the % placeholder: RAISE NOTICE 'Some message %', <variable or SQL command>; For more, see the documentation .","title":"Logging"},{"location":"Programming/PostgreSQL%20Manual/#query-diagnostics","text":"Various information about the last query can be obtained using the GET DIAGNOSTIC command. For example, the number of rows affected by the last query can be obtained using the ROW_COUNT parameter: GET DIAGNOSTIC <variable> = ROW_COUNT; The result is stored in the variable <variable> . Note that this constract is not available in the SQL queries but only in a PL/pgSQL block. For other diagnostic fields, see the documentation .","title":"Query diagnostics"},{"location":"Programming/PostgreSQL%20Manual/#psql","text":"psql is a basic command line utitiltyfor manipulating postgres database. To connect: psql -d <db name> Then the psql commands can be executed. To execute command immediatelly, use the -c parameter: psql -d <db name> -c \"<command>\" Do not forget to quote the command.","title":"psql"},{"location":"Programming/PostgreSQL%20Manual/#importing-data-from-csv","text":"The prefered mathod depends on the characte of the data: - data exactly match the table in the database: use psql COPY command - data do not match the table, but they are small: 1. load the data with pandas 2. process the data as needed 3. use pandas.to_sql to upload the data - data do not match the table and they are large: 1. preprocess the data with bach commands 2. use psql COPY to upload the data - data do not match the table and they are large and dirty : use the file_fdw module: 1. create a table for SQL mapping with tolerant column types (e.g., text for problematic columns) 2. select from the mapping to the real table","title":"Importing data from csv"},{"location":"Programming/PostgreSQL%20Manual/#psql-copy-command","text":"The COPY command can be used to copy the input into a database table. A subset of database column can be selected, but that is not true for the input, i.e, all input columns have to be used. If a subset of input columns needs to be used, or some columns requires processing, you need to perform some preprocessing. COPY manual","title":"psql COPY command"},{"location":"Programming/PostgreSQL%20Manual/#importing-data-from-a-shapefile","text":"There are multiple options: - shp2psql : simple tool that creates sql from a shapefile - easy start, almost no configuration - always imports all data from shapefile, cannot be configured to skip columns - included in the Postgres instalation - ogr2ogr - needs to be installed, part of GDAL - QGIS - The db manger can be used to export data from QGIS - data can be viewed before import - only suitable for creating new table, not for appending to an existing one","title":"Importing data from a shapefile"},{"location":"Programming/PostgreSQL%20Manual/#importing-data-from-geojson","text":"For a single geometry stored in a GeoJSON file, the function ST_GeomFromGeoJSON can be used. - just copy the geometry part of the file - change the geometry type to match the type in db - don't forget to surround the object with curly brackets For the whole document, the ogr2ogr tool can be used.","title":"Importing data from GeoJSON"},{"location":"Programming/PostgreSQL%20Manual/#lost-password-to-the-postgres-server","text":"The password for the db superuser is stored in db postgres . In order to log there and change it, the whole authentification has to be turned off, and then we can proceed with changing the password. Steps: 1. find the pg_hba.conf file - usually located in C:\\Program Files\\PostgreSQL\\13\\data 2. backup the file and replace all occurances of scram-sha-256 in the file with trust 3. restart the posgreSQL service - in the Windows service management, there should be a service for postgresql running 4. change the password for the superuser 1. psql -U postgres 2. ALTER USER postgres WITH password 'yourpassword'; (do not forget the semicolon at the end!) 5. restore the pg_hba.conf file from backup 6. restart the postgreSQL service again 7. test if the new password works","title":"Lost Password to the Postgres Server"},{"location":"Programming/PostgreSQL%20Manual/#datagrip","text":"","title":"DataGrip"},{"location":"Programming/PostgreSQL%20Manual/#import-formats","text":"DataGrip can handle imports only from separator baset files (csv, tsv).","title":"Import Formats"},{"location":"Programming/PostgreSQL%20Manual/#view-geometry","text":"In the reuslt window/tab, click on the gear wheel -> Show GeoView . However, the geoviewer has a fixed WGS84 projection, so you have to project the result to this projection first .","title":"View Geometry"},{"location":"Programming/PostgreSQL%20Manual/#create-a-spatial-index","text":"There is currently no GUI tool for that in DataGrip. Just add a normal index and modify the auto generated statement by changing <column> to USING GIST(<column>) at the end of the statement.","title":"Create a spatial Index"},{"location":"Programming/PostgreSQL%20Manual/#filter-out-store-procedures","text":"right-click on routines click open table sort by type","title":"Filter out store procedures"},{"location":"Programming/PostgreSQL%20Manual/#creating-functions-and-procedures","text":"There is no UI available currently, use Navicat or console","title":"Creating functions and procedures"},{"location":"Programming/PostgreSQL%20Manual/#duplicate-table","text":"Drag the table in the database explorer and drop it to the location you want it to copy to.","title":"Duplicate table"},{"location":"Programming/PostgreSQL%20Manual/#navicat","text":"","title":"Navicat"},{"location":"Programming/PostgreSQL%20Manual/#cannot-connect-to-db","text":"Symptoms: - cant connect to db server: Could not connect - after editing the connection and trying to save it (ok button): connection is being used Try: 1. close navicat 2. open navicat, edit connection 1. click test connection 1. click ok, and start the connection by double click","title":"Cannot connect to db"},{"location":"Programming/PostgreSQL%20Manual/#pgadmin","text":"The best way to install the PgAdmin is to use the EDB PostgreSQL installer and uncheck the database installation during the installation configuration. This way, we also install useful tools like psql","title":"PgAdmin"},{"location":"Programming/PostgreSQL%20Manual/#diagrams","text":"To create diagram from an existing database: right click on the database -> Generate ERD","title":"Diagrams"},{"location":"Programming/PostgreSQL%20Manual/#kill-a-hanging-query","text":"To kill a hanging query, we need to complete two steps: 1. identify the query PID 1. kill the query To identify the PID of the problematic query, we can use tool such as pg_activity : sudo -u postgres pg_activity -U postgres To kill the query, run: SELECT pg_cancel_backend(<PID>)","title":"Kill a hanging query"},{"location":"Programming/PostgreSQL%20Manual/#troubleshooting","text":"If the db tools are unresponsive on certain tasks/queries, check if the table needed for those queries is not locke by some problematic query.","title":"Troubleshooting"},{"location":"Programming/PostgreSQL%20Manual/#select-postgresql-version","text":"SELECT version()","title":"Select PostgreSQL version"},{"location":"Programming/PostgreSQL%20Manual/#select-postgis-version","text":"SELECT PostGIS_version()","title":"Select PostGIS version"},{"location":"Programming/PostgreSQL%20Manual/#tried-to-send-an-out-of-range-integer-as-a-2-byte-value","text":"This error is caused by a too large number of values in the insert statement. The maximum index is a 2-byte number (max value: 32767). The solution is to split the insert statement into smaller bulks.","title":"Tried to send an out-of-range integer as a 2-byte value"},{"location":"Programming/Regex/","text":"Symbol meaning \u00b6 . any character [xyz] one of these characters [c-j] any character between c and j . We can combine this and the previou syntax, e.g.: [az0-4jsd] . Note that the minus sign is interpreted as a range only if it is between two characters. [^c-g] ^ means negation: anything except the following set of characters. Note that the negation( ^ ) sign needs to be the first character in the bracket. \\ : escape character. | means OR. It has the lowest precedence, so it is evaluated last. ? lazy quantifier. It will try to match as few characters as possible (i.e., previous pattern will try to match only till the next patern matches). ?R recursive pattern. Quantifiers \u00b6 * : zero or more + : one or more ? zero or one {<from>, <to>} between from and to times. If to is omitted, it means infinity. If from is omitted, it means zero. If there is only one number, it means exact count. If both are omitted, it means one. Anchors \u00b6 ^x must start with x x$ must end with x Groups and Lookarounds \u00b6 () capture group. We can refer to it later, either in the regex, or in the result of the match, depending on the programming language. The nubering starts from 1, the 0 group is usually the whole match. in the regex we refer to group using \\1 , \\2 , etc. (?:) non-caputing group. It is useful when we want to use the quantifiers on a group, but we don't want to capture it. (?=) positive lookahead. It will try to match the pattern, but it will not consume it. (?!) negative lookahead. It is useful when we want to match a pattern, but we don't want to consume it. (?<=) positive lookbehind. Same as positive lookahead, but it looks behind. (?<!) negative lookbehind. Same as negative lookahead, but it looks behind. Principles \u00b6 Non-capturing groups \u00b6 Non-capturing groups are groups that helps to specify the match but they are not captured. They are useful when we want to use the group content to specify the match, but we don't want to capture/consume the group. Some of them can be replaced, but usually with a more complicated regex. All of the non-capturing groups start with (? and end with ) . The ? is followed by a character that specifies the type of the group. The most common are: - ?: non-capturing group - ?= positive lookahead - ?! negative lookahead - ?<= positive lookbehind - ?<! negative lookbehind The actual content of the group is specified between the group type specifier (e.g., ?= ) and the closing bracket ( ) ). Example: (?=d)a This regex will match a only if it is followed by d . The d will not be consumed. Note that some regex engines don't support variable length lookbehind . To overcome this, we can use the following tricks: - use multiple lookbehinds with fixed length - construct a more complicated regex that will match the same thing - place a marker with one regex replace and then use the lookbehind to match the marker Examples \u00b6 Any whitespace \u00b6 /[\\x{00a0}\\s]/u Non-breaking space \u00b6 ((?!&nbsp;)[^\\s\\x{00a0}]) Transform Google sheet to latex table \u00b6 naj\u00edt ([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n nahradit \\1 & \\2 & \\3 & \\4 \\\\\\\\\\r\\n CSV to Latex \u00b6 Search: ([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,\\r\\n]*)\\n Replace \\1 & \\2 & \\3 & \\4 & \\5 & \\6 & \\7 \\\\\\\\\\r\\n Name regex \u00b6 ([A\u00c1BC\u010cDEFGHIJKLMNOPQR\u0158S\u0160TU\u00daVWXYZ\u017d]{1}[a\u00e1bc\u010dd\u010fe\u00e9\u011bfghchi\u00edjklmn\u0148o\u00f3pqr\u0159s\u0161t\u0165u\u00fa\u016fvwxy\u00fdz\u017ew]+ *){2,3} Archive \u00b6 vasdomovnik \u00b6 naj\u00edt [0-9]+[ ]*([^\\r\\n]*)[\\r\\n]+ nahradit '\\1' , sloupce na pole php \u00b6 naj\u00edt ([^\\r\\n]*)([\\r\\n])+ nahradit '\\1' => ''\\2, Pogamut - jen logy od jednoho bota \u00b6 \\(TeamCTF[^2][^\\r\\n]*\\r\\n nahradit za pr\u00e1zdno","title":"Regex"},{"location":"Programming/Regex/#symbol-meaning","text":". any character [xyz] one of these characters [c-j] any character between c and j . We can combine this and the previou syntax, e.g.: [az0-4jsd] . Note that the minus sign is interpreted as a range only if it is between two characters. [^c-g] ^ means negation: anything except the following set of characters. Note that the negation( ^ ) sign needs to be the first character in the bracket. \\ : escape character. | means OR. It has the lowest precedence, so it is evaluated last. ? lazy quantifier. It will try to match as few characters as possible (i.e., previous pattern will try to match only till the next patern matches). ?R recursive pattern.","title":"Symbol meaning"},{"location":"Programming/Regex/#quantifiers","text":"* : zero or more + : one or more ? zero or one {<from>, <to>} between from and to times. If to is omitted, it means infinity. If from is omitted, it means zero. If there is only one number, it means exact count. If both are omitted, it means one.","title":"Quantifiers"},{"location":"Programming/Regex/#anchors","text":"^x must start with x x$ must end with x","title":"Anchors"},{"location":"Programming/Regex/#groups-and-lookarounds","text":"() capture group. We can refer to it later, either in the regex, or in the result of the match, depending on the programming language. The nubering starts from 1, the 0 group is usually the whole match. in the regex we refer to group using \\1 , \\2 , etc. (?:) non-caputing group. It is useful when we want to use the quantifiers on a group, but we don't want to capture it. (?=) positive lookahead. It will try to match the pattern, but it will not consume it. (?!) negative lookahead. It is useful when we want to match a pattern, but we don't want to consume it. (?<=) positive lookbehind. Same as positive lookahead, but it looks behind. (?<!) negative lookbehind. Same as negative lookahead, but it looks behind.","title":"Groups and Lookarounds"},{"location":"Programming/Regex/#principles","text":"","title":"Principles"},{"location":"Programming/Regex/#non-capturing-groups","text":"Non-capturing groups are groups that helps to specify the match but they are not captured. They are useful when we want to use the group content to specify the match, but we don't want to capture/consume the group. Some of them can be replaced, but usually with a more complicated regex. All of the non-capturing groups start with (? and end with ) . The ? is followed by a character that specifies the type of the group. The most common are: - ?: non-capturing group - ?= positive lookahead - ?! negative lookahead - ?<= positive lookbehind - ?<! negative lookbehind The actual content of the group is specified between the group type specifier (e.g., ?= ) and the closing bracket ( ) ). Example: (?=d)a This regex will match a only if it is followed by d . The d will not be consumed. Note that some regex engines don't support variable length lookbehind . To overcome this, we can use the following tricks: - use multiple lookbehinds with fixed length - construct a more complicated regex that will match the same thing - place a marker with one regex replace and then use the lookbehind to match the marker","title":"Non-capturing groups"},{"location":"Programming/Regex/#examples","text":"","title":"Examples"},{"location":"Programming/Regex/#any-whitespace","text":"/[\\x{00a0}\\s]/u","title":"Any whitespace"},{"location":"Programming/Regex/#non-breaking-space","text":"((?!&nbsp;)[^\\s\\x{00a0}])","title":"Non-breaking space"},{"location":"Programming/Regex/#transform-google-sheet-to-latex-table","text":"naj\u00edt ([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n nahradit \\1 & \\2 & \\3 & \\4 \\\\\\\\\\r\\n","title":"Transform Google sheet to latex table"},{"location":"Programming/Regex/#csv-to-latex","text":"Search: ([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,\\r\\n]*)\\n Replace \\1 & \\2 & \\3 & \\4 & \\5 & \\6 & \\7 \\\\\\\\\\r\\n","title":"CSV to Latex"},{"location":"Programming/Regex/#name-regex","text":"([A\u00c1BC\u010cDEFGHIJKLMNOPQR\u0158S\u0160TU\u00daVWXYZ\u017d]{1}[a\u00e1bc\u010dd\u010fe\u00e9\u011bfghchi\u00edjklmn\u0148o\u00f3pqr\u0159s\u0161t\u0165u\u00fa\u016fvwxy\u00fdz\u017ew]+ *){2,3}","title":"Name regex"},{"location":"Programming/Regex/#archive","text":"","title":"Archive"},{"location":"Programming/Regex/#vasdomovnik","text":"naj\u00edt [0-9]+[ ]*([^\\r\\n]*)[\\r\\n]+ nahradit '\\1' ,","title":"vasdomovnik"},{"location":"Programming/Regex/#sloupce-na-pole-php","text":"naj\u00edt ([^\\r\\n]*)([\\r\\n])+ nahradit '\\1' => ''\\2,","title":"sloupce na pole php"},{"location":"Programming/Regex/#pogamut-jen-logy-od-jednoho-bota","text":"\\(TeamCTF[^2][^\\r\\n]*\\r\\n nahradit za pr\u00e1zdno","title":"Pogamut - jen logy od jednoho bota"},{"location":"Programming/Ruby%20Workflow/","text":"Classical workflow is to use: - official Ruby distribution - Bundler to manage dependencies Installation \u00b6 Windows \u00b6 Download and run Ruby installer use the versin with DevKit at the end, a command prompt will open, confitm the prompt with Enter Project setup \u00b6 The project configuration is stored in the Gemfile file. It contains a list of dependencies, which are installed using the bundle command. Typically, the file contains the following lines: source \"https://rubygems.org\" # the source of the gems gem \"jekyll\" # the gem to install Gems \u00b6 Gems are packages for Ruby. They can be installed using the gem command, but moslty, they are installed as dependencies using the bundle command. The gem specification in the Gemfile contains the following parameters (split by spaces): - gem : the name of the gem (required) - version : the version of the gem - group : the group of the gem. It can be used run a command only for a specific group. For example, bundle install --without development will not install gems from the development group. The group parameter is a new syntax, the old syntax is to use the group command: group :development do gem \"jekyll\" end","title":"Ruby Workflow"},{"location":"Programming/Ruby%20Workflow/#installation","text":"","title":"Installation"},{"location":"Programming/Ruby%20Workflow/#windows","text":"Download and run Ruby installer use the versin with DevKit at the end, a command prompt will open, confitm the prompt with Enter","title":"Windows"},{"location":"Programming/Ruby%20Workflow/#project-setup","text":"The project configuration is stored in the Gemfile file. It contains a list of dependencies, which are installed using the bundle command. Typically, the file contains the following lines: source \"https://rubygems.org\" # the source of the gems gem \"jekyll\" # the gem to install","title":"Project setup"},{"location":"Programming/Ruby%20Workflow/#gems","text":"Gems are packages for Ruby. They can be installed using the gem command, but moslty, they are installed as dependencies using the bundle command. The gem specification in the Gemfile contains the following parameters (split by spaces): - gem : the name of the gem (required) - version : the version of the gem - group : the group of the gem. It can be used run a command only for a specific group. For example, bundle install --without development will not install gems from the development group. The group parameter is a new syntax, the old syntax is to use the group command: group :development do gem \"jekyll\" end","title":"Gems"},{"location":"Programming/SQL%20Manual/","text":"Literals \u00b6 In SQL, there are three basic types of literals: - numeric : 1 , 1.2 , 1.2e3 - string : 'string' , \"string\" - boolean : true , false When we need a constant value for other types, we usually use either a constructor function or a specifically formatted string literal. String literals can also span multiple lines , we do not need any operator to split the line (unlike in Python and despite the JetBrains IDEs adds a string concatenation operator automatically on newline). WITH \u00b6 Statement for defining variables pointing to temporary data, that can be used in the related SELECT statement. Usage: WITH <var> AS (<sql that assigns data to var>) <sql that use the variable> Note that the variable has to appear in the FROM clause ! Multiple variables in the WITH statement ahould be delimited by a comma: WITH <var> AS (<sql that assigns data to var>), <var 2> AS (<sql that assigns data to var 2>) <sql that use the variables> SELECT \u00b6 Most common SQL statement, syntax: SELECT <EXPRESSION> [AS <ALIAS>][, <EXPRESSION 2> [AS <ALIAS 2> ...]] The most common expression is just a column name. Selecting row number \u00b6 We can select row numbers using the function ROW_SELECT() in the SELECT statement: SELECT ... ROW_NUMBER() OVER([<PARTITIONING AND NUMBERING ORDER>]) AS <RESULT COLUMN NAME>, ... Inside the OVER statement, we can specify the order of the row numbering. Note however, that this does not order the result, for that, we use the ORDER BY statement. If we want the rown numbering to correspond with the row order in the result, we can left the OVER statement empty. Count selecting rows \u00b6 The count() function can be used to count the selection. The standard syntax is: SELECT count(1) FROM ... Count distinct \u00b6 To count distinct values in a selection we can use: SELECT count(DISTINCT <column name>) FROM... Select from another column if the specified column is NULL \u00b6 We can use a replacement column using the coalesce function: SELECT coalesce (<primary column>, <secondary column>) The secondary column value will be used if the primary column value in the row is NULL . UNION \u00b6 UNION and UNION ALL are important keywords that enables to merge query results verticaly, i.e., appending rows of one query to the results set of another. The difference between them is that UNION discards duplicate rows, while UNION ALL keeps them The UNUION statement appends one SELECT statement to another, but some statements that appears to be part of the SELECT needs to stay outside (i.e., be specified just once for the whole union), namely ORDER BY , and LIMIT . In contrast, the GROUP BY and HAVING statement stays inside each individual select. JOIN \u00b6 Classical syntax: JOIN <table name> [[AS] <ALIAS>] ON <CONDITION> The alias is obligatory if there are duplicate names (e.g., we are joining one table twice) Types \u00b6 INNER (default): when there is nothing to join, the row is discarded [LEFT/RIGHT/FULL] OUTER : when there is nothing to join, the missing values are set to null CROSS : creates cartesian product between tables OUTER JOIN \u00b6 The OUTER JOIN has three subtypes - LEFT : joins right table to left, fills the missing rows on right with null - RIGHT : joins left to right, fills the missing rows on left with null - FULL : both LEFT and RIGHT JOIN is performend Properties \u00b6 When there are multiple matching rows, all of them are matched (i.e., it creates duplicit rows) If you want to filter the tables before join, you need to specify the condition inside ON caluse Join Only One Row \u00b6 Joining a specific row \u00b6 Sometimes, we want to join only one row from many fulfilling some condition. One way to do that is to use a subquery: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY column_in_b DESC LIMIT 1 ) This query joins table b to table a , using the row with the highest column_in_b . Note however, that all rows from a will be joined to the same row from b . To use a different row from b depending on a , we need to look outside the subquery to filter out b according to a . The folowing query, which should do exactly that, is invalid : SELECT * FROM a JOIN ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 ) The problem here is that the subquery cannot refer the tables outside in, in the preceeeding FROM clause. Luckily, in the most use db systems, there is a magical keyword LATERAL that enables exacly that: SELECT * FROM a JOIN LATERAL ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 ) Join random row \u00b6 To join a random row, we can use the RANDOM function in ordering, e.g.: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY RANDOM() LIMIT 1 ) However, this subquery is evaluated just once, hence we have the same problem as with the first example in this section: to every row in a , we are joining the same random row from b . To force the db system to execute the subquery for each row in a , we need to refer a in the subquery, even with some useless filter (and of course we need a LATERAL join for that): SELECT * FROM a JOIN LATERAL( SELECT * FROM b WHERE a.column_in_a IS NOT NULL ORDER BY RANDOM() LIMIT 1 ) Joining any row find active nodes \u00b6 Sometimes, we need to join any matching row from B to find a set of active (referenced) rows in A. For example, we need to find a set of custommers with pending orders. IF the average number of matches in B is low, we proceed with normal join and then aggregate the results or use DISTINCT . However, sometimes, the average number of matching rows in B can be very high (e.g., finding all countries with at least one MC Donald). For those cases, the LATERAL join is again the solution: SELECT * FROM zones JOIN LATERAL ( SELECT id AS request_id FROM demand WHERE zones.id = demand.destination LIMIT 1 ) demand ON TRUE For more information about the trade offs of this solution, check the SO answer Getting All Combinations of Rows \u00b6 This can be done using the CROSS JOIN , e.g.: SELECT * FROM table_a CROSS JOIN table_b The proble arises when you want to filter one of the tables before joining, because CROSS JOIN does not support the ON clause (see more in the Oracle docs ). Then you can use the equivalent INNER JOIN : SELECT * FROM table_a INNER JOIN table_b ON true Here you replace the true value with your intended condition. Inverse JOIN \u00b6 Sometimes, it is useful to find all rows in table A that has no match in table B. The usual approach is to use LEFT JOIN and filter out non null rows after the join: SELECT ... FROM tableA LEFT JOIN tableB WHERE tableB.id is NULL Joining a table on multiple options \u00b6 There are situations, where we want to join a single table on multiple possible matches. For example, we want to match all people having birthday or name day the same day as some promotion is happanning. The foollowing query is a straighrorward solution: SELECT * FROM promo JOIN people ON promo.date = people.birthaday OR promo.date = people.name_day However, as we explain in the performance chapter, using OR in SQL is almost never a good solution. The usual way of gatting rid of OR is to use IN : SELECT * FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day) Nevertheless, this query can still lead to problems, despite being more compact. When we use any complex condition while joining tables, we risk poor performance. In general, it is better to use just simple column-to-column matches, even when there are more joins as a result. If you have performance problems, consult the \"Replacing OR \" section in the \"Performance Optimization\" chapter. GROUP BY \u00b6 wiki GROUP BY is used to aggregate data. Usual syntax: GROUP BY <LIST OF COLUMNS> Note that we need to use some aggreagte function for all columns in the SELECT clause that are not present in the GROUP BY list. On the other hand, we can use columns not present in the SELECT in the GROUP BY statement. Using aggregate functions for the whole result set while using GROUP BY \u00b6 If a query contains a GROUP BY statement, all aggregate functions (e.g., count , avg ) are applied to groups, i.e., for each row of the result set. If we need to apply an aggregate function to the whole result set while using GROUP BY , we need to specify it using the OVER statement: SELECT count(1) OVER () as result_count ... This will add a coulumn with a total count to each row of the result. If we do not need the actual groups, but only the distinct count, we can use a LIMIT statement. Get any row from each group \u00b6 Sometimes, we need a value from a non-grouped column, but we do not care which one. The reson can be, for example, that we no that the values are the same for all rows in the group. There is no dedicated aggregation for this case, but we can use some simple ones as MIN or MAX . Window functions \u00b6 Sometimes, we would need an aggregate function that somehow use two different columns (e.g., value of col A for the row where col B is largest). For that, we cannot use the classical aggregation, but we rather have to use a window function . A window function is a function that for each row returns a value computed from one or multiple rows. Syntactically, we recognize a window function by the OVER clause that determines the rows used as an input for the function. Functions with the same name can exist as aggregate and window functions. Window functions are evaluated after the GROUP BY clause and aggregate functions. Specifiing the range \u00b6 () : the whole result set (PARTITION BY <column set>) : the rows with the same values for that set of columns We can also order the result for the selected range using ORDER BY inside the parantheses. In some SQL dialects (e.g., PostgreSQL), there are even more sophisticated ways how to specify the range for the window functions. ORDER BY \u00b6 By default, there is no guarantee that the result will be in any particular order. To sort the result, we need to add an ORDER BY statement at the end of the query: ORDER BY <LIST OF COLUMNS> INSERT \u00b6 Standard syntax for the INSERT statement is INSERT INTO <table> (<col_1>, <col_2>,...) VALUES (<val_1>, <val_2>,...) If we fill all columns and we are confident with the column ordering, we can omit columns: INSERT INTO <table> VALUES (<val_1>, <val_2>,...) Sometimes, we need to handle the eroro cases, e.g., the case when the record already exists. The solutions for these cases are, however, database specific. INSERT SELECT \u00b6 If we want to duplicate the records, we can use: INSERT INTO <table> SELECT * FROM <table> [WHERE <condition>] If we need to fill some columns ourselfs: INSERT INTO <table> (<col_1>, <col_2>,...) SELECT <col_1>, <any expression> FROM <anything suported by select> [WHERE <condition>] UPDATE \u00b6 UPDATE query has the following structure: UPDATE table_name SET column1 = value1, column2 = value2...., columnN = valueN WHERE <condition> Unfortunately, the statement is ready to update N records with one set of values, but not to update N records with N set of values. To do that, we have only an option to select from another table: UPDATE table_name SET column1 = other_table.column1 FROM other_table WHERE other_table.id = table_name.id Don't forget the WHERE clause here, otherwise, you are matching the whole result set returned by the FROM clause to each row of the table. Use the table for updating itself \u00b6 If we are abou to update the table using date stored in int, we need to use aliases: UPDATE nodes_ways new SET way_id = ways.osm_id FROM nodes_ways old JOIN ways ON old.way_id = ways.id AND old.area = ways.area WHERE new.way_id = old.way_id AND new.area = old.area AND new.position = old.position DELETE \u00b6 To delete records from a table, we use the DELETE statement: DELETE FROM <table_name> WHERE <condition> If some data from another table are required for the selection of the records to be deleted, the syntax varies depending on the database engine. EXPLAIN \u00b6 Sources - official documentation - official cosumentation: usage - https://docs.gitlab.com/ee/development/understanding_explain_plans.html Remarks: \u00b6 to show the actual run times, we need to run EXPLAIN ANALYZE Nodes \u00b6 Sources - Plan nodes source code - PG documentation with nodes described Node example: -> Parallel Seq Scan on records_mon (cost=0.00..4053077.22 rows=2074111 width=6) (actual time=398.243..74465.389 rows=7221902 loops=2) Filter: ((trip_length_0_1_mi = '0'::double precision) AND (trip_length_1_2_mi = '0'::double precision) AND (trip_length_2_3_mi = '0'::double precision) AND (trip_length_3_4_mi = '0'::double precision) AND (trip_length_4_5_mi = '0'::double precision)) Rows Removed by Filter: 8639817 Buffers: shared hit=157989 read=3805864 Description: In first parantheses, there are expected values: - cost the estimated cost in arbitrary units. The format is startup cost..total cost , where startup cost is a flat cost of the node, an init cost, while total cost is the estimated cost of the node. Averege per loop. - rows : expected number of rows produced by this node. Averege per loop. - width the width of each row in bytes In the second parantheses, there are measured results: - actual time : The measured execution time in miliseconds. The format is startup time..total time . - rows The real number of rows returned. The Rows Removed by the Filter indicates the number of rows that were filtered out. The Buffers statistic shows the number of buffers used. Each buffer consists of 8 KB of data. Keys \u00b6 Keys serves as a way of identifying table rows, they are unique . There are many type of keys, see databastar article for the terminology overview. Primary key \u00b6 Most important keys in ORM are primary keys. Each table should have a single primary key. A primary key has to be non null. When choosing primary key, we can either - use a uniqu combination of database colums: a natural key - use and extra column: surogate key If we use a natural key and it is composed from multiple columns, we call it a composite key The following table summarize the adventages and disadvantages of each of the solutions: Area | Property | Natural key | Composite key | Surrogate key | |-|-|-|-|-| | usage | SQL joins | easy | hard | easy | || changing natural key columns | hard | hard | easy | | Performance | extra space | none | A lot if there are reference tables, otherwise none | one extra column | || space for indexes | normal | extra | normal || extra insertion time | no | A lot if there are reference tables, otherwise none | yes | || join performance | suboptimal due to sparse values | even more sub-optimal due to sparse values | optimal From the table above, we can see that using natural keys should be considered only if rows can be identified by a single column and we have a strong confidence in that natural id, specifically in its uniquness and timelessnes. Non-primary (UNIQUE) keys \u00b6 Sometimes, we need to enforse a uniqueness of a set of columns that does not compose a primary key (e.g., we use a surogate key). We can use a non primary key for that. One of the differences between primary and non-primary keys is that non-primary keys can be null, and each of the null values is considered unique. Indices \u00b6 Indices are essential for speeding queries containing conditions (including conditional joins). The basic syntax for creating an index is: CREATE INDEX <index name> ON <table name>(<column name>); Show Table Indices \u00b6 MySQL: SHOW INDEX FROM <tablename>; PostgreSQL SELECT * FROM pg_indexes WHERE tablename = '<tablename >'; Show Indices From All Tables \u00b6 MySQL: SELECT DISTINCT TABLE_NAME, INDEX_NAME FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = '<schemaname>'; CREATE TABLE \u00b6 The syntax is: CREATE TABLE <TABLE NAME> ( <COLUNM 1 NAME> <COLUNM 1 TYPE>, ... <COLUNM N NAME> <COLUNM N TYPE> [, PRIMARY KEY (<LIST OF KEY COLUMNS>)] ) The primary key is optional, but usually, any table should have one. Each row has to have a unique primary key. An index is created automatically for the list of primary key columns. ALTER TABLE \u00b6 Add column generated from other columns \u00b6 SQL: ALTER TABLE <table name> ADD <column name> AS (<generator expression>); In MySQL we have to add the data type: ALTER TABLE <tablename> ADD <columnname> <datatype> AS (<generator expression>); In PostgreSQL, the syntax is quite different: ALTER TABLE <tablename> ADD <columnname> <datatype> GENERATED ALWAYS AS (<generator expression>) STORED The advantage of the Postgres approach is that the column is set as generated, therefore it is generated for the new rows automatically. Procedures and functions \u00b6 In SQL there are two constructs that are able to encapsulate SQL statements to increase resusability and readability: functions and procedues. The main difference between these two is that functions are intended to be called inline from SLQ statements, while procedures cannot be used in SQL statements and instead, they are used as a wrapper of a set of SQL statement to be called repeatedly with different arguments. The extensive summary of the different capabilities of functions and procedures is on SO . Calling a procedure \u00b6 The keyword for calling a procedure differs between database system, refer to the documentation for your system for the right keyword. Creating a procedure \u00b6 The syntax for calling a procedure differs between database system, refer to the documentation for your system for the right syntax. Parameters \u00b6 Procedures and functions can have parameters similar to parameters in programming languages. We can use those parameters in the body of a function/procedure equally as a column or constant. Parameters can have default values , suplied after the = sign. We can test whether a default argument was supplied by testing the parameter for the default value. Views \u00b6 Views are basically named SQL queries stored in database. The queries are run on each view invocation, unles the view is materialized. The syntax is: CREATE VIEW <VIEW NAME> AS <QUERY> Modifying the view \u00b6 The view can be modified with CREATE OR REPLACE VIEW , however, existing columns cannot be changed . If you need to change existing columns, drop the view first. Performace Optimization \u00b6 When the query is slow, first inspect the following checklist: - Do not use OR or IN for a set of columns (see replacing OR below). - Check that all column and combination of columns used in conditions ( WHERE ) are indexed. - Check that all foreign keys are indexed. - Check that all joins are simple joins (column to column, or set of columns to a matching set of columns). If nothing from the above works, try to start with a simple query and add more complex pars to find where the problem is. If decomposing the query also does not bing light into the problem, refer to either one of the subsections below, or to the external sources. Also, note that some IDEs limits the number of returned rows automatically, which can hide serious problems and confuse you. Try to remove the limit when testing the performance this way. Replacing OR \u00b6 We can slow down the query significantly using OR or IN statements if the set of available options is not constant (e.g., IN(1, 2) is okish, while IN(origin, destination) can have drastic performance impact). To get rid of these disjunctioncs, we can use the UNION statement, basically duplicating the query. The resulting query will be double in size, but much faster: SELECT people.id FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day); -- can be revritten as SELECT * FROM promo JOIN people ON promo.date = people.birthaday UNION SELECT * FROM promo JOIN people ON promo.date = people.name_day GROUP BY people.id A specific join makes the query slow \u00b6 If a single join makes the query slow, there is a great chance that the index is not used for the join. Even if the table has an index on the referenced column(s), the join can still not use it if we are joining not to the table itself but to: - a subquery, - a variable created with a WITH statement, - a view, - or temborary table created from the indexed table. You can solve the situation by creating a materialized view or temporary table instead, and adding inedices to the table manualy. Specifically, you need to split the query into multiple queries: 1. delete the materialized view/table if exists 1. create the materialized view/table 1. create the required indices 1. perform the actual query that utilizes th view/table Of course we can skip the first three steps if the materialized view is constant for all queries, which is common during the testing phase. Slow DELETE \u00b6 When the delete is slow, the cause can be the missing index on the child table that refers to the table we are deleting from. Other possible causes are listed in the SO answer . Sources \u00b6 Database development mistakes made by application developers","title":"SQL Manual"},{"location":"Programming/SQL%20Manual/#literals","text":"In SQL, there are three basic types of literals: - numeric : 1 , 1.2 , 1.2e3 - string : 'string' , \"string\" - boolean : true , false When we need a constant value for other types, we usually use either a constructor function or a specifically formatted string literal. String literals can also span multiple lines , we do not need any operator to split the line (unlike in Python and despite the JetBrains IDEs adds a string concatenation operator automatically on newline).","title":"Literals"},{"location":"Programming/SQL%20Manual/#with","text":"Statement for defining variables pointing to temporary data, that can be used in the related SELECT statement. Usage: WITH <var> AS (<sql that assigns data to var>) <sql that use the variable> Note that the variable has to appear in the FROM clause ! Multiple variables in the WITH statement ahould be delimited by a comma: WITH <var> AS (<sql that assigns data to var>), <var 2> AS (<sql that assigns data to var 2>) <sql that use the variables>","title":"WITH"},{"location":"Programming/SQL%20Manual/#select","text":"Most common SQL statement, syntax: SELECT <EXPRESSION> [AS <ALIAS>][, <EXPRESSION 2> [AS <ALIAS 2> ...]] The most common expression is just a column name.","title":"SELECT"},{"location":"Programming/SQL%20Manual/#selecting-row-number","text":"We can select row numbers using the function ROW_SELECT() in the SELECT statement: SELECT ... ROW_NUMBER() OVER([<PARTITIONING AND NUMBERING ORDER>]) AS <RESULT COLUMN NAME>, ... Inside the OVER statement, we can specify the order of the row numbering. Note however, that this does not order the result, for that, we use the ORDER BY statement. If we want the rown numbering to correspond with the row order in the result, we can left the OVER statement empty.","title":"Selecting row number"},{"location":"Programming/SQL%20Manual/#count-selecting-rows","text":"The count() function can be used to count the selection. The standard syntax is: SELECT count(1) FROM ...","title":"Count selecting rows"},{"location":"Programming/SQL%20Manual/#count-distinct","text":"To count distinct values in a selection we can use: SELECT count(DISTINCT <column name>) FROM...","title":"Count distinct"},{"location":"Programming/SQL%20Manual/#select-from-another-column-if-the-specified-column-is-null","text":"We can use a replacement column using the coalesce function: SELECT coalesce (<primary column>, <secondary column>) The secondary column value will be used if the primary column value in the row is NULL .","title":"Select from another column if the specified column is NULL"},{"location":"Programming/SQL%20Manual/#union","text":"UNION and UNION ALL are important keywords that enables to merge query results verticaly, i.e., appending rows of one query to the results set of another. The difference between them is that UNION discards duplicate rows, while UNION ALL keeps them The UNUION statement appends one SELECT statement to another, but some statements that appears to be part of the SELECT needs to stay outside (i.e., be specified just once for the whole union), namely ORDER BY , and LIMIT . In contrast, the GROUP BY and HAVING statement stays inside each individual select.","title":"UNION"},{"location":"Programming/SQL%20Manual/#join","text":"Classical syntax: JOIN <table name> [[AS] <ALIAS>] ON <CONDITION> The alias is obligatory if there are duplicate names (e.g., we are joining one table twice)","title":"JOIN"},{"location":"Programming/SQL%20Manual/#types","text":"INNER (default): when there is nothing to join, the row is discarded [LEFT/RIGHT/FULL] OUTER : when there is nothing to join, the missing values are set to null CROSS : creates cartesian product between tables","title":"Types"},{"location":"Programming/SQL%20Manual/#outer-join","text":"The OUTER JOIN has three subtypes - LEFT : joins right table to left, fills the missing rows on right with null - RIGHT : joins left to right, fills the missing rows on left with null - FULL : both LEFT and RIGHT JOIN is performend","title":"OUTER JOIN"},{"location":"Programming/SQL%20Manual/#properties","text":"When there are multiple matching rows, all of them are matched (i.e., it creates duplicit rows) If you want to filter the tables before join, you need to specify the condition inside ON caluse","title":"Properties"},{"location":"Programming/SQL%20Manual/#join-only-one-row","text":"","title":"Join Only One Row"},{"location":"Programming/SQL%20Manual/#joining-a-specific-row","text":"Sometimes, we want to join only one row from many fulfilling some condition. One way to do that is to use a subquery: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY column_in_b DESC LIMIT 1 ) This query joins table b to table a , using the row with the highest column_in_b . Note however, that all rows from a will be joined to the same row from b . To use a different row from b depending on a , we need to look outside the subquery to filter out b according to a . The folowing query, which should do exactly that, is invalid : SELECT * FROM a JOIN ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 ) The problem here is that the subquery cannot refer the tables outside in, in the preceeeding FROM clause. Luckily, in the most use db systems, there is a magical keyword LATERAL that enables exacly that: SELECT * FROM a JOIN LATERAL ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 )","title":"Joining a specific row"},{"location":"Programming/SQL%20Manual/#join-random-row","text":"To join a random row, we can use the RANDOM function in ordering, e.g.: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY RANDOM() LIMIT 1 ) However, this subquery is evaluated just once, hence we have the same problem as with the first example in this section: to every row in a , we are joining the same random row from b . To force the db system to execute the subquery for each row in a , we need to refer a in the subquery, even with some useless filter (and of course we need a LATERAL join for that): SELECT * FROM a JOIN LATERAL( SELECT * FROM b WHERE a.column_in_a IS NOT NULL ORDER BY RANDOM() LIMIT 1 )","title":"Join random row"},{"location":"Programming/SQL%20Manual/#joining-any-row-find-active-nodes","text":"Sometimes, we need to join any matching row from B to find a set of active (referenced) rows in A. For example, we need to find a set of custommers with pending orders. IF the average number of matches in B is low, we proceed with normal join and then aggregate the results or use DISTINCT . However, sometimes, the average number of matching rows in B can be very high (e.g., finding all countries with at least one MC Donald). For those cases, the LATERAL join is again the solution: SELECT * FROM zones JOIN LATERAL ( SELECT id AS request_id FROM demand WHERE zones.id = demand.destination LIMIT 1 ) demand ON TRUE For more information about the trade offs of this solution, check the SO answer","title":"Joining any row find active nodes"},{"location":"Programming/SQL%20Manual/#getting-all-combinations-of-rows","text":"This can be done using the CROSS JOIN , e.g.: SELECT * FROM table_a CROSS JOIN table_b The proble arises when you want to filter one of the tables before joining, because CROSS JOIN does not support the ON clause (see more in the Oracle docs ). Then you can use the equivalent INNER JOIN : SELECT * FROM table_a INNER JOIN table_b ON true Here you replace the true value with your intended condition.","title":"Getting All Combinations of Rows"},{"location":"Programming/SQL%20Manual/#inverse-join","text":"Sometimes, it is useful to find all rows in table A that has no match in table B. The usual approach is to use LEFT JOIN and filter out non null rows after the join: SELECT ... FROM tableA LEFT JOIN tableB WHERE tableB.id is NULL","title":"Inverse JOIN"},{"location":"Programming/SQL%20Manual/#joining-a-table-on-multiple-options","text":"There are situations, where we want to join a single table on multiple possible matches. For example, we want to match all people having birthday or name day the same day as some promotion is happanning. The foollowing query is a straighrorward solution: SELECT * FROM promo JOIN people ON promo.date = people.birthaday OR promo.date = people.name_day However, as we explain in the performance chapter, using OR in SQL is almost never a good solution. The usual way of gatting rid of OR is to use IN : SELECT * FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day) Nevertheless, this query can still lead to problems, despite being more compact. When we use any complex condition while joining tables, we risk poor performance. In general, it is better to use just simple column-to-column matches, even when there are more joins as a result. If you have performance problems, consult the \"Replacing OR \" section in the \"Performance Optimization\" chapter.","title":"Joining a table on multiple options"},{"location":"Programming/SQL%20Manual/#group-by","text":"wiki GROUP BY is used to aggregate data. Usual syntax: GROUP BY <LIST OF COLUMNS> Note that we need to use some aggreagte function for all columns in the SELECT clause that are not present in the GROUP BY list. On the other hand, we can use columns not present in the SELECT in the GROUP BY statement.","title":"GROUP BY"},{"location":"Programming/SQL%20Manual/#using-aggregate-functions-for-the-whole-result-set-while-using-group-by","text":"If a query contains a GROUP BY statement, all aggregate functions (e.g., count , avg ) are applied to groups, i.e., for each row of the result set. If we need to apply an aggregate function to the whole result set while using GROUP BY , we need to specify it using the OVER statement: SELECT count(1) OVER () as result_count ... This will add a coulumn with a total count to each row of the result. If we do not need the actual groups, but only the distinct count, we can use a LIMIT statement.","title":"Using aggregate functions for the whole result set while using GROUP BY"},{"location":"Programming/SQL%20Manual/#get-any-row-from-each-group","text":"Sometimes, we need a value from a non-grouped column, but we do not care which one. The reson can be, for example, that we no that the values are the same for all rows in the group. There is no dedicated aggregation for this case, but we can use some simple ones as MIN or MAX .","title":"Get any row from each group"},{"location":"Programming/SQL%20Manual/#window-functions","text":"Sometimes, we would need an aggregate function that somehow use two different columns (e.g., value of col A for the row where col B is largest). For that, we cannot use the classical aggregation, but we rather have to use a window function . A window function is a function that for each row returns a value computed from one or multiple rows. Syntactically, we recognize a window function by the OVER clause that determines the rows used as an input for the function. Functions with the same name can exist as aggregate and window functions. Window functions are evaluated after the GROUP BY clause and aggregate functions.","title":"Window functions"},{"location":"Programming/SQL%20Manual/#specifiing-the-range","text":"() : the whole result set (PARTITION BY <column set>) : the rows with the same values for that set of columns We can also order the result for the selected range using ORDER BY inside the parantheses. In some SQL dialects (e.g., PostgreSQL), there are even more sophisticated ways how to specify the range for the window functions.","title":"Specifiing the range"},{"location":"Programming/SQL%20Manual/#order-by","text":"By default, there is no guarantee that the result will be in any particular order. To sort the result, we need to add an ORDER BY statement at the end of the query: ORDER BY <LIST OF COLUMNS>","title":"ORDER BY"},{"location":"Programming/SQL%20Manual/#insert","text":"Standard syntax for the INSERT statement is INSERT INTO <table> (<col_1>, <col_2>,...) VALUES (<val_1>, <val_2>,...) If we fill all columns and we are confident with the column ordering, we can omit columns: INSERT INTO <table> VALUES (<val_1>, <val_2>,...) Sometimes, we need to handle the eroro cases, e.g., the case when the record already exists. The solutions for these cases are, however, database specific.","title":"INSERT"},{"location":"Programming/SQL%20Manual/#insert-select","text":"If we want to duplicate the records, we can use: INSERT INTO <table> SELECT * FROM <table> [WHERE <condition>] If we need to fill some columns ourselfs: INSERT INTO <table> (<col_1>, <col_2>,...) SELECT <col_1>, <any expression> FROM <anything suported by select> [WHERE <condition>]","title":"INSERT SELECT"},{"location":"Programming/SQL%20Manual/#update","text":"UPDATE query has the following structure: UPDATE table_name SET column1 = value1, column2 = value2...., columnN = valueN WHERE <condition> Unfortunately, the statement is ready to update N records with one set of values, but not to update N records with N set of values. To do that, we have only an option to select from another table: UPDATE table_name SET column1 = other_table.column1 FROM other_table WHERE other_table.id = table_name.id Don't forget the WHERE clause here, otherwise, you are matching the whole result set returned by the FROM clause to each row of the table.","title":"UPDATE"},{"location":"Programming/SQL%20Manual/#use-the-table-for-updating-itself","text":"If we are abou to update the table using date stored in int, we need to use aliases: UPDATE nodes_ways new SET way_id = ways.osm_id FROM nodes_ways old JOIN ways ON old.way_id = ways.id AND old.area = ways.area WHERE new.way_id = old.way_id AND new.area = old.area AND new.position = old.position","title":"Use the table for updating itself"},{"location":"Programming/SQL%20Manual/#delete","text":"To delete records from a table, we use the DELETE statement: DELETE FROM <table_name> WHERE <condition> If some data from another table are required for the selection of the records to be deleted, the syntax varies depending on the database engine.","title":"DELETE"},{"location":"Programming/SQL%20Manual/#explain","text":"Sources - official documentation - official cosumentation: usage - https://docs.gitlab.com/ee/development/understanding_explain_plans.html","title":"EXPLAIN"},{"location":"Programming/SQL%20Manual/#remarks","text":"to show the actual run times, we need to run EXPLAIN ANALYZE","title":"Remarks:"},{"location":"Programming/SQL%20Manual/#nodes","text":"Sources - Plan nodes source code - PG documentation with nodes described Node example: -> Parallel Seq Scan on records_mon (cost=0.00..4053077.22 rows=2074111 width=6) (actual time=398.243..74465.389 rows=7221902 loops=2) Filter: ((trip_length_0_1_mi = '0'::double precision) AND (trip_length_1_2_mi = '0'::double precision) AND (trip_length_2_3_mi = '0'::double precision) AND (trip_length_3_4_mi = '0'::double precision) AND (trip_length_4_5_mi = '0'::double precision)) Rows Removed by Filter: 8639817 Buffers: shared hit=157989 read=3805864 Description: In first parantheses, there are expected values: - cost the estimated cost in arbitrary units. The format is startup cost..total cost , where startup cost is a flat cost of the node, an init cost, while total cost is the estimated cost of the node. Averege per loop. - rows : expected number of rows produced by this node. Averege per loop. - width the width of each row in bytes In the second parantheses, there are measured results: - actual time : The measured execution time in miliseconds. The format is startup time..total time . - rows The real number of rows returned. The Rows Removed by the Filter indicates the number of rows that were filtered out. The Buffers statistic shows the number of buffers used. Each buffer consists of 8 KB of data.","title":"Nodes"},{"location":"Programming/SQL%20Manual/#keys","text":"Keys serves as a way of identifying table rows, they are unique . There are many type of keys, see databastar article for the terminology overview.","title":"Keys"},{"location":"Programming/SQL%20Manual/#primary-key","text":"Most important keys in ORM are primary keys. Each table should have a single primary key. A primary key has to be non null. When choosing primary key, we can either - use a uniqu combination of database colums: a natural key - use and extra column: surogate key If we use a natural key and it is composed from multiple columns, we call it a composite key The following table summarize the adventages and disadvantages of each of the solutions: Area | Property | Natural key | Composite key | Surrogate key | |-|-|-|-|-| | usage | SQL joins | easy | hard | easy | || changing natural key columns | hard | hard | easy | | Performance | extra space | none | A lot if there are reference tables, otherwise none | one extra column | || space for indexes | normal | extra | normal || extra insertion time | no | A lot if there are reference tables, otherwise none | yes | || join performance | suboptimal due to sparse values | even more sub-optimal due to sparse values | optimal From the table above, we can see that using natural keys should be considered only if rows can be identified by a single column and we have a strong confidence in that natural id, specifically in its uniquness and timelessnes.","title":"Primary key"},{"location":"Programming/SQL%20Manual/#non-primary-unique-keys","text":"Sometimes, we need to enforse a uniqueness of a set of columns that does not compose a primary key (e.g., we use a surogate key). We can use a non primary key for that. One of the differences between primary and non-primary keys is that non-primary keys can be null, and each of the null values is considered unique.","title":"Non-primary (UNIQUE) keys"},{"location":"Programming/SQL%20Manual/#indices","text":"Indices are essential for speeding queries containing conditions (including conditional joins). The basic syntax for creating an index is: CREATE INDEX <index name> ON <table name>(<column name>);","title":"Indices"},{"location":"Programming/SQL%20Manual/#show-table-indices","text":"MySQL: SHOW INDEX FROM <tablename>; PostgreSQL SELECT * FROM pg_indexes WHERE tablename = '<tablename >';","title":"Show Table Indices"},{"location":"Programming/SQL%20Manual/#show-indices-from-all-tables","text":"MySQL: SELECT DISTINCT TABLE_NAME, INDEX_NAME FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = '<schemaname>';","title":"Show Indices From All Tables"},{"location":"Programming/SQL%20Manual/#create-table","text":"The syntax is: CREATE TABLE <TABLE NAME> ( <COLUNM 1 NAME> <COLUNM 1 TYPE>, ... <COLUNM N NAME> <COLUNM N TYPE> [, PRIMARY KEY (<LIST OF KEY COLUMNS>)] ) The primary key is optional, but usually, any table should have one. Each row has to have a unique primary key. An index is created automatically for the list of primary key columns.","title":"CREATE TABLE"},{"location":"Programming/SQL%20Manual/#alter-table","text":"","title":"ALTER TABLE"},{"location":"Programming/SQL%20Manual/#add-column-generated-from-other-columns","text":"SQL: ALTER TABLE <table name> ADD <column name> AS (<generator expression>); In MySQL we have to add the data type: ALTER TABLE <tablename> ADD <columnname> <datatype> AS (<generator expression>); In PostgreSQL, the syntax is quite different: ALTER TABLE <tablename> ADD <columnname> <datatype> GENERATED ALWAYS AS (<generator expression>) STORED The advantage of the Postgres approach is that the column is set as generated, therefore it is generated for the new rows automatically.","title":"Add column generated from other columns"},{"location":"Programming/SQL%20Manual/#procedures-and-functions","text":"In SQL there are two constructs that are able to encapsulate SQL statements to increase resusability and readability: functions and procedues. The main difference between these two is that functions are intended to be called inline from SLQ statements, while procedures cannot be used in SQL statements and instead, they are used as a wrapper of a set of SQL statement to be called repeatedly with different arguments. The extensive summary of the different capabilities of functions and procedures is on SO .","title":"Procedures and functions"},{"location":"Programming/SQL%20Manual/#calling-a-procedure","text":"The keyword for calling a procedure differs between database system, refer to the documentation for your system for the right keyword.","title":"Calling a procedure"},{"location":"Programming/SQL%20Manual/#creating-a-procedure","text":"The syntax for calling a procedure differs between database system, refer to the documentation for your system for the right syntax.","title":"Creating a procedure"},{"location":"Programming/SQL%20Manual/#parameters","text":"Procedures and functions can have parameters similar to parameters in programming languages. We can use those parameters in the body of a function/procedure equally as a column or constant. Parameters can have default values , suplied after the = sign. We can test whether a default argument was supplied by testing the parameter for the default value.","title":"Parameters"},{"location":"Programming/SQL%20Manual/#views","text":"Views are basically named SQL queries stored in database. The queries are run on each view invocation, unles the view is materialized. The syntax is: CREATE VIEW <VIEW NAME> AS <QUERY>","title":"Views"},{"location":"Programming/SQL%20Manual/#modifying-the-view","text":"The view can be modified with CREATE OR REPLACE VIEW , however, existing columns cannot be changed . If you need to change existing columns, drop the view first.","title":"Modifying the view"},{"location":"Programming/SQL%20Manual/#performace-optimization","text":"When the query is slow, first inspect the following checklist: - Do not use OR or IN for a set of columns (see replacing OR below). - Check that all column and combination of columns used in conditions ( WHERE ) are indexed. - Check that all foreign keys are indexed. - Check that all joins are simple joins (column to column, or set of columns to a matching set of columns). If nothing from the above works, try to start with a simple query and add more complex pars to find where the problem is. If decomposing the query also does not bing light into the problem, refer to either one of the subsections below, or to the external sources. Also, note that some IDEs limits the number of returned rows automatically, which can hide serious problems and confuse you. Try to remove the limit when testing the performance this way.","title":"Performace Optimization"},{"location":"Programming/SQL%20Manual/#replacing-or","text":"We can slow down the query significantly using OR or IN statements if the set of available options is not constant (e.g., IN(1, 2) is okish, while IN(origin, destination) can have drastic performance impact). To get rid of these disjunctioncs, we can use the UNION statement, basically duplicating the query. The resulting query will be double in size, but much faster: SELECT people.id FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day); -- can be revritten as SELECT * FROM promo JOIN people ON promo.date = people.birthaday UNION SELECT * FROM promo JOIN people ON promo.date = people.name_day GROUP BY people.id","title":"Replacing OR"},{"location":"Programming/SQL%20Manual/#a-specific-join-makes-the-query-slow","text":"If a single join makes the query slow, there is a great chance that the index is not used for the join. Even if the table has an index on the referenced column(s), the join can still not use it if we are joining not to the table itself but to: - a subquery, - a variable created with a WITH statement, - a view, - or temborary table created from the indexed table. You can solve the situation by creating a materialized view or temporary table instead, and adding inedices to the table manualy. Specifically, you need to split the query into multiple queries: 1. delete the materialized view/table if exists 1. create the materialized view/table 1. create the required indices 1. perform the actual query that utilizes th view/table Of course we can skip the first three steps if the materialized view is constant for all queries, which is common during the testing phase.","title":"A specific join makes the query slow"},{"location":"Programming/SQL%20Manual/#slow-delete","text":"When the delete is slow, the cause can be the missing index on the child table that refers to the table we are deleting from. Other possible causes are listed in the SO answer .","title":"Slow DELETE"},{"location":"Programming/SQL%20Manual/#sources","text":"Database development mistakes made by application developers","title":"Sources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/","text":"Compilation \u00b6 General rules \u00b6 Make the code you want to compile reachable. In C++, only reachable methods/classes are compiled! Solve errors that stop the compilation first. Warnings can stay in logs even if solved until the main error is gone and the build is finished Be aware that the cause of the error can be on a different line than the one in the error log! If the source of the compilation bug cannot be found \u00b6 read the error examples below check that the code follow the my guidelines and C++ Core Guidelines . read the cpp reference for the parts of the problematic code check the const correctness . It is a causes a lot of problems. try a different compiler, the error message can be more informative try to isolate the case in some small example copy the project and remove the stuff until the problem is gone Practical static assertions \u00b6 Test for concept satisfaction: static_assert(My_concept<My_class>); Useful Predicates \u00b6 std::is_same_as for checking the type equality. Determining type at compile time \u00b6 Sometimes, it is practical to know the exact type during compile time. There is no direct way for that, but we can trick the compiler to print the type in an error message: template <typename...> struct Get_type; class Something {}; Get_type<<TYPE TO GET>> my_type{}; This should print an error message similar to: error: variable \u2018Get_type<<TYPE TO GET RESOLVED>> my_type\u2019 has initializer but incomplete type . Errors with a missing copy constructor \u00b6 e.g. error C2280: 'Solution<N>::Solution(const Solution<N> &)': attempting to reference a deleted function These can be split into two groups: We want to copy the object, but the copy constructor is missing \u00b6 We do not want to copy the object, but the copy constructor is still called \u00b6 First, check if the class is move-constructible in the first place: static_assert(std::is_move_constructible<Solution<Cordeau_node>>::value); If the above check fails, check why the move constructor is not present. Possible reasons, why the move constructor is not available: Move constructor is not implicitly declared due to a broken rule of five, i.e., one of the other constructors/assignments/destructors is defined Implicitly declared move constructor is deleted. Possible reasons: the class have a member that cannot be moved from const member If the check passes, and the copy constructor is still being called: Errors with missing move constructor \u00b6 First, we should check whether the object's type T is movable using static_assert(std::is_move_constructible_v<T>) If the static assertion is false: 1. Check whether all base classes has move constructors available - the std::is_move_constructible_v<T> cannot be used for that, as the move operations can be protected. Instead, look for the presence of the move constructors in base classes (any base class should have them declared, as the implicit declaration of copy/move/destruction does not work with virtual classes) 2. Check whether all class members are move constructible using the std::is_move_constructible_v<T> concept. Do not forget the const qualifiers! Multiply Defined Symbols \u00b6 e.g. name already used for a template in the current scope. The source of the duplicate should be in the compiler output. Usually, this can be solved by using namespaces. Conversion is inaccessible \u00b6 This mostly happens if we forgot to add the public keyword when inheriting from a base class, resulting in the (default) private inheritance. Cannot convert from 'initializer list' to... \u00b6 This happans when there is no function signature matching the arguments. Sometimes, the specific argument can be found by using constructor instead of initializer list. Otherwise, check the arguments one by one. For each argument: 1. check if the type matches 2. check if the value type matches, i.e. value/ref/pointer 3. check if the const matches 4. check if the problem is not in wrong template argument deduction. The automatic deduction can use value type instead of reference type... Returning address of local variable or temporary \u00b6 This happens when we assign a lambda to std::function and the labda return by reference and does not have an explicit return type. The solution is to add an explicit return type. Cannot resolve symbol \u00b6 or alternatively: 'identifier' was unexpected here; expected 'type specifier' . It simply means that the type cannot be reolved from the code location. Possible reasons: - Circular dinclude Linker Errors \u00b6 Undefined \u00b6 Something like unresolved external symbol... . check that all used files are listed in CMakeLists.txt ( add_executable command\u2026) check that all templates are defined in header files check that all functions are defined correctly (even unsigned vs unsigned int can make problems...) check this list: https://stackoverflow.com/questions/12573816/what-is-an-undefined-reference-unresolved-external-symbol-error-and-how-do-i-fix? Multiply Defined \u00b6 Check the recent includes, are all of them correct? Check the multiply defined symbol. If it is a function defined in a header file, it has to be static or inline. there can be a bug even in an external library! https://stackoverflow.com/questions/30180145/general-techniques-for-debugging-the-multiple-definition-of-error Runtime errors \u00b6 First, identify the exception context . To do that, look at the line where the exception is thrown. If the throwing line is not on the call stack, it is possible that the debugger does not break on the particular exception type. to change that go to the Exception Settings and check the exception type there. If the causo of the exception is not clear from the context, it may be usefull to check the exception details Unfortunatelly, it is not possible to inspect unhandeled exception object easily in Visual Studio. to do so, add the following watch: (<exception type>*) <exception address> where <exception type> is the type of the exception (e.g. std::exception ) and <exception address> is the address from the exception dialog. Finally, if the cause of the exception is still unclear, look at the exception type, and proceed to the respective section below, if there is one. Improve debugger experience with natvis \u00b6 Natvis is a visualization system that enables to enhance the presentation of variables in debugger locals or watch windows. It is a XML file, where we can define visualization rules for any type. Structure: <Type Name=\"My_class\"> ... visualization rules </Type> The name must be fully qualified, i.e., we have to include namespaces. The big advantage is that natvis files can be changed while the debugger is running and the presentation of the type in locals/watch window is changed immediatly after saving the natvis file. Natvis expressions \u00b6 The expression in natvis are sorounded by {} . If we want curly braces in the text, we can double them {{...}} . Unfortunatelly, function calles cannot be used in natvis expressions . Natvis Errors \u00b6 Natvis errors can be displayed in the output window if turned on in settins: Debug -> Options -> Debugging -> Output Window . Existing visualisations \u00b6 Existing natvis files are stored in <VS Installation Folder>\\Common7\\Packages\\Debugger\\Visualizers folder. The STL visualizations are in stl.natvis Debugger manual \u00b6 Address breakpoints \u00b6 Address breakpoints can be used to watch a change of a variable or in general, a change of any memory location. To set an address breakpoint, we nned to first find the address of the variable. To do that, we can: - use the & operator on the variable in the watch window - use the & operator on the variable in the immediate window The addres should have a format 0x0000000000000000 . Memory Errors \u00b6 These exception are raised when an unallocated memory is accesed. The following signalize a memory error: - Read Access Violation - HEAP CORRUPTION DETECTED First, most of the memory errors can be caught by various assertions and guards in the debug mode. If possible, try to run the program in the debugg mode, even if it takes a long time, because this way, you can catch the problem when it happens, before the memory is corrupted. If that does not help, read the following sections. Other reassons are also discussed here Accessing null pointer \u00b6 A frequent cause of memory errors is accessing a null pointer object's data. In this case, the cause of the problem can be quickly determined in the debugger. Just follow the lifetime of the pointer and find the momemnt when it becom null. Read Access Violation Caused by a Demaged vtable \u00b6 In case of some previous memory mismanagement, the heap can be demaged, possibly resulting in a corrupted virtual table for objects on the heap. To check whether the virtual table is corrupted, add the following watch to the debugger: <var name>.__vfptr Where <var name> is the name of the object you want to inspect. To resolve this problem, see debugging memory errors. Using Application Verifier to find causes of memory related errors. \u00b6 A lot of memory errors can be caught just by running the program in the debugger. The STL containers, for example, containst various assertions that break the code on wrong memory access related to these collections. To add even more assert guards (e.g., for dynamic arrays), we can use the Application Verifier which is installed as a part of Windows SDK (which is typically installed together with Visual Studio). To debug the application with the Application verifier enabled: 1. Open AV 2. right click -> add executable and select the executable to test 3. select the appropriete test suite (the basic one is enouh for the memory testing) 4. click save 5. close AV 6. run the executable in the debugger, find the problem, fix it 7. open AV 8. delete the exectable from the list Using Address Sanitizer \u00b6 A linux memory tool called address sanitizer can be used to debug memory related errors. To use it from the Visual Studio: 1. Check that the libasan lib is installed on WSL 1. In Cmake Settings for the debug configuration, check Enable AddressSanitizer 1. build the project 1. run The program should now break on the first problem. The details are displayed in the output window More at Microsoft learn Using Valgrinfd to debug memory errors \u00b6 Valgrind is a toolbox for debugging C/C++ code. The most famous tool is calle Memcheck and is intended for memory error detection. Basic usage: valgrind --leak-check=yes <program> <program arguments> The explanation of the error messages can be found on the Valgrind website The most common errors and tips: - Conditional jump or move depends on uninitialised value : - triggers on the first usage (not copy) of the uninitialized data - note that the uninitialized variable can look normal (e.g. if it is a number), just the value is random. - Invalid read of size ... : - can happen to both stack and heap memory - the content can still be in memory, it just means that the memory has been freed/invalidated. There are some expected messeges not to be worried about: Warning: set address range perms: large range Logical Errors \u00b6 C++ Specific Numerical Errors \u00b6 First possible error is overflow . C++ does not handle or report overflow! The behaviour is undefined. Second potential danger is the unsigned integer overflow . In case the result below zero is stored in unsigned integer, the number is wrapped around, resulting in large positive integer. Another thing is that when signed and unsigned integers are used in one operation, the signed integer is implicitely converted to unsigned integer before the operation! This is called promotion and it also works for other types (see in a table on SO ). In general to prevent the overflow: - check each operation for the potential overflow, inluding the unsigned integer overflow with negative numbers - if the overflow can happen, cast the types before any arithmetic operation to prevent the overflow - also, one have to use the right type in templates like std::max Testing - Google Test \u00b6 Debugging tests \u00b6 To debug a test, you need to run it with the flag some flags: --gtest_break_on_failure . This flag breaks the test on a failed test or failed assertion. --gtest_catch_exceptions=0 . This flag stops google test from catching exceptions, hence the program breaks at the place where the exception occurred, which is what you want --gtest_filter=Some_test_prefix* for choosing just some tests Also, you need to run tests from Visual Studio, otherwise, the program ends when clicking on the retry button. Visual Studio Errors \u00b6 False errors appears in a file / in many files \u00b6 close visual studio delete the out folder open the visual studio again Refactoring options not present \u00b6 If the refactoring options like change signature are not present, try to send the Resharper bug reports :), or create a new project. IntalliSense false errors \u00b6 Sometimes, the errors disappears by deleting the .suo file located in <project dir>/.vs/<project name>/<VS version> Profiling \u00b6 There are multiple profiler options for C++ , but not all the tools are easy to use or maintained. CPU Profiling \u00b6 VTune \u00b6 VTune can be run ftom the Visual Studio only for VS solution projects. In case of CMake projects, we need to run the VTune GUI and configure the debugging there. Memory Profiling \u00b6 For memory profiling to work, two things needs to be taken care of: - if the application allocates a lot memory inside parallel region, disable paralelization for profiling. Otherwise, there can be too many allocation events for the profiler to handle - if you use a custom memory allocator, disable it and use a standard allocator for memory profiling Memory Profiling in Visual Studio \u00b6 To profile memory in Visul Studio 1. Set a breakpoint before the region of interest 2. Run the app and wait for the hit 3. In Diagnostic Tools tab -> Summary , click Enable heap profiling for next session 4. Restart the app and wait for the hit. 5. Take memory snapshot 6. Add a breakpoint to the end of the region of interest 7. Wait for the hit, take snapshot and check both snapshots Memory \u00b6 Written with StackEdit .","title":"C++ Debugging and Profiling"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#compilation","text":"","title":"Compilation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#general-rules","text":"Make the code you want to compile reachable. In C++, only reachable methods/classes are compiled! Solve errors that stop the compilation first. Warnings can stay in logs even if solved until the main error is gone and the build is finished Be aware that the cause of the error can be on a different line than the one in the error log!","title":"General rules"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#if-the-source-of-the-compilation-bug-cannot-be-found","text":"read the error examples below check that the code follow the my guidelines and C++ Core Guidelines . read the cpp reference for the parts of the problematic code check the const correctness . It is a causes a lot of problems. try a different compiler, the error message can be more informative try to isolate the case in some small example copy the project and remove the stuff until the problem is gone","title":"If the source of the compilation bug cannot be found"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#practical-static-assertions","text":"Test for concept satisfaction: static_assert(My_concept<My_class>);","title":"Practical static assertions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#useful-predicates","text":"std::is_same_as for checking the type equality.","title":"Useful Predicates"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#determining-type-at-compile-time","text":"Sometimes, it is practical to know the exact type during compile time. There is no direct way for that, but we can trick the compiler to print the type in an error message: template <typename...> struct Get_type; class Something {}; Get_type<<TYPE TO GET>> my_type{}; This should print an error message similar to: error: variable \u2018Get_type<<TYPE TO GET RESOLVED>> my_type\u2019 has initializer but incomplete type .","title":"Determining type at compile time"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#errors-with-a-missing-copy-constructor","text":"e.g. error C2280: 'Solution<N>::Solution(const Solution<N> &)': attempting to reference a deleted function These can be split into two groups:","title":"Errors with a missing copy constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#we-want-to-copy-the-object-but-the-copy-constructor-is-missing","text":"","title":"We want to copy the object, but the copy constructor is missing"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#we-do-not-want-to-copy-the-object-but-the-copy-constructor-is-still-called","text":"First, check if the class is move-constructible in the first place: static_assert(std::is_move_constructible<Solution<Cordeau_node>>::value); If the above check fails, check why the move constructor is not present. Possible reasons, why the move constructor is not available: Move constructor is not implicitly declared due to a broken rule of five, i.e., one of the other constructors/assignments/destructors is defined Implicitly declared move constructor is deleted. Possible reasons: the class have a member that cannot be moved from const member If the check passes, and the copy constructor is still being called:","title":"We do not want to copy the object, but the copy constructor is still called"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#errors-with-missing-move-constructor","text":"First, we should check whether the object's type T is movable using static_assert(std::is_move_constructible_v<T>) If the static assertion is false: 1. Check whether all base classes has move constructors available - the std::is_move_constructible_v<T> cannot be used for that, as the move operations can be protected. Instead, look for the presence of the move constructors in base classes (any base class should have them declared, as the implicit declaration of copy/move/destruction does not work with virtual classes) 2. Check whether all class members are move constructible using the std::is_move_constructible_v<T> concept. Do not forget the const qualifiers!","title":"Errors with missing move constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#multiply-defined-symbols","text":"e.g. name already used for a template in the current scope. The source of the duplicate should be in the compiler output. Usually, this can be solved by using namespaces.","title":"Multiply Defined Symbols"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#conversion-is-inaccessible","text":"This mostly happens if we forgot to add the public keyword when inheriting from a base class, resulting in the (default) private inheritance.","title":"Conversion is inaccessible"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#cannot-convert-from-initializer-list-to","text":"This happans when there is no function signature matching the arguments. Sometimes, the specific argument can be found by using constructor instead of initializer list. Otherwise, check the arguments one by one. For each argument: 1. check if the type matches 2. check if the value type matches, i.e. value/ref/pointer 3. check if the const matches 4. check if the problem is not in wrong template argument deduction. The automatic deduction can use value type instead of reference type...","title":"Cannot convert from 'initializer list' to..."},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#returning-address-of-local-variable-or-temporary","text":"This happens when we assign a lambda to std::function and the labda return by reference and does not have an explicit return type. The solution is to add an explicit return type.","title":"Returning address of local variable or temporary"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#cannot-resolve-symbol","text":"or alternatively: 'identifier' was unexpected here; expected 'type specifier' . It simply means that the type cannot be reolved from the code location. Possible reasons: - Circular dinclude","title":"Cannot resolve symbol"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#linker-errors","text":"","title":"Linker Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#undefined","text":"Something like unresolved external symbol... . check that all used files are listed in CMakeLists.txt ( add_executable command\u2026) check that all templates are defined in header files check that all functions are defined correctly (even unsigned vs unsigned int can make problems...) check this list: https://stackoverflow.com/questions/12573816/what-is-an-undefined-reference-unresolved-external-symbol-error-and-how-do-i-fix?","title":"Undefined"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#multiply-defined","text":"Check the recent includes, are all of them correct? Check the multiply defined symbol. If it is a function defined in a header file, it has to be static or inline. there can be a bug even in an external library! https://stackoverflow.com/questions/30180145/general-techniques-for-debugging-the-multiple-definition-of-error","title":"Multiply Defined"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#runtime-errors","text":"First, identify the exception context . To do that, look at the line where the exception is thrown. If the throwing line is not on the call stack, it is possible that the debugger does not break on the particular exception type. to change that go to the Exception Settings and check the exception type there. If the causo of the exception is not clear from the context, it may be usefull to check the exception details Unfortunatelly, it is not possible to inspect unhandeled exception object easily in Visual Studio. to do so, add the following watch: (<exception type>*) <exception address> where <exception type> is the type of the exception (e.g. std::exception ) and <exception address> is the address from the exception dialog. Finally, if the cause of the exception is still unclear, look at the exception type, and proceed to the respective section below, if there is one.","title":"Runtime errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#improve-debugger-experience-with-natvis","text":"Natvis is a visualization system that enables to enhance the presentation of variables in debugger locals or watch windows. It is a XML file, where we can define visualization rules for any type. Structure: <Type Name=\"My_class\"> ... visualization rules </Type> The name must be fully qualified, i.e., we have to include namespaces. The big advantage is that natvis files can be changed while the debugger is running and the presentation of the type in locals/watch window is changed immediatly after saving the natvis file.","title":"Improve debugger experience with natvis"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#natvis-expressions","text":"The expression in natvis are sorounded by {} . If we want curly braces in the text, we can double them {{...}} . Unfortunatelly, function calles cannot be used in natvis expressions .","title":"Natvis expressions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#natvis-errors","text":"Natvis errors can be displayed in the output window if turned on in settins: Debug -> Options -> Debugging -> Output Window .","title":"Natvis Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#existing-visualisations","text":"Existing natvis files are stored in <VS Installation Folder>\\Common7\\Packages\\Debugger\\Visualizers folder. The STL visualizations are in stl.natvis","title":"Existing visualisations"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#debugger-manual","text":"","title":"Debugger manual"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#address-breakpoints","text":"Address breakpoints can be used to watch a change of a variable or in general, a change of any memory location. To set an address breakpoint, we nned to first find the address of the variable. To do that, we can: - use the & operator on the variable in the watch window - use the & operator on the variable in the immediate window The addres should have a format 0x0000000000000000 .","title":"Address breakpoints"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#memory-errors","text":"These exception are raised when an unallocated memory is accesed. The following signalize a memory error: - Read Access Violation - HEAP CORRUPTION DETECTED First, most of the memory errors can be caught by various assertions and guards in the debug mode. If possible, try to run the program in the debugg mode, even if it takes a long time, because this way, you can catch the problem when it happens, before the memory is corrupted. If that does not help, read the following sections. Other reassons are also discussed here","title":"Memory Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#accessing-null-pointer","text":"A frequent cause of memory errors is accessing a null pointer object's data. In this case, the cause of the problem can be quickly determined in the debugger. Just follow the lifetime of the pointer and find the momemnt when it becom null.","title":"Accessing null pointer"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#read-access-violation-caused-by-a-demaged-vtable","text":"In case of some previous memory mismanagement, the heap can be demaged, possibly resulting in a corrupted virtual table for objects on the heap. To check whether the virtual table is corrupted, add the following watch to the debugger: <var name>.__vfptr Where <var name> is the name of the object you want to inspect. To resolve this problem, see debugging memory errors.","title":"Read Access Violation Caused by a Demaged vtable"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#using-application-verifier-to-find-causes-of-memory-related-errors","text":"A lot of memory errors can be caught just by running the program in the debugger. The STL containers, for example, containst various assertions that break the code on wrong memory access related to these collections. To add even more assert guards (e.g., for dynamic arrays), we can use the Application Verifier which is installed as a part of Windows SDK (which is typically installed together with Visual Studio). To debug the application with the Application verifier enabled: 1. Open AV 2. right click -> add executable and select the executable to test 3. select the appropriete test suite (the basic one is enouh for the memory testing) 4. click save 5. close AV 6. run the executable in the debugger, find the problem, fix it 7. open AV 8. delete the exectable from the list","title":"Using Application Verifier to find causes of memory related errors."},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#using-address-sanitizer","text":"A linux memory tool called address sanitizer can be used to debug memory related errors. To use it from the Visual Studio: 1. Check that the libasan lib is installed on WSL 1. In Cmake Settings for the debug configuration, check Enable AddressSanitizer 1. build the project 1. run The program should now break on the first problem. The details are displayed in the output window More at Microsoft learn","title":"Using Address Sanitizer"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#using-valgrinfd-to-debug-memory-errors","text":"Valgrind is a toolbox for debugging C/C++ code. The most famous tool is calle Memcheck and is intended for memory error detection. Basic usage: valgrind --leak-check=yes <program> <program arguments> The explanation of the error messages can be found on the Valgrind website The most common errors and tips: - Conditional jump or move depends on uninitialised value : - triggers on the first usage (not copy) of the uninitialized data - note that the uninitialized variable can look normal (e.g. if it is a number), just the value is random. - Invalid read of size ... : - can happen to both stack and heap memory - the content can still be in memory, it just means that the memory has been freed/invalidated. There are some expected messeges not to be worried about: Warning: set address range perms: large range","title":"Using Valgrinfd to debug memory errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#logical-errors","text":"","title":"Logical Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#c-specific-numerical-errors","text":"First possible error is overflow . C++ does not handle or report overflow! The behaviour is undefined. Second potential danger is the unsigned integer overflow . In case the result below zero is stored in unsigned integer, the number is wrapped around, resulting in large positive integer. Another thing is that when signed and unsigned integers are used in one operation, the signed integer is implicitely converted to unsigned integer before the operation! This is called promotion and it also works for other types (see in a table on SO ). In general to prevent the overflow: - check each operation for the potential overflow, inluding the unsigned integer overflow with negative numbers - if the overflow can happen, cast the types before any arithmetic operation to prevent the overflow - also, one have to use the right type in templates like std::max","title":"C++ Specific Numerical Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#testing-google-test","text":"","title":"Testing - Google Test"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#debugging-tests","text":"To debug a test, you need to run it with the flag some flags: --gtest_break_on_failure . This flag breaks the test on a failed test or failed assertion. --gtest_catch_exceptions=0 . This flag stops google test from catching exceptions, hence the program breaks at the place where the exception occurred, which is what you want --gtest_filter=Some_test_prefix* for choosing just some tests Also, you need to run tests from Visual Studio, otherwise, the program ends when clicking on the retry button.","title":"Debugging tests"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#visual-studio-errors","text":"","title":"Visual Studio Errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#false-errors-appears-in-a-file-in-many-files","text":"close visual studio delete the out folder open the visual studio again","title":"False errors appears in a file / in many files"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#refactoring-options-not-present","text":"If the refactoring options like change signature are not present, try to send the Resharper bug reports :), or create a new project.","title":"Refactoring options not present"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#intallisense-false-errors","text":"Sometimes, the errors disappears by deleting the .suo file located in <project dir>/.vs/<project name>/<VS version>","title":"IntalliSense false errors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#profiling","text":"There are multiple profiler options for C++ , but not all the tools are easy to use or maintained.","title":"Profiling"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#cpu-profiling","text":"","title":"CPU Profiling"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#vtune","text":"VTune can be run ftom the Visual Studio only for VS solution projects. In case of CMake projects, we need to run the VTune GUI and configure the debugging there.","title":"VTune"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#memory-profiling","text":"For memory profiling to work, two things needs to be taken care of: - if the application allocates a lot memory inside parallel region, disable paralelization for profiling. Otherwise, there can be too many allocation events for the profiler to handle - if you use a custom memory allocator, disable it and use a standard allocator for memory profiling","title":"Memory Profiling"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#memory-profiling-in-visual-studio","text":"To profile memory in Visul Studio 1. Set a breakpoint before the region of interest 2. Run the app and wait for the hit 3. In Diagnostic Tools tab -> Summary , click Enable heap profiling for next session 4. Restart the app and wait for the hit. 5. Take memory snapshot 6. Add a breakpoint to the end of the region of interest 7. Wait for the hit, take snapshot and check both snapshots","title":"Memory Profiling in Visual Studio"},{"location":"Programming/C%2B%2B/C%2B%2B%20Debugging%20and%20Profiling/#memory","text":"Written with StackEdit .","title":"Memory"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/","text":"Type System and basic types \u00b6 cppreference Type is a property of each: - object - reference - function - expression Arithmetic Types \u00b6 cppreference Integers \u00b6 Integer types varies in the sign and size. Unfortunatelly, the minimum sizes guaranteed by the standard are not usable, because the real size is different and it differs even between platforms . Especially the long type. To use an integer with a specific size, or a specific minimal size, we can use type aliases defined in cstdint Overflow and Underflow \u00b6 The overflow (and underflow) is a common problem in most programming languages. The problem in C++ is that: - overflows are not detected - overflows can happen in many unexpected situations Dangerous situations \u00b6 In addition to the usual suspects like assigning a value to a variable of a smaller type, there are some less obvious situations that can cause overflows. Some examples: - the result of an arithmetic operation is assigned to a variable of large enough type, but the overflow happens before the assignment itself: short a = 32767; short b = 1; int c = a + b; // overflow happens beffore the assignment A solution to this problem is to use a numeric cast of the opperands (even one is enouhg): short a = 32767; short b = 1; int c = static_cast<int>(a) + b; Detecting overflows \u00b6 There are some methods how to detect overflows automatically by suppliying arguments to the compiler. These are summarized here: - MSVC : not implemented - GCC : only detectes signed and floating point overflows, as the unsigned overflows are not considered as errors (the behaviour is defined in the standard). All undefined behaviour can be detected using the -fsanitize=undefined flag. Documentation - Clang : Both signed and unsigned overflow can be detected. The undefined behaviour can be detected using the -fsanitize=undefined flag. Fo all integer overflows, the -fsanitize=integer flag can be used. Documentation The reasoning behind excluding the unsigned overflows from GCC are described here . It is also possible to do an ad-hoc overflow check in the code, the possible solutions are described in this SO question Pointers \u00b6 cppreference Pointers to Functions \u00b6 Function pointers are declared as: <return_type> (*<pointer_name>)(<arg_1_type>, ..., <arg_n_type>) For example a function the_function returning bool and accepting int can be stored to pointer like this: bool (*ptr)(int) = &the_function The above example can be then simply called as bool b = ptr(2) Pointers to Member Objects \u00b6 Pointers to member objects has a cumbersome syntax - declaration: <member type> <class type>::*<pointer name> = ... - usage: <object name>.*<pointer name> = ... Example: class My_class{ public: int my_member; } int main{ // declaring the pointer int My_class::*ptr = &My_class::my_member; // creating the instance My_class inst; // using the pointer to a member object inst.*ptr = 2; } Pointers to Member Functions \u00b6 Pointers to member functions are even more scary in C++. We need to use the member object and the function adress and combine it in a obscure way: class My_class{ public: bool my_method(int par); } int main{ // creating the instance My_class inst; // assigning method address to a pointer bool (My_class::*ptr)(int) = &My_class::my_method; // using the pointer to a member function bool b = (inst.*ptr)(2) } The first unexpected change is the My_class before the name of the pointer. It's because unlike a pointer to function the_function which is of type (*)(int) , the pointer to my_method is of type (My_class::*)(int) The second difference is the call. We have t use the pointer to member binding operator .* to access the member of the specific instance inst . But this operator has a lower priority then the function call operator, so we must use the extra parantheses. References \u00b6 References serve as an alias to already existing objects. Standard ( Lvalue ) references works the same way as pointers, with two differences: - they cannot be NULL - they cannot be reassigned The second property is the most important, as the assignment is a common operation, which often happens under do hood. In conslusion, reference types cannot be used in most of the containers and objets that needs to be copied . Rvalue references \u00b6 Rvalue references are used to refer to temporary objects. They eneable to prevent copying local objets by extending lifetime of temporary objects. They are mostly used as function parameters: void f(int& x){ } f(3); // 3 needs to be copied to f, because it is a temporary variable // we can add the rvalue overload void f(int&& x){ } f(3) // rvalue overload called, no copy Forwarding references \u00b6 Forwarding references are references that preserves the value category (i.e. r/l-value reference, const ). They have two forms: - function parameter forwarding references - auto forwarding references Function parameter forwarding references \u00b6 In a function template, if we use the rvalue reference syntax for a function parameter of whose type is a function template parameter, the reference is actually a forwarding reference. Example: template<class T> void f(T&& arg) // parameter is T& or T&& depending on the supplied argument Important details: - it works only for non const references - the reference type has to be a function template argument, not a class template argument auto forwarding reference \u00b6 When we assign to `auto&&, it is a forwarding reference, not rvalue reference: auto&& a = f() // both type and value category depends on the return value of f() for(auto&& a: g(){ // same } Arrays \u00b6 cppreference There are two types of arrays: - static , i.e., their size is known at compile type, and - dynamic , the size of which is computed at runtime We can use the array name to access the first elemnt of the array as it is the pointer to that element. Static arrays \u00b6 Declaration: int a[nrows]; int a[nrwows][ncols]; // 2D int a[x_1]...[x_n]; // ND Initialization: int a[3] = {1, 2, 5} int b[3] = {} // zero initialization int c[3][2] = {{1,5}, {2,9}, {4,4}} // 2D int d[] = {1,5} // we can skip dimensions if their can be derived from data Note that the multi-dimensional syntax is just an abstraction for the programmers. The following code blocks are therefore equivalent: Matrix syntax const int rowns = 5; const int cols = 3; int matrix[rows][cols]; int main(){ for(int n = 0; n < rows; ++n){ for(int m = 0; m < cols; ++m){ Table[n][m] = (n + 1) * (m + 1); } } } Flat syntax const int rowns = 5; const int cols = 3; int matrix[rows * cols]; int main(){ for(int n = 0; n < rows; ++n){ for(int m = 0; m < cols; ++m){ Table[n * cols + m] = (n + 1) * (m + 1); } } } Using the matrix syntax adds the possibility to access the element of the array using multiple dimensions. But the underlying memory is the same. Dynamic arrays \u00b6 Declaration: int* a = new int[size] For multiple dimensions, this syntax does not scale, i.e, only one dimension can be dynamic: int(*a)[4] = new int[rows][4] // static column count int(*b)[cols] = new int[rows][cols] // does not compile unless cols is a constant! Array to pointer implicit conversion \u00b6 When we use the array name in an expression, it can be implicitly converted to a pointer to the first element of the array. This is true for both static and dynamic arrays. Example: int a[3] = {1, 2, 5} int* ptr = a; // ptr points to the first element of a This implicit conversion is called array-to-pointer decay . Mutli-dimensional dynamic arrays \u00b6 To simulate multi-dimensional dynamic arrays, we have two options: - use the flat syntax, as demonstrated on static arrays - use aray of pointers to arrays Method Pros Cons Flat Syntax Fast: single continuous allocations different access syntax than static 2D arrays Array of pointers Slow: one allocation per row, unrelated memory addresses between rows same access syntax as static 2D arrays Flat array \u00b6 int* a = new int[rows * cols] Then we can access the array as: a[x * cols + y] = 5 Array of pointers to array \u00b6 Declaration and Definition int** a= new int*[rows] for(int i = 0; i < rows; ++i){ a[i] = new int[cols] } Access is than like for static 2D array: a[x][y] = 5 . This works because the pointers can be also accessed using the array index operator ( [] ). In other words, it works \"by coincidence\", but we have not created a real 2D array. Auto dealocation of dynamic arrays \u00b6 We can replace the error-prone usage of new and delete by wraping the array into unique pointer: std:unique_ptr<int[]> a; a = std::make_unique<int[]>(size) References and Pointers to arrays \u00b6 cppreference The pointer to array is declared as <type> (*<pointer_name>)[<size>] : int a[5]; int (*ptr)[5] = &a; Analogously, the reference to array is declared as <type> (&<reference_name>)[<size>] : int a[5]; int (&ref)[5] = a; Function Type \u00b6 A function type consist from the function arguments and the return type. The function type is written as return_type(arg_1_type, ..., arg_n_type) . Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo), int(double, double)>) // TRUE Reference to Function and Pointer to Function Types \u00b6 cppreference A refrence to function has a type return_type(&)(arg_1_type, ..., arg_n_type) . Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo)&, int(&)(double, double)>); // TRUE A pointer to function has a type: return_type(*)(arg_1_type, ..., arg_n_type) Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo)*, int(*)(double, double)>); // TRUE Complete and Incomplete Types \u00b6 In many context, we have to supply a type with a requirement of being a complete type. So what types are incomplete? - The void type is always incomplete - Any structure without definition (e.g. using struct structure *ps; , without defining structure .) - An array without dimensions is an incomplete type: int a[]; is incomplete, while int a[5]; is complete. - An array of incomplete elements is incomplete. A type trait that can be used to determine whether a type is complete is described here . Aggregate types \u00b6 Aggregate types are: - array types - class types that fullfill the following conditions - no private or protected members - no constructores declared (including inherited constructors) - no private or protected base classes - no virtual member functions The elements of the aggregate types can and are ment to be constructed using the aggregate initialization (see the local variable initialization section). Type Conversion \u00b6 cppreference: implicit conversion In some context, an implicit type conversion is aplied. This happens if we use a value of one type in a context that expects a different type. The conversion is applied automatically by the compiler, but it can be also applied explicitly using the static_cast operator. In some cases where the conversion is potentially dangerous, the static_cast is the only way to prevent compiler warnings. Numeric Conversion \u00b6 There are two basic types of numeric conversion: - standard implicit conversion that can be of many types: this conversion is applied if we use an expression of type T in a context that expects a type U . Example: ```cpp void print_int(int a){ std::cout << a << std::endl; } int main(){ short a = 5; print_int(a); // a is implicitly converted to int } ``` usual arithmetic conversion which is applied when we use two different types in an arithmetic binary operation. Example: cpp int main(){ short a = 5; int b = 2; int c = a + b; // a is converted to int } Implicit Numeric Conversion \u00b6 Integral Promotion \u00b6 Integral promotion is a coversion of an integer type to a larger integer type. The promotion should be safe in a sense that it never changes the value. Important promotions are: - bool is promoted to int : false -> 0 , true -> 1 Integral Conversion \u00b6 Unlike integral promotion, integral conversion coverts to a smaller type, so the value can be changed. The conversion is safe only if the value is in the range of the target type. Important conversions are: Usual Arithmetic Conversion \u00b6 cppreference This conversion is applied when we use two different types in an arithmetic binary operation. The purpose of this conversion is convert both operands to the same type before the operation is applied. The result of the conversion is then the type of the operands. The conversion has the following steps steps: 1. lvalue to rvalue conversion of both operands 1. special step for enum types 1. special step for floating point types 1. conversion of both operands to the common type The last step: the conversion of both operands to the common type is performed using the following rules: 1. If both operands have the same type, no conversion is performed. 1. If both operands have signed integer types or both have unsigned integer types, the operand with the type of lesser integer conversion rank (size) is converted to the type of the operand with greater rank. 1. otherwise, we have a mix of signed and unsigned types. The following rules are applied: 1. If the unsigned type has conversion rank greater or equal to the rank of the signed type, then the unsigned type is used. 1. Otherwise, if the signed type can represent all values of the unsigned type, then the signed type is used. 1. Otherwise, both operands are converted to the unsigned type corresponding to the signed type (same rank). Here especially the rule 3.1 leads to many unexpected results and hard to find bugs. Example: int main(){ unsigned int a = 10; int b = -1; auto c = b - a; // c is unsigned and the value is 4294967285 } To avoid this problem, always use the static_cast operator if dealing with mixed signed/unsigned types . Show the Type \u00b6 Sometimes, it is useful to print the type, so that we can see the real type of some complicated template code. For that, the following template can be used: #include <string_view> template <typename T> constexpr auto type_name() { std::string_view name, prefix, suffix; #ifdef __clang__ name = __PRETTY_FUNCTION__; prefix = \"auto type_name() [T = \"; suffix = \"]\"; #elif defined(__GNUC__) name = __PRETTY_FUNCTION__; prefix = \"constexpr auto type_name() [with T = \"; suffix = \"]\"; #elif defined(_MSC_VER) name = __FUNCSIG__; prefix = \"auto __cdecl type_name<\"; suffix = \">(void)\"; #endif name.remove_prefix(prefix.size()); name.remove_suffix(suffix.size()); return name; } Usage: std::cout << type_name<std::remove_pointer_t<typename std::vector<std::string>::iterator::value_type>>() << std::endl; // Prints: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > Source on SO Standard Library Types \u00b6 Smart Pointers \u00b6 For managing resources in dynamic memory, smart pointers (sometimes called handles ) should be used. They manage the memory (alocation, dealocation) automatically, but their usage requires some practice. There are two types of smart pointers: - std::unique_ptr for unique ownership - std::shared_ptr for shared ownership Creation \u00b6 Usually, we create the pointer together with the target object in one call: - std::make_unique<T>(<OBJECT PARAMS>) for unique pointer - std::make_shared<T>(<OBJECT PARAMS>) for shared pointer These methods work well for objects, but cannot be used for arbitrary array initialization (only the empty/zero-initialized array can be created using these methods). For arbitrary array initialization, we need to use the smart pointer constructor: std::unique_ptr<int[]> ptr(new int[]{1, 2, 3}); Counter-intuitively, smart pointers created using the empty constructor of the respective pointer type does not default-construct the target object, but initialize the pointer to null instead: std::unique_ptr<My_class> ptr(std::null_ptr); // ptr is null std::unique_ptr<My_class> ptr(); // ptr is also null Shared Pointer \u00b6 Pointer to object with non-trivial ownership (owned by multiple objects). std::reference_wrapper \u00b6 cppreference Reference wrapper is a class template that can be used to store references in containers or aggregated objects. The disintinction from normal references is that the reference wrapper can be copied and assigned, so it does not prevent the copy/move operations on the object it belongs to. Otherwise, it behaves like a normal reference: it has to be assigned to a valid object and it cannot be null. Strings \u00b6 In C++, there are two types of strings: - std::string is an owning class for a string. - std::string_view is a non-owning class for a string. Also, there is a C-style string ( char* ), but it is not recommended to use it in modern C++. The difference between std::string and std::string_view is best explained by a table below: std::string std::string_view Owning Yes No Null-terminated Yes No Size Dynamic Static Lifetime Managed by the string Managed by the underlying char sequence Can be constexpr No Yes and the following code: std::string_view sv = \"hello\"; // sv is a view of the string literal \"hello\" std::string s = \"hello\"; // s stores a copy of the string literal \"hello\" String Literals \u00b6 The standard string literal is writen as \"literal\" . However, we need to escape some characters in such literals, therefore, a raw string literal is sometimes more desirable: R\"(literal)\" If our literal contains ( or ) , this is stil not enough, however, the delimiter can be extended to any string with a maximum length of 16 characters, for example: R\"lit(literal)lit\" . Formatting strings \u00b6 The usage of modern string formating is either - std::format from the <format> header if the compiler supports C++20 string formatting ( compiler support ) or - fmt::format from the fmt library if not. Either way, the usage is the same: format(<literal>, <arguments>) where the literal is a string literal with {} placeholders and the arguments are the values to be inserted into the placeholders. The placeholders can be filled width - argument identification, if we want to use the same argument multiple times or change the order in the string while keep the order of arguments in the function call or - format specification. These two parts are separated by : , both of them are optional. The most common format specifications are: - data type: - d for decimal integer - f for floating point number - s for string - width and precision, in the format <width>.<precision> . Both values can be dynamic: std::format(\"{:{}.{}f}\", a, b, c) formats a float number a with width b and precision c . The formating reference can be found in the cppreference Spliting the string into tokens \u00b6 If we want to split the string on patern, the easiest way is to use the split view from the ranges library: auto parts = std::ranges::views::split(str, \"-\"); Converting string to int \u00b6 There are simple functions for converting std::string to numbers, named std::stoi , std::stoul , etc. See cppreference for details. For C strings, the situation is more complicated. Substring \u00b6 A substring can be obtained using a member function substr : str.substr(str.size() - 1, 1)) // returns the last character as a string change the case \u00b6 Unfortunatelly, the STL has case changing functions only for characters, so we need to iterate over the string ourselfs. The boost has a solution, however: #include <boost/algorithm/string.hpp> auto upper = boost::to_upper(str); Date and time \u00b6 The date and time structure in C++ is std::tm. We can create it from the date and time string using std::get_time function: std::tm tm; std::istringstream ss(\"2011-Feb-18 23:12:34\"); ss >> std::get_time(&tm, \"%Y-%b-%d %H:%M:%S\"); Collections \u00b6 std::array std::vector Sets \u00b6 Normal set collection for C++ is std::unordered_set . By default, the set uses a Hash , KeyEqual and Allocator template params provided by std functions. However, they need to exist, specifically: - std::hash<Key> - std::equal_to<Key> - std::allocator<Key> So either those specializations needs to be provided by the snadard library (check cppreference), or you have to provide it. Providing custom hash function \u00b6 There are two options for providing custom hash function for a type T s: - implementing an explicit specialization of the template function std::hash<T> - providing the Hash template param when constructing the hash The first method is prefered if we want to provide a default hash function for some type for which there is no hash function specialization in the standard library. The second method is prefered only when we want some special hash function for a type T for which std::hash<T> is already defined. Implementing custom hash function \u00b6 First check whether the hash function is not provide by STL on cppreference . Then, many other hash specializations are implemented by boost, check the reference . If there is no implementation, we can implement the hash function as follows (example for set): template<> struct std::hash<std::unordered_set<const Request*>> { size_t operator()(const std::unordered_set<const Request*>& set) const { std::hash<const Request> hash_f; size_t sum{0}; for (const Request* r : set) { sum += hash_f(*r); } return sum; } }; Important implementation details: - the function needs to be implemented inside std or annonymous namespace, not inside a custom namespace - do not forget to add template<> above the function, this indicates that it is a template specialization. Maps \u00b6 The maps has similar requiremnts for keys as the requirements for set value types (see previous section). The hash map type is called std::unordered_map . Geeting value by key \u00b6 To access the map element, the array operator ( [] ) can be used. Note however, that this operator does not check the existence of the key, even if we do not provide a value. Example: std::unordered_map<int,std::string> map; map[0] = \"hello\" map[0] = \"world\" // OK, tha value is overwritten a = map[1] // a == map[1] == \"\" unintuitively, the default value is inserted if the key does not exist Therefore, if we just read from the map, it is safer to use the at() member function. Inserting into map \u00b6 There are three options: 1. map[key] = value; or 2. map.insert({key, value}); 3. map.emplace(key, value); There are some considerations with these options: - 1 inserts the value into the map even if the key already exists, overwriting the previous value. 2 and 3 do not overwrite the new value, instead, they return the position in the map and the indicator of success ( true if the insertion happend). - 1 requires the value to be default constructible and assignable - 3 avoids the creation of temporary objects, it sends references to key and value directly to the map. Tuples \u00b6 We have two standard class templates for tuples: - std::pair for pairs - std::tuple for tuples with unlimited size Although named differently, these class templates behaves mostly the same. Creating tuples \u00b6 There are two ways of creating a tuple: - constructor ( auto p = std::pair(...) ) - initializer ( auto p = {} ) Beware that by default , the deduced types are decayed, i.e., const and references are removed and the tuple stores value types . If you need to store the reference in a tuple, you have to specify the type: auto p = std::pair<int, constr std::string&>(...) Also, beware that the RVO does not apply for tuple members. This means that if we store values types in the tuple, the types are copied/moved, and in conclusion, they have to by copyable/movable! This is the reason why we frequently use smart pointers in tuples even though we would reurn directly by value if we returned a single value. Creating tuples with std::make_pair / std::make_tuple \u00b6 TLDR: from C++17, there is no reason to use make_pair / make_tuple . There are also factory methods make_pair / make_tuple . Before C++17, argument deduction did not work for constructors, so there is a dedicated method for creating tuples. However, now we can just call the constructor and the template arguments are deduced from the constructor arguments. Also, the make_pair / make_tuple functions can only produce tuples containing values, not references (even if we specify the reference type in the make_pair / make_tuple template argument, the returned tuple will be value-typed). Accessing tuple members \u00b6 The standard way to access the tuple/pair mamber is using the std::get function: auto tuple = std::tuple<int, std::string, float>(0, \"hello\", 1.5); auto hello = std::get<1>(tuple); Structured binding - unpacking tuples into variables \u00b6 If we don't need the whole tuple objects, but only its members, we can use a structured binding . Example: std::pair<int, int> get_data(); void main(){ const auto& [x, y] = get_data(); } Unpacking tuples to constructor params with std::make_from_tuple \u00b6 We cannot use structured binding to unpack tuple directly into function arguments. For normal functions, this is not a problem, as we can first use structured binding into local variables, and then we use those variables to call the function. However, it is a problem for parent/member initializer calls, as we cannot introduce any variables there. Luckily, there is a std::make_from_tuple template function prepared for this purpose. Example: std::tuple<int,float> get_data(){ ... } class Parent{ public: Parent(int a, float b){...} { class Child: public Parent{ public: Child(): Parent(std::make_from_tuple<Parent>(get_data())){} } std::optional \u00b6 cppreference std::optional<T> is a class template that can be used to store a value of type T or nothing. The advantage over other options like null pointers or is that the std::optional is a value type, so it can wrap stack objects as well. The type T must satisfy std::is_move_constructible_v<T> (must be either movable or copyable). The usage is easy as the class has a value constructor from T and a default constructor that creates an empty optional. Also, the type T is convertible to std::optional<T> , and std::nullopt is convertible to an empty optional. Finally, std::optional<T> is convertible to bool , so it can be used in if statements. A typical usage is: class My_class{ public: My_class(int a, int b); } std::optional<My_class> f(){ ... return My_class(a, b); // or return {a, b}; // or, in case of fail return std::nullopt; } std::optional<int> a = f(); if(a){ // a has a value } Value Categories \u00b6 [cppreferencepreerecege/value_category). In many contexts, the value category of an expression is important in deciding whether the code compiles or not, or which function or template overload is chosen. Therefore, it is usefull to be able to read value categories. expression value types: - lvalue , meaning left-value. An expression typically on the left side of compound expression a statement, e.g. variable, member, or function name. Also, lvalues expressions are are: - function ratoalls to fuctions returning lvalue - assignments - ++a , --a and similar pre operators - *a indirection - string literal - cast - prvalue , meaning pure rvalue. It is either a result of some operand ( + , / ) or a constructor/initializer result. The foloowing expressions are prvalues: - literals with exception of string literals, e.g.: 4 , true , nullptr - function or operator calls that return rvalue (non-reference) - a++ , a-- and other post operators - arithmetic and logical expressions - &a address of expression - this - non-type template parameters, unless they are references - lambda expressions - requires expressions and concept spetializations - xvalue , meaning expiring value. These valaues usually represent lvalues converted to rvalues. Xvalue expressions are: - function call to functions returning rvalue reference (e.g., std::move ). - member object expression ( a.m ) if a is an rvlaue and m is a non-reference type - glvalue = lvalue || xvalue . - rvalue = prvlaue || xvalue . Operators \u00b6 C++ supports almost all the standard operators known from other languages like Java or C#. Note that the standard also supports alternative tokens for some operators (e.g., && -> and , || -> or , ! -> not ). However, these are not supported by all compilers. In MSVC, the /permissive- flag needs to be used to enable these tokens. User-defined Operators \u00b6 In C++ there are more operators than in other popular es like Python or Java. Additionally, these operators can be overloaded. See cppreferencen page for detailed description. Comparison Operators \u00b6 Deafult Comparison Operators \u00b6 For details, see cppreference . The != is usually not a problem, because it is implicitely generated as a negation of the == operator. However, the == is not generated by default, even for simple classes . To force the generation of a default member-wise comparison operator, we need to write: bool operator==(const My_class&) const == default; However, to do that, all members and base classes have to ae the operator == defined, otherwise the default operator will be implicitely deleted. Functions \u00b6 cppreference Deciding between free function, member function and static member function \u00b6 Basically, you should decide as follows: 1. Function needs access to instance -> member function 2. Function - should be called only by class members (i.e., member functions), so we want to limit its visibility, or - we need to access static members of the class -> static member function 1. Otherwise -> free function Argument-parameter Conversions \u00b6 Arg/param value reference rvalue value - - std::move reference implicit copy - copy constructor rvalue - not possible - Default Parameters \u00b6 Default function parameters in C++ works similarly to other languages: int add(int a, int b = 10); add(1, 2) // 3 add(1) // 11 However, the default parameters works only if we call the function by name. Therefore, we cannot use them in std::function and similar contexts. Example: std::function<int(int,int)> addf = add; std::function<int(int)> addf = add; // does not compile addf(1) // does not compile Default Parameters and Inheritance \u00b6 TLDR: do not use default parameters in virtual functions. The default parameters are resolved at compile time. Therefore, the value does not depend on the actual type of the object, but on the declared type of the variable. This have following consequences: - the default parameters are not inherited - A* a = new B(); a->foo() will call B::foo() , with the default parameters of A::foo() To prevent confusion with inheritence we should use function overloading instead of default parameters in virtual functions (like in Java). Return values and NRVO \u00b6 For deciding the return value format, refer to the return value decision tree . Especially, note that NRVO is used in modern C++ and therefore, we can return all objects by value with no overhead most of the time. The NRVO works as follows: 1. compiler tries to just tranfer the object to the parent stack frame (i. e. to the caller) without any move or copy 2. if the above is not possible, the move constructor is called. 3. if the above is not possible, the copy constructor is called. From C++17, the RVO is mandatory, therefore, it is unlikely that the compiler use a move/copy constructor. Consequently, most of the times, we can just return the local variable and let the rest to the compiler: unique_ptr<int> f(){ auto p = std::make_unique<int>(0); return p; // works, calls the move constructor automatically in the worst case (pre C++17 compiler) // return move( p ); // also works, but prevents NRVO } The NRVO is described also on cppreference together with initializer copy elision. Function Overlaoding \u00b6 Both normal and member funcions in C++ can be overloaded. The oveload mechanic, however, is quite complicated. There can be three results of overload resolution of some function call: - no function fits -> error - one function fits the best - multiple functions fits the best -> error The whole algorithm of overload resolution can be found on cppreference . First, viable funcions are determined as functions with the same name and: - with the same number of parameters - with a greater number of parameters if the extra parameters has default arguments If there are no viable functions, the compilation fails. Otherwise, all viable functions are compared to get the best fit. The comparison has multiple levels. The basic principle is that if only one function fits the rules at certain level, it is chosen as a best fit. If there are multiple such functions, the compilation fails. Levels: 1. Better conversion priority (most of the time, the best fit is found here, see conversion priority and ranking bellow) non-template constructor priority Conversion prioritiy and ranking \u00b6 cppreference When the conversion takes priority during the best viable function search, we say it is better . The (incomplete) algorithm of determining better conversion works as follows: 1. standard conversion is better than user defined conversion 2. user defined conversion is better then elipsis ( ... ) conversion 3. comparing two standard conversions: 1. if a conversion sequence S1 is a subsequence of conversion sequence S2, S1 is better then S2 2. lower rank priority 3. rvalue over lvalue if both applicable 4. ref over const ref if both applicable Conversion sequence ranks \u00b6 exact match promotion conversion : includes class to base conversion Constructor argument type resolution in list initialization \u00b6 When we use a list initailization and it results in a constructor call, it is not immediatelly clear which types will be used for arguments as the initialization list is not an expression. These types are, however, critical for the finding of best viable constructor. The following rules are used to determine the argument types (simplified): 1. auto return type \u00b6 For functions that are defined inside declaration (template functions, lambdas), the return type can be automatically deduced if we use the auto keyword. The decision between value and reference return type is made according to the following rules: - return type auto -> return by value - return type auto& -> return by reference - return type auto* -> return by pointer - return type decltyype(auto) -> the return type is decltype(<RETURN EXPRESSION>) See more rules on cppreference Note that the auto return type is not allowed for functions defined outside the declaration (unless using the trailing return type). Function visibility \u00b6 The member function visibility is determined by the access specifier, in the same manner as the member variable visibility. For free functions , the visibility is determined by the linkage specifier . Without the specifier, the function is visible. To make it visible only in the current translation unit, we can use the static specifier. An equivalent way to make a function visible only in the current translation unit is to put it into an anonymous namespace : namespace { void f() {} } This way, the function is visible in the current translation unit, as the namespace is implicitly imported into it, but it is not visible in other translation units, because anonymous namespaces cannot be imported. One of the other approches frequently used in C++ is to put the function declaration into the source file so it cannot be included from the header. This solution is, however, flawed, unsafe, and therefore, not recommended . The problem is that this way, the function is still visible to the linker, and can be mistakenly used from another translation unit if somebody declare a function with the same name. Usefull STL functions \u00b6 std::for_each : iterates over iterable objects and call a callable for each iteration std::bind : Binds a function call to a variable that can be called some parameters of the function can be fixed in the variable, while others can be provided for each call each reference parameter has to be wrapped as a reference_wrapper std:mem_fn : Creates a variable that represents a callable that calls member function std::copy : Copy elements from one range to another. std::accumulate : computes the sum of some iterable std::transform : transforms some range and stores it to another. For the output iterator, you can use std::back_inserter . Deleting functions \u00b6 cppreference We can delete functions using the delete keyword. This is mostly used for preventing the usage of copy/move constructors and assignment operators. However, it can be used for any function, as we illustrate in the following example: class My_class{ print_integer(int a){ std::cout << a << std::endl; } // we do not want to print doubles even they can be implicitly converted to int print_integer(double a) = delete; } Classes and structs \u00b6 The only difference between a class and a struct is that in class, all members are private by default. Friend declaration \u00b6 Sometimes, we need to provide an access to privat e members of a class to some other classes. In java, for example, we can put both classes to the same package and set the members as package private (no specifier). In C++, there is an even stronger concept of friend classes. We put a friend declaration to the body of a class whose private members should be accessible from some other class. The declaratiton can look as follows: Class To_be_accesssed { friend Has_access; } Now the Has_access class has access to the To_be_accesssed 's private members. Note that the friend relation is not transitive, nor symetric, and it is not inherited. cppreference Template friends \u00b6 If we want a template to be a friend, we can modify the code above: class To_be_accesssed { template<class T> friend class Has_access; } Now every Has_access<T> is a friend of To_be_accesssed . Note thet we need to use keyword class next to friend . We can also use only a template spetialization: class To_be_accesssed { friend class Has_access<int>; } or we can bound the allowed types of two templates togehter if both Has_access and of To_be_accesssed are templates: template<class T> class To_be_accesssed { friend class Has_access<T>; } Initialization and Assignment \u00b6 Loacal variables initialization/assignment \u00b6 Initialization happens in many contexts : - in the declaration - in new expression - function parameter initialization - return value initialization The syntax can be: - (<expression list>) - = expression list - {<initializer list>} Finally, there are multiple initialization types, the resulting initialization type depends on both context and syntax: - Value initialization: std::string s{}; - Direct initialization: std::string s{\"value\"} - Copy initialization: std::string s = \"value\" - List initialization: std::string s{'v', 'a', 'l', 'u', 'e'} - Aggregate initialization: char a[3] = {'a', 'b'} - Reference initialization: char& c = a[0] - Default initialization: std::string s List initialization \u00b6 List initialization initializes an object from a list. he list initialization has many forms, including: - My_class c{arg_1, arg_2} - My_class c = {arg_1, arg_2} - my_func({arg_1, arg_2}) - return {arg_1, arg_2} The list initialization of a type T can result in various initializations/constructios depending on many aspects. Here is the simplified algorithm: 1. T is aggregate -> aggregate initialization 2. The initializer list is empty and T has a default constructor -> value initialization 3. T has an constructor accepting std::initializer_list -> this constructor is called 4. other constructors of T are considered, excluding explicit constructors Value initialization \u00b6 cppreference This initializon is performed when we do not porvide any parameters for the initialization. Depending on the object, it results in either defualt or zero initialization. Aggregate initialization \u00b6 Aggregate initialization is an initialization for aggregate types. It is a form of list initialization. Example: My_class o1{arg_1, arg_2}; My_class o2 = {arg_1, arg_2}; // equivalent The list initialization of type T from an initializer list results in aggregate initialization if these conditions are fullfilled: - the initializer list contains more then one element - T is an aggregate type Nested initialization \u00b6 It is not possible to create nested initializatio statements like: class My_class{ int a, float b; public: My_class(ina a, float b): a(a), b(b) } std::tuple<int, My_class>{2, {3, 2.5}} // does not compile std::tuple<int, My_class>{2, My_class{3, 2.5}} // correnct version Member Initialization/Assignment \u00b6 There are two ways of member initialization: - default member initialization - initialization using member initializer list And then, there is an assignment option in constructor body . Reference: - default member initialization - constructor and initializer list One way or another, all members should be initialized at the constructor body at latest , even if we assign them again during all possible use cases. Reason: - some types can have arbitrary values when unassigned. This can lead to confusion when debugging the class, i.e., the member can appear as initialized even if it is not. - easy support for overloading constructors, we can sometimes skip the call to the constructor with all arguments - we can avoid default arguments in the constructor It is important to not use virtual functions in member initialization or constructor body , because the function table is not ready yet, so the calls are hard wired, and the results can be unpredictable, possibly compiler dependent. Default Member Initialization \u00b6 Either a brace initializer : My_class{ int member{1} } or an equals initializer : My_class{ int member = 1 } Member Initializer List \u00b6 Either using direct initialization (calling constructor of member ): My_class{ My_class(): member(1){ } or list initialization : My_class{ My_class(): member{1}{ } Constructor Body \u00b6 My_class{ My_class(){ member = 1 } } Comparison Table \u00b6 Ordered by priority, i.e., each method makes the methods bellow ignored/ovewritten if applied to the same member. Type | In-place | works for const members --| --|-- Constructor body | no | no Member initializer list | yes | yes Default member initializer | yes, if we use direct initialization | yes Constructors and Special Member Functions \u00b6 cppreference Special member functions are member functions that are someetimes defined implicitely by the compiler. The special member functions are: - default (no parameter) constructor - copy Cconstructor - copy sssignment - move constructor - move assignment - destructor Along with the comparison operators, these are the only functions that can be defaulted (see below). Constructor \u00b6 Defualt Variant \u00b6 The default constructor just create an empty object. The default constructor is not implicitly generated if: - there is anothe constructor declared, including copy and move constructor - there is some member that cannot be defaulty initialized Explicit constructor \u00b6 Sometimes, a normal constructor can lead to unexpected results, especially if it has only a single argument: class My_string { public: String(std::string string); // convert from std::string String(int length); // construct empty string with a preallocated size }; String s = 10; // surprise: empty string of size 10 istead of \"10\" To prevent these surprising conversion, we can mark the constructor explicit . The explicit keyword before the constructor name prevents the assigment using this constructor. The explicit constructor has to be explicitelly called. Copy Constructor \u00b6 cppreference A copy constructor is called if an object is initialized from another object unless the move constructor is called as a better fit or the call is optimized out by copy elision . Some examples: - initializing a new object from an existing object: My_class a; My_class b = a; // copy constructor called My_class c(a); // copy constructor called passing an object to a function by value: void f(My_class a){...} My_class a; f(a); // copy constructor called returning an object by value where the type is not movable and the compiler cannot optimize the call out. we call the copy constructor directly Implicit declaration and implicit deletion \u00b6 The copy constructor for type T is implicitely-declared if T has no declared user-defined copy constructors. If some there are some user-defined copy constructors, we can still force the implicit declaration of the copy constructor using the default keyword However, the implicit declaration does not mean that the copy constructor can be used! This is because the copy constructor can be implicitely defined as deleted . This happens if any of the following conditions is true: 1. T has a non-static data member that cannot be copied. This can happen if any of the following is true: - it has a deleted copy constructor, - the copy constructor is inaccessible ( protected, private ) - the copy constructor is ambiguous (e.g., multiple inheritance) 1. T has a base class that cannot be copied, i.e., 1, 2, or 3 applies to at least one base class 1. T has a non-static data member or base class with inaccessible destructor 1. T has a rvlaue data member 1. T has a user-defined move constructor or move assignment operator (this rule does not apply for defaulted copy constructor) The default implementationof copy constructor calls recursively the copy constructor of all base classes and on all members. For a pointer member, the copy object\u2019s member points to the same object as the original object\u2019s member Checking if a class is copy constructible \u00b6 We can check if a class is copy constructible using the std::is_copy_constructible type trait. Copy Assignment \u00b6 Copy Assignment is needed when we use the = operator with the existing class instances, e.g.: Class instanceA {}; Class instanceB; instanceB = instance A Move Constructor \u00b6 cppreference Move constructor semantic is that the new object takes the ownership of the resources of the old object. The state of the old object is unspecified, but it should not be used anymore. Move constructor is typically called when the object is initaialized from xvalue (but not prvalue!) of the same type. Examples: - returning xvalue: Type f(){ Type t; return std::move(t); } passing argument as xvalue: f(Type t){ ... } Type t f(std::move(t)); initializing from xvalue: Type t; Type t2 = std::move(t); Note that for prvalues, the move call is eliminated by copy elision . Therefore, some calls that suggest move constructor call are actually optimized out: Type f(){ Type t; return t; // no move constructor call, copy elision } Type t = T(f()) // no move constructor call, copy elision Move constructor is needed: - to cheaply move the object out from function if RVO is not possible - to store the object in vector without copying it Note that a single class can have multiple move constructors, e.g.: both Type(Type&&) and Type(const Type&&) . Implicit declaration and implicit deletion \u00b6 The move constructor for type T is implicitely-declared if T has no declared copy constructors, copy assignment operators, move assignment operators, or destructors. If some of the above is declared, we can still force the implicit declaration of the move constructor using the default keyword However, that does not mean that the move constructor can be used! This is because the move constructor can be implicitely defined as deleted . This happens if any of the following conditions is true: 1. T has a non-static data member that cannot be moved. A member cannot be moved if any of the following is true: - it has a deleted, inaccessible (protected, private), or ambiguous move constructor, - it is a reference, - it is const -qualified 1. T has a base class that cannot be moved, i.e., 1, 2, or 3 applies to at least one base class 1. T has a non-static data member or base class with inaccessible destructor Checking if a class is move constructible \u00b6 We can check if a class is move constructible using the std::is_move_constructible type trait. However, the std::is_move_constructible does not check if the move constructor is accessible! Instead it checks if the call to the move constructor is valid (can success, compiles). The call can success if the move constructor is accessible, but it can also success if it is not accessible, but the class has a copy constructor, which is used instead. To check if the move constructor is accessible, we have to manually check the conditions, or disable the copy constructor. Move Assignment \u00b6 Trivial special member functions \u00b6 The special member functions are called trivial if they contain no operations other then copying/moving the members and base classes. For a special member function of type T to be trivial, all of the following conditions must be true: - it is implicitly-declared or defaulted - T has no virtual functions - T has no virtual base classes - the constructor for all direct base classes is trivial - the constructor for all non-static data members is trivial Destructor \u00b6 We need destructor only if the object owns some resources that needs to be manually deallocated Setting special member functions to default \u00b6 Rules \u00b6 if you want to be sure, delete everything you don\u2019t need most likely, either we need no custom constructors, or we need three (move and destructor), or we need all of them. Rules for Typical Object Types \u00b6 Simple Temporary Object \u00b6 the object should live only in some local context we don\u2019t need anything Unique Object \u00b6 usually represents some real object usually, we need constructors for passing the ownership: move constructor move assignment noe sin Default Object \u00b6 copyable object We need copy constructor copy assignment move constructor move assignment Operators \u00b6 In C++ there are more operators than in other popular es like Python or Java. Additionally, thsese operator s can be overloaded. See cppreferencen page for detailed description. Comparison Operators \u00b6 Deafult Comparison Operators \u00b6 For details, see cppreferencetassnpneolan . The != is usually not a problem, because it is implicitely generated as a negation of the == operator. However, the == is not generated by default, even for simple classes . To force the generation of a default member-wise comparison operator, we need to write: bool operator==(const My_class&) const = default; Mowever, to do that, all members and base classes have to have the operator == defined, otherwise the default operator will be implicitely deleted. The comparability can be checked with a std::equality_comparable<T> concept ic_assert(std::equality_comparable<My_class>); Const vs non-const \u00b6 The const keyword makes the object non-mutable. This means that: - it cannot be reassigned - non-const member functions of the object cannot be called The const keyword is usually used for local variables, function parameters, etc. For members, the const keyword should not be used , as it sometimes breaks the move operations on the object. For example we cannot move f om a const std::unique_ptr<T> object. While this is also true for local variable, in members, it can lead to hard to find compilation errors, as a single const std::unique_ptr<T> member deep in the object hierarchy breaks the move semantic for the whole class and all subclasses. Avoiding duplication between const and non-const version of the same function \u00b6 To solve this problem without threathening the const-correctness, we need to implement the const version of a function and call it from the non-const one with double type cast: - one that converts this to const, so we can call the const version of the function - another one that removes const from the return value Example: const Content& get_content(unsigned index) const { Content content = ... // complicated code to get the right content return content; } Content& get_content(unsigned index){ return const_cast<Content&>(std::as_const(this*).get_content()); } there are no common supercalss or i## Const/non const overloads and inheritance Normally, the compiler can safely choose the best match between const and non-const overloads. The problem can happen when each version is in a different place in the class hierarchy. Example: class Base { public: const int& get() const { return some; } protected: int some; }; class A : public virtual Base { public: int& get() { return some; } }; class B : public A {}; B test; test.get(); // ambiguous function error The problem is that the overload set is created for each class in the hierarchy separately. So if the overload was resolved prior the virtual function resolution, we would have only one version (non-const), which would be chosen, despite not being the best overload match in both overload sets. To prevent such unexpected result, some compilers (GCC) raise an ambiguous function error in such situations. To resolve that, we can merge the overload sets in class B : class B : public A { using Base:get; using A:get; }; Avoiding duplication between const and non-const version of the same function \u00b6 To solve this problem without threathening the const-correctness, we need to implement the const version of a function and call it from the non-const one with double type cast: - one that converts this to const, so we can call the const version of the function - another one that removes const from the return value Example: const Content& get_content(unsigned index) const { Content content = ... // complicated code to get the right content return content; } Content& get_content(unsigned index){ return const_cast<Content&>(std::as_const(this*).get_content()); } IO and Filesystem \u00b6 Standard IO \u00b6 The simple way to print to standard input is: std::cout << \"Hello world\" << std::endl; To return to the begining of the line and overwrite the previous output, we can use the '\\r' character: std::cout << \"Hello world\" << '\\r' << std::flush; File path manipulation \u00b6 Although we can use strings to work with file paths in C++, the standard format which is also easy to use is std::filesystem::path from the filesystem library . Basic operations: - To create a path , we jusct call std::filesystem::path(<string path>) . - We can easily join two paths by auto full_path = <path 1> / <path 2> ; - To get the asolute path , we call - std::filesystem::absolute(<path>) to get the path as CWD/<path> - std::filesystem::canonical(<path>) to get the dots resolved. Note that this method throws exception if the path does not exists. - The path to the current working directory can be obtained by calling std::filesystem::current_path() and set using std::filesystem::current_path(<path>) . - To change the file extension (in the C++ representation, not in the filesystem), we can call the replace_extension method. Filesystem manipulation \u00b6 cppreference Copying \u00b6 To copy, we can use std::filesystem::copy(<source path>, <destination path>[, <options>]) function. The options parameter type is std::filesystem::copy_options . This enum is a bitmask type, therefore, multiple options can be combined using the | operator. Example: auto options = std::filesystem::copy_options::recursive | std::filesystem::copy_options::overwrite_existing; std::filesystem::copy(\"C:/temp/data\", \"c:/data/new\", options); Note that unlike the unix cp command, the copy function does not copy the directoy itself , even if the destination directory exists. Suppose we have two direcories: - C:/temp/new - C:/data/ And we want to copy the new folder, so that the result is: C:/data/new/ . In bash, this will be: cp -r C:/temp/new C:/data/ While in C++, we need to do: std::filesystem::copy(\"C:/temp/new\", \"C:/data/new\", std::filesystem::copy_options::recursive); Creating directories \u00b6 To create a directory, we can use std::filesystem::create_directory(<path>) function. This function fails if the parent directory does not exist. To create the parent directories as well, we can use std::filesystem::create_directories(<path>) function. Removing files and directories \u00b6 To remove a file or an empty directory, we can use std::filesystem::remove(<path>) function. To remove a directory with all its content, we can use std::filesystem::remove_all(<path>) function listed on the same page of cppreference. Other useful functions \u00b6 std::filesystem::exists(<path>) std::filesystem::is_directory(<path>) std::filesystem::is_regular_file(<path>) std::filesystem::is_empty(<path>) Simple line by line IO \u00b6 Input \u00b6 For input, we can use std::ifstream : std::ifstream file; file.open(<path>); std::string line; while (std::getline(file, line)) { // do something with the line } file.close(); The important thing is that we need to check whether the open call was successful. The open function never throws an exception, even if the file does not exist , which is a common case. Instead, it only sets the failbit of the stream. Without some check, the failure is hidden as an ifstream in a fail state behaves as if it was empty. Output \u00b6 For line by line output, we use std::ofstream : std::ofstream file; file.open(<path>); batch_file << \"first line\" << std::endl; batch_file << \"second line\" << std::endl; ... batch_file.close(); csv \u00b6 Input \u00b6 Output \u00b6 For csv output, we can usually use the general line-by-line approach. Inheritance \u00b6 Inheritance in C++ is similar to other languages, here are the important points: - To enable overiding, a member function needs to be declared as virtual . Otherwise, it will be just hidden in a child with a function with the same name, and the override specifier cannot be used (see Shadowing). - Multiple inheritance is possible. - No interfaces. Instead, you can use abstract class with no data members. - Virtual functions without implementation needs = 0 at the end of the declaration (e.g.: virtual void print() = 0; ) Polymorphism \u00b6 Polymorphism is a concept for abstraction using which we can provide a single interface for multiple types that share the same parent. In C++, to use the polymorphism, we need to work with pointers or references . Imagine that we have these two class and a method that can process the base class: class Base { }; class Derived: public Base { }; void process_base(Base* base) { } Now we can use it lake this: Derived* derived = new Derived(); Base* base = derived; // we easilly can convert derived to base process_base(base); process_base(derived); // we can call the function that accepts a base pointer with a derived pointer We can do the same with smart pointers: void process_base_sh(std::shared_ptr<Base> base) { } std::shared_ptr<Derived> derived_sh = std::make_shared<Derived>(); std::shared_ptr<Base> base_sh = derived_sh; process_base_sh(base_sh); process_base_sh(derived_sh); Shadowing/Hiding: why is a function from parent not available in child? \u00b6 Members in child with a same name as another members in parent shadows those members (except the case when the parent member is virtual). When a member is shadowed/hiden, it is not available in the child class and it cannot be called using the child class instance. This can be counter-intuitive for functions as the shadowing considers only the name, not the signature . Example: class Base { public: void print() { printf(\"Base\\n\"); } }; class Child: public Base { public: void print(std::string type) { printf(\"Child \" + type + \"\\n\"); } }; int main() { Child child; child.print(); // does not compile, as the print() is hidden by print(std::string) return 0; } How to call a hidden function? \u00b6 There are two ways how to call a hideen function: 1. we can use the using declaration in the child to introduce the hidden function: c++ class Child: public Base { public: using Base::print; // now the print() is available in Child void print(std::string type) { printf(\"Child \" + type + \"\\n\"); } }; 2. Usiang a fully qualified name of the method: c++ int main() { Child child; child.Base::print(); return 0; } Constructors \u00b6 Parent constructor is allways called from a child. By default, an empty construcor is called. Alternatively, we can call another constructor in the initializer. When we do not call the parent constructor in the child's initializer and the parenhas no empty constructor, a compilation error is raised. Enablinging Parent Constructors in Child \u00b6 Implicitly, all methods from parent classes are visible in child, with exception of constructors. Constructors can be inherited manually with a using declaration, but only all at once. To enable only some constructors, we need to repeat them manually as child constructors and call parent construcors from them. Inheritance and Constructors/Destructors \u00b6 To prevent the future bugs with polymorphic destruction calls, it's a good habit to declare a public virtual destructor in each base class : class Base{ public: virtual ~Base() = default; } Otherwise, the following code will not call the child destructor: Child* child = new Child(); Base* base = (Base) child; delete base; But when defining destructor, constructor and move operations are not impliciotely generated. Moreover, the copy operations are generated enabling a polymorphic copy, which results in slicing. Therefore, the best approach for the base class is to herefore to: - declare the virtual destrucor as default - declare the default constructor . We need a default constructor, unless we use a diferent constructor and we want to disable the default one. - declare the copy and move operations as protected**. This way, the polymorpic copy is not possible, but proper copy/move operations are generated for every child class. Initializing base class members \u00b6 The base class members cannot be initialized in the child constructor initializer. Instead, we need to create a constructor in the base class and call it from the child constructor initializer. Slicing \u00b6 Polymorphism does not go well with value types. When a value type is copied, the only part that remains is the part writen in the code. That means that copying base_2 = base_1 result in a new Base object in base_2 , even if base_1 is an instance of child. Abstract classes therefore cannot be used as function value arguments at all . To pass a polymorphic type as a value to a library function, we need a copyable wrapper that forwards all calls to the undelying polymorphic type. Checking the Type \u00b6 There is no equivalent of Java's instanceof in C++. To check the type. it is possible to use dynamic cast: Child& child = dynamic_cast<Child&>(parent) In case of failure, std::bad_cast is thrown. To prevent exceptions (i.e., we need the type check for branching), we can use pointers: Child* child = dynamic_cast<Child*>(&parent) In this case, if the cast fails, then child == nullptr . Note that to use the dynamic_cast on a type, the type, the type needs to have at least one virtual method . However, this should not be an issue as the type should have at least a virtual destructor. Covariant Return Type \u00b6 Covariant return type is a concept of returning a narower type id derived class than the return type specified in base. Example: class Base { public: virtual Base& get() = 0; }; class Derived: public Base{ public: Derived& get() override { return *this; } }; It works with template classes too: template<class T> class Derived_template: public Base { public: Derived_template<T>& get() override { return &this; } }; Use Method from Parent to Override a Method from Other Parent \u00b6 Unlike in java, a parent method cannot be used to implement an interface of a child . Example: class Interface { public: virtual void print() = 0; }; class Base { public: virtual void print() { printf(\"Base\\n\"); } }; class Child: public Base, public Interface { public: }; int main() { Child child; // does not compile, as Child is an abstract class child.print(); return 0; } The above code does not compile as in C++, the parent print() method is not used as an impementation of print() from the interface (like it works e.g. in Java). There simplest solution to this problem is to override the method in Child and call the parent method staticaly: class Child: public Base, public Interface { public: void print() override { Base::print(); } }; Multiple inheritance and virtual base classes \u00b6 wiki cppreference Multiple inheritance is possible in C++. However, it can lead to some problems. Consider the following example: class A { public: int a; }; class B: public A {}; class C: public A {}; class D: public B, public C {}; It may not be obvious, but the class D has two instances of A in it. This is because the B and C both have their own instance of A . This is certainly not what we want as this way, we have two copies of A::a in D , which are only accessible using qualified names ( D::B::a and D::C::a ) and which can have different values. Virtual Inheritance \u00b6 To mitigate this problem, we can use the virtual inheritance . The virtual inheritance is used when we want to have only one instance of a base class in a child class, even if the base class is inherited multiple times. To use the virtual inheritance, we need to declare the base class as virtual in all child classes: class A { public: int a; }; class B: public virtual A {}; class C: public virtual A {}; class D: public B, public C {}; Multiple copy/move calls with virtual inheritance \u00b6 However, this solves only the problem of having multiple instances of the same base class. But there are also problems with the copy and move operations. In the above example, if the class D is copied or moved, it calls the copy/move operations of B and C , which in turn call the copy/move operations of A . This means that the A is copied/moved twice , which is not what we want. To solve this we need to manually define the copy/move operations of classes in the hierarchy so that the copy/move operations of the base class are called only once. However this can be a complex task. Also, it can backfire later when we extend the hierarchy. Other sources \u00b6 SO answer SO answer 2 Templates \u00b6 The templates are a powerful tool for: - generic programming, - zero-overhead interfaces, - and metaprogramming. Although they have similar syntax as generics in Java, they are principialy different both in the way they are implemented and in the way they are used. There are two types of templates: - function templates - class templates Syntax \u00b6 Template Declaration \u00b6 Both for classes and functions, the template declaration has the following form: template<<template parameters>> The template parameters can be: - type parameters: class T - value parameters: int N - concept parameters: std::integral T Template definition \u00b6 The definition of template functions or functions fo the template class requires the template declaration to be present. The definition has the following form: template<<template parameters>> <standard function definition> Here, the template parameters are the function template parameters if we define a template function, or the class template parameters if we define a function of a template class. If the template function is a member of a template class, we have to specify both the template parameters of the function and the template parameters of the class: template<<class template parameters>> template<<function template parameters>> <standard class function definition> Note that the template definition has to be in the header file , either directly or included from another header file. This includes the member function definitions of a template class, even if they are not templated themselves and does not use the template parameters of the class. Organization rules \u00b6 *.h : declarations *.tpp template definitions *.cpp non-template definitions. For simplicity, we include the tpp files at the end of corresponding header files. If we need to speed up the compilation, we can include the tpp files only in the source files that needs the implementations , as described on SE To speed up the build it is also desireble to move any non-template code to source files , even through inheritance, if needed. Providing Template Arguments \u00b6 A template can be instantiated only if all the template arguments are provided. Arguments can be: - provided explicitly: std::vector<int> v; or sum<int>(1,2) - deduced - from the initialization (classes): std::vector v = {1,2,3}; - from the context (functions): sum(1,2); - defaulted: template<class T = int> class A {}; template<class T = int> int sum<T>(T a, T b = 0) { return a + b; } auto s = sum(1, 2); A a(); If we want the template arguments to be deduced or defaulted, we usually use the <> : template<class T = int> class A {}; A<> a(); // default argument is used std::vector<A<>> v; // default argument is used In some cases, the <> can be ommited, e.g., when declaring a variable: A a; // default argument is used // but std::vector<A> v; // error, the A is considered a template here, not the instantiation The rules for omitting the <> are quite complex. Therefore, it is better to always use the <> when we want to use the default arguments. Rules for omitting the <> \u00b6 We can ommite the <> in the following cases: - when declaring a variable: A a; - when using the type in a function call: f(A()); - when instantiating a template class: class B: public A {}; We cannot ommite the <> in the following cases: - When we use the template as a nested type: std::vector<A<>> v; , not std::vector<A> v; - in the return type of a function: A<> f() , not A f() - When declaring an alias: using B = A<> not using B = A - for template template parameters. Default Template Arguments \u00b6 Default template arguments can be used to provide a default value for any template parameter except parameter packs. For template classes, there is a restriction that after a default argument is used, all the following parameters must have a default argument as well, except the last one wchich can be parameter pack. Template Argument Deduction \u00b6 Details on cppreference . Template argument deduction should work for: - constructors - function and operator calls - storing the function pointer Class Template Argument Deduction (CTAD) \u00b6 Details on cppreference . The main difference from the function templete argument deduction is that in CTAD, all the template arguments needs to be specified, or all must not be specified and must be deducible. Apart from that, there are more subtle differences arising of a complex procedure that is behind CTAD. We explain CTAD principle using a new concept (not a C++ concept :) ) called deduction guides . Deduction Guides \u00b6 The CTAD use so called deductione guides to deduce the template parameters. Deduction guides can be either implicit or explicit. To demonstrate the principle, let's first start with user-defined deduction guides. User defined deduction guides \u00b6 Let's have an iterator wrapper class below: template<class E, Iterator<E> I> class Iter_wrapper{ public: explicit Iter_wrapper(I iterator){ ... } ... }; Here, the argument E cannot be deduced from argument I , despite the use of the Iterator concept may indicate otherwise. We can still enable the deduction by adding the following deduction guide: template<class I> Iter_wrapper(I iterator) -> Iter_wrapper<decltype(*iterator),I>; Here, the part left from -> represents the constructor call that should be guided, and the part right from -> defines the argument types we want to deduce. Some more details about user defined deduction guides are also on the Microsoft Blog . Implicit deduction guides \u00b6 The vast majority of deduction guidedes used in CTAD are implicit. The most important implicit deduction guides are: - constructor deduction guides - copy deduction guides The copy deduction guide has the following form: template<<class template parameters>> <class>(<class><class template parameters> obj) -> <class><class template parameters>; For a simple wrapper class: template<class C> class Wrapper{ ... } template<class C> Wrapper(Wrapper<C> obj) -> Wrapper<C>; // implicitelly defined copy deduction guide The constructor deduction guides has the following form: template<<class template parameters>> <class>(<constructor arguments>) -> <class><class template parameters>; For a simple wrapper class: template<class C> class Wrapper{ Wrapper(T&& ref); } template<class C> Wrapper(C&&) -> Wrapper<C>; // implicitelly defined constructor deduction guide Deduction guides resolution \u00b6 Note that CTAD is a process independent of the constructor overload! . First an appropriate deduction guide is used to deduce the class template argumnets, this process can fail if there is no guide. Only then, the overload resolution begins. Most of the time, it is not so important and we can just look at the constructor that is chosen by the constructor overload resolution process and see the used deduction guids and consequently, the resulting template arguments. Sometimes, however, this simplified understanding can lead to confusing results: template<class C> class Wrapper{ Wrapper(T&& ref); Wrapper(double&& ref); // special overload for double } auto w1 = Wrapper(1.5) // the double overload is called In the above example, it may be surprising that the second constructor can be called, as it does not have the class argument present, so the implicit deduction guide cannot work: template<class C> Wrapper(double&&) -> Wrapper<C>; // C unknown! However, it compiles and works, because the deduction guide from the first constructor is used for CTAD, and then, the second constructor is chosen by the constructor overload. Template Specialization \u00b6 Template specialization is a way to provide a different implementation of a template for a specific type. For example, we can provide a different implementation of a template for a std::string type. Imagine following class: // declaration template<class T> class Object{ public: void print(T value) }; // definition template<class T> void Object<T>::print(T value){ std::cout << value << std::endl; } Now, we can provide a different implementation for std::string : // declaration template<> class Object{ public: void print(std::string value) }; template<> void Object<std::string>::print(std::string value){ std::cout << value << std::endl; } There are two types of template specialization: - full specialization : exact specification for all template arguments - partial specialization : exact specification for a subset of template arguments and/or non-type template arguments To demonstrate the difference, let's have a look at the following example: // declaration template<class T, class C> class Object{}; // primary template // full specialization template<> class Object<int, std::string>{}; // full specialization // partial specializations template<class C> class Object<int, C>{}; // not a full specialization, as C is not specified template<std::integral T, My_concept C> class Object<T, C>{}; // not a full specialization, types are not exactly specified While behaving similarly, there are some important differences between the two types: - Full specialization is a new type. Therefore, it must be defined in the source file ( .cpp ), just like any other class or function and it must have a separate declaration. On the other hand, partial specialization is still just a template, so it must be defined in the header file ( .h or .tpp ). - For functions, we cannot provide a partial specialization . For member functions we can solve this by specializing the whole class. The solution for any function is to alloow all types in the function and use if constexpr to select the correct implementation: template<class T, class C> class Object{ public: bool process(T value, C config){ if constexpr (std::is_same_v<T, std::string>){ return process_string(value, config); } else { return process_value(value, config); } } }; Note that here, the if constexpr requires the corresponding else branch. Otherwise, the code cannot be discarded during the compilation. Example: template<class T, class C> class Object{ public: bool process(T value, C config){ if constexpr (std::is_same_v<T, std::string>){ return process_string(value, config); } return process_value(value, config); // this compiles even if T is std::string } }; Templates and Namespaces \u00b6 If the templated code resides in a namespace, it can be tempting to save few lines of code by sorrounding both .h and .tpp files using one namespace expression: // structs.h hile namespace my_namespace { // declarations... #include 'structs.tpp' } // structs.tpp // definitions However, this can confuse some IDEs (e.g., false positive errors in IntelliSense), so it is better to introduce the namespace in both files: // structs.h hile namespace my_namespace { // declarations... } #include 'structs.tpp' // structs.tpp namespace my_namespace { // definitions } Don't forget to close the file and reopen it after the change to clear the errors. Using Complicated Types as Template Arguments \u00b6 Sometimes, it can be very tricky to determine the template argument we need in order to use the template. The correct argument can be for example a return value of some function, templete function, or even member function of a template instanciation which has other templates as argument... To make it easier, we can, istead of suplying the correct arguments, evaluate an expression that returns the correct type and then use the decltype specifier. For more info, see the Determining Type from Expressions section. Type Traits \u00b6 The purpose of type traits is to create predicates involving teplate parameters. Using type traits, we can ask questios about template parameters. With the answer to these questions, we can even implement conditional compilation, i.e., select a correct template based on parameter type. Most of the STL type traits are defined in header type_traits . A type trate is a template with a constant that holds the result of the predicate, i.e., the answer to the question. More about type traits Usefull Type Traits \u00b6 std::is_same std::is_base_of std::is_convertible std::conditional : enables if-else type selection Replacement for old type traits \u00b6 Some of the old type traits are no longer needed as they can be replaced by new language features, which are more readable and less error prone. Some examples: - std::enable_if can be replaced by concepts: // old: enable_if template<class T> void f(T x, typename std::enable_if_t<std::is_integral_v<T>, void> = 0) { std::cout << x << '\\n'; } // new: concepts template<std::integral T> void f(T x) { std::cout << x << '\\n'; } Concepts \u00b6 cppreference Concepts are named sets of requiremnets. They can be used instead of class / typename keywords to restrict the template types. The syntax is: template<class T, ....> concept concept-name = constraint-expression The concept can have multiple template parameters. The first one in the declaration stands for the concept itself, so it can be refered in the constraint expression. More template parameters can be optionally added and their purpose is to make the concept generic. Constraints \u00b6 Constraints can be composed using && and || operatos. For atomic constaints declaration, we can use: - Type traits: template<class T> concept Integral = std::is_integral<T>::value; Concepts: template<class T> concept UnsignedIntegral = Integral<T> && !SignedIntegral<T>; Requires expression: template<typename T> concept Addable = requires (T x) { x + x; }; Either form we chose, the atomic constraint have to always evaluate to bool. Requires Expression \u00b6 Requires expressions ar ethe most powerfull conctraints. The syntax is: requires(parameter list){requirements} There are four types of requirements that can appear in the requires expression: - simple requiremnet : a requirement that can contain any expression. Evaluates to true if the expression is valid. requires (T x) { x + x; }; type requirement : a requiremnt checking the validity of a type: requires { typename T::inner; // required nested member name typename S<T>; // required class template specialization typename Ref<T>; // required alias template substitution }; compound requirement : Checks the arguments and the return type of some call. It has the form: {expression} -> return-type-requirement; requires(T x) { {*x} -> std::convertible_to<typename T::inner>; } Nested requirement : a require expression inside another requires expression: requires(T a, size_t n) { requires Same<T*, decltype(&a)>; // nested } Auto filling the first template argument \u00b6 Concepts have a special feature that their first argument can be autoffiled from outer context. Consequentlly, you then fill only the remaining arguments. Examples: //When using the concept template<class T, class U> concept Derived = std::is_base_of<U, T>::value; template<Derived<Base> T> void f(T); // T is constrained by Derived<T, Base> // When defining the concept template<typename S> concept Stock = requires(S stock) { // return value is constrained by std::same_as<decltype(stock), double> {stock.get_value()} -> std::same_as<double>; } STL Concepts \u00b6 iterator concepts Usefull Patterns \u00b6 Constrain a Template Argument \u00b6 Imagine that you have a template function load and an abstract class Loadable_interface that works as an interface: class Loadable_interface{ virtual void load() = 0; }; template<class T> void load(T to_load){ ... to load.load() ... }; Typically you want to constraint the template argument T to the Loadable_interface type, so that other developer clearly see the interface requirement, and receives a clear error message if the requirement is not met. In Java, we have an extend keyword for this purpose that can constraint the template argument. In C++, this can be solved with concepts. First we have to define a concept that requires the interface: template<typename L> concept Loadable = std::is_base_of_v<Loadable_interface, L>; Than we can use the concept like this: template<Loadable T> void load(T to_load){ ... to load.load() ... }; Constraint a Concept Argument \u00b6 Imagine that you have a concept Loadable that requires a method load to return a type T restricted by a concept Loadable_type . One would expect to write the loadable concept like this: template<typename L, Loadable_type LT> concept Loadable = requires(L loadable) { {loadable.load()} -> LT; }; However, this is not possible, as there is a rule that concept cannot not have associated constraints . The solution is to use an unrestricted template argument and constrain it inside the concept definition: template<typename L, typename LT> concept Loadable = Loadable_type<LT> && requires(L loadable) { {loadable.load()} -> LT; }; Sources \u00b6 https://en.cppreference.com/w/cpp/language/constraints Requires expression explained Interfaces \u00b6 In programming, an interface is usualy a set of requirements that restricts the function or template parameters, so that all types fulfiling the requiremnet can be used as arguments. Therte are two ways how to create an interface in C++: - using the polymorphism - using templates argument restriction While the polymorphism is easier to implement, the templating is more powerful and it has zero overhead. The most important thing is probably that despite these concepts can be used together in one application, not all \"combinations\" are allowed especialy when using tamplates and polymorphism in the same type. Note that in C++, polymorphism option work only for function argument restriction, but we cannot directly use it to constrain template arguments (unlike in Java). To demonstrate all possible options, imagine an interface that constraints a type that it must have the following two functions: int get_value(); void set_value(int date); The following sections we will demonstrate how to achieve this using multiple techniques. Interface using polymorfism \u00b6 Unlike in java, there are no interface types in C++. However, we can implement polymorfic interface using abstract class. The following class can be used as an interface: class Value_interface{ virtual int get_value() = 0; virtual void set_value(int date) = 0; } To use this interface as a fuction argument or return value, follow this example: std::unique_ptr<Value_interface> increment(std::unique_ptr<Value_interface> orig_value){ return orig_value->set_value(orig_value->get_value() + 1); } This system works in C++ because it supports multiple inheritance. Do not forget to use the virtual keyword, otherwise, the method cannot be overriden. Note that unlike in other languages, in C++, the polymorphism cannot be directly use as a template (generic) interface. Therefore, we cannot use the polymorfism alone to restrict a type. Using template argument restriction as an interface \u00b6 To use template argument restriction as an interface, we can use concepts. The following concept impose the same requirements as the interface from the polymorphism section: To use template argument restriction as an interface, we can use concepts. The following concept impose the same requirements as the interface from the polymorphism section: template<class V> concept Value_interface = requires(V value_interface){{value_interface.get_value()} -> std::same_as<int>; } && requires(V value_interface, int value){{value_interface.set_value(value)} -> std::same_as<void>; } Remember that the return type of the function has to defined by a concept , the type cannot be used directly. Therefore, the following require statement is invalid: requires{(V value_interface){value_interface.get_value()} -> int; } To use this interface as an template argument in class use: template<Value_interface V> class ... And in function arguments and return types: template<Value_interface V> V increment(V orig_value){ return orig_value.set_value(orig_value.get_value() + 1); Restricting the member function to be const \u00b6 To restrict the member function to be const, we neet to make the type value const in the requires expression: template<class V> concept Value_interface = requires{(const V value_interface) {valvalue_interfaceue.get_value() -> std::same_as<int>;}; }; Using concepts and polymorphism together to restrict template parameters with abstract class \u00b6 We cannot restrict template parameters by polymorphic interface directly, however, we can combine it with concept. The folowing concept can be used together with the interface from the polymorphic interface section: template<class V> concept Value_interface_concept = requires std::is_base_of<Value_interface,V> Neverthless, as much as this combination can seem to be clear and elegent, it brings some problems. . We can use concepts to imposed many interfaces on a single type, but with this solution, it can lead to a polymorphic hell. While there is no problem with two concepts that directly requires the same method to be present with abstract classes, this can be problematic. Moreover, we will lose the zero overhead advantage of the concepts, as the polymorphism will be used to implement the interface. The Conflict Between Templates and Polymorphism \u00b6 As described above, messing with polymorphism and templates together can be tricky. Some examples: No Virtual Member Function with Template Parameters \u00b6 An example: a virtual (abstract) function cannot be a template function ( member template function cannot be virtual), so it cannot use template parameters outside of those defined by the class template. Polymorphism cannot be used inside template params \u00b6 If the functin accepts MyContainer<Animal> we cannot call it with MyContainer<Cat> , even if Cat is an instance of Animal. Possible solutions for conflicts \u00b6 do not use templates -> more complicated polymorphism ( type erasure for members/containers) do not use polymorphism -> use templates for interfaces an adapter can be used Polymorphic members and containers \u00b6 When we need to store various object in the same member or container, we can use both templates and polymorphism. However, both techniques has its limits, summarized in the table below: | | Polymorphism | Templates | | -- | -- | -- | | The concrete type has to be known at compile time | No | Yes | For multiple member initializations, the member can contain any element. | No , the elements have to share base class. | Yes | | For a single initialization, the containar can contain multiple types of objects | Yes , if they have the same base class | No | We can work with value members | No | Yes | When using the interface, we need to use downcasting and upcasting | Yes | No Deciding between template and polymorphism \u00b6 Frequently, we need some entity(class, function) to accept multiple objects through some interface. We have to decide, whether we use templates, or polymorphism for that interface. Some decision points: - We need to return the same type we enter to the class/function -> use templates - We have to access the interface (from outside) without knowing the exact type -> use polymorphism - We need to restrict the member/parametr type in the child -> use templates for the template parameter - if you need to fix the relation between method parameters/members or template arguments of thouse, you need to use templates - If there are space considerations, be aware that every parent class adds an 8 byte pointer to the atribute table In general, the polymorphic interface have the following adventages: - easy to implement - easy to undestand - similar to what people know from other languages On the other hand, the interface using concepts has the following adventages: - no need for type cast - all types check on compile time -> no runtime errors - zero overhead - no object slicing -> you don't have to use pointers when working with this kind of interface - we can save memory because we don't need the vtable pointers Iterators and ranges \u00b6 If we want to iterate over elements in some programming language, we need to fullfill some interface. In Java, this interface is called Iterable . Also, there is usually some interface that formalize the underlying work, in Java, for example, it is called Iterator . In C++, however, the interface for iteration is not handled by polymorphism. Instead, it is handled using type traits and concepts. On top of that, there are multiple interfaces for iteration: - legacy iteration, e.g., for (auto it = v.begin(); it != v.end(); ++it) - STL algorithms, e.g., std::find(v.begin(), v.end(), 42) - STL range algorithms, e.g., std::ranges::find(v, 42) - STL range adaptors, e.g., std::ranges::views::filter(v, [](int x){return x > 0;}) The following table summarizes the differences between the interfaces: |---| Plain iteration | STL algorithms | STL range algorithms | STL range adaptors | |---|---|---|---|---| | Interface | type traits | type traits | concepts | concepts | | Iteration | eager | eager | eager | lazy | | Modify the underlying range | no | yes | yes | no | | Can work on temporaries * | yes | yes | yes | no | *If the operation modifies the data, i.e., sorting, shuffling, transforming, etc. The examples below demonstrate the differences between the interfaces on the following task: create a vector of 10 elements with values 0,1,2,...,9, i.e., the same as Python range(10) . // plain iteration std::vector<int> vec(10); int i = 0; for (auto it = vec.begin(); it != vec.end(); ++it) { *it = i; ++i; } // legacy algorithm std::vector<int> vec(10); std::iota(vec.begin(), vec.end(), 0); // C++11 way, legacy interface using type traits // range algorithm std::vector<int> vec(10); std::ranges::iota(vec.begin(), vec.end(), 0); // basically the same, but the constructor arguments are constrained with concepts // same using adaptor auto range = std::views::iota(0, 10); std::vector vec{range.begin(), range.end()}; // in-place vector construction Terminology \u00b6 range : the object we iterate over (Iterable in Java) iterator : the object which does the real work (Iterator in Java) Usually, a range is composed from two iterators: - begin : points to the begining of the range, returne by <range_object>.begin() - end : points to the end of the object, returned by <range_object>.end() Each iterator implements the dereference ( * ) operator that acces the element of the range the iterator is pointing to. Depending on the iterator type, the iterator also supports other operations: ++ , -- to iterate along the range, array index operator ( [] ) for random access, etc. Most of the STL collections (vector, set,...) are also ranges. How to choose the correct interface? \u00b6 when deciding which interface to use, we can use the following rules: 1. If the number of tasks and the complexity of the tasks is high, use the legacy iteration . It is hard to write a 20 line for loop with various function calls as algorithm or adaptor and the result would be hard to read. 1. Otherwise, if you need to preserve the original range as it is or you need to compose multiple operations, use the STL range adaptors . 1. Otherwise, use the STL range algorithms . Note that the in this guide, we do not consider the legacy algorithms. With the availability of the STL algorithms, there is no reason to use the legacy algorithms, except for the backward compatibility or for the algorithms that are not yet implemented in the STL. Also note that some STL algorithms are principially non-modifying, e.g., std::ranges::find or std::ranges::count . These algorithms logically do not have the adaptor equivalent. STL ranges \u00b6 https://en.cppreference.com/w/cpp/ranges In C++ 20 there is a new range library that provides functional operations for iterators. It is similar to functional addon in Java 8. As explained in the beginning of thi chapter, there are two ways how to use the STL ranges: - using the range algorithms ( ranges::<alg name> ) that are invoked eagerly. - using the range adaptors ( ranges::views::<adaptor name> ) that are invoked lazily. Note that the range algorithms and adaptors cannot produce result without an input, i.e., we always need a range or collection on which we want to apply our algorithm/adapter. STL range adaptors \u00b6 The difference of range adaptors to range algorithms is that the range adapters are lazy, i.e., they do not produce any result until they are iterated over. This is similar to the Python generators. The advantage is that we can chain multiple adaptors together and the result is computed only when we iterate over the final adaptor. Note that due to the lazy nature of the adaptors, the underlying range has to be alive during the whole iteration . Therefore, we cannot use the adaptors on temporaries, e.g., we cannot use the adaptors directly in the constructor of a vector, or we cannot use the adaptors on a temporary range returned by a function. A custom view can be created so that it can be chained with STL views. However, it has to satisfy the view concept , and more importantly, it should satisfy the view semantic, i.e., it should be cheap to copy and move (without copying the underlying data). Useful range algorithms \u00b6 std::shuffle : shuffles the elements in the range (formerly std::random_shuffle ). std::adjacent_find : finds the first two adjacent elements that are equal. Can be used to find duplicates if the range is sorted. Other Resources \u00b6 https://www.modernescpp.com/index.php/c-20-the-ranges-library Boost ranges \u00b6 In addition to the STL range algorithms and adaptors, boost has it's own range library with other more complex algorithms and adaptors. Boost range requirements \u00b6 Sometimes, it is hard to say why a type does not satisfy some of the requirements for boos ranges. Fortunatelly, the boost provides concepts for checking whether a type satisfy each specific range model. Example: BOOST_CONCEPT_ASSERT(( boost::SinglePassRangeConcept<std::vector<int>> )); // true Also, it is necessary to check whether the value of the iterator can be accessed: BOOST_CONCEPT_ASSERT(( boost_concepts::ReadableIteratorConcept< typename boost::range_iterator<std::vector<int>>::type > )); // true Most likely, the compiler will complain that boost::range_iterator<R>::type does not exist for your range R . The boost range library generate this type by a macro from the R::iterator type. Therefore, make sure that your range has an iterator type defined, either as: - a type alias to an existing iterator - an iterator nested class Note that <RANGE CLASS>::iterator and <RANGE CLASS>::const_iterator has to be accessible (public). Sequence Range \u00b6 The iota algortihm/adapter is used to create a sequence: auto range = std::views::iota(0, 10); auto vec = std::vector(range.begin(), range.end()); Note that we cannot pass the view directly to the vector, as the vector does not have a range constructor. Zip range \u00b6 The classical Python like zip iteration is available using the zip adapator , which is not yet supported in MSVC. However, boost provides a similar functionality boost::combine . boost::combine \u00b6 boost::combine example: std::vector<int> va{1, 2, 3}; std::vectro<float> vb{0.5, 1, 1.5}; for(const auto& [a, b]: boost::combine(va, vb)){ ... } Each argument of combine must satisfy boost::SinglePassRange Enumerating range \u00b6 There is no function in standard library equivalent to the python enumerate. We can use a similar boost solution: #include <boost/range/adaptor/indexed.hpp> for(auto const& el: <range> | boost::adaptors::indexed(0)){ std::cout << el.index() << \": \" << el.value() << std::endl; } However, inside the loop, we have to call the index and value functions, so it is probably easier to stick to the good old extra variable: size_t i = 0; for(auto const& el: <range>) { std::cout << i << \": \" << el << std::endl; ++i; } Sorting \u00b6 There is no sorted view or something simmiler, so in order to sort a range, we need to: - really sort the object in the range - create an adaptor/view from the range, and then sort the view There are two functions for sorting in the STL algorithm library: - std::sort : old - supports parallelization directly by the policy param - std::ranges::sort : new - supports comparison using projections There are three types of sorting: - natural sorting using the < operator of T : std::sort(<RANGE<T>>) - sorting using a comparator: std::sort(<RANGE>, <COMPARATOR>) , where comparator is a fuction with parameters and return value analogous to the natural sorting operator. - sorting using projection (only availeble in std::ranges::sort ): std::ranges::sort(<RANGE>, <STANDARD GENERIC COMPARATOR>, <PROJECTION> Sorting using projection \u00b6 When we want to sort the objects by a single property different then natural sorting, the easiest way is to use projection. Example: struct Data { int a; std::string b; ... }; std::vector<Data> data = get_data(); std::sort(data, {}, &Data::b); The trick here is that we can only provide the member that we want to use for sorting, but the sorting logic ( first < second ...) is handeled by a standard comparator (the second argument initialized with {} ). We can have even more complicated projections: std::vector<My_class> objects = get_objects(); std::vector<unsigned> indexes = get_indexes(objects); auto proj = [&objects](std::size_t i) -> const std::string& { return objects[i].get_name(); }; std::ranges::sort(indexes, {}, proj) // sort indexes using the property of objects Transformation \u00b6 Transformation alg/views transforms an input range according to a callable. As with other operation, there are thre options: - classical algorithm: std::transform with a direct paralellization using the policy parameter - range algorithm: std::ranges::transform with a support for projections - range view: std::ranges::views::transform - a lazy variant The algorithms (but not the view) also supports binary transformations, i.e., create an output range using two input ranges. Transform view example: std::vector<int> in(3, 0); // [0, 0, 0] auto ad = std::ranges::transform_view(in, [](const auto in){return in + 1;}); std::vector<int> out(ad.begin(), ad.end()); The transform view can be only constructed from an object satisfying ranges::input_range . If we want to use a general range (e.g., vector), we need to call the addapter, which has a same signature like the view constructor itself. The important thing here is that the adapter return type is not a std::ranges::views::transform<<RANGE>> but std::ranges::views::transform<std::ranges::ref_view<RANGE>>> ( std::ranges::ref_view ). Supporting various collections is therefore possible only with teplates, but not with inheritance. Note that unlike in Java, it is not possible to use a member reference as a transformation function (e.g.: &MyClass::to_sting() ). We have to always use lambda functions, std::bind or similar to create the callable. Iterator Concepts \u00b6 https://en.cppreference.com/w/cpp/iterator C++ 20 has some new iterator concepts, providing various interfaces. However, range-based for loop (for each) does not require any of the new concepts, nor the legacy iterators, its requirements are smaller. Boost Iterator Templates \u00b6 The boost.iterator library provides some templates to implement iteratores easily, typically using some existing iterators and modifying just a small part of it: - for pointer to type (dereference) iterator, you can use boost indirect iterator - zip iterator for Python like iteration over multiple collections - [transform iteratorather useful iterators are also included in the boost.iterator library for using another iterator and just modify the access ( * ) opindex.html). including: - zip iterator. - counting_iterator to create number sequence like Python range - gentransform iterator There are also two general (most powerfull) classes: - iterator adapter - iterator facade Resources \u00b6 How to write a legacy iterator iter_value_t Lambda Functions \u00b6 In c++ lambda functions are defined as: [<capture>](<params>) -> <return_type> { <code> } The rerurn type is optional, but sometimes required (see below). Since C++23, the parantheses are optional if there are no functon parameters. Captures \u00b6 Anything that we want to use from outside has to appear in capture. To prevent copying, we should capture by reference, using & before the name of the variable. [&var_1] // capture by reference [var_1] // capture by value [&] // default capture by reference For the detailed explanation of the captures, see cppreference . Return type \u00b6 The return type of lambda functions can be set only using the trailing return type syntax ( -> <RETURN TYPE> after the function params). The return type can be omited. Note however, that the default return type is auto , so in case we want to return by reference, we need to add at least -> auto , or even a more specific return type. Specifiers \u00b6 Lambda functions can have special specifiers: - mutable : lambda can modify function parameters capture by copy Exceptions \u00b6 In C++, exceptions works simillarly as in other languages. Standard runtime error can be thrown using the std::runtime_error class: throw std::runtime_error(\"message\"); Always catch exception by reference! Rethrowing Exceptions \u00b6 We can rethrow an exception like this: catch(const std::exception& ex){ // do ssomething ... throw; } Note that in parallel regions, the exception have to be caught before the end of the parallel region , otherwise the thread is killed. How to Catch Any Exception \u00b6 In C++, we can catch any exception with: catch (...) { } However, this way, we cannot access the exception object. As there is no base class for exceptions in C++, there is no way to catch all kind of exception objects in C++. noexcept specification \u00b6 A lot of templates in C++ requires functions to be noexcept which is usually checked by a type trait std::is_nothrow_invocable . We can easily modify our function to satisfy this by adding a noexcept to the function declariaton. There are no requirements for a noexcept function. It can call functions without noexcept or even throw exceptions itself. The only difference it that uncought exceptions from a noexcept function are not passed to the caller. Instead the program is terminated by calling std::terminate , which otherwise happens only if the main function throws. By default, only constructors, destructors, and copy/move operations are noexcept. Stack traces \u00b6 Unlike most other languages, C++ does not print stack trace on program termination. The only way to get a stack trace for all exceptions is to set up a custom terminate handler an inside it, print the stack trace. However, as of 2023, all the stack trace printing/generating libraries requires platform dependent configuration and fails to work in some platforms or configurations. Example: void terminate_handler_with_stacktrace() { try { <stack trace generation here>; } catch (...) {} std::abort(); } std::set_terminate(&terminate_handler_with_stacktrace); To create the stacktrace, we can use one of the stacktrace libraries: - stacktrace header from the standard library if the compiler supports it (C++ 23) - cpptrace - boost stacktrace Logging \u00b6 There is no build in logging in C++. However, there are some libraries that can be used for logging. In this section we will present logging using the spdlog library. We can log using the spdlog::<LEVEL> functions: spdlog::info(\"Hello, {}!\", \"World\"); By default, the log is written to console. In order to write also to a file, we need to create loggers manually and set the list of sinks as a default logger: const auto console_sink = std::make_shared<spdlog::sinks::stdout_sink_st>(); console_sink->set_level(spdlog::level::info); // log level for console sink auto file_sink = std::make_shared<spdlog::sinks::basic_file_sink_st>(<log filepath>, true); std::initializer_list<spdlog::sink_ptr> sink_list{console_sink, file_sink}; const auto logger = std::make_shared<spdlog::logger>(<LOGGER NAME>, sink_list); logger->set_level(spdlog::level::debug); //log level for the whole logger spdlog::set_default_logger(logger); To save performance in case of an intensive logging, we can set an extended flushing period: spdlog::flush_every(std::chrono::seconds(5)); Type Aliases \u00b6 Type aliases are short names bound to some other types. We can introduce it either with typedef or with using keyword. Examples (equvalent): typedef int number; using number = int; typedef void func(int,int); using func = void(int, int) The using new syntax is more readable, as the alias is at the begining of the expression. But why to use type aliases? Two strong motivations can be: - iImprove the readebility : When we work with a type with a very long declaration, it is wise to use an alias. We can partialy solve this issue by using auto, but that is not a complete solution - Make the refactoring easier : When w work with aliases, it is easy to change the type we work with, just by redefining the alias. Note that type aliases cannot have the same name as variables in the same scope . So it is usually safer to name type aliases with this in mind, i.e., using id_type = .. insted of using id = .. Template Aliasis \u00b6 We can also create template aliases as follows: template<class A, typename B> class some_template{ ... }; template<class T> using my_template_alias = some_template<T, int>; Aliases inside classes \u00b6 The type alias can also be placed inside a class. From outside the class, it can be accessed as <CLASS NAME>::<ALIAS NAME> : class My_class{ public: using number = unsigned long long number n = 0; } My_class::number number = 5; Constant Expressions \u00b6 A constant expression is an expression that can be evaluated at compile time. The result of constant expression can be used in static context, i.e., it can be: - assigned to a constexpr variable, - tested for true using static_assert Unfortunatelly, there is no universal way how to determine if an expression is a constant expression . More on cppreference . Regular expressions \u00b6 The regex patern is stored in a std::regex object: const std::regex regex{R\"regex(Plan (\\d+))regex\"}; Note that we use the raw string so we do not have to escape the pattern. Also, note that std::regex cannot be constexpr Matching the result \u00b6 We use the std::regex_search to search for the occurence of the pattern in a string. The result is stored in a std::smatch object which contains the whole match on the 0th index and then the macthed groups on subsequent indices. A typical operation: std::smatch matches; const auto found = std::regex_search(string, matches, regex); if(found){ auto plan_id = matches[1].str(); // finds the first group } Note that matches[0] is not the first matched group, but the whole match. Namespaces \u00b6 cppreference Namespace provides duplicit-name protection, it is a similar concept to Java packages. Contrary to java packages and modules, the C++ namespaces are unrelated to the directory structure. namespace my_namespace { ... } The namespaces are used in both declaration and definition (both in header and source files). The inner namespace has access to outer namespaces. For using some namespace inside our namespace without full qualification, we can write: using namespace <NAMESPACE NAME> Anonymous namespaces \u00b6 Anonymous namespaces are declared as: namespace { ... } Each anonnymous namespaces has a different and unknown ID. Therefore, the content of the annonymous namespace cannot be accessed from outside the namespace, with exception of the file where the namespace is declared which has an implicit access to it. Namespace aliases \u00b6 We can create a namespace alias using the namespace keyword to short the nested namespace names. Typicall example: namespace fs = std::filesystem; decltype : Determining Type from Expressions \u00b6 Sometimes, it is usefull to declare a type from expression, instead of do it manualy. Using decltype specifier, we can get the resulting type of an expression as if it was evaluated. Examples: struct A { double x; }; const A* a; decltype(a->x) // evaluates to double decltype(std::accumulate(a, [](double sum, double val){return sum + val;})) // evalutes to double We can use the decltype in any context where type is required. Examples: int i = 1 decltype(i) j = 3 std::vector<decltype(j)> numbers; The Value Category of decltype \u00b6 The value category of decltype is resolved depending on the value category of an expression inside it: - deltype(<XVALUE>) -> T&& - deltype(<LVALUE>) -> T& - deltype(<RVALUE>) -> T The rvalue conversion can lead to unexpected results, in context, where the value type matters: static_assert(std::is_same_v<decltype(0), decltype(std::identity()(0))>); // error The above expressions fails because: - decltype(0) , 0 is an rvalue -> the decltype result is int - decltype(std::identity()(0)) result of std::identity() is an xvalue -> the decltype result is int&& . Determining Type from Expressions Sometimes, it is usefull to declare a type from expression, instead of do it manualy. Using decltype specifier, we can get the resulting type of an expression as if it was evaluated. Examples: struct A { double x; }; const A* a; decltype(a->x) // evaluates to double decltype(std::accumulate(a, [](double sum, double val){return sum + val;})) // evalutes to double We can use the decltype in any context where type is required. Examples: int i = 1 decltype(i) j = 3 std::vector<decltype(j)> numbers; Determining the Return Value Type of a Function \u00b6 As we can see above, we can use decltype to determine the return value type. But also, there is a type trait for that: std::invoke_result_t (formerly std::result_of ). The std::invoke_result_t should vbe equal to decltype when aplied to return type, with the following limitations: - we cannot use abstract classes as arguments of std::invoke_result_t , while we can use them inside decltype (using std::declval , see below). - Construct object inside decltype with std::declval \u00b6 std::declval is a usefull function designed to be used only in static contexts, inside decltype . It enables using member functions inside decltype without using constructors. Without std::declval , some type expressions are hard or even impossible to costruct. Example: class Complex_class{ Complex_class(int a, bool b, ...) ... int compute() } // without declval decltype<Complex_class(1, false, ...).compute()> // using declval decltype(std::declval<Complex_class>().compute()) decltype and Overloading \u00b6 in static context, there is no overloading, the vtable is not available. Therefore, we have to hint the compiler which specific overloaded function we want to evaluate. This also applies to const vs non const overloading. The following example shows how to get the const iterator type of a vector: std::vector<anything> vec // non const iter decltype(vec.begin()) // const iter decltype<std::declval<const decltype(vec)>().begin()> Another example shows how to use the const overload inside std::bind : decltype(std::bind(static_cast<const ActionData<N>&(std::vector<ActionData<N>>::*)(size_t) const>(&std::vector<ActionData<N>>::operator[]), action_data)), Above, we used static cast for choosing the const version of the vector array operator. Instead, we can use explicit template argument for std::bind : decltype(std::bind<const ActionData<N>& (std::vector<ActionData<N>>::*)(size_t) const>(&std::vector<ActionData<N>>::operator[], action_data)), Parallelization \u00b6 While there wa no support of parallelization i earlier versions of C++ , now there are many tools. Standard Threads \u00b6 For-each with Parallel Execution Policy \u00b6 The function std::for_each can be run with a parallel execution policy to process the loop in parallel. Async tasks \u00b6 Tasks for asznchronous execution, like file downloads, db queries, etc. The main function is std::async . Open-MP \u00b6 In MSVC, the Open MP library is automatically included and linked. In GCC, we need to find the libs in CmakeLists.txt : find_package(OpenMP REQUIRED) Standard Templates for Callables \u00b6 Using std::invoke to call the member function \u00b6 using std::invoke , the cal syntax bool b = (inst.*ptr)() can be replaced with longer but more straighforward call: bool b = std::invoke(ptr, inst, 2) Using std::mem_fn to Store a Pointer to Member Function in a Callable \u00b6 With std::mem_fn , we can store the pointer to a member function in a callable object. Later, we can call the object without the pointer to the member function. Example: auto mem_ptr = std::mem_fn(&My_class::my_method) bool b = mem_ptr(inst, 2) Using a Pointer to Member Function as a Functor \u00b6 A normal function can be usually send instead of functor, as it can be invoked in the same way. However, in case of member function, we usually need to somehow bind the function pointer to the instance. We can use the std::bind function exactly for that: auto functor = std::bind(&My_class::my_method, inst); bool b = functor(2) Advanteges: - we do not need an access to instance in the context from which we call the member function - we do not have to remember the complex syntax of a pointer to a member function declaration - we receive a callable object, which usage is even simpler than using std::invoke Note that in case we want to bind only some parameters, we need to supply placeholders for the remaining parameters ( std::placeholders ). Using Lambdas Instead of std::bind \u00b6 For more readable code and better compile error messages, it is usefull to replace std::bind callls with labda functions. The above example can be rewritten as: auto functor = [inst](int num){return inst.my_method(num);); bool b = functor(2) Store the Result of std::bind \u00b6 Sometimes, we need to know the return type of the std::bind . In many context, we need to provide the type instead of using auto . But luckily, there is a type exactly for that: std::function . Example: std::function<bool(int)> functor = std::bind(&My_class::my_method, inst); bool b = functor(2) A lambda can also be stored to std::function . But be carefull to add an explicit return type to it, if it returns by a reference. Example: My_class{ public: int my_member } My_class inst; std::function f = [inst](){return &inst.my_member; } // wrong, reference to a temporary due to return type deduction std::function f = [inst]() -> const int& {return &inst.my_member; } // correct More detailed information about pointers to member functions std::mem_fn and Data Members \u00b6 Data member pointers can be aslo stored as std::mem_fn . A call to this object with an instance as the only argument then return the data member value. The plain syntax is <type> <class name>.*<pointer name> = <class name>.<member name> , and the pointer is then accessed as <instance>.*<pointer name> . Example: int Car::*pSpeed = &Car::speed; c1.*pSpeed = 2; Usefull STL functions - std::for_each : iterates over iterable objects and call a callable for each iteration - std::bind : Binds a function call to a variable that can be called - some parameters of the function can be fixed in the variable, while others can be provided for each call - each reference parameter has to be wrapped as a reference_wrapper - std:mem_fn : Creates a variable that represents a callable that calls member function std::function \u00b6 The std::function template can hold any callable. It can be initialized from: - function pointer/reference, - member function pointer/reference, - lambda function - functor It can be easily passed to functions, used as template parameter, etc. The template parameters for std::function has the form of std::function<<RETURN TYPE>(<ARGUMENTS>)> . Example: auto lambda = [](std::size_t i) -> My_class { return My_class(i); }; std::function<My_class(std::size_t)> f{lambda} std::function and overloading \u00b6 one of the traps when using std::function is the ambiguity when using an overloade function: int add(int, int); double add(double, double); std::function<int(int, int)> func = add; // fails due to ambiguity. The solution is to cant the function to its type first and then assign it to the template: std::function<int(int, int)> func = static_cast<int(*)(int, int)>add; Testing with Google Test \u00b6 Private method testing \u00b6 The testing of private method is not easy with Google Test, but that is common also for other tets frameworks or even computer languages (see the common manual). Some solutions are described in this SO question . Usually, the easiest solution is to aplly some naming/namespace convention and make the function accessible. For free functions: namespace internal { void private_function(){ ... } } For member functions: class MyClass{ public: void _private_function(); Conditional Function Execution \u00b6 W know it from other languages: if the function can be run in two (or more) modes, there is a function parameter that controls the execution. Usually, most of the function is the same (otherwise, we eould create multiple fuctions), and the switch controls just a small part. Unlike in other langueges. C++ has not one, but three options how to implement this. They are described below in atable together with theai properties. function parameter template parameter compiler directive good readability yes no no compiler optimization no yes yes conditional code compilation no no yes Function Parameter \u00b6 void(bool switch = true){ if(switch){ ... } else{ ... } } Template Parameter \u00b6 template<bool S = true> void(){ if(S){ ... } else{ ... } } Compiler Directive \u00b6 void(){ #ifdef SWITCH ... #else ... #endif } Ignoring warnings for specific line of code \u00b6 Sometimes, we want to suppress some warnings, mostly in libraries we are including. The syntax is, unfortunatelly, different for each compiler. Example: #if defined(_MSC_VER) #pragma warning(push) #pragma warning(disable: <WARNING CODE>) #elif defined(__GNUC__) #pragma GCC diagnostic push #pragma GCC diagnostic ignored \"<WARNING TYPE GCC>\" #elif defined(__clang__) #pragma clang diagnostic push #pragma clang diagnostic ignored \"<WARNING TYPE CLANG>\" #endif .. affected code... #if defined(_MSC_VER) #pragma warning(pop) #elif defined(__GNUC__) #pragma GCC diagnostic pop #elif defined(__clang__) #pragma clang diagnostic pop #endif Note that warnings related to the preprocessor macros cannot be suppressed this way in GCC due to a bug (fixed in GCC 13). The same is true for conditions: #if 0 #pragma sdhdhs // unknown pragma raises warning, despite unreachcable #endif Measuring used resource \u00b6 Memory \u00b6 MSVC \u00b6 In MSVC, we can measure the peak used memory using the following code: #include <psapi.h> PROCESS_MEMORY_COUNTERS pmc; K32GetProcessMemoryInfo(GetCurrentProcess(), &pmc, sizeof(pmc)); auto max_mem = pmc.PeakWorkingSetSize Working with tabular data \u00b6 Potential libs similar to Python Pandas: - Arrow - Dataframe Executing external commands \u00b6 The support for executing external commands in C++ is unsatisfactory. The most common solution is to use the system function. However, the system calls are not portable, e.g., the quotes around the command are not supported in Windows Another option is to use the Boost Process library. Unions and Variants \u00b6 The idea of a union is to store multiple types in the same memory location. Compared to the polymorphism, when we work with pointers and to templates, where the actual type is determined at compile time, the union actually has a shared memory for all the types. The union can be therefore used in cases where nor polymorphism neither templates are suitable. One example can be storing different unrelated types (e.g., std::string and int ) in a container. We cannot use templates as that require a single type. Nor we can use polymorphism, as the types are unrelated. The big disadvantage of unions is that they are not type safe. The compiler cannot check if the type we are accessing is the same as the type we stored. Therefore, we have to be very careful when using unions. Therefore, unless some special case, we should use std::variant instead of unions . std::variant \u00b6 The declaration of std::variant is similar to the declaration of std::tuple : std::variant<int, double> v; The std::variant can store any of the types specified in the template parameters. The type of the stored value can be obtained using the std::variant::index method. The value can be accessed using the std::get_if method, which returns a pointer to the stored value. Example: std::variant<int, double> v = 1; std::cout << v.index() << std::endl; // prints 0 std::cout << *std::get_if<int>(&v) << std::endl; // prints 1 A realy usefull feature of std::variant is the std::visit method, which allows us to call a function on the stored value. The function is selected based on the type of the stored value. Example: std::variant<int, double> v = 1; std::visit([](auto&& arg) { std::cout << arg << std::endl; }, v); // prints 1 More on variants: - cppreference - cppstories","title":"C++ Manual"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#type-system-and-basic-types","text":"cppreference Type is a property of each: - object - reference - function - expression","title":"Type System and basic types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#arithmetic-types","text":"cppreference","title":"Arithmetic Types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#integers","text":"Integer types varies in the sign and size. Unfortunatelly, the minimum sizes guaranteed by the standard are not usable, because the real size is different and it differs even between platforms . Especially the long type. To use an integer with a specific size, or a specific minimal size, we can use type aliases defined in cstdint","title":"Integers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#overflow-and-underflow","text":"The overflow (and underflow) is a common problem in most programming languages. The problem in C++ is that: - overflows are not detected - overflows can happen in many unexpected situations","title":"Overflow and Underflow"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#dangerous-situations","text":"In addition to the usual suspects like assigning a value to a variable of a smaller type, there are some less obvious situations that can cause overflows. Some examples: - the result of an arithmetic operation is assigned to a variable of large enough type, but the overflow happens before the assignment itself: short a = 32767; short b = 1; int c = a + b; // overflow happens beffore the assignment A solution to this problem is to use a numeric cast of the opperands (even one is enouhg): short a = 32767; short b = 1; int c = static_cast<int>(a) + b;","title":"Dangerous situations"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#detecting-overflows","text":"There are some methods how to detect overflows automatically by suppliying arguments to the compiler. These are summarized here: - MSVC : not implemented - GCC : only detectes signed and floating point overflows, as the unsigned overflows are not considered as errors (the behaviour is defined in the standard). All undefined behaviour can be detected using the -fsanitize=undefined flag. Documentation - Clang : Both signed and unsigned overflow can be detected. The undefined behaviour can be detected using the -fsanitize=undefined flag. Fo all integer overflows, the -fsanitize=integer flag can be used. Documentation The reasoning behind excluding the unsigned overflows from GCC are described here . It is also possible to do an ad-hoc overflow check in the code, the possible solutions are described in this SO question","title":"Detecting overflows"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#pointers","text":"cppreference","title":"Pointers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#pointers-to-functions","text":"Function pointers are declared as: <return_type> (*<pointer_name>)(<arg_1_type>, ..., <arg_n_type>) For example a function the_function returning bool and accepting int can be stored to pointer like this: bool (*ptr)(int) = &the_function The above example can be then simply called as bool b = ptr(2)","title":"Pointers to Functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#pointers-to-member-objects","text":"Pointers to member objects has a cumbersome syntax - declaration: <member type> <class type>::*<pointer name> = ... - usage: <object name>.*<pointer name> = ... Example: class My_class{ public: int my_member; } int main{ // declaring the pointer int My_class::*ptr = &My_class::my_member; // creating the instance My_class inst; // using the pointer to a member object inst.*ptr = 2; }","title":"Pointers to Member Objects"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#pointers-to-member-functions","text":"Pointers to member functions are even more scary in C++. We need to use the member object and the function adress and combine it in a obscure way: class My_class{ public: bool my_method(int par); } int main{ // creating the instance My_class inst; // assigning method address to a pointer bool (My_class::*ptr)(int) = &My_class::my_method; // using the pointer to a member function bool b = (inst.*ptr)(2) } The first unexpected change is the My_class before the name of the pointer. It's because unlike a pointer to function the_function which is of type (*)(int) , the pointer to my_method is of type (My_class::*)(int) The second difference is the call. We have t use the pointer to member binding operator .* to access the member of the specific instance inst . But this operator has a lower priority then the function call operator, so we must use the extra parantheses.","title":"Pointers to Member Functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#references","text":"References serve as an alias to already existing objects. Standard ( Lvalue ) references works the same way as pointers, with two differences: - they cannot be NULL - they cannot be reassigned The second property is the most important, as the assignment is a common operation, which often happens under do hood. In conslusion, reference types cannot be used in most of the containers and objets that needs to be copied .","title":"References"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#rvalue-references","text":"Rvalue references are used to refer to temporary objects. They eneable to prevent copying local objets by extending lifetime of temporary objects. They are mostly used as function parameters: void f(int& x){ } f(3); // 3 needs to be copied to f, because it is a temporary variable // we can add the rvalue overload void f(int&& x){ } f(3) // rvalue overload called, no copy","title":"Rvalue references"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#forwarding-references","text":"Forwarding references are references that preserves the value category (i.e. r/l-value reference, const ). They have two forms: - function parameter forwarding references - auto forwarding references","title":"Forwarding references"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-parameter-forwarding-references","text":"In a function template, if we use the rvalue reference syntax for a function parameter of whose type is a function template parameter, the reference is actually a forwarding reference. Example: template<class T> void f(T&& arg) // parameter is T& or T&& depending on the supplied argument Important details: - it works only for non const references - the reference type has to be a function template argument, not a class template argument","title":"Function parameter forwarding references"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#auto-forwarding-reference","text":"When we assign to `auto&&, it is a forwarding reference, not rvalue reference: auto&& a = f() // both type and value category depends on the return value of f() for(auto&& a: g(){ // same }","title":"auto forwarding reference"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#arrays","text":"cppreference There are two types of arrays: - static , i.e., their size is known at compile type, and - dynamic , the size of which is computed at runtime We can use the array name to access the first elemnt of the array as it is the pointer to that element.","title":"Arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#static-arrays","text":"Declaration: int a[nrows]; int a[nrwows][ncols]; // 2D int a[x_1]...[x_n]; // ND Initialization: int a[3] = {1, 2, 5} int b[3] = {} // zero initialization int c[3][2] = {{1,5}, {2,9}, {4,4}} // 2D int d[] = {1,5} // we can skip dimensions if their can be derived from data Note that the multi-dimensional syntax is just an abstraction for the programmers. The following code blocks are therefore equivalent: Matrix syntax const int rowns = 5; const int cols = 3; int matrix[rows][cols]; int main(){ for(int n = 0; n < rows; ++n){ for(int m = 0; m < cols; ++m){ Table[n][m] = (n + 1) * (m + 1); } } } Flat syntax const int rowns = 5; const int cols = 3; int matrix[rows * cols]; int main(){ for(int n = 0; n < rows; ++n){ for(int m = 0; m < cols; ++m){ Table[n * cols + m] = (n + 1) * (m + 1); } } } Using the matrix syntax adds the possibility to access the element of the array using multiple dimensions. But the underlying memory is the same.","title":"Static arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#dynamic-arrays","text":"Declaration: int* a = new int[size] For multiple dimensions, this syntax does not scale, i.e, only one dimension can be dynamic: int(*a)[4] = new int[rows][4] // static column count int(*b)[cols] = new int[rows][cols] // does not compile unless cols is a constant!","title":"Dynamic arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#array-to-pointer-implicit-conversion","text":"When we use the array name in an expression, it can be implicitly converted to a pointer to the first element of the array. This is true for both static and dynamic arrays. Example: int a[3] = {1, 2, 5} int* ptr = a; // ptr points to the first element of a This implicit conversion is called array-to-pointer decay .","title":"Array to pointer implicit conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#mutli-dimensional-dynamic-arrays","text":"To simulate multi-dimensional dynamic arrays, we have two options: - use the flat syntax, as demonstrated on static arrays - use aray of pointers to arrays Method Pros Cons Flat Syntax Fast: single continuous allocations different access syntax than static 2D arrays Array of pointers Slow: one allocation per row, unrelated memory addresses between rows same access syntax as static 2D arrays","title":"Mutli-dimensional dynamic arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#flat-array","text":"int* a = new int[rows * cols] Then we can access the array as: a[x * cols + y] = 5","title":"Flat array"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#array-of-pointers-to-array","text":"Declaration and Definition int** a= new int*[rows] for(int i = 0; i < rows; ++i){ a[i] = new int[cols] } Access is than like for static 2D array: a[x][y] = 5 . This works because the pointers can be also accessed using the array index operator ( [] ). In other words, it works \"by coincidence\", but we have not created a real 2D array.","title":"Array of pointers to array"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#auto-dealocation-of-dynamic-arrays","text":"We can replace the error-prone usage of new and delete by wraping the array into unique pointer: std:unique_ptr<int[]> a; a = std::make_unique<int[]>(size)","title":"Auto dealocation of dynamic arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#references-and-pointers-to-arrays","text":"cppreference The pointer to array is declared as <type> (*<pointer_name>)[<size>] : int a[5]; int (*ptr)[5] = &a; Analogously, the reference to array is declared as <type> (&<reference_name>)[<size>] : int a[5]; int (&ref)[5] = a;","title":"References and Pointers to arrays"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-type","text":"A function type consist from the function arguments and the return type. The function type is written as return_type(arg_1_type, ..., arg_n_type) . Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo), int(double, double)>) // TRUE","title":"Function Type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#reference-to-function-and-pointer-to-function-types","text":"cppreference A refrence to function has a type return_type(&)(arg_1_type, ..., arg_n_type) . Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo)&, int(&)(double, double)>); // TRUE A pointer to function has a type: return_type(*)(arg_1_type, ..., arg_n_type) Example: int foo(double a, double b); static_assert(std::is_same_v<decltype(foo)*, int(*)(double, double)>); // TRUE","title":"Reference to Function and Pointer to Function Types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#complete-and-incomplete-types","text":"In many context, we have to supply a type with a requirement of being a complete type. So what types are incomplete? - The void type is always incomplete - Any structure without definition (e.g. using struct structure *ps; , without defining structure .) - An array without dimensions is an incomplete type: int a[]; is incomplete, while int a[5]; is complete. - An array of incomplete elements is incomplete. A type trait that can be used to determine whether a type is complete is described here .","title":"Complete and Incomplete Types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#aggregate-types","text":"Aggregate types are: - array types - class types that fullfill the following conditions - no private or protected members - no constructores declared (including inherited constructors) - no private or protected base classes - no virtual member functions The elements of the aggregate types can and are ment to be constructed using the aggregate initialization (see the local variable initialization section).","title":"Aggregate types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#type-conversion","text":"cppreference: implicit conversion In some context, an implicit type conversion is aplied. This happens if we use a value of one type in a context that expects a different type. The conversion is applied automatically by the compiler, but it can be also applied explicitly using the static_cast operator. In some cases where the conversion is potentially dangerous, the static_cast is the only way to prevent compiler warnings.","title":"Type Conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#numeric-conversion","text":"There are two basic types of numeric conversion: - standard implicit conversion that can be of many types: this conversion is applied if we use an expression of type T in a context that expects a type U . Example: ```cpp void print_int(int a){ std::cout << a << std::endl; } int main(){ short a = 5; print_int(a); // a is implicitly converted to int } ``` usual arithmetic conversion which is applied when we use two different types in an arithmetic binary operation. Example: cpp int main(){ short a = 5; int b = 2; int c = a + b; // a is converted to int }","title":"Numeric Conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implicit-numeric-conversion","text":"","title":"Implicit Numeric Conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#integral-promotion","text":"Integral promotion is a coversion of an integer type to a larger integer type. The promotion should be safe in a sense that it never changes the value. Important promotions are: - bool is promoted to int : false -> 0 , true -> 1","title":"Integral Promotion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#integral-conversion","text":"Unlike integral promotion, integral conversion coverts to a smaller type, so the value can be changed. The conversion is safe only if the value is in the range of the target type. Important conversions are:","title":"Integral Conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#usual-arithmetic-conversion","text":"cppreference This conversion is applied when we use two different types in an arithmetic binary operation. The purpose of this conversion is convert both operands to the same type before the operation is applied. The result of the conversion is then the type of the operands. The conversion has the following steps steps: 1. lvalue to rvalue conversion of both operands 1. special step for enum types 1. special step for floating point types 1. conversion of both operands to the common type The last step: the conversion of both operands to the common type is performed using the following rules: 1. If both operands have the same type, no conversion is performed. 1. If both operands have signed integer types or both have unsigned integer types, the operand with the type of lesser integer conversion rank (size) is converted to the type of the operand with greater rank. 1. otherwise, we have a mix of signed and unsigned types. The following rules are applied: 1. If the unsigned type has conversion rank greater or equal to the rank of the signed type, then the unsigned type is used. 1. Otherwise, if the signed type can represent all values of the unsigned type, then the signed type is used. 1. Otherwise, both operands are converted to the unsigned type corresponding to the signed type (same rank). Here especially the rule 3.1 leads to many unexpected results and hard to find bugs. Example: int main(){ unsigned int a = 10; int b = -1; auto c = b - a; // c is unsigned and the value is 4294967285 } To avoid this problem, always use the static_cast operator if dealing with mixed signed/unsigned types .","title":"Usual Arithmetic Conversion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#show-the-type","text":"Sometimes, it is useful to print the type, so that we can see the real type of some complicated template code. For that, the following template can be used: #include <string_view> template <typename T> constexpr auto type_name() { std::string_view name, prefix, suffix; #ifdef __clang__ name = __PRETTY_FUNCTION__; prefix = \"auto type_name() [T = \"; suffix = \"]\"; #elif defined(__GNUC__) name = __PRETTY_FUNCTION__; prefix = \"constexpr auto type_name() [with T = \"; suffix = \"]\"; #elif defined(_MSC_VER) name = __FUNCSIG__; prefix = \"auto __cdecl type_name<\"; suffix = \">(void)\"; #endif name.remove_prefix(prefix.size()); name.remove_suffix(suffix.size()); return name; } Usage: std::cout << type_name<std::remove_pointer_t<typename std::vector<std::string>::iterator::value_type>>() << std::endl; // Prints: class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > Source on SO","title":"Show the Type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#standard-library-types","text":"","title":"Standard Library Types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#smart-pointers","text":"For managing resources in dynamic memory, smart pointers (sometimes called handles ) should be used. They manage the memory (alocation, dealocation) automatically, but their usage requires some practice. There are two types of smart pointers: - std::unique_ptr for unique ownership - std::shared_ptr for shared ownership","title":"Smart Pointers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#creation","text":"Usually, we create the pointer together with the target object in one call: - std::make_unique<T>(<OBJECT PARAMS>) for unique pointer - std::make_shared<T>(<OBJECT PARAMS>) for shared pointer These methods work well for objects, but cannot be used for arbitrary array initialization (only the empty/zero-initialized array can be created using these methods). For arbitrary array initialization, we need to use the smart pointer constructor: std::unique_ptr<int[]> ptr(new int[]{1, 2, 3}); Counter-intuitively, smart pointers created using the empty constructor of the respective pointer type does not default-construct the target object, but initialize the pointer to null instead: std::unique_ptr<My_class> ptr(std::null_ptr); // ptr is null std::unique_ptr<My_class> ptr(); // ptr is also null","title":"Creation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#shared-pointer","text":"Pointer to object with non-trivial ownership (owned by multiple objects).","title":"Shared Pointer"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdreference_wrapper","text":"cppreference Reference wrapper is a class template that can be used to store references in containers or aggregated objects. The disintinction from normal references is that the reference wrapper can be copied and assigned, so it does not prevent the copy/move operations on the object it belongs to. Otherwise, it behaves like a normal reference: it has to be assigned to a valid object and it cannot be null.","title":"std::reference_wrapper"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#strings","text":"In C++, there are two types of strings: - std::string is an owning class for a string. - std::string_view is a non-owning class for a string. Also, there is a C-style string ( char* ), but it is not recommended to use it in modern C++. The difference between std::string and std::string_view is best explained by a table below: std::string std::string_view Owning Yes No Null-terminated Yes No Size Dynamic Static Lifetime Managed by the string Managed by the underlying char sequence Can be constexpr No Yes and the following code: std::string_view sv = \"hello\"; // sv is a view of the string literal \"hello\" std::string s = \"hello\"; // s stores a copy of the string literal \"hello\"","title":"Strings"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#string-literals","text":"The standard string literal is writen as \"literal\" . However, we need to escape some characters in such literals, therefore, a raw string literal is sometimes more desirable: R\"(literal)\" If our literal contains ( or ) , this is stil not enough, however, the delimiter can be extended to any string with a maximum length of 16 characters, for example: R\"lit(literal)lit\" .","title":"String Literals"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#formatting-strings","text":"The usage of modern string formating is either - std::format from the <format> header if the compiler supports C++20 string formatting ( compiler support ) or - fmt::format from the fmt library if not. Either way, the usage is the same: format(<literal>, <arguments>) where the literal is a string literal with {} placeholders and the arguments are the values to be inserted into the placeholders. The placeholders can be filled width - argument identification, if we want to use the same argument multiple times or change the order in the string while keep the order of arguments in the function call or - format specification. These two parts are separated by : , both of them are optional. The most common format specifications are: - data type: - d for decimal integer - f for floating point number - s for string - width and precision, in the format <width>.<precision> . Both values can be dynamic: std::format(\"{:{}.{}f}\", a, b, c) formats a float number a with width b and precision c . The formating reference can be found in the cppreference","title":"Formatting strings"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#spliting-the-string-into-tokens","text":"If we want to split the string on patern, the easiest way is to use the split view from the ranges library: auto parts = std::ranges::views::split(str, \"-\");","title":"Spliting the string into tokens"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#converting-string-to-int","text":"There are simple functions for converting std::string to numbers, named std::stoi , std::stoul , etc. See cppreference for details. For C strings, the situation is more complicated.","title":"Converting string to int"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#substring","text":"A substring can be obtained using a member function substr : str.substr(str.size() - 1, 1)) // returns the last character as a string","title":"Substring"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#change-the-case","text":"Unfortunatelly, the STL has case changing functions only for characters, so we need to iterate over the string ourselfs. The boost has a solution, however: #include <boost/algorithm/string.hpp> auto upper = boost::to_upper(str);","title":"change the case"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#date-and-time","text":"The date and time structure in C++ is std::tm. We can create it from the date and time string using std::get_time function: std::tm tm; std::istringstream ss(\"2011-Feb-18 23:12:34\"); ss >> std::get_time(&tm, \"%Y-%b-%d %H:%M:%S\");","title":"Date and time"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#collections","text":"std::array std::vector","title":"Collections"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#sets","text":"Normal set collection for C++ is std::unordered_set . By default, the set uses a Hash , KeyEqual and Allocator template params provided by std functions. However, they need to exist, specifically: - std::hash<Key> - std::equal_to<Key> - std::allocator<Key> So either those specializations needs to be provided by the snadard library (check cppreference), or you have to provide it.","title":"Sets"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#providing-custom-hash-function","text":"There are two options for providing custom hash function for a type T s: - implementing an explicit specialization of the template function std::hash<T> - providing the Hash template param when constructing the hash The first method is prefered if we want to provide a default hash function for some type for which there is no hash function specialization in the standard library. The second method is prefered only when we want some special hash function for a type T for which std::hash<T> is already defined.","title":"Providing custom hash function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implementing-custom-hash-function","text":"First check whether the hash function is not provide by STL on cppreference . Then, many other hash specializations are implemented by boost, check the reference . If there is no implementation, we can implement the hash function as follows (example for set): template<> struct std::hash<std::unordered_set<const Request*>> { size_t operator()(const std::unordered_set<const Request*>& set) const { std::hash<const Request> hash_f; size_t sum{0}; for (const Request* r : set) { sum += hash_f(*r); } return sum; } }; Important implementation details: - the function needs to be implemented inside std or annonymous namespace, not inside a custom namespace - do not forget to add template<> above the function, this indicates that it is a template specialization.","title":"Implementing custom hash function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#maps","text":"The maps has similar requiremnts for keys as the requirements for set value types (see previous section). The hash map type is called std::unordered_map .","title":"Maps"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#geeting-value-by-key","text":"To access the map element, the array operator ( [] ) can be used. Note however, that this operator does not check the existence of the key, even if we do not provide a value. Example: std::unordered_map<int,std::string> map; map[0] = \"hello\" map[0] = \"world\" // OK, tha value is overwritten a = map[1] // a == map[1] == \"\" unintuitively, the default value is inserted if the key does not exist Therefore, if we just read from the map, it is safer to use the at() member function.","title":"Geeting value by key"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#inserting-into-map","text":"There are three options: 1. map[key] = value; or 2. map.insert({key, value}); 3. map.emplace(key, value); There are some considerations with these options: - 1 inserts the value into the map even if the key already exists, overwriting the previous value. 2 and 3 do not overwrite the new value, instead, they return the position in the map and the indicator of success ( true if the insertion happend). - 1 requires the value to be default constructible and assignable - 3 avoids the creation of temporary objects, it sends references to key and value directly to the map.","title":"Inserting into map"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#tuples","text":"We have two standard class templates for tuples: - std::pair for pairs - std::tuple for tuples with unlimited size Although named differently, these class templates behaves mostly the same.","title":"Tuples"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#creating-tuples","text":"There are two ways of creating a tuple: - constructor ( auto p = std::pair(...) ) - initializer ( auto p = {} ) Beware that by default , the deduced types are decayed, i.e., const and references are removed and the tuple stores value types . If you need to store the reference in a tuple, you have to specify the type: auto p = std::pair<int, constr std::string&>(...) Also, beware that the RVO does not apply for tuple members. This means that if we store values types in the tuple, the types are copied/moved, and in conclusion, they have to by copyable/movable! This is the reason why we frequently use smart pointers in tuples even though we would reurn directly by value if we returned a single value.","title":"Creating tuples"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#creating-tuples-with-stdmake_pairstdmake_tuple","text":"TLDR: from C++17, there is no reason to use make_pair / make_tuple . There are also factory methods make_pair / make_tuple . Before C++17, argument deduction did not work for constructors, so there is a dedicated method for creating tuples. However, now we can just call the constructor and the template arguments are deduced from the constructor arguments. Also, the make_pair / make_tuple functions can only produce tuples containing values, not references (even if we specify the reference type in the make_pair / make_tuple template argument, the returned tuple will be value-typed).","title":"Creating tuples with std::make_pair/std::make_tuple"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#accessing-tuple-members","text":"The standard way to access the tuple/pair mamber is using the std::get function: auto tuple = std::tuple<int, std::string, float>(0, \"hello\", 1.5); auto hello = std::get<1>(tuple);","title":"Accessing tuple members"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#structured-binding-unpacking-tuples-into-variables","text":"If we don't need the whole tuple objects, but only its members, we can use a structured binding . Example: std::pair<int, int> get_data(); void main(){ const auto& [x, y] = get_data(); }","title":"Structured binding - unpacking tuples into variables"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#unpacking-tuples-to-constructor-params-with-stdmake_from_tuple","text":"We cannot use structured binding to unpack tuple directly into function arguments. For normal functions, this is not a problem, as we can first use structured binding into local variables, and then we use those variables to call the function. However, it is a problem for parent/member initializer calls, as we cannot introduce any variables there. Luckily, there is a std::make_from_tuple template function prepared for this purpose. Example: std::tuple<int,float> get_data(){ ... } class Parent{ public: Parent(int a, float b){...} { class Child: public Parent{ public: Child(): Parent(std::make_from_tuple<Parent>(get_data())){} }","title":"Unpacking tuples to constructor params with std::make_from_tuple"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdoptional","text":"cppreference std::optional<T> is a class template that can be used to store a value of type T or nothing. The advantage over other options like null pointers or is that the std::optional is a value type, so it can wrap stack objects as well. The type T must satisfy std::is_move_constructible_v<T> (must be either movable or copyable). The usage is easy as the class has a value constructor from T and a default constructor that creates an empty optional. Also, the type T is convertible to std::optional<T> , and std::nullopt is convertible to an empty optional. Finally, std::optional<T> is convertible to bool , so it can be used in if statements. A typical usage is: class My_class{ public: My_class(int a, int b); } std::optional<My_class> f(){ ... return My_class(a, b); // or return {a, b}; // or, in case of fail return std::nullopt; } std::optional<int> a = f(); if(a){ // a has a value }","title":"std::optional"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#value-categories","text":"[cppreferencepreerecege/value_category). In many contexts, the value category of an expression is important in deciding whether the code compiles or not, or which function or template overload is chosen. Therefore, it is usefull to be able to read value categories. expression value types: - lvalue , meaning left-value. An expression typically on the left side of compound expression a statement, e.g. variable, member, or function name. Also, lvalues expressions are are: - function ratoalls to fuctions returning lvalue - assignments - ++a , --a and similar pre operators - *a indirection - string literal - cast - prvalue , meaning pure rvalue. It is either a result of some operand ( + , / ) or a constructor/initializer result. The foloowing expressions are prvalues: - literals with exception of string literals, e.g.: 4 , true , nullptr - function or operator calls that return rvalue (non-reference) - a++ , a-- and other post operators - arithmetic and logical expressions - &a address of expression - this - non-type template parameters, unless they are references - lambda expressions - requires expressions and concept spetializations - xvalue , meaning expiring value. These valaues usually represent lvalues converted to rvalues. Xvalue expressions are: - function call to functions returning rvalue reference (e.g., std::move ). - member object expression ( a.m ) if a is an rvlaue and m is a non-reference type - glvalue = lvalue || xvalue . - rvalue = prvlaue || xvalue .","title":"Value Categories"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#operators","text":"C++ supports almost all the standard operators known from other languages like Java or C#. Note that the standard also supports alternative tokens for some operators (e.g., && -> and , || -> or , ! -> not ). However, these are not supported by all compilers. In MSVC, the /permissive- flag needs to be used to enable these tokens.","title":"Operators"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#user-defined-operators","text":"In C++ there are more operators than in other popular es like Python or Java. Additionally, these operators can be overloaded. See cppreferencen page for detailed description.","title":"User-defined Operators"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#comparison-operators","text":"","title":"Comparison Operators"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deafult-comparison-operators","text":"For details, see cppreference . The != is usually not a problem, because it is implicitely generated as a negation of the == operator. However, the == is not generated by default, even for simple classes . To force the generation of a default member-wise comparison operator, we need to write: bool operator==(const My_class&) const == default; However, to do that, all members and base classes have to ae the operator == defined, otherwise the default operator will be implicitely deleted.","title":"Deafult Comparison Operators"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#functions","text":"cppreference","title":"Functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deciding-between-free-function-member-function-and-static-member-function","text":"Basically, you should decide as follows: 1. Function needs access to instance -> member function 2. Function - should be called only by class members (i.e., member functions), so we want to limit its visibility, or - we need to access static members of the class -> static member function 1. Otherwise -> free function","title":"Deciding between free function, member function and static member function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#argument-parameter-conversions","text":"Arg/param value reference rvalue value - - std::move reference implicit copy - copy constructor rvalue - not possible -","title":"Argument-parameter Conversions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-parameters","text":"Default function parameters in C++ works similarly to other languages: int add(int a, int b = 10); add(1, 2) // 3 add(1) // 11 However, the default parameters works only if we call the function by name. Therefore, we cannot use them in std::function and similar contexts. Example: std::function<int(int,int)> addf = add; std::function<int(int)> addf = add; // does not compile addf(1) // does not compile","title":"Default Parameters"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-parameters-and-inheritance","text":"TLDR: do not use default parameters in virtual functions. The default parameters are resolved at compile time. Therefore, the value does not depend on the actual type of the object, but on the declared type of the variable. This have following consequences: - the default parameters are not inherited - A* a = new B(); a->foo() will call B::foo() , with the default parameters of A::foo() To prevent confusion with inheritence we should use function overloading instead of default parameters in virtual functions (like in Java).","title":"Default Parameters and Inheritance"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#return-values-and-nrvo","text":"For deciding the return value format, refer to the return value decision tree . Especially, note that NRVO is used in modern C++ and therefore, we can return all objects by value with no overhead most of the time. The NRVO works as follows: 1. compiler tries to just tranfer the object to the parent stack frame (i. e. to the caller) without any move or copy 2. if the above is not possible, the move constructor is called. 3. if the above is not possible, the copy constructor is called. From C++17, the RVO is mandatory, therefore, it is unlikely that the compiler use a move/copy constructor. Consequently, most of the times, we can just return the local variable and let the rest to the compiler: unique_ptr<int> f(){ auto p = std::make_unique<int>(0); return p; // works, calls the move constructor automatically in the worst case (pre C++17 compiler) // return move( p ); // also works, but prevents NRVO } The NRVO is described also on cppreference together with initializer copy elision.","title":"Return values and NRVO"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-overlaoding","text":"Both normal and member funcions in C++ can be overloaded. The oveload mechanic, however, is quite complicated. There can be three results of overload resolution of some function call: - no function fits -> error - one function fits the best - multiple functions fits the best -> error The whole algorithm of overload resolution can be found on cppreference . First, viable funcions are determined as functions with the same name and: - with the same number of parameters - with a greater number of parameters if the extra parameters has default arguments If there are no viable functions, the compilation fails. Otherwise, all viable functions are compared to get the best fit. The comparison has multiple levels. The basic principle is that if only one function fits the rules at certain level, it is chosen as a best fit. If there are multiple such functions, the compilation fails. Levels: 1. Better conversion priority (most of the time, the best fit is found here, see conversion priority and ranking bellow) non-template constructor priority","title":"Function Overlaoding"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#conversion-prioritiy-and-ranking","text":"cppreference When the conversion takes priority during the best viable function search, we say it is better . The (incomplete) algorithm of determining better conversion works as follows: 1. standard conversion is better than user defined conversion 2. user defined conversion is better then elipsis ( ... ) conversion 3. comparing two standard conversions: 1. if a conversion sequence S1 is a subsequence of conversion sequence S2, S1 is better then S2 2. lower rank priority 3. rvalue over lvalue if both applicable 4. ref over const ref if both applicable","title":"Conversion prioritiy and ranking"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#conversion-sequence-ranks","text":"exact match promotion conversion : includes class to base conversion","title":"Conversion sequence ranks"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constructor-argument-type-resolution-in-list-initialization","text":"When we use a list initailization and it results in a constructor call, it is not immediatelly clear which types will be used for arguments as the initialization list is not an expression. These types are, however, critical for the finding of best viable constructor. The following rules are used to determine the argument types (simplified): 1.","title":"Constructor argument type resolution in list initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#auto-return-type","text":"For functions that are defined inside declaration (template functions, lambdas), the return type can be automatically deduced if we use the auto keyword. The decision between value and reference return type is made according to the following rules: - return type auto -> return by value - return type auto& -> return by reference - return type auto* -> return by pointer - return type decltyype(auto) -> the return type is decltype(<RETURN EXPRESSION>) See more rules on cppreference Note that the auto return type is not allowed for functions defined outside the declaration (unless using the trailing return type).","title":"auto return type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-visibility","text":"The member function visibility is determined by the access specifier, in the same manner as the member variable visibility. For free functions , the visibility is determined by the linkage specifier . Without the specifier, the function is visible. To make it visible only in the current translation unit, we can use the static specifier. An equivalent way to make a function visible only in the current translation unit is to put it into an anonymous namespace : namespace { void f() {} } This way, the function is visible in the current translation unit, as the namespace is implicitly imported into it, but it is not visible in other translation units, because anonymous namespaces cannot be imported. One of the other approches frequently used in C++ is to put the function declaration into the source file so it cannot be included from the header. This solution is, however, flawed, unsafe, and therefore, not recommended . The problem is that this way, the function is still visible to the linker, and can be mistakenly used from another translation unit if somebody declare a function with the same name.","title":"Function visibility"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#usefull-stl-functions","text":"std::for_each : iterates over iterable objects and call a callable for each iteration std::bind : Binds a function call to a variable that can be called some parameters of the function can be fixed in the variable, while others can be provided for each call each reference parameter has to be wrapped as a reference_wrapper std:mem_fn : Creates a variable that represents a callable that calls member function std::copy : Copy elements from one range to another. std::accumulate : computes the sum of some iterable std::transform : transforms some range and stores it to another. For the output iterator, you can use std::back_inserter .","title":"Usefull STL functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deleting-functions","text":"cppreference We can delete functions using the delete keyword. This is mostly used for preventing the usage of copy/move constructors and assignment operators. However, it can be used for any function, as we illustrate in the following example: class My_class{ print_integer(int a){ std::cout << a << std::endl; } // we do not want to print doubles even they can be implicitly converted to int print_integer(double a) = delete; }","title":"Deleting functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#classes-and-structs","text":"The only difference between a class and a struct is that in class, all members are private by default.","title":"Classes and structs"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#friend-declaration","text":"Sometimes, we need to provide an access to privat e members of a class to some other classes. In java, for example, we can put both classes to the same package and set the members as package private (no specifier). In C++, there is an even stronger concept of friend classes. We put a friend declaration to the body of a class whose private members should be accessible from some other class. The declaratiton can look as follows: Class To_be_accesssed { friend Has_access; } Now the Has_access class has access to the To_be_accesssed 's private members. Note that the friend relation is not transitive, nor symetric, and it is not inherited. cppreference","title":"Friend declaration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-friends","text":"If we want a template to be a friend, we can modify the code above: class To_be_accesssed { template<class T> friend class Has_access; } Now every Has_access<T> is a friend of To_be_accesssed . Note thet we need to use keyword class next to friend . We can also use only a template spetialization: class To_be_accesssed { friend class Has_access<int>; } or we can bound the allowed types of two templates togehter if both Has_access and of To_be_accesssed are templates: template<class T> class To_be_accesssed { friend class Has_access<T>; }","title":"Template friends"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#initialization-and-assignment","text":"","title":"Initialization and Assignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#loacal-variables-initializationassignment","text":"Initialization happens in many contexts : - in the declaration - in new expression - function parameter initialization - return value initialization The syntax can be: - (<expression list>) - = expression list - {<initializer list>} Finally, there are multiple initialization types, the resulting initialization type depends on both context and syntax: - Value initialization: std::string s{}; - Direct initialization: std::string s{\"value\"} - Copy initialization: std::string s = \"value\" - List initialization: std::string s{'v', 'a', 'l', 'u', 'e'} - Aggregate initialization: char a[3] = {'a', 'b'} - Reference initialization: char& c = a[0] - Default initialization: std::string s","title":"Loacal variables initialization/assignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#list-initialization","text":"List initialization initializes an object from a list. he list initialization has many forms, including: - My_class c{arg_1, arg_2} - My_class c = {arg_1, arg_2} - my_func({arg_1, arg_2}) - return {arg_1, arg_2} The list initialization of a type T can result in various initializations/constructios depending on many aspects. Here is the simplified algorithm: 1. T is aggregate -> aggregate initialization 2. The initializer list is empty and T has a default constructor -> value initialization 3. T has an constructor accepting std::initializer_list -> this constructor is called 4. other constructors of T are considered, excluding explicit constructors","title":"List initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#value-initialization","text":"cppreference This initializon is performed when we do not porvide any parameters for the initialization. Depending on the object, it results in either defualt or zero initialization.","title":"Value initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#aggregate-initialization","text":"Aggregate initialization is an initialization for aggregate types. It is a form of list initialization. Example: My_class o1{arg_1, arg_2}; My_class o2 = {arg_1, arg_2}; // equivalent The list initialization of type T from an initializer list results in aggregate initialization if these conditions are fullfilled: - the initializer list contains more then one element - T is an aggregate type","title":"Aggregate initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#nested-initialization","text":"It is not possible to create nested initializatio statements like: class My_class{ int a, float b; public: My_class(ina a, float b): a(a), b(b) } std::tuple<int, My_class>{2, {3, 2.5}} // does not compile std::tuple<int, My_class>{2, My_class{3, 2.5}} // correnct version","title":"Nested initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#member-initializationassignment","text":"There are two ways of member initialization: - default member initialization - initialization using member initializer list And then, there is an assignment option in constructor body . Reference: - default member initialization - constructor and initializer list One way or another, all members should be initialized at the constructor body at latest , even if we assign them again during all possible use cases. Reason: - some types can have arbitrary values when unassigned. This can lead to confusion when debugging the class, i.e., the member can appear as initialized even if it is not. - easy support for overloading constructors, we can sometimes skip the call to the constructor with all arguments - we can avoid default arguments in the constructor It is important to not use virtual functions in member initialization or constructor body , because the function table is not ready yet, so the calls are hard wired, and the results can be unpredictable, possibly compiler dependent.","title":"Member Initialization/Assignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-member-initialization","text":"Either a brace initializer : My_class{ int member{1} } or an equals initializer : My_class{ int member = 1 }","title":"Default Member Initialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#member-initializer-list","text":"Either using direct initialization (calling constructor of member ): My_class{ My_class(): member(1){ } or list initialization : My_class{ My_class(): member{1}{ }","title":"Member Initializer List"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constructor-body","text":"My_class{ My_class(){ member = 1 } }","title":"Constructor Body"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#comparison-table","text":"Ordered by priority, i.e., each method makes the methods bellow ignored/ovewritten if applied to the same member. Type | In-place | works for const members --| --|-- Constructor body | no | no Member initializer list | yes | yes Default member initializer | yes, if we use direct initialization | yes","title":"Comparison Table"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constructors-and-special-member-functions","text":"cppreference Special member functions are member functions that are someetimes defined implicitely by the compiler. The special member functions are: - default (no parameter) constructor - copy Cconstructor - copy sssignment - move constructor - move assignment - destructor Along with the comparison operators, these are the only functions that can be defaulted (see below).","title":"Constructors and Special Member Functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constructor","text":"","title":"Constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#defualt-variant","text":"The default constructor just create an empty object. The default constructor is not implicitly generated if: - there is anothe constructor declared, including copy and move constructor - there is some member that cannot be defaulty initialized","title":"Defualt Variant"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#explicit-constructor","text":"Sometimes, a normal constructor can lead to unexpected results, especially if it has only a single argument: class My_string { public: String(std::string string); // convert from std::string String(int length); // construct empty string with a preallocated size }; String s = 10; // surprise: empty string of size 10 istead of \"10\" To prevent these surprising conversion, we can mark the constructor explicit . The explicit keyword before the constructor name prevents the assigment using this constructor. The explicit constructor has to be explicitelly called.","title":"Explicit constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#copy-constructor","text":"cppreference A copy constructor is called if an object is initialized from another object unless the move constructor is called as a better fit or the call is optimized out by copy elision . Some examples: - initializing a new object from an existing object: My_class a; My_class b = a; // copy constructor called My_class c(a); // copy constructor called passing an object to a function by value: void f(My_class a){...} My_class a; f(a); // copy constructor called returning an object by value where the type is not movable and the compiler cannot optimize the call out. we call the copy constructor directly","title":"Copy Constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implicit-declaration-and-implicit-deletion","text":"The copy constructor for type T is implicitely-declared if T has no declared user-defined copy constructors. If some there are some user-defined copy constructors, we can still force the implicit declaration of the copy constructor using the default keyword However, the implicit declaration does not mean that the copy constructor can be used! This is because the copy constructor can be implicitely defined as deleted . This happens if any of the following conditions is true: 1. T has a non-static data member that cannot be copied. This can happen if any of the following is true: - it has a deleted copy constructor, - the copy constructor is inaccessible ( protected, private ) - the copy constructor is ambiguous (e.g., multiple inheritance) 1. T has a base class that cannot be copied, i.e., 1, 2, or 3 applies to at least one base class 1. T has a non-static data member or base class with inaccessible destructor 1. T has a rvlaue data member 1. T has a user-defined move constructor or move assignment operator (this rule does not apply for defaulted copy constructor) The default implementationof copy constructor calls recursively the copy constructor of all base classes and on all members. For a pointer member, the copy object\u2019s member points to the same object as the original object\u2019s member","title":"Implicit declaration and implicit deletion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#checking-if-a-class-is-copy-constructible","text":"We can check if a class is copy constructible using the std::is_copy_constructible type trait.","title":"Checking if a class is copy constructible"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#copy-assignment","text":"Copy Assignment is needed when we use the = operator with the existing class instances, e.g.: Class instanceA {}; Class instanceB; instanceB = instance A","title":"Copy Assignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#move-constructor","text":"cppreference Move constructor semantic is that the new object takes the ownership of the resources of the old object. The state of the old object is unspecified, but it should not be used anymore. Move constructor is typically called when the object is initaialized from xvalue (but not prvalue!) of the same type. Examples: - returning xvalue: Type f(){ Type t; return std::move(t); } passing argument as xvalue: f(Type t){ ... } Type t f(std::move(t)); initializing from xvalue: Type t; Type t2 = std::move(t); Note that for prvalues, the move call is eliminated by copy elision . Therefore, some calls that suggest move constructor call are actually optimized out: Type f(){ Type t; return t; // no move constructor call, copy elision } Type t = T(f()) // no move constructor call, copy elision Move constructor is needed: - to cheaply move the object out from function if RVO is not possible - to store the object in vector without copying it Note that a single class can have multiple move constructors, e.g.: both Type(Type&&) and Type(const Type&&) .","title":"Move Constructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implicit-declaration-and-implicit-deletion_1","text":"The move constructor for type T is implicitely-declared if T has no declared copy constructors, copy assignment operators, move assignment operators, or destructors. If some of the above is declared, we can still force the implicit declaration of the move constructor using the default keyword However, that does not mean that the move constructor can be used! This is because the move constructor can be implicitely defined as deleted . This happens if any of the following conditions is true: 1. T has a non-static data member that cannot be moved. A member cannot be moved if any of the following is true: - it has a deleted, inaccessible (protected, private), or ambiguous move constructor, - it is a reference, - it is const -qualified 1. T has a base class that cannot be moved, i.e., 1, 2, or 3 applies to at least one base class 1. T has a non-static data member or base class with inaccessible destructor","title":"Implicit declaration and implicit deletion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#checking-if-a-class-is-move-constructible","text":"We can check if a class is move constructible using the std::is_move_constructible type trait. However, the std::is_move_constructible does not check if the move constructor is accessible! Instead it checks if the call to the move constructor is valid (can success, compiles). The call can success if the move constructor is accessible, but it can also success if it is not accessible, but the class has a copy constructor, which is used instead. To check if the move constructor is accessible, we have to manually check the conditions, or disable the copy constructor.","title":"Checking if a class is move constructible"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#move-assignment","text":"","title":"Move Assignment"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#trivial-special-member-functions","text":"The special member functions are called trivial if they contain no operations other then copying/moving the members and base classes. For a special member function of type T to be trivial, all of the following conditions must be true: - it is implicitly-declared or defaulted - T has no virtual functions - T has no virtual base classes - the constructor for all direct base classes is trivial - the constructor for all non-static data members is trivial","title":"Trivial special member functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#destructor","text":"We need destructor only if the object owns some resources that needs to be manually deallocated","title":"Destructor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#setting-special-member-functions-to-default","text":"","title":"Setting special member functions to default"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#rules","text":"if you want to be sure, delete everything you don\u2019t need most likely, either we need no custom constructors, or we need three (move and destructor), or we need all of them.","title":"Rules"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#rules-for-typical-object-types","text":"","title":"Rules for Typical Object Types"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#simple-temporary-object","text":"the object should live only in some local context we don\u2019t need anything","title":"Simple Temporary Object"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#unique-object","text":"usually represents some real object usually, we need constructors for passing the ownership: move constructor move assignment noe sin","title":"Unique Object"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-object","text":"copyable object We need copy constructor copy assignment move constructor move assignment","title":"Default Object"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#operators_1","text":"In C++ there are more operators than in other popular es like Python or Java. Additionally, thsese operator s can be overloaded. See cppreferencen page for detailed description.","title":"Operators"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#comparison-operators_1","text":"","title":"Comparison Operators"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deafult-comparison-operators_1","text":"For details, see cppreferencetassnpneolan . The != is usually not a problem, because it is implicitely generated as a negation of the == operator. However, the == is not generated by default, even for simple classes . To force the generation of a default member-wise comparison operator, we need to write: bool operator==(const My_class&) const = default; Mowever, to do that, all members and base classes have to have the operator == defined, otherwise the default operator will be implicitely deleted. The comparability can be checked with a std::equality_comparable<T> concept ic_assert(std::equality_comparable<My_class>);","title":"Deafult Comparison Operators"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#const-vs-non-const","text":"The const keyword makes the object non-mutable. This means that: - it cannot be reassigned - non-const member functions of the object cannot be called The const keyword is usually used for local variables, function parameters, etc. For members, the const keyword should not be used , as it sometimes breaks the move operations on the object. For example we cannot move f om a const std::unique_ptr<T> object. While this is also true for local variable, in members, it can lead to hard to find compilation errors, as a single const std::unique_ptr<T> member deep in the object hierarchy breaks the move semantic for the whole class and all subclasses.","title":"Const vs non-const"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#avoiding-duplication-between-const-and-non-const-version-of-the-same-function","text":"To solve this problem without threathening the const-correctness, we need to implement the const version of a function and call it from the non-const one with double type cast: - one that converts this to const, so we can call the const version of the function - another one that removes const from the return value Example: const Content& get_content(unsigned index) const { Content content = ... // complicated code to get the right content return content; } Content& get_content(unsigned index){ return const_cast<Content&>(std::as_const(this*).get_content()); } there are no common supercalss or i## Const/non const overloads and inheritance Normally, the compiler can safely choose the best match between const and non-const overloads. The problem can happen when each version is in a different place in the class hierarchy. Example: class Base { public: const int& get() const { return some; } protected: int some; }; class A : public virtual Base { public: int& get() { return some; } }; class B : public A {}; B test; test.get(); // ambiguous function error The problem is that the overload set is created for each class in the hierarchy separately. So if the overload was resolved prior the virtual function resolution, we would have only one version (non-const), which would be chosen, despite not being the best overload match in both overload sets. To prevent such unexpected result, some compilers (GCC) raise an ambiguous function error in such situations. To resolve that, we can merge the overload sets in class B : class B : public A { using Base:get; using A:get; };","title":"Avoiding duplication between const and non-const version of the same function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#avoiding-duplication-between-const-and-non-const-version-of-the-same-function_1","text":"To solve this problem without threathening the const-correctness, we need to implement the const version of a function and call it from the non-const one with double type cast: - one that converts this to const, so we can call the const version of the function - another one that removes const from the return value Example: const Content& get_content(unsigned index) const { Content content = ... // complicated code to get the right content return content; } Content& get_content(unsigned index){ return const_cast<Content&>(std::as_const(this*).get_content()); }","title":"Avoiding duplication between const and non-const version of the same function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#io-and-filesystem","text":"","title":"IO and Filesystem"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#standard-io","text":"The simple way to print to standard input is: std::cout << \"Hello world\" << std::endl; To return to the begining of the line and overwrite the previous output, we can use the '\\r' character: std::cout << \"Hello world\" << '\\r' << std::flush;","title":"Standard IO"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#file-path-manipulation","text":"Although we can use strings to work with file paths in C++, the standard format which is also easy to use is std::filesystem::path from the filesystem library . Basic operations: - To create a path , we jusct call std::filesystem::path(<string path>) . - We can easily join two paths by auto full_path = <path 1> / <path 2> ; - To get the asolute path , we call - std::filesystem::absolute(<path>) to get the path as CWD/<path> - std::filesystem::canonical(<path>) to get the dots resolved. Note that this method throws exception if the path does not exists. - The path to the current working directory can be obtained by calling std::filesystem::current_path() and set using std::filesystem::current_path(<path>) . - To change the file extension (in the C++ representation, not in the filesystem), we can call the replace_extension method.","title":"File path manipulation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#filesystem-manipulation","text":"cppreference","title":"Filesystem manipulation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#copying","text":"To copy, we can use std::filesystem::copy(<source path>, <destination path>[, <options>]) function. The options parameter type is std::filesystem::copy_options . This enum is a bitmask type, therefore, multiple options can be combined using the | operator. Example: auto options = std::filesystem::copy_options::recursive | std::filesystem::copy_options::overwrite_existing; std::filesystem::copy(\"C:/temp/data\", \"c:/data/new\", options); Note that unlike the unix cp command, the copy function does not copy the directoy itself , even if the destination directory exists. Suppose we have two direcories: - C:/temp/new - C:/data/ And we want to copy the new folder, so that the result is: C:/data/new/ . In bash, this will be: cp -r C:/temp/new C:/data/ While in C++, we need to do: std::filesystem::copy(\"C:/temp/new\", \"C:/data/new\", std::filesystem::copy_options::recursive);","title":"Copying"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#creating-directories","text":"To create a directory, we can use std::filesystem::create_directory(<path>) function. This function fails if the parent directory does not exist. To create the parent directories as well, we can use std::filesystem::create_directories(<path>) function.","title":"Creating directories"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#removing-files-and-directories","text":"To remove a file or an empty directory, we can use std::filesystem::remove(<path>) function. To remove a directory with all its content, we can use std::filesystem::remove_all(<path>) function listed on the same page of cppreference.","title":"Removing files and directories"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#other-useful-functions","text":"std::filesystem::exists(<path>) std::filesystem::is_directory(<path>) std::filesystem::is_regular_file(<path>) std::filesystem::is_empty(<path>)","title":"Other useful functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#simple-line-by-line-io","text":"","title":"Simple line by line IO"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#input","text":"For input, we can use std::ifstream : std::ifstream file; file.open(<path>); std::string line; while (std::getline(file, line)) { // do something with the line } file.close(); The important thing is that we need to check whether the open call was successful. The open function never throws an exception, even if the file does not exist , which is a common case. Instead, it only sets the failbit of the stream. Without some check, the failure is hidden as an ifstream in a fail state behaves as if it was empty.","title":"Input"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#output","text":"For line by line output, we use std::ofstream : std::ofstream file; file.open(<path>); batch_file << \"first line\" << std::endl; batch_file << \"second line\" << std::endl; ... batch_file.close();","title":"Output"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#csv","text":"","title":"csv"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#input_1","text":"","title":"Input"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#output_1","text":"For csv output, we can usually use the general line-by-line approach.","title":"Output"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#inheritance","text":"Inheritance in C++ is similar to other languages, here are the important points: - To enable overiding, a member function needs to be declared as virtual . Otherwise, it will be just hidden in a child with a function with the same name, and the override specifier cannot be used (see Shadowing). - Multiple inheritance is possible. - No interfaces. Instead, you can use abstract class with no data members. - Virtual functions without implementation needs = 0 at the end of the declaration (e.g.: virtual void print() = 0; )","title":"Inheritance"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#polymorphism","text":"Polymorphism is a concept for abstraction using which we can provide a single interface for multiple types that share the same parent. In C++, to use the polymorphism, we need to work with pointers or references . Imagine that we have these two class and a method that can process the base class: class Base { }; class Derived: public Base { }; void process_base(Base* base) { } Now we can use it lake this: Derived* derived = new Derived(); Base* base = derived; // we easilly can convert derived to base process_base(base); process_base(derived); // we can call the function that accepts a base pointer with a derived pointer We can do the same with smart pointers: void process_base_sh(std::shared_ptr<Base> base) { } std::shared_ptr<Derived> derived_sh = std::make_shared<Derived>(); std::shared_ptr<Base> base_sh = derived_sh; process_base_sh(base_sh); process_base_sh(derived_sh);","title":"Polymorphism"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#shadowinghiding-why-is-a-function-from-parent-not-available-in-child","text":"Members in child with a same name as another members in parent shadows those members (except the case when the parent member is virtual). When a member is shadowed/hiden, it is not available in the child class and it cannot be called using the child class instance. This can be counter-intuitive for functions as the shadowing considers only the name, not the signature . Example: class Base { public: void print() { printf(\"Base\\n\"); } }; class Child: public Base { public: void print(std::string type) { printf(\"Child \" + type + \"\\n\"); } }; int main() { Child child; child.print(); // does not compile, as the print() is hidden by print(std::string) return 0; }","title":"Shadowing/Hiding: why is a function from parent not available in child?"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#how-to-call-a-hidden-function","text":"There are two ways how to call a hideen function: 1. we can use the using declaration in the child to introduce the hidden function: c++ class Child: public Base { public: using Base::print; // now the print() is available in Child void print(std::string type) { printf(\"Child \" + type + \"\\n\"); } }; 2. Usiang a fully qualified name of the method: c++ int main() { Child child; child.Base::print(); return 0; }","title":"How to call a hidden function?"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constructors","text":"Parent constructor is allways called from a child. By default, an empty construcor is called. Alternatively, we can call another constructor in the initializer. When we do not call the parent constructor in the child's initializer and the parenhas no empty constructor, a compilation error is raised.","title":"Constructors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#enablinging-parent-constructors-in-child","text":"Implicitly, all methods from parent classes are visible in child, with exception of constructors. Constructors can be inherited manually with a using declaration, but only all at once. To enable only some constructors, we need to repeat them manually as child constructors and call parent construcors from them.","title":"Enablinging Parent Constructors in Child"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#inheritance-and-constructorsdestructors","text":"To prevent the future bugs with polymorphic destruction calls, it's a good habit to declare a public virtual destructor in each base class : class Base{ public: virtual ~Base() = default; } Otherwise, the following code will not call the child destructor: Child* child = new Child(); Base* base = (Base) child; delete base; But when defining destructor, constructor and move operations are not impliciotely generated. Moreover, the copy operations are generated enabling a polymorphic copy, which results in slicing. Therefore, the best approach for the base class is to herefore to: - declare the virtual destrucor as default - declare the default constructor . We need a default constructor, unless we use a diferent constructor and we want to disable the default one. - declare the copy and move operations as protected**. This way, the polymorpic copy is not possible, but proper copy/move operations are generated for every child class.","title":"Inheritance and Constructors/Destructors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#initializing-base-class-members","text":"The base class members cannot be initialized in the child constructor initializer. Instead, we need to create a constructor in the base class and call it from the child constructor initializer.","title":"Initializing base class members"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#slicing","text":"Polymorphism does not go well with value types. When a value type is copied, the only part that remains is the part writen in the code. That means that copying base_2 = base_1 result in a new Base object in base_2 , even if base_1 is an instance of child. Abstract classes therefore cannot be used as function value arguments at all . To pass a polymorphic type as a value to a library function, we need a copyable wrapper that forwards all calls to the undelying polymorphic type.","title":"Slicing"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#checking-the-type","text":"There is no equivalent of Java's instanceof in C++. To check the type. it is possible to use dynamic cast: Child& child = dynamic_cast<Child&>(parent) In case of failure, std::bad_cast is thrown. To prevent exceptions (i.e., we need the type check for branching), we can use pointers: Child* child = dynamic_cast<Child*>(&parent) In this case, if the cast fails, then child == nullptr . Note that to use the dynamic_cast on a type, the type, the type needs to have at least one virtual method . However, this should not be an issue as the type should have at least a virtual destructor.","title":"Checking the Type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#covariant-return-type","text":"Covariant return type is a concept of returning a narower type id derived class than the return type specified in base. Example: class Base { public: virtual Base& get() = 0; }; class Derived: public Base{ public: Derived& get() override { return *this; } }; It works with template classes too: template<class T> class Derived_template: public Base { public: Derived_template<T>& get() override { return &this; } };","title":"Covariant Return Type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#use-method-from-parent-to-override-a-method-from-other-parent","text":"Unlike in java, a parent method cannot be used to implement an interface of a child . Example: class Interface { public: virtual void print() = 0; }; class Base { public: virtual void print() { printf(\"Base\\n\"); } }; class Child: public Base, public Interface { public: }; int main() { Child child; // does not compile, as Child is an abstract class child.print(); return 0; } The above code does not compile as in C++, the parent print() method is not used as an impementation of print() from the interface (like it works e.g. in Java). There simplest solution to this problem is to override the method in Child and call the parent method staticaly: class Child: public Base, public Interface { public: void print() override { Base::print(); } };","title":"Use Method from Parent to Override a Method from Other Parent"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#multiple-inheritance-and-virtual-base-classes","text":"wiki cppreference Multiple inheritance is possible in C++. However, it can lead to some problems. Consider the following example: class A { public: int a; }; class B: public A {}; class C: public A {}; class D: public B, public C {}; It may not be obvious, but the class D has two instances of A in it. This is because the B and C both have their own instance of A . This is certainly not what we want as this way, we have two copies of A::a in D , which are only accessible using qualified names ( D::B::a and D::C::a ) and which can have different values.","title":"Multiple inheritance and virtual base classes"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#virtual-inheritance","text":"To mitigate this problem, we can use the virtual inheritance . The virtual inheritance is used when we want to have only one instance of a base class in a child class, even if the base class is inherited multiple times. To use the virtual inheritance, we need to declare the base class as virtual in all child classes: class A { public: int a; }; class B: public virtual A {}; class C: public virtual A {}; class D: public B, public C {};","title":"Virtual Inheritance"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#multiple-copymove-calls-with-virtual-inheritance","text":"However, this solves only the problem of having multiple instances of the same base class. But there are also problems with the copy and move operations. In the above example, if the class D is copied or moved, it calls the copy/move operations of B and C , which in turn call the copy/move operations of A . This means that the A is copied/moved twice , which is not what we want. To solve this we need to manually define the copy/move operations of classes in the hierarchy so that the copy/move operations of the base class are called only once. However this can be a complex task. Also, it can backfire later when we extend the hierarchy.","title":"Multiple copy/move calls with virtual inheritance"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#other-sources","text":"SO answer SO answer 2","title":"Other sources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#templates","text":"The templates are a powerful tool for: - generic programming, - zero-overhead interfaces, - and metaprogramming. Although they have similar syntax as generics in Java, they are principialy different both in the way they are implemented and in the way they are used. There are two types of templates: - function templates - class templates","title":"Templates"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#syntax","text":"","title":"Syntax"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-declaration","text":"Both for classes and functions, the template declaration has the following form: template<<template parameters>> The template parameters can be: - type parameters: class T - value parameters: int N - concept parameters: std::integral T","title":"Template Declaration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-definition","text":"The definition of template functions or functions fo the template class requires the template declaration to be present. The definition has the following form: template<<template parameters>> <standard function definition> Here, the template parameters are the function template parameters if we define a template function, or the class template parameters if we define a function of a template class. If the template function is a member of a template class, we have to specify both the template parameters of the function and the template parameters of the class: template<<class template parameters>> template<<function template parameters>> <standard class function definition> Note that the template definition has to be in the header file , either directly or included from another header file. This includes the member function definitions of a template class, even if they are not templated themselves and does not use the template parameters of the class.","title":"Template definition"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#organization-rules","text":"*.h : declarations *.tpp template definitions *.cpp non-template definitions. For simplicity, we include the tpp files at the end of corresponding header files. If we need to speed up the compilation, we can include the tpp files only in the source files that needs the implementations , as described on SE To speed up the build it is also desireble to move any non-template code to source files , even through inheritance, if needed.","title":"Organization rules"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#providing-template-arguments","text":"A template can be instantiated only if all the template arguments are provided. Arguments can be: - provided explicitly: std::vector<int> v; or sum<int>(1,2) - deduced - from the initialization (classes): std::vector v = {1,2,3}; - from the context (functions): sum(1,2); - defaulted: template<class T = int> class A {}; template<class T = int> int sum<T>(T a, T b = 0) { return a + b; } auto s = sum(1, 2); A a(); If we want the template arguments to be deduced or defaulted, we usually use the <> : template<class T = int> class A {}; A<> a(); // default argument is used std::vector<A<>> v; // default argument is used In some cases, the <> can be ommited, e.g., when declaring a variable: A a; // default argument is used // but std::vector<A> v; // error, the A is considered a template here, not the instantiation The rules for omitting the <> are quite complex. Therefore, it is better to always use the <> when we want to use the default arguments.","title":"Providing Template Arguments"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#rules-for-omitting-the","text":"We can ommite the <> in the following cases: - when declaring a variable: A a; - when using the type in a function call: f(A()); - when instantiating a template class: class B: public A {}; We cannot ommite the <> in the following cases: - When we use the template as a nested type: std::vector<A<>> v; , not std::vector<A> v; - in the return type of a function: A<> f() , not A f() - When declaring an alias: using B = A<> not using B = A - for template template parameters.","title":"Rules for omitting the &lt;&gt;"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#default-template-arguments","text":"Default template arguments can be used to provide a default value for any template parameter except parameter packs. For template classes, there is a restriction that after a default argument is used, all the following parameters must have a default argument as well, except the last one wchich can be parameter pack.","title":"Default Template Arguments"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-argument-deduction","text":"Details on cppreference . Template argument deduction should work for: - constructors - function and operator calls - storing the function pointer","title":"Template Argument Deduction"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#class-template-argument-deduction-ctad","text":"Details on cppreference . The main difference from the function templete argument deduction is that in CTAD, all the template arguments needs to be specified, or all must not be specified and must be deducible. Apart from that, there are more subtle differences arising of a complex procedure that is behind CTAD. We explain CTAD principle using a new concept (not a C++ concept :) ) called deduction guides .","title":"Class Template Argument Deduction (CTAD)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deduction-guides","text":"The CTAD use so called deductione guides to deduce the template parameters. Deduction guides can be either implicit or explicit. To demonstrate the principle, let's first start with user-defined deduction guides.","title":"Deduction Guides"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#user-defined-deduction-guides","text":"Let's have an iterator wrapper class below: template<class E, Iterator<E> I> class Iter_wrapper{ public: explicit Iter_wrapper(I iterator){ ... } ... }; Here, the argument E cannot be deduced from argument I , despite the use of the Iterator concept may indicate otherwise. We can still enable the deduction by adding the following deduction guide: template<class I> Iter_wrapper(I iterator) -> Iter_wrapper<decltype(*iterator),I>; Here, the part left from -> represents the constructor call that should be guided, and the part right from -> defines the argument types we want to deduce. Some more details about user defined deduction guides are also on the Microsoft Blog .","title":"User defined deduction guides"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#implicit-deduction-guides","text":"The vast majority of deduction guidedes used in CTAD are implicit. The most important implicit deduction guides are: - constructor deduction guides - copy deduction guides The copy deduction guide has the following form: template<<class template parameters>> <class>(<class><class template parameters> obj) -> <class><class template parameters>; For a simple wrapper class: template<class C> class Wrapper{ ... } template<class C> Wrapper(Wrapper<C> obj) -> Wrapper<C>; // implicitelly defined copy deduction guide The constructor deduction guides has the following form: template<<class template parameters>> <class>(<constructor arguments>) -> <class><class template parameters>; For a simple wrapper class: template<class C> class Wrapper{ Wrapper(T&& ref); } template<class C> Wrapper(C&&) -> Wrapper<C>; // implicitelly defined constructor deduction guide","title":"Implicit deduction guides"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deduction-guides-resolution","text":"Note that CTAD is a process independent of the constructor overload! . First an appropriate deduction guide is used to deduce the class template argumnets, this process can fail if there is no guide. Only then, the overload resolution begins. Most of the time, it is not so important and we can just look at the constructor that is chosen by the constructor overload resolution process and see the used deduction guids and consequently, the resulting template arguments. Sometimes, however, this simplified understanding can lead to confusing results: template<class C> class Wrapper{ Wrapper(T&& ref); Wrapper(double&& ref); // special overload for double } auto w1 = Wrapper(1.5) // the double overload is called In the above example, it may be surprising that the second constructor can be called, as it does not have the class argument present, so the implicit deduction guide cannot work: template<class C> Wrapper(double&&) -> Wrapper<C>; // C unknown! However, it compiles and works, because the deduction guide from the first constructor is used for CTAD, and then, the second constructor is chosen by the constructor overload.","title":"Deduction guides resolution"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-specialization","text":"Template specialization is a way to provide a different implementation of a template for a specific type. For example, we can provide a different implementation of a template for a std::string type. Imagine following class: // declaration template<class T> class Object{ public: void print(T value) }; // definition template<class T> void Object<T>::print(T value){ std::cout << value << std::endl; } Now, we can provide a different implementation for std::string : // declaration template<> class Object{ public: void print(std::string value) }; template<> void Object<std::string>::print(std::string value){ std::cout << value << std::endl; } There are two types of template specialization: - full specialization : exact specification for all template arguments - partial specialization : exact specification for a subset of template arguments and/or non-type template arguments To demonstrate the difference, let's have a look at the following example: // declaration template<class T, class C> class Object{}; // primary template // full specialization template<> class Object<int, std::string>{}; // full specialization // partial specializations template<class C> class Object<int, C>{}; // not a full specialization, as C is not specified template<std::integral T, My_concept C> class Object<T, C>{}; // not a full specialization, types are not exactly specified While behaving similarly, there are some important differences between the two types: - Full specialization is a new type. Therefore, it must be defined in the source file ( .cpp ), just like any other class or function and it must have a separate declaration. On the other hand, partial specialization is still just a template, so it must be defined in the header file ( .h or .tpp ). - For functions, we cannot provide a partial specialization . For member functions we can solve this by specializing the whole class. The solution for any function is to alloow all types in the function and use if constexpr to select the correct implementation: template<class T, class C> class Object{ public: bool process(T value, C config){ if constexpr (std::is_same_v<T, std::string>){ return process_string(value, config); } else { return process_value(value, config); } } }; Note that here, the if constexpr requires the corresponding else branch. Otherwise, the code cannot be discarded during the compilation. Example: template<class T, class C> class Object{ public: bool process(T value, C config){ if constexpr (std::is_same_v<T, std::string>){ return process_string(value, config); } return process_value(value, config); // this compiles even if T is std::string } };","title":"Template Specialization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#templates-and-namespaces","text":"If the templated code resides in a namespace, it can be tempting to save few lines of code by sorrounding both .h and .tpp files using one namespace expression: // structs.h hile namespace my_namespace { // declarations... #include 'structs.tpp' } // structs.tpp // definitions However, this can confuse some IDEs (e.g., false positive errors in IntelliSense), so it is better to introduce the namespace in both files: // structs.h hile namespace my_namespace { // declarations... } #include 'structs.tpp' // structs.tpp namespace my_namespace { // definitions } Don't forget to close the file and reopen it after the change to clear the errors.","title":"Templates and Namespaces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-complicated-types-as-template-arguments","text":"Sometimes, it can be very tricky to determine the template argument we need in order to use the template. The correct argument can be for example a return value of some function, templete function, or even member function of a template instanciation which has other templates as argument... To make it easier, we can, istead of suplying the correct arguments, evaluate an expression that returns the correct type and then use the decltype specifier. For more info, see the Determining Type from Expressions section.","title":"Using Complicated Types as Template Arguments"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#type-traits","text":"The purpose of type traits is to create predicates involving teplate parameters. Using type traits, we can ask questios about template parameters. With the answer to these questions, we can even implement conditional compilation, i.e., select a correct template based on parameter type. Most of the STL type traits are defined in header type_traits . A type trate is a template with a constant that holds the result of the predicate, i.e., the answer to the question. More about type traits","title":"Type Traits"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#usefull-type-traits","text":"std::is_same std::is_base_of std::is_convertible std::conditional : enables if-else type selection","title":"Usefull Type Traits"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#replacement-for-old-type-traits","text":"Some of the old type traits are no longer needed as they can be replaced by new language features, which are more readable and less error prone. Some examples: - std::enable_if can be replaced by concepts: // old: enable_if template<class T> void f(T x, typename std::enable_if_t<std::is_integral_v<T>, void> = 0) { std::cout << x << '\\n'; } // new: concepts template<std::integral T> void f(T x) { std::cout << x << '\\n'; }","title":"Replacement for old type traits"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#concepts","text":"cppreference Concepts are named sets of requiremnets. They can be used instead of class / typename keywords to restrict the template types. The syntax is: template<class T, ....> concept concept-name = constraint-expression The concept can have multiple template parameters. The first one in the declaration stands for the concept itself, so it can be refered in the constraint expression. More template parameters can be optionally added and their purpose is to make the concept generic.","title":"Concepts"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constraints","text":"Constraints can be composed using && and || operatos. For atomic constaints declaration, we can use: - Type traits: template<class T> concept Integral = std::is_integral<T>::value; Concepts: template<class T> concept UnsignedIntegral = Integral<T> && !SignedIntegral<T>; Requires expression: template<typename T> concept Addable = requires (T x) { x + x; }; Either form we chose, the atomic constraint have to always evaluate to bool.","title":"Constraints"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#requires-expression","text":"Requires expressions ar ethe most powerfull conctraints. The syntax is: requires(parameter list){requirements} There are four types of requirements that can appear in the requires expression: - simple requiremnet : a requirement that can contain any expression. Evaluates to true if the expression is valid. requires (T x) { x + x; }; type requirement : a requiremnt checking the validity of a type: requires { typename T::inner; // required nested member name typename S<T>; // required class template specialization typename Ref<T>; // required alias template substitution }; compound requirement : Checks the arguments and the return type of some call. It has the form: {expression} -> return-type-requirement; requires(T x) { {*x} -> std::convertible_to<typename T::inner>; } Nested requirement : a require expression inside another requires expression: requires(T a, size_t n) { requires Same<T*, decltype(&a)>; // nested }","title":"Requires Expression"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#auto-filling-the-first-template-argument","text":"Concepts have a special feature that their first argument can be autoffiled from outer context. Consequentlly, you then fill only the remaining arguments. Examples: //When using the concept template<class T, class U> concept Derived = std::is_base_of<U, T>::value; template<Derived<Base> T> void f(T); // T is constrained by Derived<T, Base> // When defining the concept template<typename S> concept Stock = requires(S stock) { // return value is constrained by std::same_as<decltype(stock), double> {stock.get_value()} -> std::same_as<double>; }","title":"Auto filling the first template argument"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stl-concepts","text":"iterator concepts","title":"STL Concepts"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#usefull-patterns","text":"","title":"Usefull Patterns"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constrain-a-template-argument","text":"Imagine that you have a template function load and an abstract class Loadable_interface that works as an interface: class Loadable_interface{ virtual void load() = 0; }; template<class T> void load(T to_load){ ... to load.load() ... }; Typically you want to constraint the template argument T to the Loadable_interface type, so that other developer clearly see the interface requirement, and receives a clear error message if the requirement is not met. In Java, we have an extend keyword for this purpose that can constraint the template argument. In C++, this can be solved with concepts. First we have to define a concept that requires the interface: template<typename L> concept Loadable = std::is_base_of_v<Loadable_interface, L>; Than we can use the concept like this: template<Loadable T> void load(T to_load){ ... to load.load() ... };","title":"Constrain a Template Argument"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constraint-a-concept-argument","text":"Imagine that you have a concept Loadable that requires a method load to return a type T restricted by a concept Loadable_type . One would expect to write the loadable concept like this: template<typename L, Loadable_type LT> concept Loadable = requires(L loadable) { {loadable.load()} -> LT; }; However, this is not possible, as there is a rule that concept cannot not have associated constraints . The solution is to use an unrestricted template argument and constrain it inside the concept definition: template<typename L, typename LT> concept Loadable = Loadable_type<LT> && requires(L loadable) { {loadable.load()} -> LT; };","title":"Constraint a Concept Argument"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#sources","text":"https://en.cppreference.com/w/cpp/language/constraints Requires expression explained","title":"Sources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#interfaces","text":"In programming, an interface is usualy a set of requirements that restricts the function or template parameters, so that all types fulfiling the requiremnet can be used as arguments. Therte are two ways how to create an interface in C++: - using the polymorphism - using templates argument restriction While the polymorphism is easier to implement, the templating is more powerful and it has zero overhead. The most important thing is probably that despite these concepts can be used together in one application, not all \"combinations\" are allowed especialy when using tamplates and polymorphism in the same type. Note that in C++, polymorphism option work only for function argument restriction, but we cannot directly use it to constrain template arguments (unlike in Java). To demonstrate all possible options, imagine an interface that constraints a type that it must have the following two functions: int get_value(); void set_value(int date); The following sections we will demonstrate how to achieve this using multiple techniques.","title":"Interfaces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#interface-using-polymorfism","text":"Unlike in java, there are no interface types in C++. However, we can implement polymorfic interface using abstract class. The following class can be used as an interface: class Value_interface{ virtual int get_value() = 0; virtual void set_value(int date) = 0; } To use this interface as a fuction argument or return value, follow this example: std::unique_ptr<Value_interface> increment(std::unique_ptr<Value_interface> orig_value){ return orig_value->set_value(orig_value->get_value() + 1); } This system works in C++ because it supports multiple inheritance. Do not forget to use the virtual keyword, otherwise, the method cannot be overriden. Note that unlike in other languages, in C++, the polymorphism cannot be directly use as a template (generic) interface. Therefore, we cannot use the polymorfism alone to restrict a type.","title":"Interface using polymorfism"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-template-argument-restriction-as-an-interface","text":"To use template argument restriction as an interface, we can use concepts. The following concept impose the same requirements as the interface from the polymorphism section: To use template argument restriction as an interface, we can use concepts. The following concept impose the same requirements as the interface from the polymorphism section: template<class V> concept Value_interface = requires(V value_interface){{value_interface.get_value()} -> std::same_as<int>; } && requires(V value_interface, int value){{value_interface.set_value(value)} -> std::same_as<void>; } Remember that the return type of the function has to defined by a concept , the type cannot be used directly. Therefore, the following require statement is invalid: requires{(V value_interface){value_interface.get_value()} -> int; } To use this interface as an template argument in class use: template<Value_interface V> class ... And in function arguments and return types: template<Value_interface V> V increment(V orig_value){ return orig_value.set_value(orig_value.get_value() + 1);","title":"Using template argument restriction as an interface"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#restricting-the-member-function-to-be-const","text":"To restrict the member function to be const, we neet to make the type value const in the requires expression: template<class V> concept Value_interface = requires{(const V value_interface) {valvalue_interfaceue.get_value() -> std::same_as<int>;}; };","title":"Restricting the member function to be const"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-concepts-and-polymorphism-together-to-restrict-template-parameters-with-abstract-class","text":"We cannot restrict template parameters by polymorphic interface directly, however, we can combine it with concept. The folowing concept can be used together with the interface from the polymorphic interface section: template<class V> concept Value_interface_concept = requires std::is_base_of<Value_interface,V> Neverthless, as much as this combination can seem to be clear and elegent, it brings some problems. . We can use concepts to imposed many interfaces on a single type, but with this solution, it can lead to a polymorphic hell. While there is no problem with two concepts that directly requires the same method to be present with abstract classes, this can be problematic. Moreover, we will lose the zero overhead advantage of the concepts, as the polymorphism will be used to implement the interface.","title":"Using concepts and polymorphism together to restrict template parameters with abstract class"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#the-conflict-between-templates-and-polymorphism","text":"As described above, messing with polymorphism and templates together can be tricky. Some examples:","title":"The Conflict Between Templates and Polymorphism"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#no-virtual-member-function-with-template-parameters","text":"An example: a virtual (abstract) function cannot be a template function ( member template function cannot be virtual), so it cannot use template parameters outside of those defined by the class template.","title":"No Virtual Member Function with Template Parameters"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#polymorphism-cannot-be-used-inside-template-params","text":"If the functin accepts MyContainer<Animal> we cannot call it with MyContainer<Cat> , even if Cat is an instance of Animal.","title":"Polymorphism cannot be used inside template params"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#possible-solutions-for-conflicts","text":"do not use templates -> more complicated polymorphism ( type erasure for members/containers) do not use polymorphism -> use templates for interfaces an adapter can be used","title":"Possible solutions for conflicts"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#polymorphic-members-and-containers","text":"When we need to store various object in the same member or container, we can use both templates and polymorphism. However, both techniques has its limits, summarized in the table below: | | Polymorphism | Templates | | -- | -- | -- | | The concrete type has to be known at compile time | No | Yes | For multiple member initializations, the member can contain any element. | No , the elements have to share base class. | Yes | | For a single initialization, the containar can contain multiple types of objects | Yes , if they have the same base class | No | We can work with value members | No | Yes | When using the interface, we need to use downcasting and upcasting | Yes | No","title":"Polymorphic members and containers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#deciding-between-template-and-polymorphism","text":"Frequently, we need some entity(class, function) to accept multiple objects through some interface. We have to decide, whether we use templates, or polymorphism for that interface. Some decision points: - We need to return the same type we enter to the class/function -> use templates - We have to access the interface (from outside) without knowing the exact type -> use polymorphism - We need to restrict the member/parametr type in the child -> use templates for the template parameter - if you need to fix the relation between method parameters/members or template arguments of thouse, you need to use templates - If there are space considerations, be aware that every parent class adds an 8 byte pointer to the atribute table In general, the polymorphic interface have the following adventages: - easy to implement - easy to undestand - similar to what people know from other languages On the other hand, the interface using concepts has the following adventages: - no need for type cast - all types check on compile time -> no runtime errors - zero overhead - no object slicing -> you don't have to use pointers when working with this kind of interface - we can save memory because we don't need the vtable pointers","title":"Deciding between template and polymorphism"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#iterators-and-ranges","text":"If we want to iterate over elements in some programming language, we need to fullfill some interface. In Java, this interface is called Iterable . Also, there is usually some interface that formalize the underlying work, in Java, for example, it is called Iterator . In C++, however, the interface for iteration is not handled by polymorphism. Instead, it is handled using type traits and concepts. On top of that, there are multiple interfaces for iteration: - legacy iteration, e.g., for (auto it = v.begin(); it != v.end(); ++it) - STL algorithms, e.g., std::find(v.begin(), v.end(), 42) - STL range algorithms, e.g., std::ranges::find(v, 42) - STL range adaptors, e.g., std::ranges::views::filter(v, [](int x){return x > 0;}) The following table summarizes the differences between the interfaces: |---| Plain iteration | STL algorithms | STL range algorithms | STL range adaptors | |---|---|---|---|---| | Interface | type traits | type traits | concepts | concepts | | Iteration | eager | eager | eager | lazy | | Modify the underlying range | no | yes | yes | no | | Can work on temporaries * | yes | yes | yes | no | *If the operation modifies the data, i.e., sorting, shuffling, transforming, etc. The examples below demonstrate the differences between the interfaces on the following task: create a vector of 10 elements with values 0,1,2,...,9, i.e., the same as Python range(10) . // plain iteration std::vector<int> vec(10); int i = 0; for (auto it = vec.begin(); it != vec.end(); ++it) { *it = i; ++i; } // legacy algorithm std::vector<int> vec(10); std::iota(vec.begin(), vec.end(), 0); // C++11 way, legacy interface using type traits // range algorithm std::vector<int> vec(10); std::ranges::iota(vec.begin(), vec.end(), 0); // basically the same, but the constructor arguments are constrained with concepts // same using adaptor auto range = std::views::iota(0, 10); std::vector vec{range.begin(), range.end()}; // in-place vector construction","title":"Iterators and ranges"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#terminology","text":"range : the object we iterate over (Iterable in Java) iterator : the object which does the real work (Iterator in Java) Usually, a range is composed from two iterators: - begin : points to the begining of the range, returne by <range_object>.begin() - end : points to the end of the object, returned by <range_object>.end() Each iterator implements the dereference ( * ) operator that acces the element of the range the iterator is pointing to. Depending on the iterator type, the iterator also supports other operations: ++ , -- to iterate along the range, array index operator ( [] ) for random access, etc. Most of the STL collections (vector, set,...) are also ranges.","title":"Terminology"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#how-to-choose-the-correct-interface","text":"when deciding which interface to use, we can use the following rules: 1. If the number of tasks and the complexity of the tasks is high, use the legacy iteration . It is hard to write a 20 line for loop with various function calls as algorithm or adaptor and the result would be hard to read. 1. Otherwise, if you need to preserve the original range as it is or you need to compose multiple operations, use the STL range adaptors . 1. Otherwise, use the STL range algorithms . Note that the in this guide, we do not consider the legacy algorithms. With the availability of the STL algorithms, there is no reason to use the legacy algorithms, except for the backward compatibility or for the algorithms that are not yet implemented in the STL. Also note that some STL algorithms are principially non-modifying, e.g., std::ranges::find or std::ranges::count . These algorithms logically do not have the adaptor equivalent.","title":"How to choose the correct interface?"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stl-ranges","text":"https://en.cppreference.com/w/cpp/ranges In C++ 20 there is a new range library that provides functional operations for iterators. It is similar to functional addon in Java 8. As explained in the beginning of thi chapter, there are two ways how to use the STL ranges: - using the range algorithms ( ranges::<alg name> ) that are invoked eagerly. - using the range adaptors ( ranges::views::<adaptor name> ) that are invoked lazily. Note that the range algorithms and adaptors cannot produce result without an input, i.e., we always need a range or collection on which we want to apply our algorithm/adapter.","title":"STL ranges"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stl-range-adaptors","text":"The difference of range adaptors to range algorithms is that the range adapters are lazy, i.e., they do not produce any result until they are iterated over. This is similar to the Python generators. The advantage is that we can chain multiple adaptors together and the result is computed only when we iterate over the final adaptor. Note that due to the lazy nature of the adaptors, the underlying range has to be alive during the whole iteration . Therefore, we cannot use the adaptors on temporaries, e.g., we cannot use the adaptors directly in the constructor of a vector, or we cannot use the adaptors on a temporary range returned by a function. A custom view can be created so that it can be chained with STL views. However, it has to satisfy the view concept , and more importantly, it should satisfy the view semantic, i.e., it should be cheap to copy and move (without copying the underlying data).","title":"STL range adaptors"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#useful-range-algorithms","text":"std::shuffle : shuffles the elements in the range (formerly std::random_shuffle ). std::adjacent_find : finds the first two adjacent elements that are equal. Can be used to find duplicates if the range is sorted.","title":"Useful range algorithms"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#other-resources","text":"https://www.modernescpp.com/index.php/c-20-the-ranges-library","title":"Other Resources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#boost-ranges","text":"In addition to the STL range algorithms and adaptors, boost has it's own range library with other more complex algorithms and adaptors.","title":"Boost ranges"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#boost-range-requirements","text":"Sometimes, it is hard to say why a type does not satisfy some of the requirements for boos ranges. Fortunatelly, the boost provides concepts for checking whether a type satisfy each specific range model. Example: BOOST_CONCEPT_ASSERT(( boost::SinglePassRangeConcept<std::vector<int>> )); // true Also, it is necessary to check whether the value of the iterator can be accessed: BOOST_CONCEPT_ASSERT(( boost_concepts::ReadableIteratorConcept< typename boost::range_iterator<std::vector<int>>::type > )); // true Most likely, the compiler will complain that boost::range_iterator<R>::type does not exist for your range R . The boost range library generate this type by a macro from the R::iterator type. Therefore, make sure that your range has an iterator type defined, either as: - a type alias to an existing iterator - an iterator nested class Note that <RANGE CLASS>::iterator and <RANGE CLASS>::const_iterator has to be accessible (public).","title":"Boost range requirements"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#sequence-range","text":"The iota algortihm/adapter is used to create a sequence: auto range = std::views::iota(0, 10); auto vec = std::vector(range.begin(), range.end()); Note that we cannot pass the view directly to the vector, as the vector does not have a range constructor.","title":"Sequence Range"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#zip-range","text":"The classical Python like zip iteration is available using the zip adapator , which is not yet supported in MSVC. However, boost provides a similar functionality boost::combine .","title":"Zip range"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#boostcombine","text":"boost::combine example: std::vector<int> va{1, 2, 3}; std::vectro<float> vb{0.5, 1, 1.5}; for(const auto& [a, b]: boost::combine(va, vb)){ ... } Each argument of combine must satisfy boost::SinglePassRange","title":"boost::combine"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#enumerating-range","text":"There is no function in standard library equivalent to the python enumerate. We can use a similar boost solution: #include <boost/range/adaptor/indexed.hpp> for(auto const& el: <range> | boost::adaptors::indexed(0)){ std::cout << el.index() << \": \" << el.value() << std::endl; } However, inside the loop, we have to call the index and value functions, so it is probably easier to stick to the good old extra variable: size_t i = 0; for(auto const& el: <range>) { std::cout << i << \": \" << el << std::endl; ++i; }","title":"Enumerating range"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#sorting","text":"There is no sorted view or something simmiler, so in order to sort a range, we need to: - really sort the object in the range - create an adaptor/view from the range, and then sort the view There are two functions for sorting in the STL algorithm library: - std::sort : old - supports parallelization directly by the policy param - std::ranges::sort : new - supports comparison using projections There are three types of sorting: - natural sorting using the < operator of T : std::sort(<RANGE<T>>) - sorting using a comparator: std::sort(<RANGE>, <COMPARATOR>) , where comparator is a fuction with parameters and return value analogous to the natural sorting operator. - sorting using projection (only availeble in std::ranges::sort ): std::ranges::sort(<RANGE>, <STANDARD GENERIC COMPARATOR>, <PROJECTION>","title":"Sorting"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#sorting-using-projection","text":"When we want to sort the objects by a single property different then natural sorting, the easiest way is to use projection. Example: struct Data { int a; std::string b; ... }; std::vector<Data> data = get_data(); std::sort(data, {}, &Data::b); The trick here is that we can only provide the member that we want to use for sorting, but the sorting logic ( first < second ...) is handeled by a standard comparator (the second argument initialized with {} ). We can have even more complicated projections: std::vector<My_class> objects = get_objects(); std::vector<unsigned> indexes = get_indexes(objects); auto proj = [&objects](std::size_t i) -> const std::string& { return objects[i].get_name(); }; std::ranges::sort(indexes, {}, proj) // sort indexes using the property of objects","title":"Sorting using projection"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#transformation","text":"Transformation alg/views transforms an input range according to a callable. As with other operation, there are thre options: - classical algorithm: std::transform with a direct paralellization using the policy parameter - range algorithm: std::ranges::transform with a support for projections - range view: std::ranges::views::transform - a lazy variant The algorithms (but not the view) also supports binary transformations, i.e., create an output range using two input ranges. Transform view example: std::vector<int> in(3, 0); // [0, 0, 0] auto ad = std::ranges::transform_view(in, [](const auto in){return in + 1;}); std::vector<int> out(ad.begin(), ad.end()); The transform view can be only constructed from an object satisfying ranges::input_range . If we want to use a general range (e.g., vector), we need to call the addapter, which has a same signature like the view constructor itself. The important thing here is that the adapter return type is not a std::ranges::views::transform<<RANGE>> but std::ranges::views::transform<std::ranges::ref_view<RANGE>>> ( std::ranges::ref_view ). Supporting various collections is therefore possible only with teplates, but not with inheritance. Note that unlike in Java, it is not possible to use a member reference as a transformation function (e.g.: &MyClass::to_sting() ). We have to always use lambda functions, std::bind or similar to create the callable.","title":"Transformation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#iterator-concepts","text":"https://en.cppreference.com/w/cpp/iterator C++ 20 has some new iterator concepts, providing various interfaces. However, range-based for loop (for each) does not require any of the new concepts, nor the legacy iterators, its requirements are smaller.","title":"Iterator Concepts"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#boost-iterator-templates","text":"The boost.iterator library provides some templates to implement iteratores easily, typically using some existing iterators and modifying just a small part of it: - for pointer to type (dereference) iterator, you can use boost indirect iterator - zip iterator for Python like iteration over multiple collections - [transform iteratorather useful iterators are also included in the boost.iterator library for using another iterator and just modify the access ( * ) opindex.html). including: - zip iterator. - counting_iterator to create number sequence like Python range - gentransform iterator There are also two general (most powerfull) classes: - iterator adapter - iterator facade","title":"Boost Iterator Templates"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#resources","text":"How to write a legacy iterator iter_value_t","title":"Resources"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#lambda-functions","text":"In c++ lambda functions are defined as: [<capture>](<params>) -> <return_type> { <code> } The rerurn type is optional, but sometimes required (see below). Since C++23, the parantheses are optional if there are no functon parameters.","title":"Lambda Functions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#captures","text":"Anything that we want to use from outside has to appear in capture. To prevent copying, we should capture by reference, using & before the name of the variable. [&var_1] // capture by reference [var_1] // capture by value [&] // default capture by reference For the detailed explanation of the captures, see cppreference .","title":"Captures"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#return-type","text":"The return type of lambda functions can be set only using the trailing return type syntax ( -> <RETURN TYPE> after the function params). The return type can be omited. Note however, that the default return type is auto , so in case we want to return by reference, we need to add at least -> auto , or even a more specific return type.","title":"Return type"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#specifiers","text":"Lambda functions can have special specifiers: - mutable : lambda can modify function parameters capture by copy","title":"Specifiers"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#exceptions","text":"In C++, exceptions works simillarly as in other languages. Standard runtime error can be thrown using the std::runtime_error class: throw std::runtime_error(\"message\"); Always catch exception by reference!","title":"Exceptions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#rethrowing-exceptions","text":"We can rethrow an exception like this: catch(const std::exception& ex){ // do ssomething ... throw; } Note that in parallel regions, the exception have to be caught before the end of the parallel region , otherwise the thread is killed.","title":"Rethrowing Exceptions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#how-to-catch-any-exception","text":"In C++, we can catch any exception with: catch (...) { } However, this way, we cannot access the exception object. As there is no base class for exceptions in C++, there is no way to catch all kind of exception objects in C++.","title":"How to Catch Any Exception"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#noexcept-specification","text":"A lot of templates in C++ requires functions to be noexcept which is usually checked by a type trait std::is_nothrow_invocable . We can easily modify our function to satisfy this by adding a noexcept to the function declariaton. There are no requirements for a noexcept function. It can call functions without noexcept or even throw exceptions itself. The only difference it that uncought exceptions from a noexcept function are not passed to the caller. Instead the program is terminated by calling std::terminate , which otherwise happens only if the main function throws. By default, only constructors, destructors, and copy/move operations are noexcept.","title":"noexcept specification"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stack-traces","text":"Unlike most other languages, C++ does not print stack trace on program termination. The only way to get a stack trace for all exceptions is to set up a custom terminate handler an inside it, print the stack trace. However, as of 2023, all the stack trace printing/generating libraries requires platform dependent configuration and fails to work in some platforms or configurations. Example: void terminate_handler_with_stacktrace() { try { <stack trace generation here>; } catch (...) {} std::abort(); } std::set_terminate(&terminate_handler_with_stacktrace); To create the stacktrace, we can use one of the stacktrace libraries: - stacktrace header from the standard library if the compiler supports it (C++ 23) - cpptrace - boost stacktrace","title":"Stack traces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#logging","text":"There is no build in logging in C++. However, there are some libraries that can be used for logging. In this section we will present logging using the spdlog library. We can log using the spdlog::<LEVEL> functions: spdlog::info(\"Hello, {}!\", \"World\"); By default, the log is written to console. In order to write also to a file, we need to create loggers manually and set the list of sinks as a default logger: const auto console_sink = std::make_shared<spdlog::sinks::stdout_sink_st>(); console_sink->set_level(spdlog::level::info); // log level for console sink auto file_sink = std::make_shared<spdlog::sinks::basic_file_sink_st>(<log filepath>, true); std::initializer_list<spdlog::sink_ptr> sink_list{console_sink, file_sink}; const auto logger = std::make_shared<spdlog::logger>(<LOGGER NAME>, sink_list); logger->set_level(spdlog::level::debug); //log level for the whole logger spdlog::set_default_logger(logger); To save performance in case of an intensive logging, we can set an extended flushing period: spdlog::flush_every(std::chrono::seconds(5));","title":"Logging"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#type-aliases","text":"Type aliases are short names bound to some other types. We can introduce it either with typedef or with using keyword. Examples (equvalent): typedef int number; using number = int; typedef void func(int,int); using func = void(int, int) The using new syntax is more readable, as the alias is at the begining of the expression. But why to use type aliases? Two strong motivations can be: - iImprove the readebility : When we work with a type with a very long declaration, it is wise to use an alias. We can partialy solve this issue by using auto, but that is not a complete solution - Make the refactoring easier : When w work with aliases, it is easy to change the type we work with, just by redefining the alias. Note that type aliases cannot have the same name as variables in the same scope . So it is usually safer to name type aliases with this in mind, i.e., using id_type = .. insted of using id = ..","title":"Type Aliases"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-aliasis","text":"We can also create template aliases as follows: template<class A, typename B> class some_template{ ... }; template<class T> using my_template_alias = some_template<T, int>;","title":"Template Aliasis"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#aliases-inside-classes","text":"The type alias can also be placed inside a class. From outside the class, it can be accessed as <CLASS NAME>::<ALIAS NAME> : class My_class{ public: using number = unsigned long long number n = 0; } My_class::number number = 5;","title":"Aliases inside classes"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#constant-expressions","text":"A constant expression is an expression that can be evaluated at compile time. The result of constant expression can be used in static context, i.e., it can be: - assigned to a constexpr variable, - tested for true using static_assert Unfortunatelly, there is no universal way how to determine if an expression is a constant expression . More on cppreference .","title":"Constant Expressions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#regular-expressions","text":"The regex patern is stored in a std::regex object: const std::regex regex{R\"regex(Plan (\\d+))regex\"}; Note that we use the raw string so we do not have to escape the pattern. Also, note that std::regex cannot be constexpr","title":"Regular expressions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#matching-the-result","text":"We use the std::regex_search to search for the occurence of the pattern in a string. The result is stored in a std::smatch object which contains the whole match on the 0th index and then the macthed groups on subsequent indices. A typical operation: std::smatch matches; const auto found = std::regex_search(string, matches, regex); if(found){ auto plan_id = matches[1].str(); // finds the first group } Note that matches[0] is not the first matched group, but the whole match.","title":"Matching the result"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#namespaces","text":"cppreference Namespace provides duplicit-name protection, it is a similar concept to Java packages. Contrary to java packages and modules, the C++ namespaces are unrelated to the directory structure. namespace my_namespace { ... } The namespaces are used in both declaration and definition (both in header and source files). The inner namespace has access to outer namespaces. For using some namespace inside our namespace without full qualification, we can write: using namespace <NAMESPACE NAME>","title":"Namespaces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#anonymous-namespaces","text":"Anonymous namespaces are declared as: namespace { ... } Each anonnymous namespaces has a different and unknown ID. Therefore, the content of the annonymous namespace cannot be accessed from outside the namespace, with exception of the file where the namespace is declared which has an implicit access to it.","title":"Anonymous namespaces"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#namespace-aliases","text":"We can create a namespace alias using the namespace keyword to short the nested namespace names. Typicall example: namespace fs = std::filesystem;","title":"Namespace aliases"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#decltype-determining-type-from-expressions","text":"Sometimes, it is usefull to declare a type from expression, instead of do it manualy. Using decltype specifier, we can get the resulting type of an expression as if it was evaluated. Examples: struct A { double x; }; const A* a; decltype(a->x) // evaluates to double decltype(std::accumulate(a, [](double sum, double val){return sum + val;})) // evalutes to double We can use the decltype in any context where type is required. Examples: int i = 1 decltype(i) j = 3 std::vector<decltype(j)> numbers;","title":"decltype: Determining Type from Expressions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#the-value-category-of-decltype","text":"The value category of decltype is resolved depending on the value category of an expression inside it: - deltype(<XVALUE>) -> T&& - deltype(<LVALUE>) -> T& - deltype(<RVALUE>) -> T The rvalue conversion can lead to unexpected results, in context, where the value type matters: static_assert(std::is_same_v<decltype(0), decltype(std::identity()(0))>); // error The above expressions fails because: - decltype(0) , 0 is an rvalue -> the decltype result is int - decltype(std::identity()(0)) result of std::identity() is an xvalue -> the decltype result is int&& . Determining Type from Expressions Sometimes, it is usefull to declare a type from expression, instead of do it manualy. Using decltype specifier, we can get the resulting type of an expression as if it was evaluated. Examples: struct A { double x; }; const A* a; decltype(a->x) // evaluates to double decltype(std::accumulate(a, [](double sum, double val){return sum + val;})) // evalutes to double We can use the decltype in any context where type is required. Examples: int i = 1 decltype(i) j = 3 std::vector<decltype(j)> numbers;","title":"The Value Category of decltype"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#determining-the-return-value-type-of-a-function","text":"As we can see above, we can use decltype to determine the return value type. But also, there is a type trait for that: std::invoke_result_t (formerly std::result_of ). The std::invoke_result_t should vbe equal to decltype when aplied to return type, with the following limitations: - we cannot use abstract classes as arguments of std::invoke_result_t , while we can use them inside decltype (using std::declval , see below). -","title":"Determining the Return Value Type of a Function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#construct-object-inside-decltype-with-stddeclval","text":"std::declval is a usefull function designed to be used only in static contexts, inside decltype . It enables using member functions inside decltype without using constructors. Without std::declval , some type expressions are hard or even impossible to costruct. Example: class Complex_class{ Complex_class(int a, bool b, ...) ... int compute() } // without declval decltype<Complex_class(1, false, ...).compute()> // using declval decltype(std::declval<Complex_class>().compute())","title":"Construct object inside decltype with std::declval"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#decltype-and-overloading","text":"in static context, there is no overloading, the vtable is not available. Therefore, we have to hint the compiler which specific overloaded function we want to evaluate. This also applies to const vs non const overloading. The following example shows how to get the const iterator type of a vector: std::vector<anything> vec // non const iter decltype(vec.begin()) // const iter decltype<std::declval<const decltype(vec)>().begin()> Another example shows how to use the const overload inside std::bind : decltype(std::bind(static_cast<const ActionData<N>&(std::vector<ActionData<N>>::*)(size_t) const>(&std::vector<ActionData<N>>::operator[]), action_data)), Above, we used static cast for choosing the const version of the vector array operator. Instead, we can use explicit template argument for std::bind : decltype(std::bind<const ActionData<N>& (std::vector<ActionData<N>>::*)(size_t) const>(&std::vector<ActionData<N>>::operator[], action_data)),","title":"decltype and Overloading"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#parallelization","text":"While there wa no support of parallelization i earlier versions of C++ , now there are many tools.","title":"Parallelization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#standard-threads","text":"","title":"Standard Threads"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#for-each-with-parallel-execution-policy","text":"The function std::for_each can be run with a parallel execution policy to process the loop in parallel.","title":"For-each with Parallel Execution Policy"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#async-tasks","text":"Tasks for asznchronous execution, like file downloads, db queries, etc. The main function is std::async .","title":"Async tasks"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#open-mp","text":"In MSVC, the Open MP library is automatically included and linked. In GCC, we need to find the libs in CmakeLists.txt : find_package(OpenMP REQUIRED)","title":"Open-MP"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#standard-templates-for-callables","text":"","title":"Standard Templates for Callables"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-stdinvoke-to-call-the-member-function","text":"using std::invoke , the cal syntax bool b = (inst.*ptr)() can be replaced with longer but more straighforward call: bool b = std::invoke(ptr, inst, 2)","title":"Using std::invoke to call the member function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-stdmem_fn-to-store-a-pointer-to-member-function-in-a-callable","text":"With std::mem_fn , we can store the pointer to a member function in a callable object. Later, we can call the object without the pointer to the member function. Example: auto mem_ptr = std::mem_fn(&My_class::my_method) bool b = mem_ptr(inst, 2)","title":"Using std::mem_fn to Store a Pointer to Member Function in a Callable"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-a-pointer-to-member-function-as-a-functor","text":"A normal function can be usually send instead of functor, as it can be invoked in the same way. However, in case of member function, we usually need to somehow bind the function pointer to the instance. We can use the std::bind function exactly for that: auto functor = std::bind(&My_class::my_method, inst); bool b = functor(2) Advanteges: - we do not need an access to instance in the context from which we call the member function - we do not have to remember the complex syntax of a pointer to a member function declaration - we receive a callable object, which usage is even simpler than using std::invoke Note that in case we want to bind only some parameters, we need to supply placeholders for the remaining parameters ( std::placeholders ).","title":"Using a Pointer to Member Function as a Functor"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#using-lambdas-instead-of-stdbind","text":"For more readable code and better compile error messages, it is usefull to replace std::bind callls with labda functions. The above example can be rewritten as: auto functor = [inst](int num){return inst.my_method(num);); bool b = functor(2)","title":"Using Lambdas Instead of std::bind"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#store-the-result-of-stdbind","text":"Sometimes, we need to know the return type of the std::bind . In many context, we need to provide the type instead of using auto . But luckily, there is a type exactly for that: std::function . Example: std::function<bool(int)> functor = std::bind(&My_class::my_method, inst); bool b = functor(2) A lambda can also be stored to std::function . But be carefull to add an explicit return type to it, if it returns by a reference. Example: My_class{ public: int my_member } My_class inst; std::function f = [inst](){return &inst.my_member; } // wrong, reference to a temporary due to return type deduction std::function f = [inst]() -> const int& {return &inst.my_member; } // correct More detailed information about pointers to member functions","title":"Store the Result of std::bind"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdmem_fn-and-data-members","text":"Data member pointers can be aslo stored as std::mem_fn . A call to this object with an instance as the only argument then return the data member value. The plain syntax is <type> <class name>.*<pointer name> = <class name>.<member name> , and the pointer is then accessed as <instance>.*<pointer name> . Example: int Car::*pSpeed = &Car::speed; c1.*pSpeed = 2; Usefull STL functions - std::for_each : iterates over iterable objects and call a callable for each iteration - std::bind : Binds a function call to a variable that can be called - some parameters of the function can be fixed in the variable, while others can be provided for each call - each reference parameter has to be wrapped as a reference_wrapper - std:mem_fn : Creates a variable that represents a callable that calls member function","title":"std::mem_fn and Data Members"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdfunction","text":"The std::function template can hold any callable. It can be initialized from: - function pointer/reference, - member function pointer/reference, - lambda function - functor It can be easily passed to functions, used as template parameter, etc. The template parameters for std::function has the form of std::function<<RETURN TYPE>(<ARGUMENTS>)> . Example: auto lambda = [](std::size_t i) -> My_class { return My_class(i); }; std::function<My_class(std::size_t)> f{lambda}","title":"std::function"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdfunction-and-overloading","text":"one of the traps when using std::function is the ambiguity when using an overloade function: int add(int, int); double add(double, double); std::function<int(int, int)> func = add; // fails due to ambiguity. The solution is to cant the function to its type first and then assign it to the template: std::function<int(int, int)> func = static_cast<int(*)(int, int)>add;","title":"std::function and overloading"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#testing-with-google-test","text":"","title":"Testing with Google Test"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#private-method-testing","text":"The testing of private method is not easy with Google Test, but that is common also for other tets frameworks or even computer languages (see the common manual). Some solutions are described in this SO question . Usually, the easiest solution is to aplly some naming/namespace convention and make the function accessible. For free functions: namespace internal { void private_function(){ ... } } For member functions: class MyClass{ public: void _private_function();","title":"Private method testing"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#conditional-function-execution","text":"W know it from other languages: if the function can be run in two (or more) modes, there is a function parameter that controls the execution. Usually, most of the function is the same (otherwise, we eould create multiple fuctions), and the switch controls just a small part. Unlike in other langueges. C++ has not one, but three options how to implement this. They are described below in atable together with theai properties. function parameter template parameter compiler directive good readability yes no no compiler optimization no yes yes conditional code compilation no no yes","title":"Conditional Function Execution"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#function-parameter","text":"void(bool switch = true){ if(switch){ ... } else{ ... } }","title":"Function Parameter"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#template-parameter","text":"template<bool S = true> void(){ if(S){ ... } else{ ... } }","title":"Template Parameter"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#compiler-directive","text":"void(){ #ifdef SWITCH ... #else ... #endif }","title":"Compiler Directive"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#ignoring-warnings-for-specific-line-of-code","text":"Sometimes, we want to suppress some warnings, mostly in libraries we are including. The syntax is, unfortunatelly, different for each compiler. Example: #if defined(_MSC_VER) #pragma warning(push) #pragma warning(disable: <WARNING CODE>) #elif defined(__GNUC__) #pragma GCC diagnostic push #pragma GCC diagnostic ignored \"<WARNING TYPE GCC>\" #elif defined(__clang__) #pragma clang diagnostic push #pragma clang diagnostic ignored \"<WARNING TYPE CLANG>\" #endif .. affected code... #if defined(_MSC_VER) #pragma warning(pop) #elif defined(__GNUC__) #pragma GCC diagnostic pop #elif defined(__clang__) #pragma clang diagnostic pop #endif Note that warnings related to the preprocessor macros cannot be suppressed this way in GCC due to a bug (fixed in GCC 13). The same is true for conditions: #if 0 #pragma sdhdhs // unknown pragma raises warning, despite unreachcable #endif","title":"Ignoring warnings for specific line of code"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#measuring-used-resource","text":"","title":"Measuring used resource"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#memory","text":"","title":"Memory"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#msvc","text":"In MSVC, we can measure the peak used memory using the following code: #include <psapi.h> PROCESS_MEMORY_COUNTERS pmc; K32GetProcessMemoryInfo(GetCurrentProcess(), &pmc, sizeof(pmc)); auto max_mem = pmc.PeakWorkingSetSize","title":"MSVC"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#working-with-tabular-data","text":"Potential libs similar to Python Pandas: - Arrow - Dataframe","title":"Working with tabular data"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#executing-external-commands","text":"The support for executing external commands in C++ is unsatisfactory. The most common solution is to use the system function. However, the system calls are not portable, e.g., the quotes around the command are not supported in Windows Another option is to use the Boost Process library.","title":"Executing external commands"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#unions-and-variants","text":"The idea of a union is to store multiple types in the same memory location. Compared to the polymorphism, when we work with pointers and to templates, where the actual type is determined at compile time, the union actually has a shared memory for all the types. The union can be therefore used in cases where nor polymorphism neither templates are suitable. One example can be storing different unrelated types (e.g., std::string and int ) in a container. We cannot use templates as that require a single type. Nor we can use polymorphism, as the types are unrelated. The big disadvantage of unions is that they are not type safe. The compiler cannot check if the type we are accessing is the same as the type we stored. Therefore, we have to be very careful when using unions. Therefore, unless some special case, we should use std::variant instead of unions .","title":"Unions and Variants"},{"location":"Programming/C%2B%2B/C%2B%2B%20Manual/#stdvariant","text":"The declaration of std::variant is similar to the declaration of std::tuple : std::variant<int, double> v; The std::variant can store any of the types specified in the template parameters. The type of the stored value can be obtained using the std::variant::index method. The value can be accessed using the std::get_if method, which returns a pointer to the stored value. Example: std::variant<int, double> v = 1; std::cout << v.index() << std::endl; // prints 0 std::cout << *std::get_if<int>(&v) << std::endl; // prints 1 A realy usefull feature of std::variant is the std::visit method, which allows us to call a function on the stored value. The function is selected based on the type of the stored value. Example: std::variant<int, double> v = 1; std::visit([](auto&& arg) { std::cout << arg << std::endl; }, v); // prints 1 More on variants: - cppreference - cppstories","title":"std::variant"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/","text":"This guide presents how to prepare the following working environment: One or more from the following toolchains MinGW MSVC GCC Cmake Clion and/or Visual studio vcpkg Toolchain \u00b6 There are various toolchains available on Windows and Linux, but we limit this guide for only some of them, specifically those which are frequently updated and works great with Clion. MSYS2 (Windows only) \u00b6 download follow the installation guide on the homepage install MinGW64 using: pacman -S mingw-w64-x86_64-gcc MSCV (Windows only) \u00b6 install Visual Studio 2019 Comunity Edition Common Compiler Flags \u00b6 /MD , /MT , and similar : these determines the version of the standard run-time library. The /MD flag is the default and also prefered. /nologo : do not print the copyright banner and information messages /EH : exception handeling flags GCC (Linux/WSL) \u00b6 If on Windows, Install the WSL first (preferably WSL 2) sudo apt-get update sudo apt-get upgrade sudo apt-get install build-essential rsync zip ninja-build make 3 For using gcc 10: - sudo apt-get install gcc-10 g++-10 Cmake \u00b6 Windows: Install CMake from https://cmake.org/download/ if your CMake is too old (e.g. error: \u201cCMake 3.15 or higher is required\u201d), update CMake (same as new install) Linux: If cmake is installed already, uninstall it! Do not use the cmake from linux repositories!! Download CMake sh installer from https://cmake.org/download/ install: sudo chmod +x <INSTALLER> sudo <INSTALLER> sudo rm <INSTALLER> add cmake executable to path Other details about CMake can be found in the CMake Manual. vcpkg \u00b6 follow the installation guide , including the user and PowerShell/bash integration add the vcpkg directory to PATH , so the program can be run from anywhere Beware that to run it with sudo on linux, it is not that easy . add a new system variable VCPKG_DEFAULT_TRIPLET , so your default library version installed with vcpkg will be x64 (like our builds), set it to: x64-linux for Linux Compilers x64-windows for MSVC x64-MinGW for MinGW CMake Integration \u00b6 By default, CMake does not see the vcpkg. To set up the appropriate enviroment variables, paths, etc., we need to run cmake commands with path to cmake toolchain file: vcpkg/scripts/buildsystems/vcpkg.cmake . See the IDE and command line section for the detailed instructions how to execute cmake with the path to the vcpkg toolchain file. The toolchain file is executed early on, so it is safe to assume that the environment will be correctly set up before the commands in yor cmake script. Update \u00b6 git pull bootstrap vcpkg again Windows: bootstrap-vcpkg.bat Linux: bootstrap-vcpkg.sh Update package \u00b6 Update vcpkg vcpkg update to get a list of available updates vcpkg upgrade --no-dry-run to actually perform the upgrade you can supply the name of the package (e.g., zlib:x64-windows) as an argument to upgrade just one package Package Features \u00b6 Some libraries have optional features, which are not installed by default, but we can install them explicitely. For example llvm . After vcpkg install llvm and typing vcpkg search llvm : llvm 10.0.0#6 The LLVM Compiler Infrastructure llvm[clang] Build C Language Family Front-end. llvm[clang-tools-extra] Build Clang tools. ... llvm[target-all] Build with all backends. llvm[target-amdgpu] Build with AMDGPU backend. llvm[target-arm] Build with ARM backend. Above, we can see that there are a lot of optional targets. To install the the arm target, for example, we can use vcpkg install llvm[target-arm] . Sometimes, a new build of the main package is required, in that case, we need to type vcpkg install llvm[target-arm] --recurse . Directory Structure \u00b6 Module Installation Scripts \u00b6 They are located in the ports directory. There is no special way how to update just the port dir, so update the whole vcpkg by git pull in case you need to update the list of available packages. Modules \u00b6 Vcpkg has it s own find_package macro in the toolchain file. It executes the script: vcpkg/installed/<tripplet>/share/<package name>/vcpkg-cmake-wrapper.cmake , if exists. Then, it executes the cmake scripts in that directory using the standard find_package , like a cmake config package. IDE \u00b6 Clion \u00b6 Configuration \u00b6 Set default layout \u00b6 Window -> Layouts -> Save changes in current layout Set up new surround with template \u00b6 In Clion, there are two types of surround with templates: surrond with and surround with live template . The first type use simple predefined templates and cannot be modified. However, the second type can be modified and new templates can be added. Toolchain configuration \u00b6 Go to settings -> Build, Execution, Deployment -> toolchain , add new toolchain and set: - Name to whatever you want - The environment should point to your toolchain: - MSVC: C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community - MSYS: C:\\MSYS2 - WSL: From the drop-down list, choose the environment you configured for using with CLion in the previous steps - Credentials (WSL) click to the setting button next to the credentials and fill - host: localhost - port: 2222 - user and password according to your WSL system credentials - Architecture (non WSL): amd64 - CMake: C:\\Program Files\\CMake\\bin\\cmake.exe , for WSL, leave it as it is - other fields should be filled automatically Project configuration \u00b6 Most project settings resides (hereinafter Project settings ) in settings -> Build, Execution, Deployment -> CMake . For each build configuration, add a new template and set: - Name to whatever you want - Build type to debug - To Cmake options , add: - path to vcpkg toolchain file: - Linux: -DCMAKE_TOOLCHAIN_FILE=/opt/vcpkg/scripts/buildsystems/vcpkg.cmake - Windows: -DCMAKE_TOOLCHAIN_FILE=C:/vcpkg/scripts/buildsystems/vcpkg.cmake - Set the correct vcpkg triplet - MSVC: -DVCPKG_TARGET_TRIPLET=x64-windows - MinGW: -DVCPKG_TARGET_TRIPLET=x64-MinGW - Linux: -DVCPKG_TARGET_TRIPLET=x64-linux WSL extra configuration \u00b6 The CLion does not see the WSL's environment variables (as of 2023-03, see here ). To fix it, go to Project settings and set add the necessary environment variables to Environment field. WSL configuration - Deprecated \u00b6 Clion connects to WSL through SSH. Therefore, you need to configure SSH in WSL. To do it, run the following script: wget https://raw.githubusercontent.com/JetBrains/clion-wsl/master/ubuntu_setup_env.sh && bash ubuntu_setup_env.sh Next, It\u2019s necessary to modify the WSL/create the WSL initialization script to fix a CMake issue when connecting from CLion. Download the wsl.conf file, and put it in /etc/. The restart the WSL (wsl.exe -t Ubuntu-20.04) Visual Studio \u00b6 Installation \u00b6 Install Visual Studio Open/Create a CMake project Install ReSharper C++ Setting Synchronization \u00b6 Sign-in in Visual Studio using a Mictosoft account. A lot of settings should be synchronized automatically . Apply the layout: Window -> Apply Window Layout -> <Layout Name> Sync ReSharper settings: you can share the file: %APPDATA%\\JetBrains\\Shared\\vAny\\ ( ~\\AppData\\Roaming\\JetBrains\\Shared\\vAny\\ ). This does not work good though as the files are changed on both sides constantly. unfortunately, as of 01/2023, there is no good way how to share resharper settings Install roamed plugins Basic Configuration \u00b6 Add 120 char guideline install the extension add the guideline in command window: Edit.AddGuideline 120 if there is an error extension ... did not load properly , you need to install the developer analytic tools package to the Visual Studio: Visual Studio Installer -> modify Go to the Individual Components tab search for the extension and select it proceed with the Visual Studio Modification If you need to use the system CMake, configure it now (described below) If you use *.tpp file, configure a support for them (described below). installation Enable template implementation files ( .*tpp ) syntax highlighting: \u00b6 Go to Tools -> Options -> Text Editor -> File Extension select Microsoft Visual C++ write tpp to the field and click add (reopen the file to see changes) To Change the Build Verbosity \u00b6 Go to Tools -> Options -> Projects and Solutions -> Build and Run Change the value of the MSBuild project build output verbosity. Project Setting \u00b6 Configure Visual Studio to use system CMake: \u00b6 Go to Project -> CMake Settings it should open the CMakeSettings.json file Scroll to the bottom and click on show advanced settings Set the CMake executable to point to the cmake.exe file of your system CMake Build Setting and Enviromental Variables \u00b6 The build configuration is in the file CMakePresets.json , located in the root of the project. The file can be also opened by right clicking on CMakeLists.txt ad selecting Edit CMake presets . Set the CMake Toolchain File \u00b6 To set the vcpkg toolchain file add the following value to the base configuration cacheVariables dictionary: \"CMAKE_TOOLCHAIN_FILE\": { \"value\": \"C:/vcpkg/scripts/buildsystems/vcpkg.cmake\", \"type\": \"FILEPATH\" } Set the Compiler \u00b6 The MSVC toolchain has two compiler executables, default one, and clang. The default compiler configuration looks like this: \"cacheVariables\": { ... \"CMAKE_C_COMPILER\": \"cl.exe\", \"CMAKE_CXX_COMPILER\": \"cl.exe\" ... }, To change the compiler to clang, replace cl.exe by clang-cl.exe in both rows. Old Method Using CMakeSettings.json \u00b6 We can open the build setting by right click on CMakeList.txt -> Cmake Settings To configure configure vcpkg toolchain file: Under General , fill to the Cmake toolchain file the following: C:/vcpkg/scripts/buildsystems/vcpkg.cmake To configure the enviromental variable, edit the CmakeSettings.json file directly. The global variables can be set in the environments array, the per configuration ones in <config object>/environments ( exmaple ). Launch Setting \u00b6 The launch settings determins the launch configuration, most importantly, the run arguments. To modify the run arguments: 1. open the launch.vs.json file: - use the context menu: - Right-click on CMakeLists.txt -> Add Debug Configuration - select default - or open the file directly, it is stored in <PROJECT DIR>/.vs/ 2. in launch.vs.json configure: - type : default for MSVC or cppgdb for WSL - projectTarget : the name of the target (executable) - name : the display name in Visual Studio - args : json array with arguments as strings - arguments with spaces have to be quoted with escaped quotes - cwd : the working directory 3. Select the launch configuration in the drop-down menu next to the play button launch.vs.json reference Other launch.vs.json options \u00b6 cwd : the working directory Microsoft reference for launch.vs.json WSL Configuration \u00b6 For using GCC 10: - go to CmakeSettings.json -> CMake variables and cache - select show advanced variables checkbox - set CMAKE_CXX_COMPILER variable to /usr/bin/g++-10 Other Configuration \u00b6 show white spaces: Edit -> Advanced -> View White Space . configure indentation: described here Determine Visual Studio version \u00b6 At total, there are 5 different versionigs related to Visual Studio . The version which the compiler support table refers to is the version of the compiler ( cl.exe ). we can find it be examining the compiler executable stored in C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.34.31933\\bin\\Hostx64\\x64 . Problems & solutions \u00b6 Cannot regenerate Cmak cache \u00b6 go to ./vs and look for file named CmakeWorkspaceSettings . It most likelz contains a line with disable = true . Just delete the file, or the specific line. Installing Library Dependencies \u00b6 Vcpkg Libraries \u00b6 type vcpkg list , if the library you need is not listed, continue to the next steps type vcpkg search <library simple name> and inspect the result to determine the exact name of the package you need if the library is not listed, check the presence in vcpkg repo if the library is in repo, but search does not find it, update vcpkg type vcpkg install <exact name> to install the package at the end of the installation log, there will be a cmake command needed to integrate the library, put it to the appropriate place to your CMakeList.txt file Boost \u00b6 With boost, we should install only the necessary components. Then to include boost, we need: - find_package(Boost REQUIRED) - with all compiled components listed - target_include_directories(<YOUR TARGET NAME> PUBLIC ${Boost_INCLUDE_DIRS}) JNI \u00b6 for JNI, a JAVA_HOME system property needs to be set to the absolute path to the JDK, e.g., C:\\Program Files\\Java\\jdk-15.0.1 Gurobi \u00b6 If you don\u2019t have Gurobi installed, do it now, and check that the installation is working Windows: just install as usual Linux: download the archive to /opt sudo tar xvfz <gurobi archive> add the the file that introduce environment variables needed for gurobi to etc/profile.d Linux only: it is necessary to build the C++ library for your version of the compiler. Steps: 1. cd <GUROBI DIR>/linux64/src/build/ 2. make 3. mv libgurobi_c++.a ../../lib/libgurobi_c++_<some id for you, like version>.a 4. cd ../../lib/ 5. ln -sf ./libgurobi_c++<some id for you, like version>.a libgurobi_c++.a Follow this guide , specifically: 1. put the attached our custom FindGUROBI script to: - Windows: C:\\Program Files\\CMake\\share\\cmake-<your cmake version>\\Modules/ - Linux: /opt/<CMAKNAME>/share/cmake-<VERSION>/Modules 2. to your CMakeLists.txt , add: - find_package(GUROBI REQUIRED) - target_include_directories(<your executable> PRIVATE ${GUROBI_INCLUDE_DIRS}) - target_link_libraries(<your executable> PRIVATE ${GUROBI_LIBRARY}) - target_link_libraries(<your executable> PRIVATE optimized ${GUROBI_CXX_LIBRARY} debug ${GUROBI_CXX_DEBUG_LIBRARY}) 3. try to load the cmake projects (i.e., generate the build scripts using cmake). 4. if the C++ library is not found ( Gurobi c++ library not found ), check whether the correct C++ library is in the gurobi home, the file <library name>.lib has to be in the lib directory of the gurobi installation. If the file is not there, it is possible that your gurobi version is too old Update Gurobi \u00b6 Updating is done by installing the new version and generating and using new licence key . after update, you need to delete your build dir in order to prevent using of cached path to old Gurobi install Also, you need to update the library name on line 10 of the FindGUROBI.cmake script. Other Libraries Not Available in vcpkg \u00b6 Test Library linking/inclusion \u00b6 For testing purposes, we can follow this simple pattern: 1. build the library 2. include the library: target_include_directories(<target name> PUBLIC <path to include dir>) , where include dir is the directory with the main header file of the library. 3. if the library is not the header only library, we need to: 3.1 link the library: target_link_libraries(<target name> PUBLIC <path to lib file>) , where path to lib file is the path to the dynamic library file used for linking ( .so on Linux, .lib on Windows). 3.2. add the dynamic library to some path visible for the executable - here the library file is .so on Linux and .dll on Windows - there are plenty options for the visible path, the most common being the system PATH variable, or the directory with the executable. Dependencies with WSL and CLion \u00b6 In WSL, when combined with CLion, some find scripts does not work, because they depend on system variables, that are not correctly passed from CLIon SSH connection to CMake. Therefore, it is necessary to add hints with absolute path to these scripts. Some of them can be downloaded here . Package that require these hints: - JNI - Gurobi Refactoring \u00b6 The refactoring of C++ code is a complex process, so the number of supported refactoring operations is limited. In Visual Studio, the only supported refactoring operation is renaming. In IntelliJ tools (CLion, ReSharper C++), there are more tools available, but still, the refactoring is not as powerful nor reliable as in Java or Python. Other alternative is to implement the refactoring manually, with a help of some compiler tools like clang Refactoring Engine ( example project ). Changing Method Signature \u00b6 As of 2023-10, there is no reliable way how to change the method signature in C++. The most efficient tool is the method signature refactorin in either CLion or ReSharper C++. However, it does not work in all cases, so it is necessary to check and fix the code manually. # Compilation for a specific CPU ## MSVC MSVC cannot compile for a specific CPU or CPU series. It can, however, use new instructions sets more efficiently if it compiles the code without the support for CPUs thad does not support these instruction sets. The command for the compiler is: `/arch: (see MSVC documentation for details). ## GCC In GCC, the march option enables compilation for a specific hardware. ml) option enables compilation for a specific hardware. pects that you use vcpkg in a per-project configuration. To make it work, add: -DCMAKE_TOOLCHAIN_FILE=<vcpkg location>/scripts/buildsystems/vcpkg.cmake - To change build options ( option in CMakeLists.txt ), run cmake with -D <option name>=<option value> <build dir> . Example: cmake -D BUILD_TESTING=OFF . Building \u00b6 For building, use: cmake --build <build dir> where build dir is the directory containing the build scripts ( CmakeFiles folder). To list the build options: cmake -L Specify the build type (Debug, Release) \u00b6 To build in release mode, or any other build mode except for the default, we need to specify the parameters for CMake. Unfortunately, these parameters depends on the build system: - Single-configuration systems (Unix, MinGW) - Multi-configuration systems (Visual Studio) Single-configuration systems \u00b6 Single configuration systems have the build type hardcoded in the build scripts. Therefore, we need to specify the build type for CMake when we generate the build scripts: cmake ../ -DCMAKE_BUILD_TYPE=Release By default, the build type is Release . Multi-configuration systems \u00b6 In multi-configuration systems, the -DCMAKE_BUILD_TYPE parameter is ignored, because the build configuration is supposed to be determined when building the code (i.e., same build scripts for debug and for release). Therefore, we omit it, and instead specify the --config parameter when building the code: cmake --build . --config Release Specify the target \u00b6 We can use the --target parameter for that: cmake --build . --target <TARGET NAME> Clean the source files \u00b6 Run: cmake --build . --target clean Handling Case Insensitivity \u00b6 Windows builds are, in line with the OS, case insensitive. Moreover, the Visual Studio does some magic with names internally, so the build is case insensitive even on VS WSL builds. The case insensitivity can bring inconsistencies that later breake Unix builds. Therefore, it is desirable to have the build case sensitive even on Windows. Fortunatelly, we can toggle the case sensitivity at the OS level using this PowerShell command: Get-ChildItem <PROJECT ROOT PATH> -Recurse -Directory | ForEach-Object { fsutil.exe file setCaseSensitiveInfo $_.FullName enable } Note that this can break the git commits, so it is necessary to also configure git in your case-sensitive repo: git config core.ignorecase false # Compilation for a specific CPU ## MSVC MSVC cannot compile for a specific CPU or CPU series. It can, however, use new instructions sets more efficiently if it compiles the code without the support for CPUs thad does not support these instruction sets. The command for the compiler is: `/arch: (see MSVC documentation for details). ## GCC In GCC, the march option enables compilation for a specific hardware. ml) option enables compilation for a specific hardware.","title":"C++ Workflow"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#toolchain","text":"There are various toolchains available on Windows and Linux, but we limit this guide for only some of them, specifically those which are frequently updated and works great with Clion.","title":"Toolchain"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#msys2-windows-only","text":"download follow the installation guide on the homepage install MinGW64 using: pacman -S mingw-w64-x86_64-gcc","title":"MSYS2 (Windows only)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#mscv-windows-only","text":"install Visual Studio 2019 Comunity Edition","title":"MSCV (Windows only)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#common-compiler-flags","text":"/MD , /MT , and similar : these determines the version of the standard run-time library. The /MD flag is the default and also prefered. /nologo : do not print the copyright banner and information messages /EH : exception handeling flags","title":"Common Compiler Flags"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#gcc-linuxwsl","text":"If on Windows, Install the WSL first (preferably WSL 2) sudo apt-get update sudo apt-get upgrade sudo apt-get install build-essential rsync zip ninja-build make 3 For using gcc 10: - sudo apt-get install gcc-10 g++-10","title":"GCC (Linux/WSL)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#cmake","text":"Windows: Install CMake from https://cmake.org/download/ if your CMake is too old (e.g. error: \u201cCMake 3.15 or higher is required\u201d), update CMake (same as new install) Linux: If cmake is installed already, uninstall it! Do not use the cmake from linux repositories!! Download CMake sh installer from https://cmake.org/download/ install: sudo chmod +x <INSTALLER> sudo <INSTALLER> sudo rm <INSTALLER> add cmake executable to path Other details about CMake can be found in the CMake Manual.","title":"Cmake"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#vcpkg","text":"follow the installation guide , including the user and PowerShell/bash integration add the vcpkg directory to PATH , so the program can be run from anywhere Beware that to run it with sudo on linux, it is not that easy . add a new system variable VCPKG_DEFAULT_TRIPLET , so your default library version installed with vcpkg will be x64 (like our builds), set it to: x64-linux for Linux Compilers x64-windows for MSVC x64-MinGW for MinGW","title":"vcpkg"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#cmake-integration","text":"By default, CMake does not see the vcpkg. To set up the appropriate enviroment variables, paths, etc., we need to run cmake commands with path to cmake toolchain file: vcpkg/scripts/buildsystems/vcpkg.cmake . See the IDE and command line section for the detailed instructions how to execute cmake with the path to the vcpkg toolchain file. The toolchain file is executed early on, so it is safe to assume that the environment will be correctly set up before the commands in yor cmake script.","title":"CMake Integration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#update","text":"git pull bootstrap vcpkg again Windows: bootstrap-vcpkg.bat Linux: bootstrap-vcpkg.sh","title":"Update"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#update-package","text":"Update vcpkg vcpkg update to get a list of available updates vcpkg upgrade --no-dry-run to actually perform the upgrade you can supply the name of the package (e.g., zlib:x64-windows) as an argument to upgrade just one package","title":"Update package"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#package-features","text":"Some libraries have optional features, which are not installed by default, but we can install them explicitely. For example llvm . After vcpkg install llvm and typing vcpkg search llvm : llvm 10.0.0#6 The LLVM Compiler Infrastructure llvm[clang] Build C Language Family Front-end. llvm[clang-tools-extra] Build Clang tools. ... llvm[target-all] Build with all backends. llvm[target-amdgpu] Build with AMDGPU backend. llvm[target-arm] Build with ARM backend. Above, we can see that there are a lot of optional targets. To install the the arm target, for example, we can use vcpkg install llvm[target-arm] . Sometimes, a new build of the main package is required, in that case, we need to type vcpkg install llvm[target-arm] --recurse .","title":"Package Features"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#directory-structure","text":"","title":"Directory Structure"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#module-installation-scripts","text":"They are located in the ports directory. There is no special way how to update just the port dir, so update the whole vcpkg by git pull in case you need to update the list of available packages.","title":"Module Installation Scripts"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#modules","text":"Vcpkg has it s own find_package macro in the toolchain file. It executes the script: vcpkg/installed/<tripplet>/share/<package name>/vcpkg-cmake-wrapper.cmake , if exists. Then, it executes the cmake scripts in that directory using the standard find_package , like a cmake config package.","title":"Modules"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#ide","text":"","title":"IDE"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#clion","text":"","title":"Clion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#configuration","text":"","title":"Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#set-default-layout","text":"Window -> Layouts -> Save changes in current layout","title":"Set default layout"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#set-up-new-surround-with-template","text":"In Clion, there are two types of surround with templates: surrond with and surround with live template . The first type use simple predefined templates and cannot be modified. However, the second type can be modified and new templates can be added.","title":"Set up new surround with template"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#toolchain-configuration","text":"Go to settings -> Build, Execution, Deployment -> toolchain , add new toolchain and set: - Name to whatever you want - The environment should point to your toolchain: - MSVC: C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community - MSYS: C:\\MSYS2 - WSL: From the drop-down list, choose the environment you configured for using with CLion in the previous steps - Credentials (WSL) click to the setting button next to the credentials and fill - host: localhost - port: 2222 - user and password according to your WSL system credentials - Architecture (non WSL): amd64 - CMake: C:\\Program Files\\CMake\\bin\\cmake.exe , for WSL, leave it as it is - other fields should be filled automatically","title":"Toolchain configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#project-configuration","text":"Most project settings resides (hereinafter Project settings ) in settings -> Build, Execution, Deployment -> CMake . For each build configuration, add a new template and set: - Name to whatever you want - Build type to debug - To Cmake options , add: - path to vcpkg toolchain file: - Linux: -DCMAKE_TOOLCHAIN_FILE=/opt/vcpkg/scripts/buildsystems/vcpkg.cmake - Windows: -DCMAKE_TOOLCHAIN_FILE=C:/vcpkg/scripts/buildsystems/vcpkg.cmake - Set the correct vcpkg triplet - MSVC: -DVCPKG_TARGET_TRIPLET=x64-windows - MinGW: -DVCPKG_TARGET_TRIPLET=x64-MinGW - Linux: -DVCPKG_TARGET_TRIPLET=x64-linux","title":"Project configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#wsl-extra-configuration","text":"The CLion does not see the WSL's environment variables (as of 2023-03, see here ). To fix it, go to Project settings and set add the necessary environment variables to Environment field.","title":"WSL extra configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#wsl-configuration-deprecated","text":"Clion connects to WSL through SSH. Therefore, you need to configure SSH in WSL. To do it, run the following script: wget https://raw.githubusercontent.com/JetBrains/clion-wsl/master/ubuntu_setup_env.sh && bash ubuntu_setup_env.sh Next, It\u2019s necessary to modify the WSL/create the WSL initialization script to fix a CMake issue when connecting from CLion. Download the wsl.conf file, and put it in /etc/. The restart the WSL (wsl.exe -t Ubuntu-20.04)","title":"WSL configuration - Deprecated"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#visual-studio","text":"","title":"Visual Studio"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#installation","text":"Install Visual Studio Open/Create a CMake project Install ReSharper C++","title":"Installation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#setting-synchronization","text":"Sign-in in Visual Studio using a Mictosoft account. A lot of settings should be synchronized automatically . Apply the layout: Window -> Apply Window Layout -> <Layout Name> Sync ReSharper settings: you can share the file: %APPDATA%\\JetBrains\\Shared\\vAny\\ ( ~\\AppData\\Roaming\\JetBrains\\Shared\\vAny\\ ). This does not work good though as the files are changed on both sides constantly. unfortunately, as of 01/2023, there is no good way how to share resharper settings Install roamed plugins","title":"Setting Synchronization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#basic-configuration","text":"Add 120 char guideline install the extension add the guideline in command window: Edit.AddGuideline 120 if there is an error extension ... did not load properly , you need to install the developer analytic tools package to the Visual Studio: Visual Studio Installer -> modify Go to the Individual Components tab search for the extension and select it proceed with the Visual Studio Modification If you need to use the system CMake, configure it now (described below) If you use *.tpp file, configure a support for them (described below). installation","title":"Basic Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#enable-template-implementation-files-tpp-syntax-highlighting","text":"Go to Tools -> Options -> Text Editor -> File Extension select Microsoft Visual C++ write tpp to the field and click add (reopen the file to see changes)","title":"Enable template implementation files (.*tpp) syntax highlighting:"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#to-change-the-build-verbosity","text":"Go to Tools -> Options -> Projects and Solutions -> Build and Run Change the value of the MSBuild project build output verbosity.","title":"To Change the Build Verbosity"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#project-setting","text":"","title":"Project Setting"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#configure-visual-studio-to-use-system-cmake","text":"Go to Project -> CMake Settings it should open the CMakeSettings.json file Scroll to the bottom and click on show advanced settings Set the CMake executable to point to the cmake.exe file of your system CMake","title":"Configure Visual Studio to use system CMake:"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#build-setting-and-enviromental-variables","text":"The build configuration is in the file CMakePresets.json , located in the root of the project. The file can be also opened by right clicking on CMakeLists.txt ad selecting Edit CMake presets .","title":"Build Setting and Enviromental Variables"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#set-the-cmake-toolchain-file","text":"To set the vcpkg toolchain file add the following value to the base configuration cacheVariables dictionary: \"CMAKE_TOOLCHAIN_FILE\": { \"value\": \"C:/vcpkg/scripts/buildsystems/vcpkg.cmake\", \"type\": \"FILEPATH\" }","title":"Set the CMake Toolchain File"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#set-the-compiler","text":"The MSVC toolchain has two compiler executables, default one, and clang. The default compiler configuration looks like this: \"cacheVariables\": { ... \"CMAKE_C_COMPILER\": \"cl.exe\", \"CMAKE_CXX_COMPILER\": \"cl.exe\" ... }, To change the compiler to clang, replace cl.exe by clang-cl.exe in both rows.","title":"Set the Compiler"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#old-method-using-cmakesettingsjson","text":"We can open the build setting by right click on CMakeList.txt -> Cmake Settings To configure configure vcpkg toolchain file: Under General , fill to the Cmake toolchain file the following: C:/vcpkg/scripts/buildsystems/vcpkg.cmake To configure the enviromental variable, edit the CmakeSettings.json file directly. The global variables can be set in the environments array, the per configuration ones in <config object>/environments ( exmaple ).","title":"Old Method Using CMakeSettings.json"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#launch-setting","text":"The launch settings determins the launch configuration, most importantly, the run arguments. To modify the run arguments: 1. open the launch.vs.json file: - use the context menu: - Right-click on CMakeLists.txt -> Add Debug Configuration - select default - or open the file directly, it is stored in <PROJECT DIR>/.vs/ 2. in launch.vs.json configure: - type : default for MSVC or cppgdb for WSL - projectTarget : the name of the target (executable) - name : the display name in Visual Studio - args : json array with arguments as strings - arguments with spaces have to be quoted with escaped quotes - cwd : the working directory 3. Select the launch configuration in the drop-down menu next to the play button launch.vs.json reference","title":"Launch Setting"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#other-launchvsjson-options","text":"cwd : the working directory Microsoft reference for launch.vs.json","title":"Other launch.vs.json options"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#wsl-configuration","text":"For using GCC 10: - go to CmakeSettings.json -> CMake variables and cache - select show advanced variables checkbox - set CMAKE_CXX_COMPILER variable to /usr/bin/g++-10","title":"WSL Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#other-configuration","text":"show white spaces: Edit -> Advanced -> View White Space . configure indentation: described here","title":"Other Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#determine-visual-studio-version","text":"At total, there are 5 different versionigs related to Visual Studio . The version which the compiler support table refers to is the version of the compiler ( cl.exe ). we can find it be examining the compiler executable stored in C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.34.31933\\bin\\Hostx64\\x64 .","title":"Determine Visual Studio version"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#problems-solutions","text":"","title":"Problems &amp; solutions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#cannot-regenerate-cmak-cache","text":"go to ./vs and look for file named CmakeWorkspaceSettings . It most likelz contains a line with disable = true . Just delete the file, or the specific line.","title":"Cannot regenerate Cmak cache"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#installing-library-dependencies","text":"","title":"Installing Library Dependencies"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#vcpkg-libraries","text":"type vcpkg list , if the library you need is not listed, continue to the next steps type vcpkg search <library simple name> and inspect the result to determine the exact name of the package you need if the library is not listed, check the presence in vcpkg repo if the library is in repo, but search does not find it, update vcpkg type vcpkg install <exact name> to install the package at the end of the installation log, there will be a cmake command needed to integrate the library, put it to the appropriate place to your CMakeList.txt file","title":"Vcpkg Libraries"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#boost","text":"With boost, we should install only the necessary components. Then to include boost, we need: - find_package(Boost REQUIRED) - with all compiled components listed - target_include_directories(<YOUR TARGET NAME> PUBLIC ${Boost_INCLUDE_DIRS})","title":"Boost"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#jni","text":"for JNI, a JAVA_HOME system property needs to be set to the absolute path to the JDK, e.g., C:\\Program Files\\Java\\jdk-15.0.1","title":"JNI"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#gurobi","text":"If you don\u2019t have Gurobi installed, do it now, and check that the installation is working Windows: just install as usual Linux: download the archive to /opt sudo tar xvfz <gurobi archive> add the the file that introduce environment variables needed for gurobi to etc/profile.d Linux only: it is necessary to build the C++ library for your version of the compiler. Steps: 1. cd <GUROBI DIR>/linux64/src/build/ 2. make 3. mv libgurobi_c++.a ../../lib/libgurobi_c++_<some id for you, like version>.a 4. cd ../../lib/ 5. ln -sf ./libgurobi_c++<some id for you, like version>.a libgurobi_c++.a Follow this guide , specifically: 1. put the attached our custom FindGUROBI script to: - Windows: C:\\Program Files\\CMake\\share\\cmake-<your cmake version>\\Modules/ - Linux: /opt/<CMAKNAME>/share/cmake-<VERSION>/Modules 2. to your CMakeLists.txt , add: - find_package(GUROBI REQUIRED) - target_include_directories(<your executable> PRIVATE ${GUROBI_INCLUDE_DIRS}) - target_link_libraries(<your executable> PRIVATE ${GUROBI_LIBRARY}) - target_link_libraries(<your executable> PRIVATE optimized ${GUROBI_CXX_LIBRARY} debug ${GUROBI_CXX_DEBUG_LIBRARY}) 3. try to load the cmake projects (i.e., generate the build scripts using cmake). 4. if the C++ library is not found ( Gurobi c++ library not found ), check whether the correct C++ library is in the gurobi home, the file <library name>.lib has to be in the lib directory of the gurobi installation. If the file is not there, it is possible that your gurobi version is too old","title":"Gurobi"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#update-gurobi","text":"Updating is done by installing the new version and generating and using new licence key . after update, you need to delete your build dir in order to prevent using of cached path to old Gurobi install Also, you need to update the library name on line 10 of the FindGUROBI.cmake script.","title":"Update Gurobi"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#other-libraries-not-available-in-vcpkg","text":"","title":"Other Libraries Not Available in vcpkg"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#test-library-linkinginclusion","text":"For testing purposes, we can follow this simple pattern: 1. build the library 2. include the library: target_include_directories(<target name> PUBLIC <path to include dir>) , where include dir is the directory with the main header file of the library. 3. if the library is not the header only library, we need to: 3.1 link the library: target_link_libraries(<target name> PUBLIC <path to lib file>) , where path to lib file is the path to the dynamic library file used for linking ( .so on Linux, .lib on Windows). 3.2. add the dynamic library to some path visible for the executable - here the library file is .so on Linux and .dll on Windows - there are plenty options for the visible path, the most common being the system PATH variable, or the directory with the executable.","title":"Test Library linking/inclusion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#dependencies-with-wsl-and-clion","text":"In WSL, when combined with CLion, some find scripts does not work, because they depend on system variables, that are not correctly passed from CLIon SSH connection to CMake. Therefore, it is necessary to add hints with absolute path to these scripts. Some of them can be downloaded here . Package that require these hints: - JNI - Gurobi","title":"Dependencies with WSL and CLion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#refactoring","text":"The refactoring of C++ code is a complex process, so the number of supported refactoring operations is limited. In Visual Studio, the only supported refactoring operation is renaming. In IntelliJ tools (CLion, ReSharper C++), there are more tools available, but still, the refactoring is not as powerful nor reliable as in Java or Python. Other alternative is to implement the refactoring manually, with a help of some compiler tools like clang Refactoring Engine ( example project ).","title":"Refactoring"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#changing-method-signature","text":"As of 2023-10, there is no reliable way how to change the method signature in C++. The most efficient tool is the method signature refactorin in either CLion or ReSharper C++. However, it does not work in all cases, so it is necessary to check and fix the code manually. # Compilation for a specific CPU ## MSVC MSVC cannot compile for a specific CPU or CPU series. It can, however, use new instructions sets more efficiently if it compiles the code without the support for CPUs thad does not support these instruction sets. The command for the compiler is: `/arch: (see MSVC documentation for details). ## GCC In GCC, the march option enables compilation for a specific hardware. ml) option enables compilation for a specific hardware. pects that you use vcpkg in a per-project configuration. To make it work, add: -DCMAKE_TOOLCHAIN_FILE=<vcpkg location>/scripts/buildsystems/vcpkg.cmake - To change build options ( option in CMakeLists.txt ), run cmake with -D <option name>=<option value> <build dir> . Example: cmake -D BUILD_TESTING=OFF .","title":"Changing Method Signature"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#building","text":"For building, use: cmake --build <build dir> where build dir is the directory containing the build scripts ( CmakeFiles folder). To list the build options: cmake -L","title":"Building"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#specify-the-build-type-debug-release","text":"To build in release mode, or any other build mode except for the default, we need to specify the parameters for CMake. Unfortunately, these parameters depends on the build system: - Single-configuration systems (Unix, MinGW) - Multi-configuration systems (Visual Studio)","title":"Specify the build type (Debug, Release)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#single-configuration-systems","text":"Single configuration systems have the build type hardcoded in the build scripts. Therefore, we need to specify the build type for CMake when we generate the build scripts: cmake ../ -DCMAKE_BUILD_TYPE=Release By default, the build type is Release .","title":"Single-configuration systems"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#multi-configuration-systems","text":"In multi-configuration systems, the -DCMAKE_BUILD_TYPE parameter is ignored, because the build configuration is supposed to be determined when building the code (i.e., same build scripts for debug and for release). Therefore, we omit it, and instead specify the --config parameter when building the code: cmake --build . --config Release","title":"Multi-configuration systems"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#specify-the-target","text":"We can use the --target parameter for that: cmake --build . --target <TARGET NAME>","title":"Specify the target"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#clean-the-source-files","text":"Run: cmake --build . --target clean","title":"Clean the source files"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#handling-case-insensitivity","text":"Windows builds are, in line with the OS, case insensitive. Moreover, the Visual Studio does some magic with names internally, so the build is case insensitive even on VS WSL builds. The case insensitivity can bring inconsistencies that later breake Unix builds. Therefore, it is desirable to have the build case sensitive even on Windows. Fortunatelly, we can toggle the case sensitivity at the OS level using this PowerShell command: Get-ChildItem <PROJECT ROOT PATH> -Recurse -Directory | ForEach-Object { fsutil.exe file setCaseSensitiveInfo $_.FullName enable } Note that this can break the git commits, so it is necessary to also configure git in your case-sensitive repo: git config core.ignorecase false # Compilation for a specific CPU ## MSVC MSVC cannot compile for a specific CPU or CPU series. It can, however, use new instructions sets more efficiently if it compiles the code without the support for CPUs thad does not support these instruction sets. The command for the compiler is: `/arch: (see MSVC documentation for details). ## GCC In GCC, the march option enables compilation for a specific hardware. ml) option enables compilation for a specific hardware.","title":"Handling Case Insensitivity"},{"location":"Programming/C%2B%2B/CMake%20Manual/","text":"Commands \u00b6 Generating Build scripts \u00b6 General syntax is: cmake <dir> Here <dir> is the CMakeLists.txt directory. The build scripts are build in current directory. Toolchain file \u00b6 To work with package managers, a link to toolchain file has to be provided as an argument. For vcpkg , the argument is as follows: # new version cmake <dir> --toolchain <vcpkg location>/scripts/buildsystems/vcpkg.cmake # old version cmake <dir> -DCMAKE_TOOLCHAIN_FILE=<vcpkg location>/scripts/buildsystems/vcpkg.cmake Note that the toolchain is only loaded at the beginnning of the generation process. Once you forgot it, you need to delete the build scripts diectory content to make this argument work for subsequent cmake commands. Usefull arguments \u00b6 -LH to see cmake nonadvanced variables together with the description. -LHA to see also the advanced variables. Note that this prints only cached variables, to print all variables, we have to edit the CmakeLists.txt. -D : To change build options ( option in CMakeLists.txt ), run cmake with -D <option name>=<option value> <build dir> . Example: cmake -D BUILD_TESTING=OFF . Legacy arguments \u00b6 -H : to specify the source directory (where the CMakeLists.txt file is located). Now it is specified as the positional argument or using -S . Building \u00b6 For building, use: cmake --build <build dir> where build dir is the directory containing the build scripts ( CmakeFiles folder). To list the build options: cmake -L Specify the target \u00b6 We can use the --target parameter for that: cmake --build . --target <TARGET NAME> Specify the build type (Debug, Release) \u00b6 In CMake, we use a specific build type string instead of compiler and linker flags: - Debug - Debug build - Release - Release build - RelWithDebInfo - Release build with debug information - MinSizeRel - Release build with minimal size Unfortunately, the way how the build type should be specified depends on the build system: - Single-configuration systems (GCC, Clang, MinGW) - Multi-configuration systems (MSVC) Single-configuration systems \u00b6 Single configuration systems have the compiler flags hardcoded in the build scripts. Therefore, we need to specify the build type for CMake when we generate the build scripts: cmake ../ -DCMAKE_BUILD_TYPE=Release By default, the build type is an empty string . This means that no extra flags are added to the compiler and linker so the compiler and linker run with their default settings. Interesting info can be found in this SO question . Multi-configuration systems \u00b6 In multi-configuration systems, the -DCMAKE_BUILD_TYPE parameter is ignored, because the build configuration is supposed to be determined when building the code (i.e., same build scripts for debug and for release). Therefore, we omit it, and instead specify the --config parameter when building the code: cmake --build . --config Release Clean the source files \u00b6 Run: cmake --build . --target clean Syntax \u00b6 Variables \u00b6 We can use the variable using their name: if(DEFINED <name>) ... In string, we need to wrap the variable in ${} : message(STATUS \"dir=${dir}\") Enviromental variables \u00b6 We can use environmental variables using the ENV variable: if(DEFINED ENV{<name>}) ... Be aware that in string, we use only one pair of curly braces (see variable references manual ): message(STATUS \"dir=$ENV{dir}\") Print all variables \u00b6 To print all variables, the following function can be used: function(dump_cmake_variables) if (ARGV0) message(STATUS \"Printing variables matching '${ARGV0}'\") else() message(STATUS \"Printing all variables\") endif() get_cmake_property(_variableNames VARIABLES) list (SORT _variableNames) foreach (_variableName ${_variableNames}) if (ARGV0) unset(MATCHED) string(REGEX MATCH ${ARGV0} MATCHED ${_variableName}) if (NOT MATCHED) continue() endif() endif() message(STATUS \"${_variableName}=${${_variableName}}\") endforeach() message(STATUS \"Printing variables - END\") endfunction() To print all variables related to HDF5 lib, call dump_cmake_variables(HDF) after the find_package call. Control structures \u00b6 if \u00b6 The if command has the following syntax: if(<condition>) ... elseif(<condition>) ... else() ... endif() Generator expressions \u00b6 Manual Generator expressions are a very useful tool to control the build process based on the build type, compiler type, or similar properties. CMake use them to generate mutliple build scripts from a single CMakeLists.txt file. The syntax for a basic condition expression is: \"$<$<condition>:<this will be printed if condition is satisfied>>\" CMakeLists.txt \u00b6 The typical structure of the CMakeLists.txt file is as follows: 1. Top section contains project wide setting like name, minimum cmake version, and the language specification. Targets sections containing: the target definition together with sources used target includes target linking Typical Top section content \u00b6 The typical content of the top section is: - minimum cmake version: cmake_minimum_required(VERSION <version>) - project name and version: project(<name> VERSION <version>) - language specification: enable_language(<language>) - language standard, e.g.: set(CMAKE_CXX_STANDARD <version>) - compile options: add_compile_options(<option 1> <option 2> ...) - cmake module inclusion: include(<module name>) Language standards \u00b6 The language standard is set using the set command together with the CMAKE_<LANG>_STANDARD variable. Example: set(CMAKE_CXX_STANDARD 17) This way, the standard is set for all targets and the compiler should be configured for that standard. However, if the compiler does not support the standard, the build script generation continues and the failure will appear later during the compilation. To avoid that, we can make the standard a requirement using the set command together with the CMAKE_<LANG>_STANDARD_REQUIRED variable. Example: set(CMAKE_CXX_STANDARD_REQUIRED ON) Compile options \u00b6 Most of the compile options are now sets automatically based on the declarations in the CMakeLists.txt file. However, some notable exceptions exists. To set such options, we have to use the add_compile_options command: add_compile_options(<option 1> <option 2> ...) MSVC \u00b6 /permissive- to enable the strictest mode of the compiler GCC \u00b6 -pedantic-errors to report all cases where non-standard GCC extension is used and treat them as errors Linker Options \u00b6 Linker options can be set with add_link_options command. Example: add_link_options(\"/STACK: 10000000\") Dependency management \u00b6 There are many ways how to manage dependencies in CMake, for complete overview, see the documentation . Although it is possible to hard-code the paths for includes and linking, it is usually better to initialize the paths automatically using a rich set of commands cmake offers. It has the following advatages: - Hardcoding the paths is error-prone, while cmake commands usually deliver correct paths - It boost the productivity as we do not have to investigate where each library is installed - The resulting CMakeLists.txt file is more portable - And most importantly, potential errors concerning missing libraries are reported prior to the compilation/linking . Most of the libraries have CMake support, so their CMake variables can be initialized simply by calling the find_package command described below. These packages have either: - their own cmake config (cmake-aware libs usually installed through the package manager like vcpkg ) - or they have a Find<package name> script created by someone else that heuristically search for the packege (The default location for these scripts is CMake/share/cmake-<version>/Modules ). For packages without the CMake support, we have to use lower-level cmake commands like find_path or find_libraries . For convinience, we can put these command to our own Find<name> script taht can be used by multiple project or even shared. Standard way: find_package \u00b6 The find_package command is the primary way of obtaining correct variables for a library including: - include paths - linking paths - platform/toolchain specific enviromental variables There are two types of package (library) info search: - module , which uses cmake scripts provided by CMake or OS. The modules are typically provided only for the most used libraries (e.g. boost). All modules provided by CMake are listed in the documentation . - config which uses CMake scripts provided by the developers of the package. They are typically distributed with the source code and downloaded by the package manager. Unless specified, the module mode is used. To force a speciic mode, we can use the MODULE / CONFIG parameters. Config packages \u00b6 Config packages are CMake modules that were created as cmake projects by their developers. They are therefore naturally integrated into Cmake. The configuration files are executed as follows: 1. Package version file: <package name>-config-version.cmake or <package name>ConfigVersion.cmake . This file handles the version compatibility, i.e., it ensures that the installed version of the package is compatible with the version requested in the find_package command. 1. Package configuration file: <package name>-config.cmake or <package name>Config.cmake . Module Packages \u00b6 Module packages are packages that are not cmake projects themselves, but are hooked into cmake using custom find module scrips. These scripts are automatically executed by find_package . They are located in e.g.: CMake/share/cmake-3.22/Modules/Find<package name>.cmake . Searching for include directories with find_path \u00b6 The find_path command is intended to find the path (e.g., an include directory). A simple syntax is: find_path( <var name> NAMES <file names> PATHS <paths> ) Here: - <var name> is the name of the resulting variable - <file names> are all possible file names split by space. At least one of the files needs to be present in a path for it to be considered to be the found path. - <paths> are candidate paths split by space Low level command: find_library \u00b6 The find_library command is used to populate a variable with a result of a specific file search optimized for libraries. The search algorithm works as follows: 1. ? 2. Search package paths - order: 1. <CurrentPackage>_ROOT , 2. ENV{<CurrentPackage>_ROOT} , 3. <ParentPackage>_ROOT , 4. ENV{<ParentPackage>_ROOT} - this only happens if the find_library command is called from within a find_<module> or find_package - this step can be skipped using the NO_PACKAGE_ROOT_PATH parameter 3. Search path from cmake cache. During a clean cmake generation, these can be only supplied by command line. - Considered variables: - CMAKE_LIBRARY_ARCHITECTURE - CMAKE_PREFIX_PATH - CMAKE_LIBRARY_PATH - CMAKE_FRAMEWORK_PATH - this step can be skipped using the NO_CMAKE_PATH parameter 4. Same as step 3, but the variables are searched among system environmental variables instead - this step can be skipped using the NO_CMAKE_ENVIRONMENT_PATH parameter 5. Search paths specified by the HINTS option 6. Search the standard system environmental paths - variables considered are LIB and PATH - this step can be skipped using the NO_SYSTEM_ENVIRONMENT_PATH parameter 7. Search in system paths - Considered variables: - CMAKE_LIBRARY_ARCHITECTURE - CMAKE_SYSTEM_PREFIX_PATH - CMAKE_SYSTEM_LIBRARY_PATH - CMAKE_SYSTEM_FRAMEWORK_PATH - this step can be skipped using the NO_CMAKE_SYSTEM_PATH parameter 8. Search the paths specified by the PATHS option Searching for libraries in the project dir \u00b6 Note that the project dir is not searched by default. To include in the search, use: HINTS ${PROJECT_SOURCE_DIR} . Full example on the Gurobi lib stored in <CMAKE LISTS DIR/lib/gurobi_c++.a> : find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED ) Creating a custom find script \u00b6 The structure of a simple find scripts is described in the documentation . We can either put the find script to the default location, so it will be available for all projects, or we can put it in the project directory and add that directory to the CMAKE_MODULE_PATH : list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\") The usual structure of the find script is: 1. the comment section describing the file: #[=======================================================================[.rst: FindMOSEK ------- Finds the MOSEK library. Result Variables ^^^^^^^^^^^^^^^^ This will define the following variables: ``MOESK_FOUND`` True if the system has the MOSEK library. ``MOSEK_INCLUDE_DIRS`` Include directories needed to use MOSEK. ``MOSEK_LIBRARIES`` Libraries needed to link to MOSEK. #]=======================================================================] find commands that fils some temp variables: find_path( MOSEK_INCLUDE_DIR NAMES mosek.h PATHS \"$ENV{MOSEK_HOME}/h\" ) find_library( MOSEK_LIBRARY NAMES libmosek64.so.10.0 libfusion64.so.10.0 PATHS \"$ENV{MOSEK_HOME}/bin\" ) handeling of the result of the file commands. The standard approach is: include(FindPackageHandleStandardArgs) find_package_handle_standard_args(MOSEK FOUND_VAR MOSEK_FOUND REQUIRED_VARS MOSEK_LIBRARY MOSEK_INCLUDE_DIR ) setting the final variables: if(MOSEK_FOUND) set(MOSEK_LIBRARIES ${MOSEK_LIBRARY}) set(MOSEK_INCLUDE_DIRS ${MOSEK_INCLUDE_DIR}) endif() Downolad dependencies during configuration and build them from source \u00b6 To download and build dependencies during the configuration, we can use the FetchContent module. For details, see the dependency management documentation . The usual workflow is: 1. Download the dependency using the FetchContent_Declare command cmake FetchContent_Declare( <NAME> <SPECIFICATION> ) - The specification can be either a URL or a git repository. 2. Build the dependency using the FetchContent_MakeAvailable command: cmake FetchContent_MakeAvailable(<NAME>) Target definition \u00b6 The target definition is done using the add_executable command. The syntax is: add_executable(<target name> <source file 1> <source file 2> ...) The target name is used to refer to the target in other commands. The target name is also used to name the output file. The list of source files should contain all the source files that are needed to build the target. There are some automatic mechanisms that can be used to add the source files (discussed e.g. on cmake forums ), but they are not recommended. Setting include directories \u00b6 To inlude the headers, we need to use a inlude_directories (global), or better target_include_directories command. Linking configuration \u00b6 For linkink, use the target_link_libraries command. Make sure that you always link against all the libraries that are needed for the target to work! Do not rely on the linker errors, these may not appear due to library preloading, indirect linkage, advenced linker heuristics, etc. The result is that on one machine the code will work, but on another, it will fail. To find out if and how to link against a library, refer to the documentation of the library. Include directories \u00b6 All the global include directories are stored in the INCLUDE_DIRECTORIES property, to print them, add this to the CMakeLists.txt file: get_property(dirs DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} PROPERTY INCLUDE_DIRECTORIES) foreach(dir ${dirs}) message(STATUS \"dir='${dir}'\") endforeach() CMake Directory Structure \u00b6 System Find_XXX.cmake files \u00b6 The system find scripts are located in the CMake/share/cmake-<version>/Modules/ directory.","title":"CMake Manual"},{"location":"Programming/C%2B%2B/CMake%20Manual/#commands","text":"","title":"Commands"},{"location":"Programming/C%2B%2B/CMake%20Manual/#generating-build-scripts","text":"General syntax is: cmake <dir> Here <dir> is the CMakeLists.txt directory. The build scripts are build in current directory.","title":"Generating Build scripts"},{"location":"Programming/C%2B%2B/CMake%20Manual/#toolchain-file","text":"To work with package managers, a link to toolchain file has to be provided as an argument. For vcpkg , the argument is as follows: # new version cmake <dir> --toolchain <vcpkg location>/scripts/buildsystems/vcpkg.cmake # old version cmake <dir> -DCMAKE_TOOLCHAIN_FILE=<vcpkg location>/scripts/buildsystems/vcpkg.cmake Note that the toolchain is only loaded at the beginnning of the generation process. Once you forgot it, you need to delete the build scripts diectory content to make this argument work for subsequent cmake commands.","title":"Toolchain file"},{"location":"Programming/C%2B%2B/CMake%20Manual/#usefull-arguments","text":"-LH to see cmake nonadvanced variables together with the description. -LHA to see also the advanced variables. Note that this prints only cached variables, to print all variables, we have to edit the CmakeLists.txt. -D : To change build options ( option in CMakeLists.txt ), run cmake with -D <option name>=<option value> <build dir> . Example: cmake -D BUILD_TESTING=OFF .","title":"Usefull arguments"},{"location":"Programming/C%2B%2B/CMake%20Manual/#legacy-arguments","text":"-H : to specify the source directory (where the CMakeLists.txt file is located). Now it is specified as the positional argument or using -S .","title":"Legacy arguments"},{"location":"Programming/C%2B%2B/CMake%20Manual/#building","text":"For building, use: cmake --build <build dir> where build dir is the directory containing the build scripts ( CmakeFiles folder). To list the build options: cmake -L","title":"Building"},{"location":"Programming/C%2B%2B/CMake%20Manual/#specify-the-target","text":"We can use the --target parameter for that: cmake --build . --target <TARGET NAME>","title":"Specify the target"},{"location":"Programming/C%2B%2B/CMake%20Manual/#specify-the-build-type-debug-release","text":"In CMake, we use a specific build type string instead of compiler and linker flags: - Debug - Debug build - Release - Release build - RelWithDebInfo - Release build with debug information - MinSizeRel - Release build with minimal size Unfortunately, the way how the build type should be specified depends on the build system: - Single-configuration systems (GCC, Clang, MinGW) - Multi-configuration systems (MSVC)","title":"Specify the build type (Debug, Release)"},{"location":"Programming/C%2B%2B/CMake%20Manual/#single-configuration-systems","text":"Single configuration systems have the compiler flags hardcoded in the build scripts. Therefore, we need to specify the build type for CMake when we generate the build scripts: cmake ../ -DCMAKE_BUILD_TYPE=Release By default, the build type is an empty string . This means that no extra flags are added to the compiler and linker so the compiler and linker run with their default settings. Interesting info can be found in this SO question .","title":"Single-configuration systems"},{"location":"Programming/C%2B%2B/CMake%20Manual/#multi-configuration-systems","text":"In multi-configuration systems, the -DCMAKE_BUILD_TYPE parameter is ignored, because the build configuration is supposed to be determined when building the code (i.e., same build scripts for debug and for release). Therefore, we omit it, and instead specify the --config parameter when building the code: cmake --build . --config Release","title":"Multi-configuration systems"},{"location":"Programming/C%2B%2B/CMake%20Manual/#clean-the-source-files","text":"Run: cmake --build . --target clean","title":"Clean the source files"},{"location":"Programming/C%2B%2B/CMake%20Manual/#syntax","text":"","title":"Syntax"},{"location":"Programming/C%2B%2B/CMake%20Manual/#variables","text":"We can use the variable using their name: if(DEFINED <name>) ... In string, we need to wrap the variable in ${} : message(STATUS \"dir=${dir}\")","title":"Variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#enviromental-variables","text":"We can use environmental variables using the ENV variable: if(DEFINED ENV{<name>}) ... Be aware that in string, we use only one pair of curly braces (see variable references manual ): message(STATUS \"dir=$ENV{dir}\")","title":"Enviromental variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#print-all-variables","text":"To print all variables, the following function can be used: function(dump_cmake_variables) if (ARGV0) message(STATUS \"Printing variables matching '${ARGV0}'\") else() message(STATUS \"Printing all variables\") endif() get_cmake_property(_variableNames VARIABLES) list (SORT _variableNames) foreach (_variableName ${_variableNames}) if (ARGV0) unset(MATCHED) string(REGEX MATCH ${ARGV0} MATCHED ${_variableName}) if (NOT MATCHED) continue() endif() endif() message(STATUS \"${_variableName}=${${_variableName}}\") endforeach() message(STATUS \"Printing variables - END\") endfunction() To print all variables related to HDF5 lib, call dump_cmake_variables(HDF) after the find_package call.","title":"Print all variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#control-structures","text":"","title":"Control structures"},{"location":"Programming/C%2B%2B/CMake%20Manual/#if","text":"The if command has the following syntax: if(<condition>) ... elseif(<condition>) ... else() ... endif()","title":"if"},{"location":"Programming/C%2B%2B/CMake%20Manual/#generator-expressions","text":"Manual Generator expressions are a very useful tool to control the build process based on the build type, compiler type, or similar properties. CMake use them to generate mutliple build scripts from a single CMakeLists.txt file. The syntax for a basic condition expression is: \"$<$<condition>:<this will be printed if condition is satisfied>>\"","title":"Generator expressions"},{"location":"Programming/C%2B%2B/CMake%20Manual/#cmakeliststxt","text":"The typical structure of the CMakeLists.txt file is as follows: 1. Top section contains project wide setting like name, minimum cmake version, and the language specification. Targets sections containing: the target definition together with sources used target includes target linking","title":"CMakeLists.txt"},{"location":"Programming/C%2B%2B/CMake%20Manual/#typical-top-section-content","text":"The typical content of the top section is: - minimum cmake version: cmake_minimum_required(VERSION <version>) - project name and version: project(<name> VERSION <version>) - language specification: enable_language(<language>) - language standard, e.g.: set(CMAKE_CXX_STANDARD <version>) - compile options: add_compile_options(<option 1> <option 2> ...) - cmake module inclusion: include(<module name>)","title":"Typical Top section content"},{"location":"Programming/C%2B%2B/CMake%20Manual/#language-standards","text":"The language standard is set using the set command together with the CMAKE_<LANG>_STANDARD variable. Example: set(CMAKE_CXX_STANDARD 17) This way, the standard is set for all targets and the compiler should be configured for that standard. However, if the compiler does not support the standard, the build script generation continues and the failure will appear later during the compilation. To avoid that, we can make the standard a requirement using the set command together with the CMAKE_<LANG>_STANDARD_REQUIRED variable. Example: set(CMAKE_CXX_STANDARD_REQUIRED ON)","title":"Language standards"},{"location":"Programming/C%2B%2B/CMake%20Manual/#compile-options","text":"Most of the compile options are now sets automatically based on the declarations in the CMakeLists.txt file. However, some notable exceptions exists. To set such options, we have to use the add_compile_options command: add_compile_options(<option 1> <option 2> ...)","title":"Compile options"},{"location":"Programming/C%2B%2B/CMake%20Manual/#msvc","text":"/permissive- to enable the strictest mode of the compiler","title":"MSVC"},{"location":"Programming/C%2B%2B/CMake%20Manual/#gcc","text":"-pedantic-errors to report all cases where non-standard GCC extension is used and treat them as errors","title":"GCC"},{"location":"Programming/C%2B%2B/CMake%20Manual/#linker-options","text":"Linker options can be set with add_link_options command. Example: add_link_options(\"/STACK: 10000000\")","title":"Linker Options"},{"location":"Programming/C%2B%2B/CMake%20Manual/#dependency-management","text":"There are many ways how to manage dependencies in CMake, for complete overview, see the documentation . Although it is possible to hard-code the paths for includes and linking, it is usually better to initialize the paths automatically using a rich set of commands cmake offers. It has the following advatages: - Hardcoding the paths is error-prone, while cmake commands usually deliver correct paths - It boost the productivity as we do not have to investigate where each library is installed - The resulting CMakeLists.txt file is more portable - And most importantly, potential errors concerning missing libraries are reported prior to the compilation/linking . Most of the libraries have CMake support, so their CMake variables can be initialized simply by calling the find_package command described below. These packages have either: - their own cmake config (cmake-aware libs usually installed through the package manager like vcpkg ) - or they have a Find<package name> script created by someone else that heuristically search for the packege (The default location for these scripts is CMake/share/cmake-<version>/Modules ). For packages without the CMake support, we have to use lower-level cmake commands like find_path or find_libraries . For convinience, we can put these command to our own Find<name> script taht can be used by multiple project or even shared.","title":"Dependency management"},{"location":"Programming/C%2B%2B/CMake%20Manual/#standard-way-find_package","text":"The find_package command is the primary way of obtaining correct variables for a library including: - include paths - linking paths - platform/toolchain specific enviromental variables There are two types of package (library) info search: - module , which uses cmake scripts provided by CMake or OS. The modules are typically provided only for the most used libraries (e.g. boost). All modules provided by CMake are listed in the documentation . - config which uses CMake scripts provided by the developers of the package. They are typically distributed with the source code and downloaded by the package manager. Unless specified, the module mode is used. To force a speciic mode, we can use the MODULE / CONFIG parameters.","title":"Standard way: find_package"},{"location":"Programming/C%2B%2B/CMake%20Manual/#config-packages","text":"Config packages are CMake modules that were created as cmake projects by their developers. They are therefore naturally integrated into Cmake. The configuration files are executed as follows: 1. Package version file: <package name>-config-version.cmake or <package name>ConfigVersion.cmake . This file handles the version compatibility, i.e., it ensures that the installed version of the package is compatible with the version requested in the find_package command. 1. Package configuration file: <package name>-config.cmake or <package name>Config.cmake .","title":"Config packages"},{"location":"Programming/C%2B%2B/CMake%20Manual/#module-packages","text":"Module packages are packages that are not cmake projects themselves, but are hooked into cmake using custom find module scrips. These scripts are automatically executed by find_package . They are located in e.g.: CMake/share/cmake-3.22/Modules/Find<package name>.cmake .","title":"Module Packages"},{"location":"Programming/C%2B%2B/CMake%20Manual/#searching-for-include-directories-with-find_path","text":"The find_path command is intended to find the path (e.g., an include directory). A simple syntax is: find_path( <var name> NAMES <file names> PATHS <paths> ) Here: - <var name> is the name of the resulting variable - <file names> are all possible file names split by space. At least one of the files needs to be present in a path for it to be considered to be the found path. - <paths> are candidate paths split by space","title":"Searching for include directories with find_path"},{"location":"Programming/C%2B%2B/CMake%20Manual/#low-level-command-find_library","text":"The find_library command is used to populate a variable with a result of a specific file search optimized for libraries. The search algorithm works as follows: 1. ? 2. Search package paths - order: 1. <CurrentPackage>_ROOT , 2. ENV{<CurrentPackage>_ROOT} , 3. <ParentPackage>_ROOT , 4. ENV{<ParentPackage>_ROOT} - this only happens if the find_library command is called from within a find_<module> or find_package - this step can be skipped using the NO_PACKAGE_ROOT_PATH parameter 3. Search path from cmake cache. During a clean cmake generation, these can be only supplied by command line. - Considered variables: - CMAKE_LIBRARY_ARCHITECTURE - CMAKE_PREFIX_PATH - CMAKE_LIBRARY_PATH - CMAKE_FRAMEWORK_PATH - this step can be skipped using the NO_CMAKE_PATH parameter 4. Same as step 3, but the variables are searched among system environmental variables instead - this step can be skipped using the NO_CMAKE_ENVIRONMENT_PATH parameter 5. Search paths specified by the HINTS option 6. Search the standard system environmental paths - variables considered are LIB and PATH - this step can be skipped using the NO_SYSTEM_ENVIRONMENT_PATH parameter 7. Search in system paths - Considered variables: - CMAKE_LIBRARY_ARCHITECTURE - CMAKE_SYSTEM_PREFIX_PATH - CMAKE_SYSTEM_LIBRARY_PATH - CMAKE_SYSTEM_FRAMEWORK_PATH - this step can be skipped using the NO_CMAKE_SYSTEM_PATH parameter 8. Search the paths specified by the PATHS option","title":"Low level command: find_library"},{"location":"Programming/C%2B%2B/CMake%20Manual/#searching-for-libraries-in-the-project-dir","text":"Note that the project dir is not searched by default. To include in the search, use: HINTS ${PROJECT_SOURCE_DIR} . Full example on the Gurobi lib stored in <CMAKE LISTS DIR/lib/gurobi_c++.a> : find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED )","title":"Searching for libraries in the project dir"},{"location":"Programming/C%2B%2B/CMake%20Manual/#creating-a-custom-find-script","text":"The structure of a simple find scripts is described in the documentation . We can either put the find script to the default location, so it will be available for all projects, or we can put it in the project directory and add that directory to the CMAKE_MODULE_PATH : list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\") The usual structure of the find script is: 1. the comment section describing the file: #[=======================================================================[.rst: FindMOSEK ------- Finds the MOSEK library. Result Variables ^^^^^^^^^^^^^^^^ This will define the following variables: ``MOESK_FOUND`` True if the system has the MOSEK library. ``MOSEK_INCLUDE_DIRS`` Include directories needed to use MOSEK. ``MOSEK_LIBRARIES`` Libraries needed to link to MOSEK. #]=======================================================================] find commands that fils some temp variables: find_path( MOSEK_INCLUDE_DIR NAMES mosek.h PATHS \"$ENV{MOSEK_HOME}/h\" ) find_library( MOSEK_LIBRARY NAMES libmosek64.so.10.0 libfusion64.so.10.0 PATHS \"$ENV{MOSEK_HOME}/bin\" ) handeling of the result of the file commands. The standard approach is: include(FindPackageHandleStandardArgs) find_package_handle_standard_args(MOSEK FOUND_VAR MOSEK_FOUND REQUIRED_VARS MOSEK_LIBRARY MOSEK_INCLUDE_DIR ) setting the final variables: if(MOSEK_FOUND) set(MOSEK_LIBRARIES ${MOSEK_LIBRARY}) set(MOSEK_INCLUDE_DIRS ${MOSEK_INCLUDE_DIR}) endif()","title":"Creating a custom find script"},{"location":"Programming/C%2B%2B/CMake%20Manual/#downolad-dependencies-during-configuration-and-build-them-from-source","text":"To download and build dependencies during the configuration, we can use the FetchContent module. For details, see the dependency management documentation . The usual workflow is: 1. Download the dependency using the FetchContent_Declare command cmake FetchContent_Declare( <NAME> <SPECIFICATION> ) - The specification can be either a URL or a git repository. 2. Build the dependency using the FetchContent_MakeAvailable command: cmake FetchContent_MakeAvailable(<NAME>)","title":"Downolad dependencies during configuration and build them from source"},{"location":"Programming/C%2B%2B/CMake%20Manual/#target-definition","text":"The target definition is done using the add_executable command. The syntax is: add_executable(<target name> <source file 1> <source file 2> ...) The target name is used to refer to the target in other commands. The target name is also used to name the output file. The list of source files should contain all the source files that are needed to build the target. There are some automatic mechanisms that can be used to add the source files (discussed e.g. on cmake forums ), but they are not recommended.","title":"Target definition"},{"location":"Programming/C%2B%2B/CMake%20Manual/#setting-include-directories","text":"To inlude the headers, we need to use a inlude_directories (global), or better target_include_directories command.","title":"Setting include directories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#linking-configuration","text":"For linkink, use the target_link_libraries command. Make sure that you always link against all the libraries that are needed for the target to work! Do not rely on the linker errors, these may not appear due to library preloading, indirect linkage, advenced linker heuristics, etc. The result is that on one machine the code will work, but on another, it will fail. To find out if and how to link against a library, refer to the documentation of the library.","title":"Linking configuration"},{"location":"Programming/C%2B%2B/CMake%20Manual/#include-directories","text":"All the global include directories are stored in the INCLUDE_DIRECTORIES property, to print them, add this to the CMakeLists.txt file: get_property(dirs DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} PROPERTY INCLUDE_DIRECTORIES) foreach(dir ${dirs}) message(STATUS \"dir='${dir}'\") endforeach()","title":"Include directories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#cmake-directory-structure","text":"","title":"CMake Directory Structure"},{"location":"Programming/C%2B%2B/CMake%20Manual/#system-find_xxxcmake-files","text":"The system find scripts are located in the CMake/share/cmake-<version>/Modules/ directory.","title":"System Find_XXX.cmake files"},{"location":"Programming/C%2B%2B/Google%20Test/","text":"Assertions \u00b6 To determine whether a test should pass or fail, we use assertion macros. There are two main types of assertions: - ASSERT_* - generates a fatal failure when it fails, aborting the current function - EXPECT_* - generates a nonfatal failure, allowing the function to continue running Basic Assertions \u00b6 _EQ(computed, expected) - tests that expected == actual . == must be defined for the type of the arguments.","title":"Google Test"},{"location":"Programming/C%2B%2B/Google%20Test/#assertions","text":"To determine whether a test should pass or fail, we use assertion macros. There are two main types of assertions: - ASSERT_* - generates a fatal failure when it fails, aborting the current function - EXPECT_* - generates a nonfatal failure, allowing the function to continue running","title":"Assertions"},{"location":"Programming/C%2B%2B/Google%20Test/#basic-assertions","text":"_EQ(computed, expected) - tests that expected == actual . == must be defined for the type of the arguments.","title":"Basic Assertions"},{"location":"Programming/Java/Debugging%20Java/","text":"General rules \u00b6 Warnings can help you to spot the problems. Check that they are enabled ( Xlint ). Sometimes, warnings may help to understand the problem. However, they are not emmited due to compilation error. Try to comment out the errorneous code and compile the code to see all warnings. Missing Resources \u00b6 First, check if the resources are where you expected in the jar or in the target folder. The structure is described here on SO . If the resources are not there, try to rebuild the project.","title":"Debugging Java"},{"location":"Programming/Java/Debugging%20Java/#general-rules","text":"Warnings can help you to spot the problems. Check that they are enabled ( Xlint ). Sometimes, warnings may help to understand the problem. However, they are not emmited due to compilation error. Try to comment out the errorneous code and compile the code to see all warnings.","title":"General rules"},{"location":"Programming/Java/Debugging%20Java/#missing-resources","text":"First, check if the resources are where you expected in the jar or in the target folder. The structure is described here on SO . If the resources are not there, try to rebuild the project.","title":"Missing Resources"},{"location":"Programming/Java/Java%20Programming%20Guide/","text":"Strings \u00b6 Classical string literals has to be enclosed in quotes ( \"my string\" ). They cannot contain some characters (newline, \" ). The backslash ( \\ ) character is reserved for Java escape sequences and need to be ecaped as \\\\ . In addition, since Java 15, there are text blocks, enclosed in three quotes: \"\"\" My long \"text block\" \"\"\" The properties of textt blocks: - can be used as standard string literals - can contain newlines, quotes, and some other symbols that have to be escaped in standard string literals. The new line after opening triple quoute is obligatory. Note that backslash ( \\ ) is still interpreted as Java escape character, so we need to escape it using \\\\ . Overloading \u00b6 When calling an oveladed method with a null argument, we receive a compilation error: reference to <METHOD> is ambiguous . A solution to this problem is to cast the null pointer to the desired type: public set(string s){ ... } public set(MyClass c){ .... } set(null) // doesn't work set((MyClass) null) // works set((String) null) // works Regular expressions \u00b6 First thing we need to know about regexes in java is that we need to escape all backslashes (refer to the Strings chapter), there are no raw strings in Java. We can work with regexes either by using specialized high level methods that accepts regex as string, or by using the tools from the java.util.regex package. Testing if string matches a regex \u00b6 We can do it simply by: String MyString = \"tests\"; String MyRegex = \"t\\\\p+\" boolean matches = myString.matches(myRegex); Exceptions \u00b6 The exception mechanism in Java is similar to other languages. The big difference is, however, that in Java, all exception not derived from the RuntimeException class are checked , which means that their handeling is enforced by the compiler. The following code does not compile, because the java.langException class is a checked exception class. private testMethod{ ... throw new Exception(); } Therefore, we need to handle the exception using the try cache block or add throws <EXCEPTION TYPE> to the method declaration. When deciding between try/catch and throws , the rule of thumb is to use the try/cache if we can handle the exception and throws if we want to leave it for the caller. The problem arises when the method we are in implements an interface that does not have the throws declaration we need. Then the only solution is to use try/cache with a cache that does not handle the exception, and indicate the problem to the caller differently, e.g, by returning a null pointer. Genericity \u00b6 Oracle official tutorial for Java 8 Like many other languages, Java offers generic types with the classical syntax: class GenericClass<T>{ ... }; An object is then created as: GenericClass<OtherClass> gc = new GenericClass<OtherClass>(); Diamond operator \u00b6 At all places where the type can be inferred, we can use the dianmond operator ( <> ) without specifiyng a type: GenericClass<OtherClass> gc = new GenericClass<>(); Raw types \u00b6 specification For the backward compatibility of Java library classes that were converted to generic classes a concept of raw types was introduces. However, in addition, this concept brings a lot of very subtle compile errors, which are caused by a nonintentional creation of raw type instead of a specific type. GenericClass gc = new GenericClass(); // raw type The parameterized type can be assigned to the raw type, but when we do it the other way arround, the code emmits warning and is not runtime safe: GenericClass<OtherClass> generic = new GenericClass<>(); GenericClass raw = generic // OK GenericClass raw = new GenericClass(); GenericClass<OtherClass> generic = raw // unsafe Calling the generic methods on raw types has a simmilar consequences. But the worst property of raw types is that all generic non-static members of raw types that are not inherited and are also eraw types. Therefore, the genericity is lost even for the completely unrelated type parameters, even specific types are erased to Object . The consequence can be seen on the following example: class GenericClass<T>{ public List<String> getListofStrings() }; GenericClass raw = new GenericClass(); raw.getListofStrings(); // returns List<Object> !!! Generic method \u00b6 A generic method in Java is written as: public <<GENERIC PARAM NAME>> <RESULT TYPE> <METHOD NAME>(<PARAMS>){ ... } example: public static <T> ProcedureParameterValidationResult<T> error(String msg){ return new ProcedureParameterValidationResult<>(false, msg, null); } if the generic parameter cannot be infered from the method arguments, we can call the method with explicit generic argument like this: <<GENERIC PARAM VALUE>><METHOD NAME>(<ARGUMENTS>); example: ProcedureParameterValidationResult.<String>error(\"Value cannot be empty.\"); Retrieving the type of a generic argument \u00b6 Because the genericity in Java is resolved at runtime, it is not possible to use the generic parameter as a type. What does it mean? Whille it is possible to use Myclass.class it is not possible to use T.class . The same applies for the class methods. The usual solution is to pass the class to the generic method as a method parameter: void processGeneric(Class<T> classType){ ... } Default generic argument \u00b6 It is not possible to set the default value of a generic parameter. If we want to achive such behavior, we need to create a new class: class GeneriClass<T>{ ... } class StringClass extends GenericClass<String>{}; Wildcards \u00b6 The wildcard symbol ( ? ) represents a concrete but unspecified type. It can be bounded by superclass/interface ( ? exctends MyInterface ), or by sublcass ( ? super MyClass ). The typical use cases: - single non-generic method accepting generic types of multiple generic parameter values: myMethod(Wrapper<? extends MyInterface> wrapper){ MyInterface mi = wrapper.get(); ... } method that can process generic class but does not need to work with the generic parameters at all: getLength(List<?> list){ return list.size(); } Difference between MyClass , MyClass<Object> , and MyClass<?> \u00b6 class MyClass<T>{ private T content; private set(T content){ this.content = content; } private List<String> getList; } // raw type processMyClass(MyClass myClass){...}; // My class of object types processMyClass(MyClass<Object> myClass){...}; // My class of any specific type processMyClass(MyClass<?> myClass){...}; MyClass (raw type) MyClass<Object> MyClass<?> type safe (Check types at compile time) no yes yes set can be called with any Object any Object null only getList() returns List<Object> List<String> List<String> processMyClass can be called with any List List<Object> only any List Collections \u00b6 Set \u00b6 From java 9, sets can be simply initialized as: Set<int> nums = Set.of(1,2,3); Note that this method returns an immutable set. In the earlier versions of Java, the Collections.singleton method can be used. Enums \u00b6 Java enumerations are declared using the enum keyword: public enum Semaphore{ RED, ORAGNGE, GREEN } Iterating over enum items \u00b6 we can iterate over enum items using the values method: for(Semaphore s: Semaphore.values()){ ... } The above code iterates over values of a specific enum. If we need an iteration over any enum (generic), we need to use a class method: Class<E> enumClass; for(E item: enumClass.getEnumConstants()){ } Converting a string to enum \u00b6 We can convert a String to enum using the valueOf method that is generated for each enum class> Semaphore sem = Semaphore.valueOf(\"RED\"); Note that the string has to match the enum value exactly, including the case. Extra whitespaces are not permited. There is also a generic static valueOf method in the Enum class, which we can use for generic enums: Class<E> enumClass; E genEnumInst = Enum.valueOf(enumClass, \"RED\"); Note that here E has to be a properly typed enum ( Enum<E> ), not the raw enum ( Enum ). Lambda expressions \u00b6 wiki An important thing about lambda expressions in Java is that we can only use them to create types satisfying some functional interface. This means that: - They can be used only in a context where a functional interface is expected - They need to be convertible to that interface. The example that demonstrate this behavior is bellow. Usually, we use some standard interface, but here we create a new one for clarity: interface OurFunctionalInterface(){ int operation(int a) } ... public void process(int num, OurFunctionalInterface op){ ... } With the above interface and method that uses it, we can call process(0, a -> a + 5) Which is an equivalent of writing OurFunctionalInterface addFive = a -> a + 5; process(0, addFive); Syntax \u00b6 A full syntax for lambda expressions in Java is: (<PARAMS>) -> { <EXPRESSIONS> } If there is only one expression, we can ommit the code block: (<PARAMS>) -> <EXPRESSION> And also, we can ommit the return statement from that expression. The two lambda epressions below are equivalent: (a, b) -> {return a + b;} (a, b) -> a + b If there is only one parameter, we can ommit the parantheses: <PARAM> -> <EXPRESSION> By default, the parameter types are infered from the functional interface. If we need more specific parameters for our function, we can specify the parameter type, we have to specify all of them however: (a, b) -> a + b // valid (int a, int b) -> a + b // valid (int a, b) -> a + b // invalid Also, it is necesary to use the parantheses, if we use the param type. Method references \u00b6 We can also create lambda functions from existing mehods (if they satisfy the desired interface) using a mechanism called method reference . For example, we can use the Integer.sum(int a, int b) method in conext where the IntBinaryOperator interface is required. Instead of IntBinaryOperator op = (a, b) -> a + b; // new lambda body we can write: IntBinaryOperator op = Integer::sum; // lambda from existing function Exceptions in lambda expressions \u00b6 Beware that the checked exceptions thrown inside lambda expressions has to be caught inside the lambda expression. The following code does not compile: try{ process(0, a -> a.methodThatThrows()) catch(exception e){ ... } Instead, we have to write: process(0, a -> { try{ return a.methodThatThrows()); catch(exception e){ ... } } Iteration \u00b6 Java for each loop has the following syntax: for(<TYPE> <VARNAME>: <ITERABLE>){ ... } Here, the <ITERABLE> can be either a Java Iterable , or an array. Enumerated iteration \u00b6 There is no enumerate equivalent in Java. One can use a stream API range method , however, it is less readable than standard for loop because the code execuded in loop has to be in a separate function. Iterating using an iterator \u00b6 The easiest way how to iterate if we have a given iterator is to use its forEachRemaining method. It takes a Consumer object as an argument, iterates using the iterator, and calls Consumer.accept method on each iteration. The example below uses a lambda function that satisfies the consumer interface: class Our{ void process(){ ... } ... } Iterator<Our> objectsIterator = ...; objectsIterator.forEachRemaining(o -> o.process()); Functional Programming \u00b6 Turning array to stream \u00b6 Arrays.stream(<ARRAY>); Creating stream from iterator \u00b6 The easiest way is to first create an Iterable from the iterator and then use the StreamSupport.stream method: Iterable<JsonNode> iterable = () -> iterator; var stream = StreamSupport.stream(iterable.spliterator(), false); Filtering \u00b6 A lambda function can be supplied as a filter: stream.filter(number -> number > 5) returns a stream with numbers greater then five. Transformation \u00b6 We can transform the stream with the map function: transformed = stream.map(object -> doSomething(object)) Materialize stream \u00b6 We can materrialize stream with the collect metod: List<String> result = stringStream.collect(Collectors.toList()); Which can be, in case of List shorten to: List<String> result = stringStream.toList(); var \u00b6 openjdk document Using var is similar to auto in C++. Unlike in C++, it can be used only for local variables. The var can be tricky when using together with diamond operator or generic methods. The compilation works fine, however, the raw type will be created. Type cast \u00b6 Java use a tradidional casting syntax: (<TARGET TYPE>) <VAR> There are two different types of cast: - value cast , which is used for value types (only primitive types in Java) and change the data - reference cast , which is used for Java objects and does not change it, it just change the objects interface Reference cast \u00b6 The upcasting (casting to a supertype) is done implicitely by the compiler, so instead of Object o = (Object) new MyClass(); we can do Object o = new MyClass(); Typically, we need to use the explicit cast for downcasting : Object o = ...; MyClass c = (MyClass) o; // when we are sure that o is an instance of MyClass... Downcasting to an incorrect type leads to a runtime ClassCastException . Casting to an unrelated type is not allowed by the compiler. Casting generic types \u00b6 Casting with generic types is notoriously dangerous due to type erasure. The real types of generic arguments are not known at runtime, therefore, an incorrect cast does not raise an error: Object o = new Object(); String s = (String) o; // ClassCatsException List<Object> lo = new ArrayList<>(); List<String> ls = (List<String>) lo; // OK at runtime, therefore, it emmits warning at compile time. Casting case expression \u00b6 Often, when we work with an interface, we have to treat each implementation differently. The best way to handle that is to use the polymorphism, i.e., add a dedicated method for treating the object to the interface and handle the differences in each implementation. However, sometimes, we cannot modify the interface as it comes from the outside of our codebase, or we do not want to put the code to the interface because it belongs to a completely different part of the application (e.g., GUI vs database connection). A typicall solution for this is to use branching on instanceof and a consecutive cast: if(obj instance of Apple){ Apple apple = (Apple) obj; ... } else if(obj instance of Peach){ Peach peach = (Peach) obj; ... } ... Casting switch \u00b6 This approach works, it is safe, but it is also error prone. A new preview swich statement is ready to replace this technique: switch(obj) case Apple apple: ... break; case Peach peach: ... break; ... } Note that unsafe type opperations are not allowed in these new switch statements, and an error is emitted instead of warning: List list = ... List<String> ls = (List<String>) list; // warning: unchecked switch(list){ case List<String> ls: // error: cannot be safely cast ... } Random numbers \u00b6 Generate random integer \u00b6 To get an infinite iterator of random integers, we can use the Random.ints method: var it = new Random().ints(0, map.nodesFromAllGraphs.size()).iterator();\\ int a = it.nextInt(); Reflection \u00b6 Reflection is a toolset for working with Java types. Its methods can be accessed from a class object associated with all classes. The class object represents the type and it can be accessed either by MyClass.class or by calling the MyClass.getClass method. The type of the class object is Class<> Test if a class is a superclass or interface of another class \u00b6 It can be tested simply as: ClassA.isAssignableFrom(ClassB) Get field by name \u00b6 Field field = MyClass().getDeclaredField(\"fieldName\"); SQL \u00b6 Java has a build in support for SQL in package java.sql . The typical operation: 1. 1. 1. create a PreparedStatement from the cursor using the SQL string as an input 1. Fill the parameters of the PreparedStatement 1. Execute the query Filling the query parameters \u00b6 This process consist of safe replacement of ? marks in the SQL string with real values. The PreparedStatement class has dedicated methods for that: - setString for strings - setObject for complex objects Each method has the index of the ? to be replaced as the first parameter. The index start from 1 . Note that these methods can be used to supply arguments for data part of the SQL statement. If the ? is used for table names or SQL keywords, the query execution will fail. Therefore, if you need to dynamically set the non-data part of an SQL query (e.g., table name), you need to sanitize the argument manually and add it to the SQl querry. Processing the result \u00b6 The java sql framework returns the result of the SQL query in form of a ResultSet . To process the result set, we need to iterate the rows using the next method and for each row, we can access the column values using one of the methods (depending on the dtat type in the column), each of which accepts either column index or label as a parameter. Example: var result = statement.executeQuery(); while(result.next()){ var str = result.getString(\"column_name\")); ... } Jackson \u00b6 Documentation Jackson is a (de)serialization library primarily focused to the JSON format. It supports annotations for automatic serialization and deserialization of Java objects. Ignore specific fields \u00b6 If you want to ignore specific fields during serialization or deserialization, you can use the @JsonIgnoreProperties annotation . Use it as a class or type annotation. Example: @JsonIgnoreProperties({ \"par1\", \"par2\" }) public class ConfigModel { // \"par1\" and \"par2\" will be ignored } This annotation can also prevent the \"Unrecognized field\" error during deserialization, as the ignored fields does not have to be present as Java class members. Represent a class by a single member \u00b6 If we want the java class to be represented by a single value in the serialized file, we can achieve that by adding the @JsonValue annotation above the member or method that should represent the class. Note, however, that this only works for simple values, because the member serializers are not called, the members is serialized as a simple value instead. If you want to represent a class by a single but complex member, use a custom serializer instead . An equivalent annotation for deserialization is the @JsonCreator annotation which should be placed above a constructor or factory method. Deserialization \u00b6 The standard usage is: ObjectMapper mapper = new ObjectMapper(); // ... configure mapper File file = new File(<PATH TO FILE>); Instance instance = mapper.readValue(file, Instance.class); By default, new objects are created for all members in the object hierarchy that are either present in the serialized file. New objects are created using the setter, if exists, otherwise, the costructor is called. Multiple Setters \u00b6 If there are multiple setters, we need to specify the one that should be used for deserialization by marking it with the @JsonSetter annotation. Update existing instance \u00b6 You can update an existing instance using the readerForUpdating method: ObjectReader or = mapper.readerForUpdating(instance); // special reader or.readValue(file) // we can use the method we already know on the object reader Note that by default, the update is shalow . Only the instance object itself is updated, but its members are brand new objects. If you want to keep all objects from an existing object hierarchy, you need to use the @JsonMerge annotation. You should put this annotation above any member of the root object you want to update instead of replacing it. The @JsonUpgrade annotation is recursive : the members of the member annotated with @JsonUpgrade are updated as well and so on. Updating polymorphic types \u00b6 For updating polymorpic type, the rule is that the exact type has to match. Also, you need jackson-databind version 2.14.0 or greater. Read just part of the file \u00b6 For reading just part of the file, use the at selector taht is available in the ObjectReader class. We need to first obtain the reader from a mapper, and then use the selector: ObjectReader reader = mapper.readerFor(Instance.class); Instance instance = reader.at(\"data\").readValue(file) Note that if the path parameter of the at method is incorrect, the method throws an exception with the message: \"no content to map due to end-of-input\". Check that some node is present \u00b6 To check for presence of a node, we should use the JsonPointer class: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); We can also use the JsonPointer in the at method: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); if(found){ Instance instance = reader.at(jsonPointer).readValue(file, Instance.class) } Interface or abstract class \u00b6 When serializing interface or abstract class, it is important to include the implementation type into serialization. Otherwise, the deserialization fails, because it cannot determine the concreate type. To serialize the concrete type, we can use the @JsonRypeInfo and JsonSubTypes annotations: @JsonTypeInfo( use = JsonTypeInfo.Id.CLASS, // what value should we store, here the class name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) public interface Interface { ... } Te above code will work, the full class name will be serialized in the file, however. If we want to use a shorter syntax, i.e., some codename for the class, we need to specify a mapping between this codename and the conreate class: @JsonTypeInfo( use = JsonTypeInfo.Id.NAME, // what value should we store, here a custom name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) @JsonSubTypes({ @JsonSubTypes.Type(value = Implementation.class, name = \"Implementation name\") }) public interface Interface { ... } Custom deserializer \u00b6 An alternative to @JsonTypeInfo is to use a custom deserializer: https://stackoverflow.com/questions/44122782/jackson-deserialize-based-on-type Custom deserializer \u00b6 If our architecture is so complex or specific that none of the Jackson annotations can help us to achieve the desired behavior, we can use a custom deserializer. For that we need to: 1. Implement a custom deserializer by extending the JsonDeserializer class 1. Registering the deserializer in the ObjectMapper Creating a custom deserializer for class \u00b6 The only method we need to implement is the: T deserialize(JsonParser jsonParser, DeserializationContext deserializationContext) where T is the type of the object we are deserializing. To get the Jaksons representation of the JSON tree, we can call: JsonNode node = jsonParser.getCodec().readTree(jsonParser); We can get all the fields by calling using the node.fields() method. For arrays, there is a method node.elements() ; Registering the deserializer \u00b6 ObjectMapper mapper = new ObjectMapper(); SimpleModule module = new SimpleModule(); module.addDeserializer(OurClass.class, new OurClassDeserializer()); mapper.registerModule(module); Custom deserializer with generic types \u00b6 When we need a custom deserializer for a generic class, we need to use a wildcard to cover multiple values of generic argument: public class OurDeserializer extends JsonDeserializer<OurGenericClass<?>>{ ... } If we also need to get the generic argument type from JSON, we need to implement the ContextualDeserializer interface. This is discribed in a SO answer . Custom deserializer with inheritance \u00b6 We can have a common deserializer for both parent and child class, or multiple child classes. However, it is necessary to - make the deserializer generic and - register the deserializer for all classes, not just for the parent. Example: public class PolymorphicDeserializer<T extends Parent> extends JsonDeserializer<T> @Override public T deserialize(JsonParser p, DeserializationContext ctxt) throws IOException, JsonProcessingException { ... } } module.addDeserializer(ChildA.class, new PolymorphicDeserializer<>()); module.addDeserializer(ChildB.class, new ProcedureParameterDeserializer<>()); Serialization \u00b6 Standard serialization: // compressed mapper.writer().writeValue(file, object); // or with indents, etc. mapper.writerWithDefaultPrettyPrinter().writeValue(file, object); By default, new objects are created for all members in the object hierarchy that are either: - public value mebers (fields) - public getters: public methods with name get<NAME> , where <NAME> is a name of some value member Other getters with different name are called only if there is an annotation above them. A special annotation dedicated for this is @JsonProperty . Appending to an existing file \u00b6 To append to an existing file, we need to create an output stream in the append mode and then use it in jackson: ObjectMapper mapper = ... JsonGenerator generator = mapper.getFactory().createGenerator(new FileOutputStream(new File(<FILENAME>), true)); mapper.writeValue(generator, output); Complex member filters \u00b6 Insted of adding annotations to each member we want to ignore, we can also apply some more compex filters, to do that, we need to: 1. add a @JsonFilter(\"<FILTER NAME>\") annotation to all classes for which we want to use the filter 2. create the filter 3. pass a set of all filters we want to use to the writer we are using for serialization The example below keeps only members inherited from MyClass : // object on which we apply the filter @JsonFilter(\"myFilter\") class targetClass{ ... } // filter PropertyFilter filter = new SimpleBeanPropertyFilter() { @Override public void serializeAsField( Object pojo, JsonGenerator jgen, SerializerProvider provider, PropertyWriter writer ) throws Exception { if(writer.getType().isTypeOrSubTypeOf(MyClass.class)){ writer.serializeAsField(pojo, jgen, provider); } } }; FilterProvider filters = new SimpleFilterProvider().addFilter(\"myFilter\", filter); // use a writer created with filters mapper.writer(filters).writeValue(generator, output); ... Flatting the hierarchy \u00b6 When we desire to simplify the object hierarchy, we can use the @JsonUnwrapped annotation above a member of a class. With this annotation, the annotated member object will be skipped while all its members will be serialized into its parent. Custom serializer \u00b6 If the serialization requirements are too complex to be expressed using Jackson annotations, we can use a custom serialzier: public class MyCustomSerializer extends JsonSerializer<MyClass> { @Override public void serialize(MyClass myClass, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException { ... } } Analogously to custom deserializer, we can register custom serializer either in the object mapper: SimpleModule module = new SimpleModule(); module.addSerializer(MyClass.class, new MyCustomSerializer()); mapper.registerModule(module); or by annotating the class: @JsonSerialize(using = MyCustomSerializer.class) public class MyClass{ ... } You can call standard serializers from custom serializers using the SerializerProvider and JsonGenerator instances supplied as a parameters of the serialize method. For example the standard serialized value of som inner/member object class can be obtained using: serializerProvider.defaultSerializeValue(myInnerInstance, jsonGenerator); Annotations \u00b6 Multiple objects \u00b6 If there are multiple objects involved in the (de)serializetion, we can use the @JsonCreator and @JsonProperty annotations to split the work: @JsonCreator public ConfigModel( @JsonProperty(\"first\") ClassA instanceA, @JsonProperty(\"second\") ClassB instanceB ) { ... } in the above examples, the first and second are keys mapping the objects in the serialized file from which instanceA and instanceB should be created.","title":"Java Programming Guide"},{"location":"Programming/Java/Java%20Programming%20Guide/#strings","text":"Classical string literals has to be enclosed in quotes ( \"my string\" ). They cannot contain some characters (newline, \" ). The backslash ( \\ ) character is reserved for Java escape sequences and need to be ecaped as \\\\ . In addition, since Java 15, there are text blocks, enclosed in three quotes: \"\"\" My long \"text block\" \"\"\" The properties of textt blocks: - can be used as standard string literals - can contain newlines, quotes, and some other symbols that have to be escaped in standard string literals. The new line after opening triple quoute is obligatory. Note that backslash ( \\ ) is still interpreted as Java escape character, so we need to escape it using \\\\ .","title":"Strings"},{"location":"Programming/Java/Java%20Programming%20Guide/#overloading","text":"When calling an oveladed method with a null argument, we receive a compilation error: reference to <METHOD> is ambiguous . A solution to this problem is to cast the null pointer to the desired type: public set(string s){ ... } public set(MyClass c){ .... } set(null) // doesn't work set((MyClass) null) // works set((String) null) // works","title":"Overloading"},{"location":"Programming/Java/Java%20Programming%20Guide/#regular-expressions","text":"First thing we need to know about regexes in java is that we need to escape all backslashes (refer to the Strings chapter), there are no raw strings in Java. We can work with regexes either by using specialized high level methods that accepts regex as string, or by using the tools from the java.util.regex package.","title":"Regular expressions"},{"location":"Programming/Java/Java%20Programming%20Guide/#testing-if-string-matches-a-regex","text":"We can do it simply by: String MyString = \"tests\"; String MyRegex = \"t\\\\p+\" boolean matches = myString.matches(myRegex);","title":"Testing if string matches a regex"},{"location":"Programming/Java/Java%20Programming%20Guide/#exceptions","text":"The exception mechanism in Java is similar to other languages. The big difference is, however, that in Java, all exception not derived from the RuntimeException class are checked , which means that their handeling is enforced by the compiler. The following code does not compile, because the java.langException class is a checked exception class. private testMethod{ ... throw new Exception(); } Therefore, we need to handle the exception using the try cache block or add throws <EXCEPTION TYPE> to the method declaration. When deciding between try/catch and throws , the rule of thumb is to use the try/cache if we can handle the exception and throws if we want to leave it for the caller. The problem arises when the method we are in implements an interface that does not have the throws declaration we need. Then the only solution is to use try/cache with a cache that does not handle the exception, and indicate the problem to the caller differently, e.g, by returning a null pointer.","title":"Exceptions"},{"location":"Programming/Java/Java%20Programming%20Guide/#genericity","text":"Oracle official tutorial for Java 8 Like many other languages, Java offers generic types with the classical syntax: class GenericClass<T>{ ... }; An object is then created as: GenericClass<OtherClass> gc = new GenericClass<OtherClass>();","title":"Genericity"},{"location":"Programming/Java/Java%20Programming%20Guide/#diamond-operator","text":"At all places where the type can be inferred, we can use the dianmond operator ( <> ) without specifiyng a type: GenericClass<OtherClass> gc = new GenericClass<>();","title":"Diamond operator"},{"location":"Programming/Java/Java%20Programming%20Guide/#raw-types","text":"specification For the backward compatibility of Java library classes that were converted to generic classes a concept of raw types was introduces. However, in addition, this concept brings a lot of very subtle compile errors, which are caused by a nonintentional creation of raw type instead of a specific type. GenericClass gc = new GenericClass(); // raw type The parameterized type can be assigned to the raw type, but when we do it the other way arround, the code emmits warning and is not runtime safe: GenericClass<OtherClass> generic = new GenericClass<>(); GenericClass raw = generic // OK GenericClass raw = new GenericClass(); GenericClass<OtherClass> generic = raw // unsafe Calling the generic methods on raw types has a simmilar consequences. But the worst property of raw types is that all generic non-static members of raw types that are not inherited and are also eraw types. Therefore, the genericity is lost even for the completely unrelated type parameters, even specific types are erased to Object . The consequence can be seen on the following example: class GenericClass<T>{ public List<String> getListofStrings() }; GenericClass raw = new GenericClass(); raw.getListofStrings(); // returns List<Object> !!!","title":"Raw types"},{"location":"Programming/Java/Java%20Programming%20Guide/#generic-method","text":"A generic method in Java is written as: public <<GENERIC PARAM NAME>> <RESULT TYPE> <METHOD NAME>(<PARAMS>){ ... } example: public static <T> ProcedureParameterValidationResult<T> error(String msg){ return new ProcedureParameterValidationResult<>(false, msg, null); } if the generic parameter cannot be infered from the method arguments, we can call the method with explicit generic argument like this: <<GENERIC PARAM VALUE>><METHOD NAME>(<ARGUMENTS>); example: ProcedureParameterValidationResult.<String>error(\"Value cannot be empty.\");","title":"Generic method"},{"location":"Programming/Java/Java%20Programming%20Guide/#retrieving-the-type-of-a-generic-argument","text":"Because the genericity in Java is resolved at runtime, it is not possible to use the generic parameter as a type. What does it mean? Whille it is possible to use Myclass.class it is not possible to use T.class . The same applies for the class methods. The usual solution is to pass the class to the generic method as a method parameter: void processGeneric(Class<T> classType){ ... }","title":"Retrieving the type of a generic argument"},{"location":"Programming/Java/Java%20Programming%20Guide/#default-generic-argument","text":"It is not possible to set the default value of a generic parameter. If we want to achive such behavior, we need to create a new class: class GeneriClass<T>{ ... } class StringClass extends GenericClass<String>{};","title":"Default generic argument"},{"location":"Programming/Java/Java%20Programming%20Guide/#wildcards","text":"The wildcard symbol ( ? ) represents a concrete but unspecified type. It can be bounded by superclass/interface ( ? exctends MyInterface ), or by sublcass ( ? super MyClass ). The typical use cases: - single non-generic method accepting generic types of multiple generic parameter values: myMethod(Wrapper<? extends MyInterface> wrapper){ MyInterface mi = wrapper.get(); ... } method that can process generic class but does not need to work with the generic parameters at all: getLength(List<?> list){ return list.size(); }","title":"Wildcards"},{"location":"Programming/Java/Java%20Programming%20Guide/#difference-between-myclass-myclassobject-and-myclass","text":"class MyClass<T>{ private T content; private set(T content){ this.content = content; } private List<String> getList; } // raw type processMyClass(MyClass myClass){...}; // My class of object types processMyClass(MyClass<Object> myClass){...}; // My class of any specific type processMyClass(MyClass<?> myClass){...}; MyClass (raw type) MyClass<Object> MyClass<?> type safe (Check types at compile time) no yes yes set can be called with any Object any Object null only getList() returns List<Object> List<String> List<String> processMyClass can be called with any List List<Object> only any List","title":"Difference between MyClass, MyClass&lt;Object&gt;, and MyClass&lt;?&gt;"},{"location":"Programming/Java/Java%20Programming%20Guide/#collections","text":"","title":"Collections"},{"location":"Programming/Java/Java%20Programming%20Guide/#set","text":"From java 9, sets can be simply initialized as: Set<int> nums = Set.of(1,2,3); Note that this method returns an immutable set. In the earlier versions of Java, the Collections.singleton method can be used.","title":"Set"},{"location":"Programming/Java/Java%20Programming%20Guide/#enums","text":"Java enumerations are declared using the enum keyword: public enum Semaphore{ RED, ORAGNGE, GREEN }","title":"Enums"},{"location":"Programming/Java/Java%20Programming%20Guide/#iterating-over-enum-items","text":"we can iterate over enum items using the values method: for(Semaphore s: Semaphore.values()){ ... } The above code iterates over values of a specific enum. If we need an iteration over any enum (generic), we need to use a class method: Class<E> enumClass; for(E item: enumClass.getEnumConstants()){ }","title":"Iterating over enum items"},{"location":"Programming/Java/Java%20Programming%20Guide/#converting-a-string-to-enum","text":"We can convert a String to enum using the valueOf method that is generated for each enum class> Semaphore sem = Semaphore.valueOf(\"RED\"); Note that the string has to match the enum value exactly, including the case. Extra whitespaces are not permited. There is also a generic static valueOf method in the Enum class, which we can use for generic enums: Class<E> enumClass; E genEnumInst = Enum.valueOf(enumClass, \"RED\"); Note that here E has to be a properly typed enum ( Enum<E> ), not the raw enum ( Enum ).","title":"Converting a string to enum"},{"location":"Programming/Java/Java%20Programming%20Guide/#lambda-expressions","text":"wiki An important thing about lambda expressions in Java is that we can only use them to create types satisfying some functional interface. This means that: - They can be used only in a context where a functional interface is expected - They need to be convertible to that interface. The example that demonstrate this behavior is bellow. Usually, we use some standard interface, but here we create a new one for clarity: interface OurFunctionalInterface(){ int operation(int a) } ... public void process(int num, OurFunctionalInterface op){ ... } With the above interface and method that uses it, we can call process(0, a -> a + 5) Which is an equivalent of writing OurFunctionalInterface addFive = a -> a + 5; process(0, addFive);","title":"Lambda expressions"},{"location":"Programming/Java/Java%20Programming%20Guide/#syntax","text":"A full syntax for lambda expressions in Java is: (<PARAMS>) -> { <EXPRESSIONS> } If there is only one expression, we can ommit the code block: (<PARAMS>) -> <EXPRESSION> And also, we can ommit the return statement from that expression. The two lambda epressions below are equivalent: (a, b) -> {return a + b;} (a, b) -> a + b If there is only one parameter, we can ommit the parantheses: <PARAM> -> <EXPRESSION> By default, the parameter types are infered from the functional interface. If we need more specific parameters for our function, we can specify the parameter type, we have to specify all of them however: (a, b) -> a + b // valid (int a, int b) -> a + b // valid (int a, b) -> a + b // invalid Also, it is necesary to use the parantheses, if we use the param type.","title":"Syntax"},{"location":"Programming/Java/Java%20Programming%20Guide/#method-references","text":"We can also create lambda functions from existing mehods (if they satisfy the desired interface) using a mechanism called method reference . For example, we can use the Integer.sum(int a, int b) method in conext where the IntBinaryOperator interface is required. Instead of IntBinaryOperator op = (a, b) -> a + b; // new lambda body we can write: IntBinaryOperator op = Integer::sum; // lambda from existing function","title":"Method references"},{"location":"Programming/Java/Java%20Programming%20Guide/#exceptions-in-lambda-expressions","text":"Beware that the checked exceptions thrown inside lambda expressions has to be caught inside the lambda expression. The following code does not compile: try{ process(0, a -> a.methodThatThrows()) catch(exception e){ ... } Instead, we have to write: process(0, a -> { try{ return a.methodThatThrows()); catch(exception e){ ... } }","title":"Exceptions in lambda expressions"},{"location":"Programming/Java/Java%20Programming%20Guide/#iteration","text":"Java for each loop has the following syntax: for(<TYPE> <VARNAME>: <ITERABLE>){ ... } Here, the <ITERABLE> can be either a Java Iterable , or an array.","title":"Iteration"},{"location":"Programming/Java/Java%20Programming%20Guide/#enumerated-iteration","text":"There is no enumerate equivalent in Java. One can use a stream API range method , however, it is less readable than standard for loop because the code execuded in loop has to be in a separate function.","title":"Enumerated iteration"},{"location":"Programming/Java/Java%20Programming%20Guide/#iterating-using-an-iterator","text":"The easiest way how to iterate if we have a given iterator is to use its forEachRemaining method. It takes a Consumer object as an argument, iterates using the iterator, and calls Consumer.accept method on each iteration. The example below uses a lambda function that satisfies the consumer interface: class Our{ void process(){ ... } ... } Iterator<Our> objectsIterator = ...; objectsIterator.forEachRemaining(o -> o.process());","title":"Iterating using an iterator"},{"location":"Programming/Java/Java%20Programming%20Guide/#functional-programming","text":"","title":"Functional Programming"},{"location":"Programming/Java/Java%20Programming%20Guide/#turning-array-to-stream","text":"Arrays.stream(<ARRAY>);","title":"Turning array to stream"},{"location":"Programming/Java/Java%20Programming%20Guide/#creating-stream-from-iterator","text":"The easiest way is to first create an Iterable from the iterator and then use the StreamSupport.stream method: Iterable<JsonNode> iterable = () -> iterator; var stream = StreamSupport.stream(iterable.spliterator(), false);","title":"Creating stream from iterator"},{"location":"Programming/Java/Java%20Programming%20Guide/#filtering","text":"A lambda function can be supplied as a filter: stream.filter(number -> number > 5) returns a stream with numbers greater then five.","title":"Filtering"},{"location":"Programming/Java/Java%20Programming%20Guide/#transformation","text":"We can transform the stream with the map function: transformed = stream.map(object -> doSomething(object))","title":"Transformation"},{"location":"Programming/Java/Java%20Programming%20Guide/#materialize-stream","text":"We can materrialize stream with the collect metod: List<String> result = stringStream.collect(Collectors.toList()); Which can be, in case of List shorten to: List<String> result = stringStream.toList();","title":"Materialize stream"},{"location":"Programming/Java/Java%20Programming%20Guide/#var","text":"openjdk document Using var is similar to auto in C++. Unlike in C++, it can be used only for local variables. The var can be tricky when using together with diamond operator or generic methods. The compilation works fine, however, the raw type will be created.","title":"var"},{"location":"Programming/Java/Java%20Programming%20Guide/#type-cast","text":"Java use a tradidional casting syntax: (<TARGET TYPE>) <VAR> There are two different types of cast: - value cast , which is used for value types (only primitive types in Java) and change the data - reference cast , which is used for Java objects and does not change it, it just change the objects interface","title":"Type cast"},{"location":"Programming/Java/Java%20Programming%20Guide/#reference-cast","text":"The upcasting (casting to a supertype) is done implicitely by the compiler, so instead of Object o = (Object) new MyClass(); we can do Object o = new MyClass(); Typically, we need to use the explicit cast for downcasting : Object o = ...; MyClass c = (MyClass) o; // when we are sure that o is an instance of MyClass... Downcasting to an incorrect type leads to a runtime ClassCastException . Casting to an unrelated type is not allowed by the compiler.","title":"Reference cast"},{"location":"Programming/Java/Java%20Programming%20Guide/#casting-generic-types","text":"Casting with generic types is notoriously dangerous due to type erasure. The real types of generic arguments are not known at runtime, therefore, an incorrect cast does not raise an error: Object o = new Object(); String s = (String) o; // ClassCatsException List<Object> lo = new ArrayList<>(); List<String> ls = (List<String>) lo; // OK at runtime, therefore, it emmits warning at compile time.","title":"Casting generic types"},{"location":"Programming/Java/Java%20Programming%20Guide/#casting-case-expression","text":"Often, when we work with an interface, we have to treat each implementation differently. The best way to handle that is to use the polymorphism, i.e., add a dedicated method for treating the object to the interface and handle the differences in each implementation. However, sometimes, we cannot modify the interface as it comes from the outside of our codebase, or we do not want to put the code to the interface because it belongs to a completely different part of the application (e.g., GUI vs database connection). A typicall solution for this is to use branching on instanceof and a consecutive cast: if(obj instance of Apple){ Apple apple = (Apple) obj; ... } else if(obj instance of Peach){ Peach peach = (Peach) obj; ... } ...","title":"Casting case expression"},{"location":"Programming/Java/Java%20Programming%20Guide/#casting-switch","text":"This approach works, it is safe, but it is also error prone. A new preview swich statement is ready to replace this technique: switch(obj) case Apple apple: ... break; case Peach peach: ... break; ... } Note that unsafe type opperations are not allowed in these new switch statements, and an error is emitted instead of warning: List list = ... List<String> ls = (List<String>) list; // warning: unchecked switch(list){ case List<String> ls: // error: cannot be safely cast ... }","title":"Casting switch"},{"location":"Programming/Java/Java%20Programming%20Guide/#random-numbers","text":"","title":"Random numbers"},{"location":"Programming/Java/Java%20Programming%20Guide/#generate-random-integer","text":"To get an infinite iterator of random integers, we can use the Random.ints method: var it = new Random().ints(0, map.nodesFromAllGraphs.size()).iterator();\\ int a = it.nextInt();","title":"Generate random integer"},{"location":"Programming/Java/Java%20Programming%20Guide/#reflection","text":"Reflection is a toolset for working with Java types. Its methods can be accessed from a class object associated with all classes. The class object represents the type and it can be accessed either by MyClass.class or by calling the MyClass.getClass method. The type of the class object is Class<>","title":"Reflection"},{"location":"Programming/Java/Java%20Programming%20Guide/#test-if-a-class-is-a-superclass-or-interface-of-another-class","text":"It can be tested simply as: ClassA.isAssignableFrom(ClassB)","title":"Test if a class is a superclass or interface of another class"},{"location":"Programming/Java/Java%20Programming%20Guide/#get-field-by-name","text":"Field field = MyClass().getDeclaredField(\"fieldName\");","title":"Get field by name"},{"location":"Programming/Java/Java%20Programming%20Guide/#sql","text":"Java has a build in support for SQL in package java.sql . The typical operation: 1. 1. 1. create a PreparedStatement from the cursor using the SQL string as an input 1. Fill the parameters of the PreparedStatement 1. Execute the query","title":"SQL"},{"location":"Programming/Java/Java%20Programming%20Guide/#filling-the-query-parameters","text":"This process consist of safe replacement of ? marks in the SQL string with real values. The PreparedStatement class has dedicated methods for that: - setString for strings - setObject for complex objects Each method has the index of the ? to be replaced as the first parameter. The index start from 1 . Note that these methods can be used to supply arguments for data part of the SQL statement. If the ? is used for table names or SQL keywords, the query execution will fail. Therefore, if you need to dynamically set the non-data part of an SQL query (e.g., table name), you need to sanitize the argument manually and add it to the SQl querry.","title":"Filling the query parameters"},{"location":"Programming/Java/Java%20Programming%20Guide/#processing-the-result","text":"The java sql framework returns the result of the SQL query in form of a ResultSet . To process the result set, we need to iterate the rows using the next method and for each row, we can access the column values using one of the methods (depending on the dtat type in the column), each of which accepts either column index or label as a parameter. Example: var result = statement.executeQuery(); while(result.next()){ var str = result.getString(\"column_name\")); ... }","title":"Processing the result"},{"location":"Programming/Java/Java%20Programming%20Guide/#jackson","text":"Documentation Jackson is a (de)serialization library primarily focused to the JSON format. It supports annotations for automatic serialization and deserialization of Java objects.","title":"Jackson"},{"location":"Programming/Java/Java%20Programming%20Guide/#ignore-specific-fields","text":"If you want to ignore specific fields during serialization or deserialization, you can use the @JsonIgnoreProperties annotation . Use it as a class or type annotation. Example: @JsonIgnoreProperties({ \"par1\", \"par2\" }) public class ConfigModel { // \"par1\" and \"par2\" will be ignored } This annotation can also prevent the \"Unrecognized field\" error during deserialization, as the ignored fields does not have to be present as Java class members.","title":"Ignore specific fields"},{"location":"Programming/Java/Java%20Programming%20Guide/#represent-a-class-by-a-single-member","text":"If we want the java class to be represented by a single value in the serialized file, we can achieve that by adding the @JsonValue annotation above the member or method that should represent the class. Note, however, that this only works for simple values, because the member serializers are not called, the members is serialized as a simple value instead. If you want to represent a class by a single but complex member, use a custom serializer instead . An equivalent annotation for deserialization is the @JsonCreator annotation which should be placed above a constructor or factory method.","title":"Represent a class by a single member"},{"location":"Programming/Java/Java%20Programming%20Guide/#deserialization","text":"The standard usage is: ObjectMapper mapper = new ObjectMapper(); // ... configure mapper File file = new File(<PATH TO FILE>); Instance instance = mapper.readValue(file, Instance.class); By default, new objects are created for all members in the object hierarchy that are either present in the serialized file. New objects are created using the setter, if exists, otherwise, the costructor is called.","title":"Deserialization"},{"location":"Programming/Java/Java%20Programming%20Guide/#multiple-setters","text":"If there are multiple setters, we need to specify the one that should be used for deserialization by marking it with the @JsonSetter annotation.","title":"Multiple Setters"},{"location":"Programming/Java/Java%20Programming%20Guide/#update-existing-instance","text":"You can update an existing instance using the readerForUpdating method: ObjectReader or = mapper.readerForUpdating(instance); // special reader or.readValue(file) // we can use the method we already know on the object reader Note that by default, the update is shalow . Only the instance object itself is updated, but its members are brand new objects. If you want to keep all objects from an existing object hierarchy, you need to use the @JsonMerge annotation. You should put this annotation above any member of the root object you want to update instead of replacing it. The @JsonUpgrade annotation is recursive : the members of the member annotated with @JsonUpgrade are updated as well and so on.","title":"Update existing instance"},{"location":"Programming/Java/Java%20Programming%20Guide/#updating-polymorphic-types","text":"For updating polymorpic type, the rule is that the exact type has to match. Also, you need jackson-databind version 2.14.0 or greater.","title":"Updating polymorphic types"},{"location":"Programming/Java/Java%20Programming%20Guide/#read-just-part-of-the-file","text":"For reading just part of the file, use the at selector taht is available in the ObjectReader class. We need to first obtain the reader from a mapper, and then use the selector: ObjectReader reader = mapper.readerFor(Instance.class); Instance instance = reader.at(\"data\").readValue(file) Note that if the path parameter of the at method is incorrect, the method throws an exception with the message: \"no content to map due to end-of-input\".","title":"Read just part of the file"},{"location":"Programming/Java/Java%20Programming%20Guide/#check-that-some-node-is-present","text":"To check for presence of a node, we should use the JsonPointer class: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); We can also use the JsonPointer in the at method: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); if(found){ Instance instance = reader.at(jsonPointer).readValue(file, Instance.class) }","title":"Check that some node is present"},{"location":"Programming/Java/Java%20Programming%20Guide/#interface-or-abstract-class","text":"When serializing interface or abstract class, it is important to include the implementation type into serialization. Otherwise, the deserialization fails, because it cannot determine the concreate type. To serialize the concrete type, we can use the @JsonRypeInfo and JsonSubTypes annotations: @JsonTypeInfo( use = JsonTypeInfo.Id.CLASS, // what value should we store, here the class name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) public interface Interface { ... } Te above code will work, the full class name will be serialized in the file, however. If we want to use a shorter syntax, i.e., some codename for the class, we need to specify a mapping between this codename and the conreate class: @JsonTypeInfo( use = JsonTypeInfo.Id.NAME, // what value should we store, here a custom name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) @JsonSubTypes({ @JsonSubTypes.Type(value = Implementation.class, name = \"Implementation name\") }) public interface Interface { ... }","title":"Interface or abstract class"},{"location":"Programming/Java/Java%20Programming%20Guide/#custom-deserializer","text":"An alternative to @JsonTypeInfo is to use a custom deserializer: https://stackoverflow.com/questions/44122782/jackson-deserialize-based-on-type","title":"Custom deserializer"},{"location":"Programming/Java/Java%20Programming%20Guide/#custom-deserializer_1","text":"If our architecture is so complex or specific that none of the Jackson annotations can help us to achieve the desired behavior, we can use a custom deserializer. For that we need to: 1. Implement a custom deserializer by extending the JsonDeserializer class 1. Registering the deserializer in the ObjectMapper","title":"Custom deserializer"},{"location":"Programming/Java/Java%20Programming%20Guide/#creating-a-custom-deserializer-for-class","text":"The only method we need to implement is the: T deserialize(JsonParser jsonParser, DeserializationContext deserializationContext) where T is the type of the object we are deserializing. To get the Jaksons representation of the JSON tree, we can call: JsonNode node = jsonParser.getCodec().readTree(jsonParser); We can get all the fields by calling using the node.fields() method. For arrays, there is a method node.elements() ;","title":"Creating a custom deserializer for class"},{"location":"Programming/Java/Java%20Programming%20Guide/#registering-the-deserializer","text":"ObjectMapper mapper = new ObjectMapper(); SimpleModule module = new SimpleModule(); module.addDeserializer(OurClass.class, new OurClassDeserializer()); mapper.registerModule(module);","title":"Registering the deserializer"},{"location":"Programming/Java/Java%20Programming%20Guide/#custom-deserializer-with-generic-types","text":"When we need a custom deserializer for a generic class, we need to use a wildcard to cover multiple values of generic argument: public class OurDeserializer extends JsonDeserializer<OurGenericClass<?>>{ ... } If we also need to get the generic argument type from JSON, we need to implement the ContextualDeserializer interface. This is discribed in a SO answer .","title":"Custom deserializer with generic types"},{"location":"Programming/Java/Java%20Programming%20Guide/#custom-deserializer-with-inheritance","text":"We can have a common deserializer for both parent and child class, or multiple child classes. However, it is necessary to - make the deserializer generic and - register the deserializer for all classes, not just for the parent. Example: public class PolymorphicDeserializer<T extends Parent> extends JsonDeserializer<T> @Override public T deserialize(JsonParser p, DeserializationContext ctxt) throws IOException, JsonProcessingException { ... } } module.addDeserializer(ChildA.class, new PolymorphicDeserializer<>()); module.addDeserializer(ChildB.class, new ProcedureParameterDeserializer<>());","title":"Custom deserializer with inheritance"},{"location":"Programming/Java/Java%20Programming%20Guide/#serialization","text":"Standard serialization: // compressed mapper.writer().writeValue(file, object); // or with indents, etc. mapper.writerWithDefaultPrettyPrinter().writeValue(file, object); By default, new objects are created for all members in the object hierarchy that are either: - public value mebers (fields) - public getters: public methods with name get<NAME> , where <NAME> is a name of some value member Other getters with different name are called only if there is an annotation above them. A special annotation dedicated for this is @JsonProperty .","title":"Serialization"},{"location":"Programming/Java/Java%20Programming%20Guide/#appending-to-an-existing-file","text":"To append to an existing file, we need to create an output stream in the append mode and then use it in jackson: ObjectMapper mapper = ... JsonGenerator generator = mapper.getFactory().createGenerator(new FileOutputStream(new File(<FILENAME>), true)); mapper.writeValue(generator, output);","title":"Appending to an existing file"},{"location":"Programming/Java/Java%20Programming%20Guide/#complex-member-filters","text":"Insted of adding annotations to each member we want to ignore, we can also apply some more compex filters, to do that, we need to: 1. add a @JsonFilter(\"<FILTER NAME>\") annotation to all classes for which we want to use the filter 2. create the filter 3. pass a set of all filters we want to use to the writer we are using for serialization The example below keeps only members inherited from MyClass : // object on which we apply the filter @JsonFilter(\"myFilter\") class targetClass{ ... } // filter PropertyFilter filter = new SimpleBeanPropertyFilter() { @Override public void serializeAsField( Object pojo, JsonGenerator jgen, SerializerProvider provider, PropertyWriter writer ) throws Exception { if(writer.getType().isTypeOrSubTypeOf(MyClass.class)){ writer.serializeAsField(pojo, jgen, provider); } } }; FilterProvider filters = new SimpleFilterProvider().addFilter(\"myFilter\", filter); // use a writer created with filters mapper.writer(filters).writeValue(generator, output); ...","title":"Complex member filters"},{"location":"Programming/Java/Java%20Programming%20Guide/#flatting-the-hierarchy","text":"When we desire to simplify the object hierarchy, we can use the @JsonUnwrapped annotation above a member of a class. With this annotation, the annotated member object will be skipped while all its members will be serialized into its parent.","title":"Flatting the hierarchy"},{"location":"Programming/Java/Java%20Programming%20Guide/#custom-serializer","text":"If the serialization requirements are too complex to be expressed using Jackson annotations, we can use a custom serialzier: public class MyCustomSerializer extends JsonSerializer<MyClass> { @Override public void serialize(MyClass myClass, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException { ... } } Analogously to custom deserializer, we can register custom serializer either in the object mapper: SimpleModule module = new SimpleModule(); module.addSerializer(MyClass.class, new MyCustomSerializer()); mapper.registerModule(module); or by annotating the class: @JsonSerialize(using = MyCustomSerializer.class) public class MyClass{ ... } You can call standard serializers from custom serializers using the SerializerProvider and JsonGenerator instances supplied as a parameters of the serialize method. For example the standard serialized value of som inner/member object class can be obtained using: serializerProvider.defaultSerializeValue(myInnerInstance, jsonGenerator);","title":"Custom serializer"},{"location":"Programming/Java/Java%20Programming%20Guide/#annotations","text":"","title":"Annotations"},{"location":"Programming/Java/Java%20Programming%20Guide/#multiple-objects","text":"If there are multiple objects involved in the (de)serializetion, we can use the @JsonCreator and @JsonProperty annotations to split the work: @JsonCreator public ConfigModel( @JsonProperty(\"first\") ClassA instanceA, @JsonProperty(\"second\") ClassB instanceB ) { ... } in the above examples, the first and second are keys mapping the objects in the serialized file from which instanceA and instanceB should be created.","title":"Multiple objects"},{"location":"Programming/Java/Java%20Workflow/","text":"Java developement stack \u00b6 We use this stack: - Toolchain: JDK - Package manager: standalone Maven - IDE: Netbeans, Idea JDK \u00b6 Java developem kit is the standard Java toolchain. Most comon tools are: - javac for compilation - java for execution javac \u00b6 The syntax is: javac [options] [sourcefiles] Warning control \u00b6 By default, all warnings are disabled and only a summary of the warnings is displayed in the output (i.e., all warning types encountered, without specific lines where they occur). To enable all warnings use the -Xlint argument. To enable/disable specific warnings, use -Xlint:<warning name> and -Xlint:-<warning name> , respectively. Maven \u00b6 Download maven from the official website and extract the archive somewhere (e.g. C:/ ) Add absolute path to <mavendir>/bin to PATH Netbeans \u00b6 Install the latest version of Apache Netbeans Configuration: Autosaving: Editor -> Autosave Tab size, tabs instead of spaces, 120 char marker line: Editor -> Formatting -> All Languages Multi-row tabs: Appearance -> DocumentTabs Git labels on projects: Team -> Versioning -> Show Versioning Labels Internet Browser: General -> web browser Javadoc config: if the javadoc for java SE does not work out of the box, maybe there is a wrong URL. Go to Tools -> Java Platforms -> Javadoc and enter there the path where the Javadoc is accessible online Install the basic plugins Markdown Project configuration \u00b6 Configure Maven Goals \u00b6 These can be configured in Project properties -> Actions Troubleshooting \u00b6 If there is a serious problem, one way to solve it can be to delete the Netbeans cache located in ~\\AppData\\Local\\NetBeans\\Cache . Idea \u00b6 Configuration \u00b6 Settings synchronization \u00b6 Log in into JetBrains Toolbox or to the App Click on the gear icon on the top-right and choose Sync Check all categories and click on pull settings from the cloud Resatart Ida to update all the settings More on Jetbrains Project configuration \u00b6 The correct JDK has to be set up in various places: - compielr has to be at least target jdk, set it in: File -> Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Project bytecode version and Per-module bytecode version - language level should be the same as target jdk: File -> Project Structure... -> Modules -> Language Level Compilation \u00b6 Everything is compiled in the background automatically. However, if we need to compile manually using maven, e.g., to activate certain pluginns, we need to: Set Java version for project \u00b6 There are two options to set: - source version determines the least version of java that supports all language tools used in the source code. It is the least version that can be used to compile the source code. - target version determines the least version of java that can be used to run your java application. The target version can be higher than the source version, but not he other way around. most of the time, however, we use the same version for source and for target. Usually, these versions needs to be set: 1. in the project compilation tool (maven) to configure the project compilation 2. in the IDE project properties, to configure the compilation of individual files, which is executed in the background to report the compilation errors at real time. Sometimes, the Java version used for running Maven has to be also set because it cannot be lower then the Java version used for project compilation using Maven. SO explanation Setting Java version in Maven \u00b6 Since Java 9, both Java versions can be set at once with the release option: <properties> <maven.compiler.release>10</maven.compiler.release> </properties> or equivalently <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> <release>10</release> </configuration> </plugin> The old way is to use separate properties: <properties> <maven.compiler.target>10</maven.compiler.target> <maven.compiler.source>10</maven.compiler.source> </properties> Note that the Java version configured in the pom file cannot be greater then the Java version used to run the Maven. Setting Java version in Netbeans \u00b6 Note that for maven projects, this is automatically set to match the properties in the pom file. For the Netbeans real-time compiler, the cross compilation does not make sense, so both source and target Java version is set in one place: 1. Right click on project -> Properties -> Sources 2. In the bottom, change the Source/Binary Format Setting Java version used for executing Maven \u00b6 If the Maven is executed from command line, edit the JAVA_HOME system property. If the Maven is executed from Netbeans, edit Project Properties -> Build -> Compile -> Java Platform Enabling preview features \u00b6 The preview features can be enabled using the --enable-preview argument. In Maven, this has to be passed to the compiler plugin: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <enablePreview>true</enablePreview> </configuration> </plugin> </plugins> </build> For Maven compiler plugin older then version 3.10.1, use: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <compilerArgs> <arg>--enable-preview</arg> </compilerArgs> </configuration> </plugin> </plugins> </build> Unfortunatelly, it is also necessary to enable preview features in the IDE configuration: - In Netbeans, add --enable-preview to Project Properties -> Run -> VM options . - In IDEA: 1. add --enable-preview to Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Override compiler parameters per-module 2. add --enable-preview to run configuration -> Build and run vm options Gurobi \u00b6 Gurobi is a commercial project not contained in any public maven repositories. It is necessary to install the gurobi maven artifact manualy. Potential problems \u00b6 unsatisfied linker error : Check if the gurobi version in the error log matches the gurobi version installed. Archive \u00b6 AIC maven repo access \u00b6 To Access the AIC maven repo, copy maven settings from another computer (located in ~/.m2) Written with StackEdit .","title":"Java Workflow"},{"location":"Programming/Java/Java%20Workflow/#java-developement-stack","text":"We use this stack: - Toolchain: JDK - Package manager: standalone Maven - IDE: Netbeans, Idea","title":"Java developement stack"},{"location":"Programming/Java/Java%20Workflow/#jdk","text":"Java developem kit is the standard Java toolchain. Most comon tools are: - javac for compilation - java for execution","title":"JDK"},{"location":"Programming/Java/Java%20Workflow/#javac","text":"The syntax is: javac [options] [sourcefiles]","title":"javac"},{"location":"Programming/Java/Java%20Workflow/#warning-control","text":"By default, all warnings are disabled and only a summary of the warnings is displayed in the output (i.e., all warning types encountered, without specific lines where they occur). To enable all warnings use the -Xlint argument. To enable/disable specific warnings, use -Xlint:<warning name> and -Xlint:-<warning name> , respectively.","title":"Warning control"},{"location":"Programming/Java/Java%20Workflow/#maven","text":"Download maven from the official website and extract the archive somewhere (e.g. C:/ ) Add absolute path to <mavendir>/bin to PATH","title":"Maven"},{"location":"Programming/Java/Java%20Workflow/#netbeans","text":"Install the latest version of Apache Netbeans Configuration: Autosaving: Editor -> Autosave Tab size, tabs instead of spaces, 120 char marker line: Editor -> Formatting -> All Languages Multi-row tabs: Appearance -> DocumentTabs Git labels on projects: Team -> Versioning -> Show Versioning Labels Internet Browser: General -> web browser Javadoc config: if the javadoc for java SE does not work out of the box, maybe there is a wrong URL. Go to Tools -> Java Platforms -> Javadoc and enter there the path where the Javadoc is accessible online Install the basic plugins Markdown","title":"Netbeans"},{"location":"Programming/Java/Java%20Workflow/#project-configuration","text":"","title":"Project configuration"},{"location":"Programming/Java/Java%20Workflow/#configure-maven-goals","text":"These can be configured in Project properties -> Actions","title":"Configure Maven Goals"},{"location":"Programming/Java/Java%20Workflow/#troubleshooting","text":"If there is a serious problem, one way to solve it can be to delete the Netbeans cache located in ~\\AppData\\Local\\NetBeans\\Cache .","title":"Troubleshooting"},{"location":"Programming/Java/Java%20Workflow/#idea","text":"","title":"Idea"},{"location":"Programming/Java/Java%20Workflow/#configuration","text":"","title":"Configuration"},{"location":"Programming/Java/Java%20Workflow/#settings-synchronization","text":"Log in into JetBrains Toolbox or to the App Click on the gear icon on the top-right and choose Sync Check all categories and click on pull settings from the cloud Resatart Ida to update all the settings More on Jetbrains","title":"Settings synchronization"},{"location":"Programming/Java/Java%20Workflow/#project-configuration_1","text":"The correct JDK has to be set up in various places: - compielr has to be at least target jdk, set it in: File -> Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Project bytecode version and Per-module bytecode version - language level should be the same as target jdk: File -> Project Structure... -> Modules -> Language Level","title":"Project configuration"},{"location":"Programming/Java/Java%20Workflow/#compilation","text":"Everything is compiled in the background automatically. However, if we need to compile manually using maven, e.g., to activate certain pluginns, we need to:","title":"Compilation"},{"location":"Programming/Java/Java%20Workflow/#set-java-version-for-project","text":"There are two options to set: - source version determines the least version of java that supports all language tools used in the source code. It is the least version that can be used to compile the source code. - target version determines the least version of java that can be used to run your java application. The target version can be higher than the source version, but not he other way around. most of the time, however, we use the same version for source and for target. Usually, these versions needs to be set: 1. in the project compilation tool (maven) to configure the project compilation 2. in the IDE project properties, to configure the compilation of individual files, which is executed in the background to report the compilation errors at real time. Sometimes, the Java version used for running Maven has to be also set because it cannot be lower then the Java version used for project compilation using Maven. SO explanation","title":"Set Java version for project"},{"location":"Programming/Java/Java%20Workflow/#setting-java-version-in-maven","text":"Since Java 9, both Java versions can be set at once with the release option: <properties> <maven.compiler.release>10</maven.compiler.release> </properties> or equivalently <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> <release>10</release> </configuration> </plugin> The old way is to use separate properties: <properties> <maven.compiler.target>10</maven.compiler.target> <maven.compiler.source>10</maven.compiler.source> </properties> Note that the Java version configured in the pom file cannot be greater then the Java version used to run the Maven.","title":"Setting Java version in Maven"},{"location":"Programming/Java/Java%20Workflow/#setting-java-version-in-netbeans","text":"Note that for maven projects, this is automatically set to match the properties in the pom file. For the Netbeans real-time compiler, the cross compilation does not make sense, so both source and target Java version is set in one place: 1. Right click on project -> Properties -> Sources 2. In the bottom, change the Source/Binary Format","title":"Setting Java version in Netbeans"},{"location":"Programming/Java/Java%20Workflow/#setting-java-version-used-for-executing-maven","text":"If the Maven is executed from command line, edit the JAVA_HOME system property. If the Maven is executed from Netbeans, edit Project Properties -> Build -> Compile -> Java Platform","title":"Setting Java version used for executing Maven"},{"location":"Programming/Java/Java%20Workflow/#enabling-preview-features","text":"The preview features can be enabled using the --enable-preview argument. In Maven, this has to be passed to the compiler plugin: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <enablePreview>true</enablePreview> </configuration> </plugin> </plugins> </build> For Maven compiler plugin older then version 3.10.1, use: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <compilerArgs> <arg>--enable-preview</arg> </compilerArgs> </configuration> </plugin> </plugins> </build> Unfortunatelly, it is also necessary to enable preview features in the IDE configuration: - In Netbeans, add --enable-preview to Project Properties -> Run -> VM options . - In IDEA: 1. add --enable-preview to Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Override compiler parameters per-module 2. add --enable-preview to run configuration -> Build and run vm options","title":"Enabling preview features"},{"location":"Programming/Java/Java%20Workflow/#gurobi","text":"Gurobi is a commercial project not contained in any public maven repositories. It is necessary to install the gurobi maven artifact manualy.","title":"Gurobi"},{"location":"Programming/Java/Java%20Workflow/#potential-problems","text":"unsatisfied linker error : Check if the gurobi version in the error log matches the gurobi version installed.","title":"Potential problems"},{"location":"Programming/Java/Java%20Workflow/#archive","text":"","title":"Archive"},{"location":"Programming/Java/Java%20Workflow/#aic-maven-repo-access","text":"To Access the AIC maven repo, copy maven settings from another computer (located in ~/.m2) Written with StackEdit .","title":"AIC maven repo access"},{"location":"Programming/Java/Maven/","text":"Dependencies \u00b6 All Maven dependencies should work out of the box. If some dependencies cannot be resolved: - check that the dependencies are on the maven central. - if not, check that they are in some special repo and check that the repo is present in the pom of the project that requires the dependency Compilation \u00b6 reference Compilation is handeled by the Maven compiler plugin. Usually, typing mvn compile is enough to compile the project. If you need any customization, add it to the compiler plugin configuration <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> .... </configuration> </plugin> Passing arguments to javac \u00b6 A single argument can be passed using the <compilerArgument> property. For more compiler arguments, use <compilerArgs> : <compilerArgs> <arg>-Xmaxerrs=1000</arg> <arg>-Xlint</arg> </compilerArgs> Tests \u00b6 Tests are usually executed with the Maven Surefire plugin using the test goal. Run a single tests file \u00b6 To run a single test file, use: mvn test -Dtest=\"<TEST CLASS NAME>\" The name should be just a class name without the package and without file extension: mvn test -Dtest=\"OSMFileTest\" We can also use a fully qualified name, if there are more test classes with the same name: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.OSMFileTest\" Run tests using a pattern \u00b6 More test can be run with a pattern, for example: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.**\" runs all tests within the input package. Execute Programs from Maven \u00b6 For executing programs, Maven has the exec target. Mostly, you need to exacute java programs from maven. Basic example: mvn exec:java -Dexec.mainClass=test.Main Other usefull arguments: - -Dexec.args=\"arg1 arg2 arg3\" If you want to pass the arguments like Xmx to jvm, you cannot use the java subgoal of the exec command. That is because with the java subgoal, Maven uses the same jvm it is running in to execute the program. To conigure jvm, we need to start a new instance. That is possible with the exec subgoal: mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"Xmx30g -classpath %classpath test.Main\" Note that here, the -Dexec.args parameter is used both for vm and runtime arguments. We can also use -Dexec.mainClass with exec:exec , but we need to refer it in the -classpath argument. The following three maven commands run the same Java program: mvn exec:java -Dexec.mainClass=test.Main mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"-classpath %classpath test.Main\" mvn exec:exec -Dexec.executable=\"java\" -Dexec.mainClass=test.Main -Dexec.args=\"-classpath %classpath ${exec.mainClass}\" HTTPS certificates \u00b6 Sometimes, it can happen that maven cannot connect to a repository with this error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target This error signals that the server SSL certificate of the maven repo (when using HTTPS) is not present in the local SSL certificate keystore. This can have two reasons, to disintiguish between them, try to access the repo from your browser over https: - If you can access the repo from your browser, it means that the server has a valid SSL certificate, but it is not in zour local keystore (just in the browser keystore). You can solve this problem by adding the certificate to your java SSL keystore (see below). - if you cannot access the server from your browser, it is likely that the server does not have a valid SSL certificate, and you have to solve it on the serer side. Adding a new SSL certificate to your keystore \u00b6 Open the repo URL in your browser Export the certificate: Chrome: click on the padlock icon left to address in address bar, select Certificate -> Details -> Copy to File and save in format \"Der-encoded binary, single certificate\". Firefox: click on HTTPS certificate chain (the lock icon right next to URL address). Click more info -> security -> show certificate -> details -> export.. . Pickup the name and choose file type *.cer Determine the keystore location: 3.1 Using maven --version , find out the location of the java used by Maven 3.2 The keystore is saved in file: <JAVA LOCATION>/lib/security/cacerts Open console as administrator and add the certificate to the keystore using: keytool -import -keystore \"<PATH TO cacerts>\" -file \"PATH TO TH EXPORTED *.cer FILE\" You can check that the operation was sucessful by listing all certificates: keytool -keystore \"<PATH TO cacerts>\" -list Debugging maven \u00b6 First, try to look at the versions of related dependencies and plugins. Old versions of these can cause many problems. No tests found using the Dtest argument of the test goal \u00b6 Check the class name/path/pattern If the name works, but pattern does not, it can be caused by an old version of the surefire plugin that use a different patten syntax. uncompilable source code \u00b6 Try to clean and compile again","title":"Maven"},{"location":"Programming/Java/Maven/#dependencies","text":"All Maven dependencies should work out of the box. If some dependencies cannot be resolved: - check that the dependencies are on the maven central. - if not, check that they are in some special repo and check that the repo is present in the pom of the project that requires the dependency","title":"Dependencies"},{"location":"Programming/Java/Maven/#compilation","text":"reference Compilation is handeled by the Maven compiler plugin. Usually, typing mvn compile is enough to compile the project. If you need any customization, add it to the compiler plugin configuration <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> .... </configuration> </plugin>","title":"Compilation"},{"location":"Programming/Java/Maven/#passing-arguments-to-javac","text":"A single argument can be passed using the <compilerArgument> property. For more compiler arguments, use <compilerArgs> : <compilerArgs> <arg>-Xmaxerrs=1000</arg> <arg>-Xlint</arg> </compilerArgs>","title":"Passing arguments to javac"},{"location":"Programming/Java/Maven/#tests","text":"Tests are usually executed with the Maven Surefire plugin using the test goal.","title":"Tests"},{"location":"Programming/Java/Maven/#run-a-single-tests-file","text":"To run a single test file, use: mvn test -Dtest=\"<TEST CLASS NAME>\" The name should be just a class name without the package and without file extension: mvn test -Dtest=\"OSMFileTest\" We can also use a fully qualified name, if there are more test classes with the same name: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.OSMFileTest\"","title":"Run a single tests file"},{"location":"Programming/Java/Maven/#run-tests-using-a-pattern","text":"More test can be run with a pattern, for example: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.**\" runs all tests within the input package.","title":"Run tests using a pattern"},{"location":"Programming/Java/Maven/#execute-programs-from-maven","text":"For executing programs, Maven has the exec target. Mostly, you need to exacute java programs from maven. Basic example: mvn exec:java -Dexec.mainClass=test.Main Other usefull arguments: - -Dexec.args=\"arg1 arg2 arg3\" If you want to pass the arguments like Xmx to jvm, you cannot use the java subgoal of the exec command. That is because with the java subgoal, Maven uses the same jvm it is running in to execute the program. To conigure jvm, we need to start a new instance. That is possible with the exec subgoal: mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"Xmx30g -classpath %classpath test.Main\" Note that here, the -Dexec.args parameter is used both for vm and runtime arguments. We can also use -Dexec.mainClass with exec:exec , but we need to refer it in the -classpath argument. The following three maven commands run the same Java program: mvn exec:java -Dexec.mainClass=test.Main mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"-classpath %classpath test.Main\" mvn exec:exec -Dexec.executable=\"java\" -Dexec.mainClass=test.Main -Dexec.args=\"-classpath %classpath ${exec.mainClass}\"","title":"Execute Programs from Maven"},{"location":"Programming/Java/Maven/#https-certificates","text":"Sometimes, it can happen that maven cannot connect to a repository with this error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target This error signals that the server SSL certificate of the maven repo (when using HTTPS) is not present in the local SSL certificate keystore. This can have two reasons, to disintiguish between them, try to access the repo from your browser over https: - If you can access the repo from your browser, it means that the server has a valid SSL certificate, but it is not in zour local keystore (just in the browser keystore). You can solve this problem by adding the certificate to your java SSL keystore (see below). - if you cannot access the server from your browser, it is likely that the server does not have a valid SSL certificate, and you have to solve it on the serer side.","title":"HTTPS certificates"},{"location":"Programming/Java/Maven/#adding-a-new-ssl-certificate-to-your-keystore","text":"Open the repo URL in your browser Export the certificate: Chrome: click on the padlock icon left to address in address bar, select Certificate -> Details -> Copy to File and save in format \"Der-encoded binary, single certificate\". Firefox: click on HTTPS certificate chain (the lock icon right next to URL address). Click more info -> security -> show certificate -> details -> export.. . Pickup the name and choose file type *.cer Determine the keystore location: 3.1 Using maven --version , find out the location of the java used by Maven 3.2 The keystore is saved in file: <JAVA LOCATION>/lib/security/cacerts Open console as administrator and add the certificate to the keystore using: keytool -import -keystore \"<PATH TO cacerts>\" -file \"PATH TO TH EXPORTED *.cer FILE\" You can check that the operation was sucessful by listing all certificates: keytool -keystore \"<PATH TO cacerts>\" -list","title":"Adding a new SSL certificate to your keystore"},{"location":"Programming/Java/Maven/#debugging-maven","text":"First, try to look at the versions of related dependencies and plugins. Old versions of these can cause many problems.","title":"Debugging maven"},{"location":"Programming/Java/Maven/#no-tests-found-using-the-dtest-argument-of-the-test-goal","text":"Check the class name/path/pattern If the name works, but pattern does not, it can be caused by an old version of the surefire plugin that use a different patten syntax.","title":"No tests found using the Dtest argument of the test goal"},{"location":"Programming/Java/Maven/#uncompilable-source-code","text":"Try to clean and compile again","title":"uncompilable  source code"},{"location":"Programming/Python/Matplotlib%20Manual/","text":"","title":"Matplotlib Manual"},{"location":"Programming/Python/Pandas%20Manual/","text":"Creating a DataFrame \u00b6 The DataFrame class has a constructor that supports multiple formats of input data as well as many configuration parameters. Therefore , for most formats of input data, we can create a dataframe using the constructor. However, we can also crete a dataframe using the from_* functions, and for some formats, these functions are the only way to create a dataframe. From a dictionary \u00b6 When having a dictionary, we can choose between two options the constructor and the from_dict function. The required syntax depend on the shape of the dictionary with respect to the required dataframe. Keys are column names, values are list of column values \u00b6 df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) # or equivalently df.DataFrame.from_dict({'col1': [1, 2], 'col2': [3, 4]}) Note that the values of the dictionary have to be lists. If we have a dictionary with values that are not lists (i.e., only one row), we have to use the orient parameter to specify the orientation of the data and then transpose the dataframe: d = {'col1': 1, 'col2': 2} df = pd.DataFrame.from_dict(d, orient='index').T # or equivalently df = pd.DataFrame([d], columns=d.keys()) Keys are indices, values are values of a single column \u00b6 df = pd.DataFrame.from_dict({'row1': 1, 'row2': 2}, orient='index', columns=['Values']) Keys are indices, values are values of single row \u00b6 df = pd.DataFrame.from_dict({'row1': [1, 2], 'row2': [3, 4]}, orient='index') Keys are one column, values are another column \u00b6 d = {'row1 col1': 'row1 col2', 'row2 col1': 'row2 col2' df = pd.DataFrame.from_dict(d.items()) # or equivalently df = pd.DataFrame({'col1': d.keys(), 'col2': d.values()}) From a list of dictionaries \u00b6 df = pd.DataFrame([{'col1': 1, 'col2': 3}, {'col1': 2, 'col2': 4}]) From a list of lists \u00b6 df = pd.DataFrame([[1, 3], [2, 4]], columns=['col1', 'col2']) From \u00b6 Obtaining info about dataset \u00b6 For a DataFrame df : - column names: df.columns - column types: df.dtypes - number of rows: len(df) Iteration \u00b6 Standard Iteration \u00b6 https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas Iteration without modifying the dataframe. From the fastest to the slowest. Vector operations \u00b6 List Comprehensions \u00b6 Apply \u00b6 itertuples() \u00b6 https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.htm Returns dataframe rows as pandas named tuples with index as the first member of the tuple. iterrows() \u00b6 https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html returns a tuple (index, data) it does not preserve the dtype items() \u00b6 https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.items.html Iterates over columns Iteration with modification \u00b6 When doing some modifications, we need to copy the dataframe and do the modifications on the copy. Filtration \u00b6 filtered = df[df['max_delay'] == x] or equivalently: filtered = df[df.max_delay == x] Filtration by Multiple Columns \u00b6 Example: filtered = df[(df['max_delay'] == x) & (df['exp_length'] == y)] Accept multiple values \u00b6 For that, we can use the isin function: filtered = df[df['max_delay'].isin([x, y])] Using the query function \u00b6 The query function can be used for more complicated filters. It is more flexible and the syntax is less verbose. The above filter can be rewriten as: filtered = df.query('max_delay == x and exp_length == y'] Filtering Series \u00b6 A seris can be filtered even simpler then the dataframe: s = df['col'] sf = s[s <= 10] # now we have a Series with values from df['col'] less than 10 Useful filter functions \u00b6 non null/nan values: <column selection>.notnull() filtring using the string value: <column selection>.str.<string function> Selection \u00b6 If we want to select a part of the dataframe (a set of rows and columns) independently of the values of the dataframe (for that, see filtration ), we can use these methods: - loc : select by label, works for both rows and columns - iloc : select by position, works for both rows and columns - [] : select by label, works only for columns There are also other methods that works for selection but does not work for setting values, such as: - xs : select by label, works for both rows and columns loc \u00b6 The operator loc has has many possible input parameters, the most common syntax is df.loc[<row selection>, <column selection>] each selection has the form of <start label>:<end label> . For the whole column, we therefore use: df.loc[:, <column name>] Difference between array operator on dataframe and on loc \u00b6 Both methods can be used both for getting and setting the column: a = df['col'] # or equivalently a = df.loc[:, 'col'] df2['col'] = a # or equivalently df2.loc[:, 'col'] = a The difference between these two methods is apparent when we want to use a chained selection, i.e., selecting from a selection. While the loc selects the appropriate columns in one step, so we know that we still refer to the original dataframe, the array operator operations are separate, and therefore, the result value can refer to a temporary: dfmi.loc[:, ('one', 'second')] = value # we set a value of a part of dfmi dfmi['one']['second'] = value # can be dangerous, we can set value to a temporary This problem is indicated by a SettingWithCopy warning. Sometimes it is not obvious that we use a chain of array operator selections, e.g.: sel = df[['a', 'b']] ..... sel['a'] = ... # we possibly edit a temporary! For more, see the dovumentation . iloc \u00b6 The iloc method works similarly to the loc method, but it uses the position instead of the label. Be aware that if the iloc operator selects by single value (e.g.: df.iloc[3] ), it returns the single row as series . To get a dataframe slice, we need to use a list of values (e.g.: df.iloc[[3]] ). Selecting all columns but one \u00b6 If we do not mind copying the dataframe, we can use the drop function. Otherwise, we can use the loc method and supply the filtered column lables obtained using the columns property: df.loc[:, df.columns != '<column to skip>'] Multi-index selection \u00b6 documentation When selecting from a dataframe with a multi-index, things get a bit more complicated. We can specify index levels using the level argument. Example: df.loc[<row selection>, <column selection>, level=<level number>] If we want to specify more than one level, we can use a tuple: df.loc[(<row index level 1>, <row index level 2>, ...), (<col index level 1>, <col index level 2>, ...)] If we select an upper level only, all lover level values are selected. For more complex cases where we wanto to select all from upper level but limit the lower level, we can use the slice function: df.loc[(slice(None), slice('15', '30')), ...] We can obtain the same result with a more readable syntax using the IndexSlice object: idx = pd.IndexSlice dft.loc[idx[:, '15':'30'], ...] Also, note that for multi-index slicing, the index needs to be sorted. If it is not, we can use the sort_index function. Sorting \u00b6 for sorting the dataframe, we can use the sort_values function. The first argument is the list of columns to sort by, starting with the most important column. Example: df.sort_values(['col1', 'col2']) If we want to use a custom sorting function, we can use the key argument. The key function should satisfy the classical python sorting interface (see Python manual) and additionaly, it should be a vector function, i.e., instead of returning a single position for a given value, it should return a vector of positions for a given vector of values. Example key function: def key_fn(l: list): return [len(x) for x in l] Working with columns \u00b6 Adding a column \u00b6 The preferable way is to use the assign function: # adds a column named 'instance_length' with constant value result_df_5_nyc_mv.assign(instance_length = 5) Multiple columns can be added at once: trips = trips.assign(dropoff_datetime = 0, dropoff_GPS_lon = 0, dropoff_GPS_lat = 0, pickup_GPS_lon = 0, pickup_GPS_lat = 0) Rename a column \u00b6 To rename a column, we can use the pandas rename function: df.rename(columns={'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}) # or equivalently df.rename({'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}, axis='columns') Working with the index \u00b6 Index of a dataframe df can be accessed by df.index . Standard range operation can be applied to index. Changing the index \u00b6 For that, we can use the set_index function. Renaming the index \u00b6 The Index.rename function can be used for that. Aggregation \u00b6 Analogously to SQL, pandas has a groupby function for aggreagting rows. The usage is as follows: group = df.groupby(<columns>) # returns a groupby object grouped by the columns sel = group[<columns>] # we can select only some columns from the groupby object agg = sel.<aggregation function> # we apply an aggregation function to the selected columns we can skip the sel step and apply the aggregation function directly to the groupby object. This way, the aggregation function is applied to all columns. For the aggregate function, we can use one of the prepared aggregation functions, for example: - sum - mean - median - min - max - count Full example (sum): df.groupby('col').sum() Sums the results for each group (column by column) To get a count, we can call the size function: df.groupby('col').size() Custom aggegate function \u00b6 Also, there is a general agg function that can be used to apply a custom aggregation function. Example: df.groupby('col').agg({'col1': 'sum', 'col2': 'mean'}) We can also use the apply method. This function takes a dataframe as an argument and returns a series. Example: def agg_fn(df): return pd.Series([df['col1'].sum(), df['col2'].mean()], index=['sum', 'mean']) The difference between agg and apply is summarized in the following table: property agg apply applied to each specified column whole dataframe output dataframe series can use multiple aggregate functions yes no can be applied to dataframe no yes Joins \u00b6 Appending one dataframe to another \u00b6 We can use the concat function for that: pd.concat([df1, df2]) I/O \u00b6 csv \u00b6 For reading csv files, we can use the read_csv function. Important params: - sep : separator - header : row number to use as column names. If None , no header is used - skiprows : number of rows to skip from the beginning - delim_whitespace : if True , the whitespace is used as a separator. The sep parameter is ignored in that case. This is a way how to read a file with variable number of whitespaces between columns. For export, we can use the to_csv method for that: df.to_csv(<file name> [, <other params>]) Insert dataframe into db \u00b6 We can use the to_sql method for that: df.to_sql(<table name>, <sql alchemy engine> [, <other params>]) Important params: - to append, not replace existing records: if_exists='append' - do not import dataframe index: index=False For larger datasets, it is important to not insert everything at once, while also tracking the progress. The following code does exactly that def chunker(seq, size): return (seq[pos:pos + size] for pos in range(0, len(seq), size)) chunksize = int(len(data) / 1000) # 0.1% with tqdm(total=len(data)) as pbar: for i, cdf in enumerate(chunker(data, chunksize)): cdf.to_sql(<table name>, <sqlalchemy_engine>) pbar.update(chunksize) If the speed is slow, it can be caused by a low upload speed of your internet connection. Note that due to the SQL syntax, the size of the SQL strings may be much larger than the size of the dataframe. Latex export \u00b6 Currently, the to_latex function is deprecated. The Styler class should be used for latex exports instead. You can get the Styler from the DataFrame using the style property. The usual workfolow is: 1. Create a Styler object from the dataframe using the style property. 1. Apply the desired formatting to the styler object. 1. Export DataFrame to latex using the to_latex method. Keep in mind that the Styler object is immutable, so you need to assign the result of each formatting operation to a new variable or chain the calls . Example: # wrong, the format is not apllied df.style.format(...) df.style.to_latex(...) # correct: temp var s = df.style.format(...) s.to_latex(...) # correct: chain calls df.style.format(...).to_latex(...) Formatting the index: columns and row labels \u00b6 The columns' and row labels' format is configures by the format_index function. Important parameters: - axis : 0 for rows, 1 for columns (cannot be both) - escape : by default, the index is not escaped, to do so, we need to set escape to 'latex' . Formatting or changing the values \u00b6 The values are formated by the format function. Important parameters: - escape : by default, the values are not escaped, to do so, we need to set escape to 'latex' . - na_rep : the string to use for missing values - precision : the number of decimal places to use for floats Replacing values \u00b6 For replace some values for the presentation with something else, we can also use the format function.For example, to change the boolean presentation in column col we call: df.style.format({'col': lambda x: 'yes' if x else 'no'}) Hihglighting min/max values \u00b6 For highlighting the min/max values, we can use the highlight_min and highlight_max functions. Important parameters: - subset : the columns in which the highlighting should be applied - props : the css properties to apply to the highlighted cells Hiding some columns, rows, or indices \u00b6 For hiding some columns, rows, or indices, we can use the hide function. Format: df.style.hide(<index name>) # hide the index with the given name By default, the <index_name> refers to the row index. To hide a column : df.style.hide_columns(<column name>, axis=1) To hide row index: df.style.hide(axis='index') Changing the header (column labels) \u00b6 There is no equivalent to the header parameter of the old to_latex function in the new style system. Instead, it is necessary to change the column names of the dataframe. Exporting to latex \u00b6 For the export, we use the to_latex function. Important parameters: - convert_css : if True , the css properties are converted to latex commands - multirow_align : the alignment of the multirow cells. Options are t , c , b - hrules : if set to True , the horizontal lines are added to the table, specifically to the top, bottom, and between the header and the body. Note that these hrules are realized as the \\toprule , \\midrule , and \\bottomrule commands from the booktabs package, so the package has to be imported . - clines : configuration for hlines between rows. It is a string composed of two parts divided by ; (e.g.: skip-last;data ). The parts are: - whether to skip last row or not ( skip-last or all ) - whether to draw the lines between indices or the whole rows ( index or data ) Displaying the dataframe in console \u00b6 We can display the dataframe in the conslo print or int the log just by supplying the dataframe as an argument because it implements the __repr__ method. Sometimes, however, the default display parameters are not sufficient. In that case, we can use the set_option function to change the display parameters: pd.set_option('display.max_rows', 1000) Important parameters: - display.max_rows : the maximum number of rows to display - display.max_columns : the maximum number of columns to display - display.max_colwidth : the maximum width of a column Other useful functions \u00b6 drop_duplicates to quickly drop duplicate rows based on a subset of columns. factorize to encode a series values as a categorical variable, i.e., assigns a different number to each unique value in series. pivot_table : function that can aggragate and transform a dataframe in one step. with this function, one can create a pivot table, but also a lot more. cut : function that can be used to discretize a continuous variable into bins. pivot_table \u00b6 The pivot table (mega)function do a lot of things at once: - it aggregates the data - it transforms the data - it sorts the data due to reindexing Although this function is very powerfall there are also many pitfalls. The most important ones are: - column data type change for columns with missing values Column data type change for columns with missing values \u00b6 The tranformation often creates row-column combinations that do not exist in the original data. These are filled with NaN values. But some data types does not support NaN values, and in conclusion, the data type of the columns with missing values is changed to float . Possible solutions: - we can use the fill_value parameter to fill the missing values with some value that is supported by the data type (e.g. -1 for integers) - we can use the dropna parameter to drop the rows with missing values - we can change the data type of the columns with missing values prior to calling the pivot_table function. For example, the pandas integer data types support NaN values. Geopandas \u00b6 Geopandas is a GIS addon to pandas, an equivalent to PostGIS. Unfortunately, it currently supports only one geometry column per table. Do not ever copy paste the geometries from jupyter notebook as the coordinates are rounded! Use the to_wkt function instead. Create a geodataframe from CSV \u00b6 Geopandas has it's own read_csv function, however, it requires a very specific csv format, so it is usually easier to first import csv to pandas and then create geopandas dataframe from pandas dataframe. Converting pandas Dataframe to geopandas Dataframe \u00b6 The geopandas dataframe constructor accepts pandas dataframes, we just need to specify the geometry column and the coordinate system: gdf = gpd.GeoDataFrame( <PANDAS DATAFRAME> geometry=gpd.points_from_xy(<X COLUMN>, <Y COLUMN>), crs=<SRID> ) Create geodataframe from shapely \u00b6 To load data from shapely, execute gdf = gpd.read_file(<PATH TO FOLDER WITH SHAPEFILES>) Working with the geometry \u00b6 The geometry can be accessed using the geometry property of the geodataframe. Spliting multi-geometry columns \u00b6 If the geometry column contains multi-geometries, we can split them into separate rows using the explode function: gdf = gdf.explode() Insert geodataframe into db \u00b6 preprocesssing \u00b6 Before inserting a geodataframe into the database, we need to process it a little bit: 1. set the SRID: gdf.set_crs(epsg=<SRID>, allow_override=True, inplace=True) 2. set the geometry: gdf.set_geometry('geom', inplace=True) 3. select, rename, or add columns so that the resulting geodataframe match the corresponding database table. This process is same as when working with pandas Simple insertion \u00b6 When the data are in the correct format and we don|t need any customization for the db query, we can use the to_postgis method: gdf.to_postgis(<TABLE NAME>, <SQL ALCHEMY CONNECTION>, if_exists='append') Customized Insertion: geoalchemy \u00b6 If we need some special insert statement, we cannot rely on the geodataframe.to_postgis function, as it is not flexible enough. The pandas dataframe.to_sql function is more flexible, however, it has trouble when working with geodata. The easiest options is therefore to use geoalchemy , the database wraper used in geopandas (extension of sqlalchemy , which is a database wrapper for pandas ). First, we need to create the insert statement. The example here uses a modification for handeling duplicite elements. meta = sqlalchemy.MetaData() # create a collection for geoalchemy database # objects table = geoalchemy2.Table( '<TABLE NAME>', meta, autoload_with=<SQL ALCHEMY CONNECTION>) insert_statement = sqlalchemy.dialects.postgresql.insert(table).on_conflict_do_nothing() In the above example, we create a geoalchemy representation of a table and then we use this representation to create a customized insert statement (the on_conflict_do_nothing is the speciality here.). Note that we use a speciatl PostgreSQL insert statement instead of the standard SQLAlchemy insert statement. Second, we need to prepare the data as a list of dictionary entries: list_to_insert = [ {'id': 0, 'geom': <GEOM>, ...}, {'id': 0, 'geom': <GEOM>, ...}, .... ] Note that the geometry in the geodataframe is in the shapely format. Therefore, we need to convert it to string using the geoalchemy from_shape function: geoalchemy2.shape.from_shape(<GEOMETRY>, srid=<SRID>) Finally, we can execute the query using an sqlalchemy connection: sqlalchemy_connection.execute(insert_statement, list_to_insert)","title":"Pandas Manual"},{"location":"Programming/Python/Pandas%20Manual/#creating-a-dataframe","text":"The DataFrame class has a constructor that supports multiple formats of input data as well as many configuration parameters. Therefore , for most formats of input data, we can create a dataframe using the constructor. However, we can also crete a dataframe using the from_* functions, and for some formats, these functions are the only way to create a dataframe.","title":"Creating a DataFrame"},{"location":"Programming/Python/Pandas%20Manual/#from-a-dictionary","text":"When having a dictionary, we can choose between two options the constructor and the from_dict function. The required syntax depend on the shape of the dictionary with respect to the required dataframe.","title":"From a dictionary"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-column-names-values-are-list-of-column-values","text":"df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) # or equivalently df.DataFrame.from_dict({'col1': [1, 2], 'col2': [3, 4]}) Note that the values of the dictionary have to be lists. If we have a dictionary with values that are not lists (i.e., only one row), we have to use the orient parameter to specify the orientation of the data and then transpose the dataframe: d = {'col1': 1, 'col2': 2} df = pd.DataFrame.from_dict(d, orient='index').T # or equivalently df = pd.DataFrame([d], columns=d.keys())","title":"Keys are column names, values are list of column values"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-indices-values-are-values-of-a-single-column","text":"df = pd.DataFrame.from_dict({'row1': 1, 'row2': 2}, orient='index', columns=['Values'])","title":"Keys are indices, values are values of a single column"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-indices-values-are-values-of-single-row","text":"df = pd.DataFrame.from_dict({'row1': [1, 2], 'row2': [3, 4]}, orient='index')","title":"Keys are indices, values are values of single row"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-one-column-values-are-another-column","text":"d = {'row1 col1': 'row1 col2', 'row2 col1': 'row2 col2' df = pd.DataFrame.from_dict(d.items()) # or equivalently df = pd.DataFrame({'col1': d.keys(), 'col2': d.values()})","title":"Keys are one column, values are another column"},{"location":"Programming/Python/Pandas%20Manual/#from-a-list-of-dictionaries","text":"df = pd.DataFrame([{'col1': 1, 'col2': 3}, {'col1': 2, 'col2': 4}])","title":"From a list of dictionaries"},{"location":"Programming/Python/Pandas%20Manual/#from-a-list-of-lists","text":"df = pd.DataFrame([[1, 3], [2, 4]], columns=['col1', 'col2'])","title":"From a list of lists"},{"location":"Programming/Python/Pandas%20Manual/#from","text":"","title":"From"},{"location":"Programming/Python/Pandas%20Manual/#obtaining-info-about-dataset","text":"For a DataFrame df : - column names: df.columns - column types: df.dtypes - number of rows: len(df)","title":"Obtaining info about dataset"},{"location":"Programming/Python/Pandas%20Manual/#iteration","text":"","title":"Iteration"},{"location":"Programming/Python/Pandas%20Manual/#standard-iteration","text":"https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas Iteration without modifying the dataframe. From the fastest to the slowest.","title":"Standard Iteration"},{"location":"Programming/Python/Pandas%20Manual/#vector-operations","text":"","title":"Vector operations"},{"location":"Programming/Python/Pandas%20Manual/#list-comprehensions","text":"","title":"List Comprehensions"},{"location":"Programming/Python/Pandas%20Manual/#apply","text":"","title":"Apply"},{"location":"Programming/Python/Pandas%20Manual/#itertuples","text":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.htm Returns dataframe rows as pandas named tuples with index as the first member of the tuple.","title":"itertuples()"},{"location":"Programming/Python/Pandas%20Manual/#iterrows","text":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html returns a tuple (index, data) it does not preserve the dtype","title":"iterrows()"},{"location":"Programming/Python/Pandas%20Manual/#items","text":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.items.html Iterates over columns","title":"items()"},{"location":"Programming/Python/Pandas%20Manual/#iteration-with-modification","text":"When doing some modifications, we need to copy the dataframe and do the modifications on the copy.","title":"Iteration with modification"},{"location":"Programming/Python/Pandas%20Manual/#filtration","text":"filtered = df[df['max_delay'] == x] or equivalently: filtered = df[df.max_delay == x]","title":"Filtration"},{"location":"Programming/Python/Pandas%20Manual/#filtration-by-multiple-columns","text":"Example: filtered = df[(df['max_delay'] == x) & (df['exp_length'] == y)]","title":"Filtration by Multiple Columns"},{"location":"Programming/Python/Pandas%20Manual/#accept-multiple-values","text":"For that, we can use the isin function: filtered = df[df['max_delay'].isin([x, y])]","title":"Accept multiple values"},{"location":"Programming/Python/Pandas%20Manual/#using-the-query-function","text":"The query function can be used for more complicated filters. It is more flexible and the syntax is less verbose. The above filter can be rewriten as: filtered = df.query('max_delay == x and exp_length == y']","title":"Using the query function"},{"location":"Programming/Python/Pandas%20Manual/#filtering-series","text":"A seris can be filtered even simpler then the dataframe: s = df['col'] sf = s[s <= 10] # now we have a Series with values from df['col'] less than 10","title":"Filtering Series"},{"location":"Programming/Python/Pandas%20Manual/#useful-filter-functions","text":"non null/nan values: <column selection>.notnull() filtring using the string value: <column selection>.str.<string function>","title":"Useful filter functions"},{"location":"Programming/Python/Pandas%20Manual/#selection","text":"If we want to select a part of the dataframe (a set of rows and columns) independently of the values of the dataframe (for that, see filtration ), we can use these methods: - loc : select by label, works for both rows and columns - iloc : select by position, works for both rows and columns - [] : select by label, works only for columns There are also other methods that works for selection but does not work for setting values, such as: - xs : select by label, works for both rows and columns","title":"Selection"},{"location":"Programming/Python/Pandas%20Manual/#loc","text":"The operator loc has has many possible input parameters, the most common syntax is df.loc[<row selection>, <column selection>] each selection has the form of <start label>:<end label> . For the whole column, we therefore use: df.loc[:, <column name>]","title":"loc"},{"location":"Programming/Python/Pandas%20Manual/#difference-between-array-operator-on-dataframe-and-on-loc","text":"Both methods can be used both for getting and setting the column: a = df['col'] # or equivalently a = df.loc[:, 'col'] df2['col'] = a # or equivalently df2.loc[:, 'col'] = a The difference between these two methods is apparent when we want to use a chained selection, i.e., selecting from a selection. While the loc selects the appropriate columns in one step, so we know that we still refer to the original dataframe, the array operator operations are separate, and therefore, the result value can refer to a temporary: dfmi.loc[:, ('one', 'second')] = value # we set a value of a part of dfmi dfmi['one']['second'] = value # can be dangerous, we can set value to a temporary This problem is indicated by a SettingWithCopy warning. Sometimes it is not obvious that we use a chain of array operator selections, e.g.: sel = df[['a', 'b']] ..... sel['a'] = ... # we possibly edit a temporary! For more, see the dovumentation .","title":"Difference between array operator on dataframe and on loc"},{"location":"Programming/Python/Pandas%20Manual/#iloc","text":"The iloc method works similarly to the loc method, but it uses the position instead of the label. Be aware that if the iloc operator selects by single value (e.g.: df.iloc[3] ), it returns the single row as series . To get a dataframe slice, we need to use a list of values (e.g.: df.iloc[[3]] ).","title":"iloc"},{"location":"Programming/Python/Pandas%20Manual/#selecting-all-columns-but-one","text":"If we do not mind copying the dataframe, we can use the drop function. Otherwise, we can use the loc method and supply the filtered column lables obtained using the columns property: df.loc[:, df.columns != '<column to skip>']","title":"Selecting all columns but one"},{"location":"Programming/Python/Pandas%20Manual/#multi-index-selection","text":"documentation When selecting from a dataframe with a multi-index, things get a bit more complicated. We can specify index levels using the level argument. Example: df.loc[<row selection>, <column selection>, level=<level number>] If we want to specify more than one level, we can use a tuple: df.loc[(<row index level 1>, <row index level 2>, ...), (<col index level 1>, <col index level 2>, ...)] If we select an upper level only, all lover level values are selected. For more complex cases where we wanto to select all from upper level but limit the lower level, we can use the slice function: df.loc[(slice(None), slice('15', '30')), ...] We can obtain the same result with a more readable syntax using the IndexSlice object: idx = pd.IndexSlice dft.loc[idx[:, '15':'30'], ...] Also, note that for multi-index slicing, the index needs to be sorted. If it is not, we can use the sort_index function.","title":"Multi-index selection"},{"location":"Programming/Python/Pandas%20Manual/#sorting","text":"for sorting the dataframe, we can use the sort_values function. The first argument is the list of columns to sort by, starting with the most important column. Example: df.sort_values(['col1', 'col2']) If we want to use a custom sorting function, we can use the key argument. The key function should satisfy the classical python sorting interface (see Python manual) and additionaly, it should be a vector function, i.e., instead of returning a single position for a given value, it should return a vector of positions for a given vector of values. Example key function: def key_fn(l: list): return [len(x) for x in l]","title":"Sorting"},{"location":"Programming/Python/Pandas%20Manual/#working-with-columns","text":"","title":"Working with columns"},{"location":"Programming/Python/Pandas%20Manual/#adding-a-column","text":"The preferable way is to use the assign function: # adds a column named 'instance_length' with constant value result_df_5_nyc_mv.assign(instance_length = 5) Multiple columns can be added at once: trips = trips.assign(dropoff_datetime = 0, dropoff_GPS_lon = 0, dropoff_GPS_lat = 0, pickup_GPS_lon = 0, pickup_GPS_lat = 0)","title":"Adding a column"},{"location":"Programming/Python/Pandas%20Manual/#rename-a-column","text":"To rename a column, we can use the pandas rename function: df.rename(columns={'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}) # or equivalently df.rename({'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}, axis='columns')","title":"Rename a column"},{"location":"Programming/Python/Pandas%20Manual/#working-with-the-index","text":"Index of a dataframe df can be accessed by df.index . Standard range operation can be applied to index.","title":"Working with the index"},{"location":"Programming/Python/Pandas%20Manual/#changing-the-index","text":"For that, we can use the set_index function.","title":"Changing the index"},{"location":"Programming/Python/Pandas%20Manual/#renaming-the-index","text":"The Index.rename function can be used for that.","title":"Renaming the index"},{"location":"Programming/Python/Pandas%20Manual/#aggregation","text":"Analogously to SQL, pandas has a groupby function for aggreagting rows. The usage is as follows: group = df.groupby(<columns>) # returns a groupby object grouped by the columns sel = group[<columns>] # we can select only some columns from the groupby object agg = sel.<aggregation function> # we apply an aggregation function to the selected columns we can skip the sel step and apply the aggregation function directly to the groupby object. This way, the aggregation function is applied to all columns. For the aggregate function, we can use one of the prepared aggregation functions, for example: - sum - mean - median - min - max - count Full example (sum): df.groupby('col').sum() Sums the results for each group (column by column) To get a count, we can call the size function: df.groupby('col').size()","title":"Aggregation"},{"location":"Programming/Python/Pandas%20Manual/#custom-aggegate-function","text":"Also, there is a general agg function that can be used to apply a custom aggregation function. Example: df.groupby('col').agg({'col1': 'sum', 'col2': 'mean'}) We can also use the apply method. This function takes a dataframe as an argument and returns a series. Example: def agg_fn(df): return pd.Series([df['col1'].sum(), df['col2'].mean()], index=['sum', 'mean']) The difference between agg and apply is summarized in the following table: property agg apply applied to each specified column whole dataframe output dataframe series can use multiple aggregate functions yes no can be applied to dataframe no yes","title":"Custom aggegate function"},{"location":"Programming/Python/Pandas%20Manual/#joins","text":"","title":"Joins"},{"location":"Programming/Python/Pandas%20Manual/#appending-one-dataframe-to-another","text":"We can use the concat function for that: pd.concat([df1, df2])","title":"Appending one dataframe to another"},{"location":"Programming/Python/Pandas%20Manual/#io","text":"","title":"I/O"},{"location":"Programming/Python/Pandas%20Manual/#csv","text":"For reading csv files, we can use the read_csv function. Important params: - sep : separator - header : row number to use as column names. If None , no header is used - skiprows : number of rows to skip from the beginning - delim_whitespace : if True , the whitespace is used as a separator. The sep parameter is ignored in that case. This is a way how to read a file with variable number of whitespaces between columns. For export, we can use the to_csv method for that: df.to_csv(<file name> [, <other params>])","title":"csv"},{"location":"Programming/Python/Pandas%20Manual/#insert-dataframe-into-db","text":"We can use the to_sql method for that: df.to_sql(<table name>, <sql alchemy engine> [, <other params>]) Important params: - to append, not replace existing records: if_exists='append' - do not import dataframe index: index=False For larger datasets, it is important to not insert everything at once, while also tracking the progress. The following code does exactly that def chunker(seq, size): return (seq[pos:pos + size] for pos in range(0, len(seq), size)) chunksize = int(len(data) / 1000) # 0.1% with tqdm(total=len(data)) as pbar: for i, cdf in enumerate(chunker(data, chunksize)): cdf.to_sql(<table name>, <sqlalchemy_engine>) pbar.update(chunksize) If the speed is slow, it can be caused by a low upload speed of your internet connection. Note that due to the SQL syntax, the size of the SQL strings may be much larger than the size of the dataframe.","title":"Insert dataframe into db"},{"location":"Programming/Python/Pandas%20Manual/#latex-export","text":"Currently, the to_latex function is deprecated. The Styler class should be used for latex exports instead. You can get the Styler from the DataFrame using the style property. The usual workfolow is: 1. Create a Styler object from the dataframe using the style property. 1. Apply the desired formatting to the styler object. 1. Export DataFrame to latex using the to_latex method. Keep in mind that the Styler object is immutable, so you need to assign the result of each formatting operation to a new variable or chain the calls . Example: # wrong, the format is not apllied df.style.format(...) df.style.to_latex(...) # correct: temp var s = df.style.format(...) s.to_latex(...) # correct: chain calls df.style.format(...).to_latex(...)","title":"Latex export"},{"location":"Programming/Python/Pandas%20Manual/#formatting-the-index-columns-and-row-labels","text":"The columns' and row labels' format is configures by the format_index function. Important parameters: - axis : 0 for rows, 1 for columns (cannot be both) - escape : by default, the index is not escaped, to do so, we need to set escape to 'latex' .","title":"Formatting the index: columns and row labels"},{"location":"Programming/Python/Pandas%20Manual/#formatting-or-changing-the-values","text":"The values are formated by the format function. Important parameters: - escape : by default, the values are not escaped, to do so, we need to set escape to 'latex' . - na_rep : the string to use for missing values - precision : the number of decimal places to use for floats","title":"Formatting or changing the values"},{"location":"Programming/Python/Pandas%20Manual/#replacing-values","text":"For replace some values for the presentation with something else, we can also use the format function.For example, to change the boolean presentation in column col we call: df.style.format({'col': lambda x: 'yes' if x else 'no'})","title":"Replacing values"},{"location":"Programming/Python/Pandas%20Manual/#hihglighting-minmax-values","text":"For highlighting the min/max values, we can use the highlight_min and highlight_max functions. Important parameters: - subset : the columns in which the highlighting should be applied - props : the css properties to apply to the highlighted cells","title":"Hihglighting min/max values"},{"location":"Programming/Python/Pandas%20Manual/#hiding-some-columns-rows-or-indices","text":"For hiding some columns, rows, or indices, we can use the hide function. Format: df.style.hide(<index name>) # hide the index with the given name By default, the <index_name> refers to the row index. To hide a column : df.style.hide_columns(<column name>, axis=1) To hide row index: df.style.hide(axis='index')","title":"Hiding some columns, rows, or indices"},{"location":"Programming/Python/Pandas%20Manual/#changing-the-header-column-labels","text":"There is no equivalent to the header parameter of the old to_latex function in the new style system. Instead, it is necessary to change the column names of the dataframe.","title":"Changing the header (column labels)"},{"location":"Programming/Python/Pandas%20Manual/#exporting-to-latex","text":"For the export, we use the to_latex function. Important parameters: - convert_css : if True , the css properties are converted to latex commands - multirow_align : the alignment of the multirow cells. Options are t , c , b - hrules : if set to True , the horizontal lines are added to the table, specifically to the top, bottom, and between the header and the body. Note that these hrules are realized as the \\toprule , \\midrule , and \\bottomrule commands from the booktabs package, so the package has to be imported . - clines : configuration for hlines between rows. It is a string composed of two parts divided by ; (e.g.: skip-last;data ). The parts are: - whether to skip last row or not ( skip-last or all ) - whether to draw the lines between indices or the whole rows ( index or data )","title":"Exporting to latex"},{"location":"Programming/Python/Pandas%20Manual/#displaying-the-dataframe-in-console","text":"We can display the dataframe in the conslo print or int the log just by supplying the dataframe as an argument because it implements the __repr__ method. Sometimes, however, the default display parameters are not sufficient. In that case, we can use the set_option function to change the display parameters: pd.set_option('display.max_rows', 1000) Important parameters: - display.max_rows : the maximum number of rows to display - display.max_columns : the maximum number of columns to display - display.max_colwidth : the maximum width of a column","title":"Displaying the dataframe in console"},{"location":"Programming/Python/Pandas%20Manual/#other-useful-functions","text":"drop_duplicates to quickly drop duplicate rows based on a subset of columns. factorize to encode a series values as a categorical variable, i.e., assigns a different number to each unique value in series. pivot_table : function that can aggragate and transform a dataframe in one step. with this function, one can create a pivot table, but also a lot more. cut : function that can be used to discretize a continuous variable into bins.","title":"Other useful functions"},{"location":"Programming/Python/Pandas%20Manual/#pivot_table","text":"The pivot table (mega)function do a lot of things at once: - it aggregates the data - it transforms the data - it sorts the data due to reindexing Although this function is very powerfall there are also many pitfalls. The most important ones are: - column data type change for columns with missing values","title":"pivot_table"},{"location":"Programming/Python/Pandas%20Manual/#column-data-type-change-for-columns-with-missing-values","text":"The tranformation often creates row-column combinations that do not exist in the original data. These are filled with NaN values. But some data types does not support NaN values, and in conclusion, the data type of the columns with missing values is changed to float . Possible solutions: - we can use the fill_value parameter to fill the missing values with some value that is supported by the data type (e.g. -1 for integers) - we can use the dropna parameter to drop the rows with missing values - we can change the data type of the columns with missing values prior to calling the pivot_table function. For example, the pandas integer data types support NaN values.","title":"Column data type change for columns with missing values"},{"location":"Programming/Python/Pandas%20Manual/#geopandas","text":"Geopandas is a GIS addon to pandas, an equivalent to PostGIS. Unfortunately, it currently supports only one geometry column per table. Do not ever copy paste the geometries from jupyter notebook as the coordinates are rounded! Use the to_wkt function instead.","title":"Geopandas"},{"location":"Programming/Python/Pandas%20Manual/#create-a-geodataframe-from-csv","text":"Geopandas has it's own read_csv function, however, it requires a very specific csv format, so it is usually easier to first import csv to pandas and then create geopandas dataframe from pandas dataframe.","title":"Create a geodataframe from CSV"},{"location":"Programming/Python/Pandas%20Manual/#converting-pandas-dataframe-to-geopandas-dataframe","text":"The geopandas dataframe constructor accepts pandas dataframes, we just need to specify the geometry column and the coordinate system: gdf = gpd.GeoDataFrame( <PANDAS DATAFRAME> geometry=gpd.points_from_xy(<X COLUMN>, <Y COLUMN>), crs=<SRID> )","title":"Converting pandas Dataframe to geopandas Dataframe"},{"location":"Programming/Python/Pandas%20Manual/#create-geodataframe-from-shapely","text":"To load data from shapely, execute gdf = gpd.read_file(<PATH TO FOLDER WITH SHAPEFILES>)","title":"Create geodataframe from shapely"},{"location":"Programming/Python/Pandas%20Manual/#working-with-the-geometry","text":"The geometry can be accessed using the geometry property of the geodataframe.","title":"Working with the geometry"},{"location":"Programming/Python/Pandas%20Manual/#spliting-multi-geometry-columns","text":"If the geometry column contains multi-geometries, we can split them into separate rows using the explode function: gdf = gdf.explode()","title":"Spliting multi-geometry columns"},{"location":"Programming/Python/Pandas%20Manual/#insert-geodataframe-into-db","text":"","title":"Insert geodataframe into db"},{"location":"Programming/Python/Pandas%20Manual/#preprocesssing","text":"Before inserting a geodataframe into the database, we need to process it a little bit: 1. set the SRID: gdf.set_crs(epsg=<SRID>, allow_override=True, inplace=True) 2. set the geometry: gdf.set_geometry('geom', inplace=True) 3. select, rename, or add columns so that the resulting geodataframe match the corresponding database table. This process is same as when working with pandas","title":"preprocesssing"},{"location":"Programming/Python/Pandas%20Manual/#simple-insertion","text":"When the data are in the correct format and we don|t need any customization for the db query, we can use the to_postgis method: gdf.to_postgis(<TABLE NAME>, <SQL ALCHEMY CONNECTION>, if_exists='append')","title":"Simple insertion"},{"location":"Programming/Python/Pandas%20Manual/#customized-insertion-geoalchemy","text":"If we need some special insert statement, we cannot rely on the geodataframe.to_postgis function, as it is not flexible enough. The pandas dataframe.to_sql function is more flexible, however, it has trouble when working with geodata. The easiest options is therefore to use geoalchemy , the database wraper used in geopandas (extension of sqlalchemy , which is a database wrapper for pandas ). First, we need to create the insert statement. The example here uses a modification for handeling duplicite elements. meta = sqlalchemy.MetaData() # create a collection for geoalchemy database # objects table = geoalchemy2.Table( '<TABLE NAME>', meta, autoload_with=<SQL ALCHEMY CONNECTION>) insert_statement = sqlalchemy.dialects.postgresql.insert(table).on_conflict_do_nothing() In the above example, we create a geoalchemy representation of a table and then we use this representation to create a customized insert statement (the on_conflict_do_nothing is the speciality here.). Note that we use a speciatl PostgreSQL insert statement instead of the standard SQLAlchemy insert statement. Second, we need to prepare the data as a list of dictionary entries: list_to_insert = [ {'id': 0, 'geom': <GEOM>, ...}, {'id': 0, 'geom': <GEOM>, ...}, .... ] Note that the geometry in the geodataframe is in the shapely format. Therefore, we need to convert it to string using the geoalchemy from_shape function: geoalchemy2.shape.from_shape(<GEOMETRY>, srid=<SRID>) Finally, we can execute the query using an sqlalchemy connection: sqlalchemy_connection.execute(insert_statement, list_to_insert)","title":"Customized Insertion: geoalchemy"},{"location":"Programming/Python/Plotly%20Manual/","text":"In plotly, we have two options: - plot quickly with plotly express - full control using graph objects Note that thes options can hardly be mixed. For example, we cannot use plotly express to create a figure and then add a subplot to it. Similarly, we cannot use the make_subplots function to create a figure and then add a plotly express plot to it. In general, it is easier to use plotly express, so we should use it if we are not affected by its limitations. The plotly express cannot - create custom subplots . Howwever, automatic \"facet\" subplots (same plot divided between multiple plots using some data attribute) are possible. Plotly Express \u00b6 documentation Plotly express modul is loaded as: import plotly.express as px Common Parameters For All Types of Plots \u00b6 data_frame : the dataframe to use. Mandatory, first positional parameter. x : the name of the column to use as x axis. Mandatory, second positional parameter. color : the name of the column to use as color. facet_col : the name of the column to use as facet column. facet_row : the name of the column to use as facet row. color_discrete_sequence : the list of colors in hexadecimal format to use for the color column. If the number of colors is less than the number of categories, the colors are reused. If the number of colors is greater, the colors are truncated. Histogram \u00b6 documentation Plotly express has histogram function for creating histograms. The basic syntax is: px.histogram(<dataframe>, <xcol name>) The y is then the number of occurences of each value in the x column. Important parameters: - nbins : number of bins. The plotly histogram is usually only good for simple cases or quick plotting. For more complex figures, it is better to generate the histogram manually (using numpy or pandas) and then plot it using the px.bar function. Bar Chart \u00b6 documentation For bar charts, we use the px.bar function. The basic syntax is: px.bar(<dataframe>, <xcol name>, <y col name>) Important parameters: - barmode : how to combine the bars in case of multiple traces. Can be group (default), stack (default in facet plots ), relative or overlay . - bargap : the gap between bars. - bar_groupgap : the gap between the bars from the same group (only for barmode = group ). Unfortunately, there is no way how to represent missing values in the bar chart (they appear as y = 0 ). To mark the missing values, we can use annotations. Automatic Subplots: Facet plots \u00b6 Facet plots can be created using the same plot function as for normal plotly express plots and supplying the facet_col and/or facet_row parameters. Example: fig = px.histogram(df, x=\"column\", facet_row=\"<col 1>\", facet_col=\"<col 2>\") Here, the figure will be devided into subplotts. Each row will share the <column 1> values, and each column will share the <column 2> values. The number of rows and columns will be determined automatically as the number of unique values in <column 1> and <column 2> , respectively. Independent axes between rows and columns \u00b6 It can happen that each row or column should have its own x or y axis due to a different scale. We can accomplish this by calling the update_xaxes and update_yaxes functions on the figure. Example: fig.update_xaxes(matches=None) fig.update_yaxes(matches=None) Removing the column name from the row/column annotations \u00b6 For each row and column, a label is added to the subplot. This label has a format of <column name> = <value> . To remove the column name, we can use the for_each_annotation function. Example: fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1])) Sharing the axes titles between rows and columns \u00b6 Unfortunately, the axes titles cannot be easily shared between rows and columns. The only way is to delete the titles and add shared titles manually using the add_annotation function. Example: # shared y axis title fig.for_each_yaxis(lambda y: y.update(title = '')) fig.add_annotation(x=-0.05, y=0.5, text=\"Vehicle hours\", textangle=-90, xref=\"paper\", yref=\"paper\", showarrow=False) # shared x axis title fig.for_each_xaxis(lambda y: y.update(title = '')) fig.add_annotation(x=0.5, y=-0.12, text=\"Occupancy\", xref=\"paper\", yref=\"paper\", showarrow=False) Plotly Graph Objects \u00b6 Bar Chart \u00b6 The basic syntax is: go.Bar(x, y, ...) Some more complicated examples are in the documentation . Line Chart \u00b6 The line chart is created using the go.Scatter function. Example: go.Scatter(x, y, ...) documentation Create subplots \u00b6 documentation and examples To create a figure with multiple subplots, we use the make_subplots function. Example: fig = make_subplots(rows=2, cols=2, start_cell=\"bottom-left\") fig.add_trace(go.Scatter(x=[1, 2, 3], y=[4, 5, 6]), row=1, col=1) ... fig.show() Importanta parameters: - shared_xaxes and shared_yaxes : if True , the subplots will share the same x and y axes. - x_title and y_title : the titles of the x and y axes. - horizontal_spacing and vertical_spacing : the spacing between the subplots. Customizing the Figure \u00b6 The figure object can be custommized in many ways. Size and margins \u00b6 The size and margins can be set using the figure.update_layout function. The specific parameters are: - width : the width of the figure in pixels - height : the height of the figure in pixels - autosize : needs to be False if we want to set the width and height manually - margin : dictionary containing the margins. The format is: l , r , t , b : the left, right, top and bottom margins in pixels - pad : the padding between the plot and the margins in pixels documentation Customize axes \u00b6 For customizing the axes, we can use the figure.update_xaxes and figure.update_yaxes functions. By default, the functions will update all axes. To update only a specific axis, we can use the row and col parameters. axis reference The most important parameters are: - dtick : the distance between the ticks - tickvals : the exact values of the ticks. This overrides the dtick parameter. - title_text : the title of the axis. Note that this text is only used if the tickavls are set manually . - range : e.g.: range=[0, 1] ` Legend \u00b6 documentation reference The legend can be styled using the figure.update_layout function. The most important parameters are: - legend_title_text : the title of the legend - legend : dictionary containing many parameters - orientation : h or v for horizontal or vertical legend - x , y : the position of the legend from the bottom left corner of the figure - xanchor , yanchor : the position of the legend box relative to the x and y coordinates - title : if string, the title of the legend (equivalent to the legend_title_text parameter). If dictionary, multiple legent title parameters can be set. Unfortunately, there is no way how to customize the padding between the legend items and the legend box . Hide legend \u00b6 To hide the legend, we can use the showlegend parameter. Example: fig.update_layout(showlegend=False) Adding Figure Annotations \u00b6 For adding annotations to the whole Figure, we can use the add_annotation function. Important parameters: - x , y : the x and y coordinates of the annotation - text : the text of the annotation - xref , yref : the coordinate system of the x and y coordinates. Can be \"paper\" or \"data\" . - showarrow : if True , an arrow will be added to the annotation - textangle : the angle of the text in degrees By default the annotation is meant to annotate the data. Therefore, the x and y coordinates use the coordinate system of the data (x and y axes). To align the annotation with respect to the whole figure, we need to set the xref and yref parameters to \"paper\" . In this case, the x and y coordinates are in the range [0, 1] and the origin is the bottom left corner of the figure. Annotations in facet plots \u00b6 When using facet plots, the annotations are added to the whole figure. Therefore, the x and y coordinates are in the range [0, 1] and the origin is the bottom left corner of the figure. To align the annotation with respect to the subplot, we need to set the xref and yref parameters to the x and y axes of the subplot. Example: fig.add_annotation(x=0.5, y=0.5, text=\"Title\", xref=\"x5\", yref=\"y5\", showarrow=False) Unfortunately, there is no way how to set the xref and yref parameters automatically . Therefore, we need to compute them manually for each annotation. Markers \u00b6 documentation Adding a marker to a hard-coded location \u00b6 To add a marker to a hard-coded location, we can add it as a new trace. Note that we can add new traces even to a figure created using plotly express. Text \u00b6 documentation Subscript and superscript \u00b6 To add a subscript or superscript to a text, we can use HTML tags. Example: fig.add_annotation(x=0.5, y=0.5, text=\"Title<sub>subscript</sub>\", xref=\"x5\", yref=\"y5\", showarrow=False) Exporting the Figure \u00b6 documentation The static export is handeled using the figure's write_image function. Example: fig.write_image(\"figure.png\") The output format is determined by the extension. The margins of the figure should be set for the figure itself, not for the export. Export hangs out \u00b6 It can be cause by kaleido. The solution si to install an older version, specifically 0.1.0.post1 . https://community.plotly.com/t/static-image-export-hangs-using-kaleido/61519/4","title":"Plotly Manual"},{"location":"Programming/Python/Plotly%20Manual/#plotly-express","text":"documentation Plotly express modul is loaded as: import plotly.express as px","title":"Plotly Express"},{"location":"Programming/Python/Plotly%20Manual/#common-parameters-for-all-types-of-plots","text":"data_frame : the dataframe to use. Mandatory, first positional parameter. x : the name of the column to use as x axis. Mandatory, second positional parameter. color : the name of the column to use as color. facet_col : the name of the column to use as facet column. facet_row : the name of the column to use as facet row. color_discrete_sequence : the list of colors in hexadecimal format to use for the color column. If the number of colors is less than the number of categories, the colors are reused. If the number of colors is greater, the colors are truncated.","title":"Common Parameters For All Types of Plots"},{"location":"Programming/Python/Plotly%20Manual/#histogram","text":"documentation Plotly express has histogram function for creating histograms. The basic syntax is: px.histogram(<dataframe>, <xcol name>) The y is then the number of occurences of each value in the x column. Important parameters: - nbins : number of bins. The plotly histogram is usually only good for simple cases or quick plotting. For more complex figures, it is better to generate the histogram manually (using numpy or pandas) and then plot it using the px.bar function.","title":"Histogram"},{"location":"Programming/Python/Plotly%20Manual/#bar-chart","text":"documentation For bar charts, we use the px.bar function. The basic syntax is: px.bar(<dataframe>, <xcol name>, <y col name>) Important parameters: - barmode : how to combine the bars in case of multiple traces. Can be group (default), stack (default in facet plots ), relative or overlay . - bargap : the gap between bars. - bar_groupgap : the gap between the bars from the same group (only for barmode = group ). Unfortunately, there is no way how to represent missing values in the bar chart (they appear as y = 0 ). To mark the missing values, we can use annotations.","title":"Bar Chart"},{"location":"Programming/Python/Plotly%20Manual/#automatic-subplots-facet-plots","text":"Facet plots can be created using the same plot function as for normal plotly express plots and supplying the facet_col and/or facet_row parameters. Example: fig = px.histogram(df, x=\"column\", facet_row=\"<col 1>\", facet_col=\"<col 2>\") Here, the figure will be devided into subplotts. Each row will share the <column 1> values, and each column will share the <column 2> values. The number of rows and columns will be determined automatically as the number of unique values in <column 1> and <column 2> , respectively.","title":"Automatic Subplots: Facet plots"},{"location":"Programming/Python/Plotly%20Manual/#independent-axes-between-rows-and-columns","text":"It can happen that each row or column should have its own x or y axis due to a different scale. We can accomplish this by calling the update_xaxes and update_yaxes functions on the figure. Example: fig.update_xaxes(matches=None) fig.update_yaxes(matches=None)","title":"Independent axes between rows and columns"},{"location":"Programming/Python/Plotly%20Manual/#removing-the-column-name-from-the-rowcolumn-annotations","text":"For each row and column, a label is added to the subplot. This label has a format of <column name> = <value> . To remove the column name, we can use the for_each_annotation function. Example: fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))","title":"Removing the column name from the row/column annotations"},{"location":"Programming/Python/Plotly%20Manual/#sharing-the-axes-titles-between-rows-and-columns","text":"Unfortunately, the axes titles cannot be easily shared between rows and columns. The only way is to delete the titles and add shared titles manually using the add_annotation function. Example: # shared y axis title fig.for_each_yaxis(lambda y: y.update(title = '')) fig.add_annotation(x=-0.05, y=0.5, text=\"Vehicle hours\", textangle=-90, xref=\"paper\", yref=\"paper\", showarrow=False) # shared x axis title fig.for_each_xaxis(lambda y: y.update(title = '')) fig.add_annotation(x=0.5, y=-0.12, text=\"Occupancy\", xref=\"paper\", yref=\"paper\", showarrow=False)","title":"Sharing the axes titles between rows and columns"},{"location":"Programming/Python/Plotly%20Manual/#plotly-graph-objects","text":"","title":"Plotly Graph Objects"},{"location":"Programming/Python/Plotly%20Manual/#bar-chart_1","text":"The basic syntax is: go.Bar(x, y, ...) Some more complicated examples are in the documentation .","title":"Bar Chart"},{"location":"Programming/Python/Plotly%20Manual/#line-chart","text":"The line chart is created using the go.Scatter function. Example: go.Scatter(x, y, ...) documentation","title":"Line Chart"},{"location":"Programming/Python/Plotly%20Manual/#create-subplots","text":"documentation and examples To create a figure with multiple subplots, we use the make_subplots function. Example: fig = make_subplots(rows=2, cols=2, start_cell=\"bottom-left\") fig.add_trace(go.Scatter(x=[1, 2, 3], y=[4, 5, 6]), row=1, col=1) ... fig.show() Importanta parameters: - shared_xaxes and shared_yaxes : if True , the subplots will share the same x and y axes. - x_title and y_title : the titles of the x and y axes. - horizontal_spacing and vertical_spacing : the spacing between the subplots.","title":"Create subplots"},{"location":"Programming/Python/Plotly%20Manual/#customizing-the-figure","text":"The figure object can be custommized in many ways.","title":"Customizing the Figure"},{"location":"Programming/Python/Plotly%20Manual/#size-and-margins","text":"The size and margins can be set using the figure.update_layout function. The specific parameters are: - width : the width of the figure in pixels - height : the height of the figure in pixels - autosize : needs to be False if we want to set the width and height manually - margin : dictionary containing the margins. The format is: l , r , t , b : the left, right, top and bottom margins in pixels - pad : the padding between the plot and the margins in pixels documentation","title":"Size and margins"},{"location":"Programming/Python/Plotly%20Manual/#customize-axes","text":"For customizing the axes, we can use the figure.update_xaxes and figure.update_yaxes functions. By default, the functions will update all axes. To update only a specific axis, we can use the row and col parameters. axis reference The most important parameters are: - dtick : the distance between the ticks - tickvals : the exact values of the ticks. This overrides the dtick parameter. - title_text : the title of the axis. Note that this text is only used if the tickavls are set manually . - range : e.g.: range=[0, 1] `","title":"Customize axes"},{"location":"Programming/Python/Plotly%20Manual/#legend","text":"documentation reference The legend can be styled using the figure.update_layout function. The most important parameters are: - legend_title_text : the title of the legend - legend : dictionary containing many parameters - orientation : h or v for horizontal or vertical legend - x , y : the position of the legend from the bottom left corner of the figure - xanchor , yanchor : the position of the legend box relative to the x and y coordinates - title : if string, the title of the legend (equivalent to the legend_title_text parameter). If dictionary, multiple legent title parameters can be set. Unfortunately, there is no way how to customize the padding between the legend items and the legend box .","title":"Legend"},{"location":"Programming/Python/Plotly%20Manual/#hide-legend","text":"To hide the legend, we can use the showlegend parameter. Example: fig.update_layout(showlegend=False)","title":"Hide legend"},{"location":"Programming/Python/Plotly%20Manual/#adding-figure-annotations","text":"For adding annotations to the whole Figure, we can use the add_annotation function. Important parameters: - x , y : the x and y coordinates of the annotation - text : the text of the annotation - xref , yref : the coordinate system of the x and y coordinates. Can be \"paper\" or \"data\" . - showarrow : if True , an arrow will be added to the annotation - textangle : the angle of the text in degrees By default the annotation is meant to annotate the data. Therefore, the x and y coordinates use the coordinate system of the data (x and y axes). To align the annotation with respect to the whole figure, we need to set the xref and yref parameters to \"paper\" . In this case, the x and y coordinates are in the range [0, 1] and the origin is the bottom left corner of the figure.","title":"Adding Figure Annotations"},{"location":"Programming/Python/Plotly%20Manual/#annotations-in-facet-plots","text":"When using facet plots, the annotations are added to the whole figure. Therefore, the x and y coordinates are in the range [0, 1] and the origin is the bottom left corner of the figure. To align the annotation with respect to the subplot, we need to set the xref and yref parameters to the x and y axes of the subplot. Example: fig.add_annotation(x=0.5, y=0.5, text=\"Title\", xref=\"x5\", yref=\"y5\", showarrow=False) Unfortunately, there is no way how to set the xref and yref parameters automatically . Therefore, we need to compute them manually for each annotation.","title":"Annotations in facet plots"},{"location":"Programming/Python/Plotly%20Manual/#markers","text":"documentation","title":"Markers"},{"location":"Programming/Python/Plotly%20Manual/#adding-a-marker-to-a-hard-coded-location","text":"To add a marker to a hard-coded location, we can add it as a new trace. Note that we can add new traces even to a figure created using plotly express.","title":"Adding a marker to a hard-coded location"},{"location":"Programming/Python/Plotly%20Manual/#text","text":"documentation","title":"Text"},{"location":"Programming/Python/Plotly%20Manual/#subscript-and-superscript","text":"To add a subscript or superscript to a text, we can use HTML tags. Example: fig.add_annotation(x=0.5, y=0.5, text=\"Title<sub>subscript</sub>\", xref=\"x5\", yref=\"y5\", showarrow=False)","title":"Subscript and superscript"},{"location":"Programming/Python/Plotly%20Manual/#exporting-the-figure","text":"documentation The static export is handeled using the figure's write_image function. Example: fig.write_image(\"figure.png\") The output format is determined by the extension. The margins of the figure should be set for the figure itself, not for the export.","title":"Exporting the Figure"},{"location":"Programming/Python/Plotly%20Manual/#export-hangs-out","text":"It can be cause by kaleido. The solution si to install an older version, specifically 0.1.0.post1 . https://community.plotly.com/t/static-image-export-hangs-using-kaleido/61519/4","title":"Export hangs out"},{"location":"Programming/Python/Python%20Debugging/","text":"Pycharm has a build in debugger, however, there are some tricky problems described below. Breaking on Exception \u00b6 Breaking on exception is one of the most important debugger tools. However, there are some problems with Pycharm exception debugging. Breake on Termination vs on Raise \u00b6 By default, the program breakes on termination (unhandled exception). This is usually a correct configuration. However, in jupyter, all exceptions are caught to not break the jupyter itself. Therefore, in jupyter, all exceptions are ignored by the debugger if the breakpoins are set to break on termination. To break on exceptions in jupyter, we have to breake on raise. By this setting, however, we stop even on expected/handeled exceptions, stoping potentially on hundereds breakpoints in library code. Another issue is with the setting itself. To propagate the change between breaking on raise/termination, we have to deactivate and then activate again the exception breakpoints, otherwise, the setting is ignored.","title":"Python Debugging"},{"location":"Programming/Python/Python%20Debugging/#breaking-on-exception","text":"Breaking on exception is one of the most important debugger tools. However, there are some problems with Pycharm exception debugging.","title":"Breaking on Exception"},{"location":"Programming/Python/Python%20Debugging/#breake-on-termination-vs-on-raise","text":"By default, the program breakes on termination (unhandled exception). This is usually a correct configuration. However, in jupyter, all exceptions are caught to not break the jupyter itself. Therefore, in jupyter, all exceptions are ignored by the debugger if the breakpoins are set to break on termination. To break on exceptions in jupyter, we have to breake on raise. By this setting, however, we stop even on expected/handeled exceptions, stoping potentially on hundereds breakpoints in library code. Another issue is with the setting itself. To propagate the change between breaking on raise/termination, we have to deactivate and then activate again the exception breakpoints, otherwise, the setting is ignored.","title":"Breake on Termination vs on Raise"},{"location":"Programming/Python/Python%20Manual/","text":"Data types \u00b6 Numbers \u00b6 Python has the following numeric types: - int - integer - float - floating point number The int type is unlimited, i.e., it can represent any integer number. The float type is limited by the machine precision, i.e., it can represent only a finite number of real numbers. Check whether a float number is integer \u00b6 To check whether a float number is integer, we can use the is_integer function: Check whether a number is NaN \u00b6 To check whether a number is NaN, we can use the math.isnan function or the numpyp.isnan function: Strings \u00b6 Strings in Python can be enclosed in single or double quotes (equivalent). The triple quotes can be used for multiline strings. String formatting \u00b6 The string formatting can be done in several ways: - using the f prefix to string literal: f'{<VAR>}' - using the format method: '{}'.format(<VAR>) Each variable can be formatted for that, Python has a string formatting mini language . The format is specified after the : character (e.g., f'{47:4}' set the width of the number 47 to 4 characters). Most of the format specifiers have default values, so we can omit them (e.g., f'{47:4}' is equivalent to f'{47:4d}' ). The following are the most common options: Checking the type \u00b6 To check the exact type: if type(<VAR>) is <TYPE>: # e.g. if type(o) is str: To check the type in the polymorphic way, including the subtypes: if isinstance(<VAR>, <TYPE>): # e.g. if isinstance(o, str): Conditions and boolean context \u00b6 Comparison operators \u00b6 Python uses the standard set of comparison operators ( == , != , < , > , <= , >= ). They are functionally similar to C++ operators: they can be overloaded and the semantic meaning of == is equality, not identity (in contrast to Java). Automatic conversion to bool \u00b6 Unlike in other languages, any expression can be used in boolean context in python, as there are rules how to convert any type to bool . The following statement is valid, foor example: s = 'hello' if s: print(s) The code above prints 'hello', as the variable s evaluates to True . Any object in Python evaluates to True , with exeption of: - False - None - numerically zero values (e.g., 0 , 0.0 ) - standard library types that are empty (e.g., empty string, list , dict ) The automatic conversion to bool in boolean context has some couner intuitive consequences. The following conditions are not equal: s = 'hello' if s: # s evaluates to True if s == True: # the result of s == True is False, then False evaluete to False Functions \u00b6 Argument unpacking \u00b6 if we need to conditionaly execute function with a different set of parameters (supposed the function has optional/default parameters), we can avoid multiple function calls inside the branching tree by using argument unpacking. Suppose we have a function with three optional parameters: a , b , c . If we skip only last n parameters, we can use a list for parameters and unpack it using * : def call_me(a, b, c): ... l = ['param A', True] call_me(*l) # calls the function with a = 'param A' and b = True If we need to skip some parameters in the middle, we have to use a dict and unpack it using ** : d = {'c': 142} call_me(**d) # calls the function with c = 142 # String formatting To format python strings we can use the format function of the string or the equivalen fstring: ```Python a = 'world' message = \"Hello {} world\".format(a) message = f\"Hello {a}\" # equivalent If we need to a special formatting for a variable, we can specify it behind : as we can see in the following example that padds the number from left: uid = '47' message = \"Hello user {:0>4d}\".format(a) # prints \"Hello user 0047\" message = f\"Hello {a:0>4d}\" # equivalent More formating optios can be found in the Python string formatting cookbook . Exceptions \u00b6 documentation Syntax: try: <code that can raise exception> except <ERROR TYPE> as <ERROR VAR>: <ERROR HANDELING> Date and time \u00b6 Python documentation The base object for date and time is datetime datetime construction \u00b6 The datetime object can be directly constructed from the parts: from daterime import datetime d = datetime(2022, 12, 20, 22, 30, 0) # 2022-12-20 22:30:00 The time part can be ommited. We can load the datetime from string using the strptime function: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') For all possible time formats, check the strftime cheatsheet Accessing the parts of datetime \u00b6 The datetime object has the following attributes: - year - month - day - hour - minute - second We can also query the day of the week using the weekday() method. The day of the week is represented as an integer, where Monday is 0 and Sunday is 6. Intervals \u00b6 There is also a dedicated object for time interval named timedelta . It can be constructed from parts (seconds to days), all parts are optional. We can obtain a timedelta by substracting a datetime from another datetime : d1 = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') d2 = datetime.strptime('2022-05-20 18:30', '%Y-%m-%d %H:%M') interval = d2 - d1 # 30 minutes We can also add or substract a timedelta object from the datetime object: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') interval = timedelta(hours=1) d2 = d + interval # '2022-05-20 19:00' Filesystem \u00b6 There are three ways commonlz used to work with filesystem in Python: - manipulate paths as strings - os.path - pathlib The folowing code compares both approaches for path concatenation: # string path concatenation a = \"C:/workspace\" b = \"project/file.txt\" c = f\"{a}/{b}\" # os.path concatenation a = \"C:/workspace\" b = \"project/file.txt\" c = os.path.join(a, b) # pathlib concatentation a = Path(\"C:/workspace\") b = Path(\"project/file.txt\") c = a / b As the pathlib is the most modern approach, we will use it in the following examples. Appart from pathlib documentation, there is also a cheat sheet available on github . Path editing \u00b6 Computing relative path \u00b6 To prevent misetakes, it is better to compute relative paths beteen directories than to hard-code them. Fortunately, there are methods we can use for that. If the desired relative path is a child of the start path, we can simply use the relative_to method of the Path object: a = Path(\"C:/workspace\") b = Path(\"C:/workspace/project/file.txt\") rel = b.relative_to(a) # rel = 'project/file.txt' However, if we need to go back in the filetree, we need a more sophisticated method from os.path : a = Path(\"C:/Users\") b = Path(\"C:/workspace/project/file.txt\") rel = os.path.relpath(a, b) # rel = '../Workspaces/project/file.txt' Get parent directory \u00b6 We can use the parent property of the Path object: p = Path(\"C:/workspace/project/file.txt\") parent = p.parent # 'C:\\\\workspace\\\\project' Absolute and canonical path \u00b6 We can use the absolute method of the Path object to get the absolute path. To get the canonical path, we can use the resolve method. Splitting paths and working with path parts \u00b6 To read the file extension , we can use the suffix property of the Path object. The property returns the extension with the dot . To change the extension, we can use the with_suffix method: p = Path(\"C:/workspace/project/file.txt\") p = p.with_suffix('.csv') # 'C:\\\\workspace\\\\project\\\\file.csv' We can split the path into parts using the parts property: p = Path(\"C:/workspace/project/file.txt\") parts = p.parts # ('C:\\\\', 'workspace', 'project', 'file.txt') To find the index of some specific part, we can use the index method: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 Later, we can use the index to manipulate the path: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 p = Path(*p.parts[:index]) # 'C:\\\\workspace' Changing path separators \u00b6 To change the path separators to forward slashes, we can use the as_posix and method: p = Path(r\"C:\\workspace\\project\\file.txt\") p = p.as_posix() # 'C:/workspace/project/file.txt' Geting and setting the working directory \u00b6 os.getcwd() - get the current working directory os.chdir(<path>) - set the current working directory Iterating over files \u00b6 The pathlib module provides a convenient way to iterate over files in a directory. The particular methods are: - glob - iterate over files in a single directory - rglob - iterate over files in a directory and all its subdirectories In general, the files will be sorted alphabetically. Single directory iteration \u00b6 Using pathlib, we can iterate over files using a filter with the glob method: p = Path(\"C:/workspace/project\") for filepath in p.glob('*.txt') # iterate over all txt files in the project directory The old way is to use the os.listdir method: p = Path(\"C:/workspace/project\") for filename in os.listdir(p): if filename.endswith('.txt'): filepath = p / filename Recursive iteration \u00b6 Using pathlib, we can iterate over files using a filter with the rglob method: p = Path(\"C:/workspace/project\") for filepath in p.rglob('*.txt') # iterate over all txt files in the project directory and all its subdirectories The old way is to use the os.walk method: p = Path(\"C:/workspace/project\") for root, dirs, files in os.walk(p): for filename in files: if filename.endswith('.txt'): filepath = Path(root) / filename Iterate only directories/files \u00b6 There is no specific filter for files/directories, but we can use the is_file or is_dir method to filter out directories: p = Path(\"C:/workspace/project\") for filepath in p.glob('*'): if filepath.is_file(): # do something Use more complex filters \u00b6 Unfortunately, the glob and rglob methods do not support more complex filters (like regex). However, we can easily apply the regex filter manually: p = Path(\"C:/workspace/project\") for filepath in p.glob('*'): if not re.match(r'^config.yaml$', filepath.name): # do something Get the path to the current script \u00b6 Path(__file__).resolve().parent Checking write permissions for a directory \u00b6 Unfortunatelly, most of the methods for checking write permissions are not reliable outside Unix systems. The most reliable way is to try to create a file in the directory: p = Path(\"C:/workspace/project\") try: with open(p / 'test.txt', 'w') as f: pass p.unlink() return True except PermissionError: return False except: raise # re-raise the exception Other methods like os.access or using tempfile module are not reliable on Windows (see e.g.: https://github.com/python/cpython/issues/66305). Deleting files and directories \u00b6 To delete a file, we can use the unlink method of the Path object: p = Path(\"C:/workspace/project/file.txt\") p.unlink() for deleting directories, we can use the rmdir method: p = Path(\"C:/workspace/project\") p.rmdir() However, the rmdir method can delete only empty directories. To delete a directory with content, we can use the shutil module: p = Path(\"C:/workspace/project\") shutil.rmtree(p) Deleting Windows read-only files (i.e. Access Denied error) \u00b6 On Windows, all the delete methods can fail because lot of files and directories are read-only. This is not a problem for most application, but it breaks Python delete methods. One way to solve this is to handle the error and change the attribute in the habdler. Example for shutil: import os import stat import shutil p = Path(\"C:/workspace/project\") shutil.rmtree(p, onerror=lambda func, path, _: (os.chmod(path, stat.S_IWRITE), func(path))) Built-in data structures and generators \u00b6 Python has several built-in data structures, most notably list , tuple , dict , and set . These are less efficient then comparable structures in other languages, but they are very convenient to use. Also, there is a special generator type. It does not store the data it is only a convinient way how to access data generated by some function. Generator \u00b6 Python wiki Generators are mostly used in the iteration, we can iterte them the same way as lists. To get the first item of the generator, we can use the next function: g = (x for x in range(10)) first = next(g) # 0 To create a generator function (a function that returns a generator), we can use the yield keyword. The following function returns a generator that yields the numbers 1, 2, and 3: def gen(): yield 1 yield 2 yield 3 Dictionary \u00b6 Official Manual Disctionaries are initialized using curly braces ( {} ) and the : operator: d = { 'key1': 'value1', 'key2': 'value2', ... } Two dictionaries can be merged using the | operator: d3 = d1 | d2 Comprehensions \u00b6 In addition to literals, Python has a convinient way of creating basic data structures: the comprehensions. The basic syntax is: <struct var> = <op. brace> <member var expr.> for <member var> in <iterable><cl. brace> As for literals, we use square braces ( [] ) for lists, curly braces ( {} ) for sets, and curly braces with colons for dictionaries. In contrast, we get a generator expression when using round braces ( () ), not a tuple. We can also use the if keyword to filter the elements: a = [it for it in range(10) if it % 2 == 0] # [0, 2, 4, 6, 8] Sorting \u00b6 Official Manual For sorting, you can use the sorted function. Instead of using comparators, Python has a different concept of key functions for custom sorting. The key function is a function that is applied to each element before sorting. For any expected object, the key function should return a value that can be compared. I/O \u00b6 CSV \u00b6 Official Manual The csv module provides a Python interface for working with CSV files. The basic usage is: import csv with open('file.csv', 'r') as f: reader = csv.reader(f) for row in reader: # do something Reader parameters: - delimiter - the delimiter character HDF5 \u00b6 HDF5 is a binary file format for storing large amounts of data. The h5py module provides a Python interface for working with HDF5 files. An example of reading a dataset from an HDF5 file on SO Command line arguments \u00b6 The sys module provides access to the command line arguments. They are stored in the argv list with the first element being the name of the script. Logging \u00b6 Official Manual A simple logging configuration: import logging logging.basicConfig( level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s', handlers=[ logging.FileHandler(\"log.txt\"), logging.StreamHandler() ] ) The logging itself is then done using the logging module methods: logging.info(\"message\") logging.warning(\"message %s\", \"with parameter\") Type hints \u00b6 Official Manual Type hints are a useful way to help the IDE and other tools to understand the code so that they can provide better support (autocompletion, type checking, refactoring, etc.). The type hints are not enforced at runtime, so they do not affect the performance of the code. We can specify the type of a variable using the : operator: a: int = 1 Apart from the basic types, we can also use the typing module to specify more complex types: from typing import List, Dict, Tuple, Set, Optional, Union, Any a: List[int] = [1, 2, 3] We can also specify the type of a function argument and return value: def foo(a: int, b: str) -> List[int]: return [a, b] Type hints in loops \u00b6 The type of the loop variable is usually inferred by IDE from the type of the iterable. However, this sometimes fails, e.g., for zip objects. In such cases, we need to specify the type of the loop variable. However, we cannot use the : directly in the loop, but instead, we have to declare the variable before the loop : for a: int in ... # error a: int for a in ... # ok Numpy \u00b6 Sorting \u00b6 for sorting, we use the sort function. There is no way how to set the sorting order, we have to use a trick for that: a = np.array([1, 2, 3, 4, 5]) a[::-1].sort() # sort in reverse order Usefull array properties: \u00b6 size : number of array items unlike len, it counts all items in the mutli-dimensional array itemsize : memory (bytes) needed to store one item in the array nbytes : array size in bytes. Should be equal to size * itemsize . Usefull functions \u00b6 Regular expressions \u00b6 In Python, the regex patterns are not compiled by default. Therefore we can use strings to store them. The basic syntax for regex search is: result = re.search(<pattern>, <string>) if result: # pattern matches group = result.group(<group index>)) # print the first group The 0th group is the whole match, as usual. Decorators \u00b6 Decorators are a special type of function that can be used to modify other functions. When we write an annotation with the name of a function above another function, the annotated function is decorated . It means that when we call the annotated function, a wrapper function is called instead. The wrapper function is the function returned by the decorater : the function with the same name as the annotation. If we want to also keep the original function functionality, we have to pass the function to the decorator and call it inside the wrapper function. In the following example, we create a dummy decorator that keeps the original function functionality: Example: def decorator(func): def wrapper(): result = func() return result return wrapper @decorator def my_func(): # do something return result Decorator with arguments \u00b6 If the original function has arguments, we have to pass them to the wrapper function. Example: def decorator(func): def wrapper(param_1, param_2): result = func(param_1, param_2) return result return wrapper @decorator def my_func(param_1, param_2): # do something return result Jupyter \u00b6 Memory \u00b6 Most of the time, when the memory allocated by the notebook is larger than expected, it is caused by some library objects (plots, tables...]). However sometimes, it can be forgotten user objects. To list all user objects, from the largest: # These are the usual ipython objects, including this one you are creating ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars'] # Get a sorted list of the objects and their sizes sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True) Matplotlib \u00b6 Official Manual Saving figures \u00b6 To save a figure, we can use the savefig function. The savefig function has to be called before the show function, otherwise the figure will be empty . Docstrings \u00b6 For documenting Python code, we use docstrings, special comments soroudned by three quotation marks: \"\"\" docstring \"\"\" Unlike in other languages, there are multiple styles for docstring content. Progress bars \u00b6 For displaying progress bars, we can use the tqdm library. It is very simple to use: from tqdm import tqdm for i in tqdm(range(100)): ... Important parameters: - desc : description of the progress bar PostgreSQL \u00b6 When working with PostgreSQL databases, we usually use either - the psycopg2 adapter or, - the sqlalchemy . SQLAlchemy \u00b6 Simple query: sqlalchemy_engine.execute(\"<sql>\") Working with GIS \u00b6 when working with gis data, we usually change the pandas library for its GIS extension called geopandas . For more,, see the pandas manual.","title":"Python Manual"},{"location":"Programming/Python/Python%20Manual/#data-types","text":"","title":"Data types"},{"location":"Programming/Python/Python%20Manual/#numbers","text":"Python has the following numeric types: - int - integer - float - floating point number The int type is unlimited, i.e., it can represent any integer number. The float type is limited by the machine precision, i.e., it can represent only a finite number of real numbers.","title":"Numbers"},{"location":"Programming/Python/Python%20Manual/#check-whether-a-float-number-is-integer","text":"To check whether a float number is integer, we can use the is_integer function:","title":"Check whether a float number is integer"},{"location":"Programming/Python/Python%20Manual/#check-whether-a-number-is-nan","text":"To check whether a number is NaN, we can use the math.isnan function or the numpyp.isnan function:","title":"Check whether a number is NaN"},{"location":"Programming/Python/Python%20Manual/#strings","text":"Strings in Python can be enclosed in single or double quotes (equivalent). The triple quotes can be used for multiline strings.","title":"Strings"},{"location":"Programming/Python/Python%20Manual/#string-formatting","text":"The string formatting can be done in several ways: - using the f prefix to string literal: f'{<VAR>}' - using the format method: '{}'.format(<VAR>) Each variable can be formatted for that, Python has a string formatting mini language . The format is specified after the : character (e.g., f'{47:4}' set the width of the number 47 to 4 characters). Most of the format specifiers have default values, so we can omit them (e.g., f'{47:4}' is equivalent to f'{47:4d}' ). The following are the most common options:","title":"String formatting"},{"location":"Programming/Python/Python%20Manual/#checking-the-type","text":"To check the exact type: if type(<VAR>) is <TYPE>: # e.g. if type(o) is str: To check the type in the polymorphic way, including the subtypes: if isinstance(<VAR>, <TYPE>): # e.g. if isinstance(o, str):","title":"Checking the type"},{"location":"Programming/Python/Python%20Manual/#conditions-and-boolean-context","text":"","title":"Conditions and boolean context"},{"location":"Programming/Python/Python%20Manual/#comparison-operators","text":"Python uses the standard set of comparison operators ( == , != , < , > , <= , >= ). They are functionally similar to C++ operators: they can be overloaded and the semantic meaning of == is equality, not identity (in contrast to Java).","title":"Comparison operators"},{"location":"Programming/Python/Python%20Manual/#automatic-conversion-to-bool","text":"Unlike in other languages, any expression can be used in boolean context in python, as there are rules how to convert any type to bool . The following statement is valid, foor example: s = 'hello' if s: print(s) The code above prints 'hello', as the variable s evaluates to True . Any object in Python evaluates to True , with exeption of: - False - None - numerically zero values (e.g., 0 , 0.0 ) - standard library types that are empty (e.g., empty string, list , dict ) The automatic conversion to bool in boolean context has some couner intuitive consequences. The following conditions are not equal: s = 'hello' if s: # s evaluates to True if s == True: # the result of s == True is False, then False evaluete to False","title":"Automatic conversion to bool"},{"location":"Programming/Python/Python%20Manual/#functions","text":"","title":"Functions"},{"location":"Programming/Python/Python%20Manual/#argument-unpacking","text":"if we need to conditionaly execute function with a different set of parameters (supposed the function has optional/default parameters), we can avoid multiple function calls inside the branching tree by using argument unpacking. Suppose we have a function with three optional parameters: a , b , c . If we skip only last n parameters, we can use a list for parameters and unpack it using * : def call_me(a, b, c): ... l = ['param A', True] call_me(*l) # calls the function with a = 'param A' and b = True If we need to skip some parameters in the middle, we have to use a dict and unpack it using ** : d = {'c': 142} call_me(**d) # calls the function with c = 142 # String formatting To format python strings we can use the format function of the string or the equivalen fstring: ```Python a = 'world' message = \"Hello {} world\".format(a) message = f\"Hello {a}\" # equivalent If we need to a special formatting for a variable, we can specify it behind : as we can see in the following example that padds the number from left: uid = '47' message = \"Hello user {:0>4d}\".format(a) # prints \"Hello user 0047\" message = f\"Hello {a:0>4d}\" # equivalent More formating optios can be found in the Python string formatting cookbook .","title":"Argument unpacking"},{"location":"Programming/Python/Python%20Manual/#exceptions","text":"documentation Syntax: try: <code that can raise exception> except <ERROR TYPE> as <ERROR VAR>: <ERROR HANDELING>","title":"Exceptions"},{"location":"Programming/Python/Python%20Manual/#date-and-time","text":"Python documentation The base object for date and time is datetime","title":"Date and time"},{"location":"Programming/Python/Python%20Manual/#datetime-construction","text":"The datetime object can be directly constructed from the parts: from daterime import datetime d = datetime(2022, 12, 20, 22, 30, 0) # 2022-12-20 22:30:00 The time part can be ommited. We can load the datetime from string using the strptime function: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') For all possible time formats, check the strftime cheatsheet","title":"datetime construction"},{"location":"Programming/Python/Python%20Manual/#accessing-the-parts-of-datetime","text":"The datetime object has the following attributes: - year - month - day - hour - minute - second We can also query the day of the week using the weekday() method. The day of the week is represented as an integer, where Monday is 0 and Sunday is 6.","title":"Accessing the parts of datetime"},{"location":"Programming/Python/Python%20Manual/#intervals","text":"There is also a dedicated object for time interval named timedelta . It can be constructed from parts (seconds to days), all parts are optional. We can obtain a timedelta by substracting a datetime from another datetime : d1 = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') d2 = datetime.strptime('2022-05-20 18:30', '%Y-%m-%d %H:%M') interval = d2 - d1 # 30 minutes We can also add or substract a timedelta object from the datetime object: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') interval = timedelta(hours=1) d2 = d + interval # '2022-05-20 19:00'","title":"Intervals"},{"location":"Programming/Python/Python%20Manual/#filesystem","text":"There are three ways commonlz used to work with filesystem in Python: - manipulate paths as strings - os.path - pathlib The folowing code compares both approaches for path concatenation: # string path concatenation a = \"C:/workspace\" b = \"project/file.txt\" c = f\"{a}/{b}\" # os.path concatenation a = \"C:/workspace\" b = \"project/file.txt\" c = os.path.join(a, b) # pathlib concatentation a = Path(\"C:/workspace\") b = Path(\"project/file.txt\") c = a / b As the pathlib is the most modern approach, we will use it in the following examples. Appart from pathlib documentation, there is also a cheat sheet available on github .","title":"Filesystem"},{"location":"Programming/Python/Python%20Manual/#path-editing","text":"","title":"Path editing"},{"location":"Programming/Python/Python%20Manual/#computing-relative-path","text":"To prevent misetakes, it is better to compute relative paths beteen directories than to hard-code them. Fortunately, there are methods we can use for that. If the desired relative path is a child of the start path, we can simply use the relative_to method of the Path object: a = Path(\"C:/workspace\") b = Path(\"C:/workspace/project/file.txt\") rel = b.relative_to(a) # rel = 'project/file.txt' However, if we need to go back in the filetree, we need a more sophisticated method from os.path : a = Path(\"C:/Users\") b = Path(\"C:/workspace/project/file.txt\") rel = os.path.relpath(a, b) # rel = '../Workspaces/project/file.txt'","title":"Computing relative path"},{"location":"Programming/Python/Python%20Manual/#get-parent-directory","text":"We can use the parent property of the Path object: p = Path(\"C:/workspace/project/file.txt\") parent = p.parent # 'C:\\\\workspace\\\\project'","title":"Get parent directory"},{"location":"Programming/Python/Python%20Manual/#absolute-and-canonical-path","text":"We can use the absolute method of the Path object to get the absolute path. To get the canonical path, we can use the resolve method.","title":"Absolute and canonical path"},{"location":"Programming/Python/Python%20Manual/#splitting-paths-and-working-with-path-parts","text":"To read the file extension , we can use the suffix property of the Path object. The property returns the extension with the dot . To change the extension, we can use the with_suffix method: p = Path(\"C:/workspace/project/file.txt\") p = p.with_suffix('.csv') # 'C:\\\\workspace\\\\project\\\\file.csv' We can split the path into parts using the parts property: p = Path(\"C:/workspace/project/file.txt\") parts = p.parts # ('C:\\\\', 'workspace', 'project', 'file.txt') To find the index of some specific part, we can use the index method: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 Later, we can use the index to manipulate the path: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 p = Path(*p.parts[:index]) # 'C:\\\\workspace'","title":"Splitting paths and working with path parts"},{"location":"Programming/Python/Python%20Manual/#changing-path-separators","text":"To change the path separators to forward slashes, we can use the as_posix and method: p = Path(r\"C:\\workspace\\project\\file.txt\") p = p.as_posix() # 'C:/workspace/project/file.txt'","title":"Changing path separators"},{"location":"Programming/Python/Python%20Manual/#geting-and-setting-the-working-directory","text":"os.getcwd() - get the current working directory os.chdir(<path>) - set the current working directory","title":"Geting and setting the working directory"},{"location":"Programming/Python/Python%20Manual/#iterating-over-files","text":"The pathlib module provides a convenient way to iterate over files in a directory. The particular methods are: - glob - iterate over files in a single directory - rglob - iterate over files in a directory and all its subdirectories In general, the files will be sorted alphabetically.","title":"Iterating over files"},{"location":"Programming/Python/Python%20Manual/#single-directory-iteration","text":"Using pathlib, we can iterate over files using a filter with the glob method: p = Path(\"C:/workspace/project\") for filepath in p.glob('*.txt') # iterate over all txt files in the project directory The old way is to use the os.listdir method: p = Path(\"C:/workspace/project\") for filename in os.listdir(p): if filename.endswith('.txt'): filepath = p / filename","title":"Single directory iteration"},{"location":"Programming/Python/Python%20Manual/#recursive-iteration","text":"Using pathlib, we can iterate over files using a filter with the rglob method: p = Path(\"C:/workspace/project\") for filepath in p.rglob('*.txt') # iterate over all txt files in the project directory and all its subdirectories The old way is to use the os.walk method: p = Path(\"C:/workspace/project\") for root, dirs, files in os.walk(p): for filename in files: if filename.endswith('.txt'): filepath = Path(root) / filename","title":"Recursive iteration"},{"location":"Programming/Python/Python%20Manual/#iterate-only-directoriesfiles","text":"There is no specific filter for files/directories, but we can use the is_file or is_dir method to filter out directories: p = Path(\"C:/workspace/project\") for filepath in p.glob('*'): if filepath.is_file(): # do something","title":"Iterate only directories/files"},{"location":"Programming/Python/Python%20Manual/#use-more-complex-filters","text":"Unfortunately, the glob and rglob methods do not support more complex filters (like regex). However, we can easily apply the regex filter manually: p = Path(\"C:/workspace/project\") for filepath in p.glob('*'): if not re.match(r'^config.yaml$', filepath.name): # do something","title":"Use more complex filters"},{"location":"Programming/Python/Python%20Manual/#get-the-path-to-the-current-script","text":"Path(__file__).resolve().parent","title":"Get the path to the current script"},{"location":"Programming/Python/Python%20Manual/#checking-write-permissions-for-a-directory","text":"Unfortunatelly, most of the methods for checking write permissions are not reliable outside Unix systems. The most reliable way is to try to create a file in the directory: p = Path(\"C:/workspace/project\") try: with open(p / 'test.txt', 'w') as f: pass p.unlink() return True except PermissionError: return False except: raise # re-raise the exception Other methods like os.access or using tempfile module are not reliable on Windows (see e.g.: https://github.com/python/cpython/issues/66305).","title":"Checking write permissions for a directory"},{"location":"Programming/Python/Python%20Manual/#deleting-files-and-directories","text":"To delete a file, we can use the unlink method of the Path object: p = Path(\"C:/workspace/project/file.txt\") p.unlink() for deleting directories, we can use the rmdir method: p = Path(\"C:/workspace/project\") p.rmdir() However, the rmdir method can delete only empty directories. To delete a directory with content, we can use the shutil module: p = Path(\"C:/workspace/project\") shutil.rmtree(p)","title":"Deleting files and directories"},{"location":"Programming/Python/Python%20Manual/#deleting-windows-read-only-files-ie-access-denied-error","text":"On Windows, all the delete methods can fail because lot of files and directories are read-only. This is not a problem for most application, but it breaks Python delete methods. One way to solve this is to handle the error and change the attribute in the habdler. Example for shutil: import os import stat import shutil p = Path(\"C:/workspace/project\") shutil.rmtree(p, onerror=lambda func, path, _: (os.chmod(path, stat.S_IWRITE), func(path)))","title":"Deleting Windows read-only files (i.e. Access Denied error)"},{"location":"Programming/Python/Python%20Manual/#built-in-data-structures-and-generators","text":"Python has several built-in data structures, most notably list , tuple , dict , and set . These are less efficient then comparable structures in other languages, but they are very convenient to use. Also, there is a special generator type. It does not store the data it is only a convinient way how to access data generated by some function.","title":"Built-in data structures and generators"},{"location":"Programming/Python/Python%20Manual/#generator","text":"Python wiki Generators are mostly used in the iteration, we can iterte them the same way as lists. To get the first item of the generator, we can use the next function: g = (x for x in range(10)) first = next(g) # 0 To create a generator function (a function that returns a generator), we can use the yield keyword. The following function returns a generator that yields the numbers 1, 2, and 3: def gen(): yield 1 yield 2 yield 3","title":"Generator"},{"location":"Programming/Python/Python%20Manual/#dictionary","text":"Official Manual Disctionaries are initialized using curly braces ( {} ) and the : operator: d = { 'key1': 'value1', 'key2': 'value2', ... } Two dictionaries can be merged using the | operator: d3 = d1 | d2","title":"Dictionary"},{"location":"Programming/Python/Python%20Manual/#comprehensions","text":"In addition to literals, Python has a convinient way of creating basic data structures: the comprehensions. The basic syntax is: <struct var> = <op. brace> <member var expr.> for <member var> in <iterable><cl. brace> As for literals, we use square braces ( [] ) for lists, curly braces ( {} ) for sets, and curly braces with colons for dictionaries. In contrast, we get a generator expression when using round braces ( () ), not a tuple. We can also use the if keyword to filter the elements: a = [it for it in range(10) if it % 2 == 0] # [0, 2, 4, 6, 8]","title":"Comprehensions"},{"location":"Programming/Python/Python%20Manual/#sorting","text":"Official Manual For sorting, you can use the sorted function. Instead of using comparators, Python has a different concept of key functions for custom sorting. The key function is a function that is applied to each element before sorting. For any expected object, the key function should return a value that can be compared.","title":"Sorting"},{"location":"Programming/Python/Python%20Manual/#io","text":"","title":"I/O"},{"location":"Programming/Python/Python%20Manual/#csv","text":"Official Manual The csv module provides a Python interface for working with CSV files. The basic usage is: import csv with open('file.csv', 'r') as f: reader = csv.reader(f) for row in reader: # do something Reader parameters: - delimiter - the delimiter character","title":"CSV"},{"location":"Programming/Python/Python%20Manual/#hdf5","text":"HDF5 is a binary file format for storing large amounts of data. The h5py module provides a Python interface for working with HDF5 files. An example of reading a dataset from an HDF5 file on SO","title":"HDF5"},{"location":"Programming/Python/Python%20Manual/#command-line-arguments","text":"The sys module provides access to the command line arguments. They are stored in the argv list with the first element being the name of the script.","title":"Command line arguments"},{"location":"Programming/Python/Python%20Manual/#logging","text":"Official Manual A simple logging configuration: import logging logging.basicConfig( level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s', handlers=[ logging.FileHandler(\"log.txt\"), logging.StreamHandler() ] ) The logging itself is then done using the logging module methods: logging.info(\"message\") logging.warning(\"message %s\", \"with parameter\")","title":"Logging"},{"location":"Programming/Python/Python%20Manual/#type-hints","text":"Official Manual Type hints are a useful way to help the IDE and other tools to understand the code so that they can provide better support (autocompletion, type checking, refactoring, etc.). The type hints are not enforced at runtime, so they do not affect the performance of the code. We can specify the type of a variable using the : operator: a: int = 1 Apart from the basic types, we can also use the typing module to specify more complex types: from typing import List, Dict, Tuple, Set, Optional, Union, Any a: List[int] = [1, 2, 3] We can also specify the type of a function argument and return value: def foo(a: int, b: str) -> List[int]: return [a, b]","title":"Type hints"},{"location":"Programming/Python/Python%20Manual/#type-hints-in-loops","text":"The type of the loop variable is usually inferred by IDE from the type of the iterable. However, this sometimes fails, e.g., for zip objects. In such cases, we need to specify the type of the loop variable. However, we cannot use the : directly in the loop, but instead, we have to declare the variable before the loop : for a: int in ... # error a: int for a in ... # ok","title":"Type hints in loops"},{"location":"Programming/Python/Python%20Manual/#numpy","text":"","title":"Numpy"},{"location":"Programming/Python/Python%20Manual/#sorting_1","text":"for sorting, we use the sort function. There is no way how to set the sorting order, we have to use a trick for that: a = np.array([1, 2, 3, 4, 5]) a[::-1].sort() # sort in reverse order","title":"Sorting"},{"location":"Programming/Python/Python%20Manual/#usefull-array-properties","text":"size : number of array items unlike len, it counts all items in the mutli-dimensional array itemsize : memory (bytes) needed to store one item in the array nbytes : array size in bytes. Should be equal to size * itemsize .","title":"Usefull array properties:"},{"location":"Programming/Python/Python%20Manual/#usefull-functions","text":"","title":"Usefull functions"},{"location":"Programming/Python/Python%20Manual/#regular-expressions","text":"In Python, the regex patterns are not compiled by default. Therefore we can use strings to store them. The basic syntax for regex search is: result = re.search(<pattern>, <string>) if result: # pattern matches group = result.group(<group index>)) # print the first group The 0th group is the whole match, as usual.","title":"Regular expressions"},{"location":"Programming/Python/Python%20Manual/#decorators","text":"Decorators are a special type of function that can be used to modify other functions. When we write an annotation with the name of a function above another function, the annotated function is decorated . It means that when we call the annotated function, a wrapper function is called instead. The wrapper function is the function returned by the decorater : the function with the same name as the annotation. If we want to also keep the original function functionality, we have to pass the function to the decorator and call it inside the wrapper function. In the following example, we create a dummy decorator that keeps the original function functionality: Example: def decorator(func): def wrapper(): result = func() return result return wrapper @decorator def my_func(): # do something return result","title":"Decorators"},{"location":"Programming/Python/Python%20Manual/#decorator-with-arguments","text":"If the original function has arguments, we have to pass them to the wrapper function. Example: def decorator(func): def wrapper(param_1, param_2): result = func(param_1, param_2) return result return wrapper @decorator def my_func(param_1, param_2): # do something return result","title":"Decorator with arguments"},{"location":"Programming/Python/Python%20Manual/#jupyter","text":"","title":"Jupyter"},{"location":"Programming/Python/Python%20Manual/#memory","text":"Most of the time, when the memory allocated by the notebook is larger than expected, it is caused by some library objects (plots, tables...]). However sometimes, it can be forgotten user objects. To list all user objects, from the largest: # These are the usual ipython objects, including this one you are creating ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars'] # Get a sorted list of the objects and their sizes sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)","title":"Memory"},{"location":"Programming/Python/Python%20Manual/#matplotlib","text":"Official Manual","title":"Matplotlib"},{"location":"Programming/Python/Python%20Manual/#saving-figures","text":"To save a figure, we can use the savefig function. The savefig function has to be called before the show function, otherwise the figure will be empty .","title":"Saving figures"},{"location":"Programming/Python/Python%20Manual/#docstrings","text":"For documenting Python code, we use docstrings, special comments soroudned by three quotation marks: \"\"\" docstring \"\"\" Unlike in other languages, there are multiple styles for docstring content.","title":"Docstrings"},{"location":"Programming/Python/Python%20Manual/#progress-bars","text":"For displaying progress bars, we can use the tqdm library. It is very simple to use: from tqdm import tqdm for i in tqdm(range(100)): ... Important parameters: - desc : description of the progress bar","title":"Progress bars"},{"location":"Programming/Python/Python%20Manual/#postgresql","text":"When working with PostgreSQL databases, we usually use either - the psycopg2 adapter or, - the sqlalchemy .","title":"PostgreSQL"},{"location":"Programming/Python/Python%20Manual/#sqlalchemy","text":"Simple query: sqlalchemy_engine.execute(\"<sql>\")","title":"SQLAlchemy"},{"location":"Programming/Python/Python%20Manual/#working-with-gis","text":"when working with gis data, we usually change the pandas library for its GIS extension called geopandas . For more,, see the pandas manual.","title":"Working with GIS"},{"location":"Programming/Python/Python%20Workflow/","text":"Dev Stack \u00b6 I use the following stack: - the latest Python, 64 bit - pip as the package manager - Pycharm IDE - pytest test suite - Visual Studio for deugging native code Python \u00b6 Python should be installed from the official web page , not using any package manager. Steps: 1. Dowload the 64-bit installer 2. Run the installer, choose advanced install - Include the GUI tools (Tkinter, TK) - Includ *.py launcher, but only if there is no newer python version installed. If this is checked and there is a newer vesrsion of Python installed, the setup will fail. - Include documentation - Check download debug symbols to enable native code debugging The source code for python can be inspected on GitHub Command line \u00b6 We execute python scripts from the command line as: python <path to the .py file> . Parameters: - -m executes a module as a script, e.g. python -m venv . This is useful for executing scripts whithout knowing the path to the script. Installing Packages \u00b6 Normal packages are installed using: pip install <package name> . However, if a package uses a C/C++ backend and does not contain the compiled wheel on PyPy, this approach will fail on Windows. Instead, you have to download the wheel from the Chris Gohlke page and install it: pip install <path to wheel> . Also, you have to install the dependencies mentioned on that page. Uninstalling packages \u00b6 To uninstall a package, use pip uninstall <package name> . There is no way how to uninstall more packages using some wildcard. To uninstall more packages efficiently (not one by one): 1. create a file with the list of all installed packages: pip freeze > packages.txt 2. edit the file and remove all packages you want to keep 3. uninstall all packages from the file: pip uninstall -r packages.txt -y Troubleshooting \u00b6 If the installation fails, check the following: 1. if you installed the package by name, check for the wheel on the Chris Golthke page. 2. if you installed the package from a wheel, check the notes/requirement info on Chris Golthke page 3. Check the log. Specifically, no building should appear there whatsoever. If a build starts, it means that some dependency that should be installed as a prebuild wheel is missing. Possible reasons: 1. you forget to install the dependency, go back to step 2 2. the dependency version does not correspond with the version required by the package you are installing. Check the log for the required version. Pycharm \u00b6 Configuration \u00b6 Settings synchronization \u00b6 Same as in IDEA: 1. Log in into JetBrains Toolbox or to the App 1. Click on the gear icon on the top-right and choose Sync 1. Check all categories and click on pull settings from the cloud Do Not Run scripts in Python Console by Default \u00b6 Run configuration select box -> Edit Configurations... -> Edit configuration templates -> Python -> uncheck the Run with Python Console Enable Progress Bars in output console \u00b6 Run configuration select box -> Edit Configurations... -> Select the configuration -> check the Emulate terminal in output console Project Configuration \u00b6 configure the correct test suite in File -> Settings -> Tools -> Python Integrated Tools -> testing Known problems & solutions \u00b6 Non deterministic output in the run window \u00b6 Problem: It can happen that the output printing/logging can be reordered randomly (not matching the order of calls in the source, neither the system console output). Solution: Edit Configurations... -> select configuration for the script -> check Emulate terminal in output console . Pycharm does not recognize a locally installed package \u00b6 It can happen that a locally installed package ( -e ) is not recognized by Pycharm. This can be solved by adding the path to the package to the interpreter paths: 1. File -> Settings -> Project: <project name> -> Python Interpreter 1. Click on the arrow next to the interpreter name and choose Show All... 1. Click on the desired interpreter and click on the filetree icon on the top of the window 1. Add the path to the package to the list of paths Jupyter \u00b6 Jupyter can be used both in Pycharm and in a web browser. Jupyter in Pycharm \u00b6 The key for the effectivity of Jupyter in Pycharm is using the command mode . To enter the command mode, press Esc . To enter the edit mode, pres enter. Command mode shortcuts \u00b6 m : change cell type to markdown Ctrl + C : copy cell Ctrl + V : paste cell Ctrl + Shift + Up : move cell up Ctrl + Shift + Down : move cell down Ctrl + - : split cell Web Browser Configuration \u00b6 Install Extension Manager with basic extensions \u00b6 Best use the official installation guide . The extensions then can be toggled on in the Nbextensions tab in the jupyter homepage. Be sure to unselect the disable configuration for nbextensions without explicit compatibility (they may break your notebook environment, but can be useful to show for nbextension development) checkbox, otherwise, all extensions will be disabled. Debugging \u00b6 Pycharm contains a good debuger for python code. However, it cannot step into most standard library functions, as those are native, implemented in C/C++. For that, we need mixed python/native debugging. Testing \u00b6 To run pytest, simply go to the folder and run pytest . Arguments: - -x : stop on first failure Mixed Python-native debugging \u00b6 In theory, there are two ways how to debug python native code: - use a tool that can step from python to C++ (only Visual Studio offers this possibility AFAIK) - use two debuggers, start with a python debugger, and attach a native debugger to the python process. This way, the debuggers can be independent. However, one needs to put a breakpoint in the native code, and for that, we need to know the location of the code that will be executed from python (non-trivial for standard library) Python-native debugger in Visual Studio \u00b6 First check and install the requirements: - Python 3.5-3.9, including debugging symbols - Python 3.10 is not compatible yet - in Visual Studio, a Python development component needs to be installed, including Python native development tools Then, configure the Visual studio: - Go to Tools -> Options -> Python -> Debugging and check: Enable debugging of the Python standard library . Finally, create a new Python project (either clean, or from existing code) and configure it: - use the interpreter directly, do not create a virtual environment - enable native code debugging: 1. Go to project properties -> Debug 2. Check enable native code debugging - use the -i flag to always see the debug console, even if the program ends without breaking 1. Go to project properties -> Debug 2. Add the -i flag to the Interpreter Arguments field Known issues \u00b6 The reference assemblies for .NETFramework,Version=v4.7.2 were not found. -> Install this component using the visual studio installer Other Sources \u00b6 Microsoft official documentation Python tools for Visual Studio GitHub page Releasing libraries to PyPi \u00b6 Steps \u00b6 add license headers: https://github.com/johann-petrak/licenseheaders check that setup.py contains all requirements - pipreqs release update the min version in dependencies Release \u00b6 raise version run sdist upload to pypi: twine upload dist/*","title":"Python Workflow"},{"location":"Programming/Python/Python%20Workflow/#dev-stack","text":"I use the following stack: - the latest Python, 64 bit - pip as the package manager - Pycharm IDE - pytest test suite - Visual Studio for deugging native code","title":"Dev Stack"},{"location":"Programming/Python/Python%20Workflow/#python","text":"Python should be installed from the official web page , not using any package manager. Steps: 1. Dowload the 64-bit installer 2. Run the installer, choose advanced install - Include the GUI tools (Tkinter, TK) - Includ *.py launcher, but only if there is no newer python version installed. If this is checked and there is a newer vesrsion of Python installed, the setup will fail. - Include documentation - Check download debug symbols to enable native code debugging The source code for python can be inspected on GitHub","title":"Python"},{"location":"Programming/Python/Python%20Workflow/#command-line","text":"We execute python scripts from the command line as: python <path to the .py file> . Parameters: - -m executes a module as a script, e.g. python -m venv . This is useful for executing scripts whithout knowing the path to the script.","title":"Command line"},{"location":"Programming/Python/Python%20Workflow/#installing-packages","text":"Normal packages are installed using: pip install <package name> . However, if a package uses a C/C++ backend and does not contain the compiled wheel on PyPy, this approach will fail on Windows. Instead, you have to download the wheel from the Chris Gohlke page and install it: pip install <path to wheel> . Also, you have to install the dependencies mentioned on that page.","title":"Installing Packages"},{"location":"Programming/Python/Python%20Workflow/#uninstalling-packages","text":"To uninstall a package, use pip uninstall <package name> . There is no way how to uninstall more packages using some wildcard. To uninstall more packages efficiently (not one by one): 1. create a file with the list of all installed packages: pip freeze > packages.txt 2. edit the file and remove all packages you want to keep 3. uninstall all packages from the file: pip uninstall -r packages.txt -y","title":"Uninstalling packages"},{"location":"Programming/Python/Python%20Workflow/#troubleshooting","text":"If the installation fails, check the following: 1. if you installed the package by name, check for the wheel on the Chris Golthke page. 2. if you installed the package from a wheel, check the notes/requirement info on Chris Golthke page 3. Check the log. Specifically, no building should appear there whatsoever. If a build starts, it means that some dependency that should be installed as a prebuild wheel is missing. Possible reasons: 1. you forget to install the dependency, go back to step 2 2. the dependency version does not correspond with the version required by the package you are installing. Check the log for the required version.","title":"Troubleshooting"},{"location":"Programming/Python/Python%20Workflow/#pycharm","text":"","title":"Pycharm"},{"location":"Programming/Python/Python%20Workflow/#configuration","text":"","title":"Configuration"},{"location":"Programming/Python/Python%20Workflow/#settings-synchronization","text":"Same as in IDEA: 1. Log in into JetBrains Toolbox or to the App 1. Click on the gear icon on the top-right and choose Sync 1. Check all categories and click on pull settings from the cloud","title":"Settings synchronization"},{"location":"Programming/Python/Python%20Workflow/#do-not-run-scripts-in-python-console-by-default","text":"Run configuration select box -> Edit Configurations... -> Edit configuration templates -> Python -> uncheck the Run with Python Console","title":"Do Not Run scripts in Python Console by Default"},{"location":"Programming/Python/Python%20Workflow/#enable-progress-bars-in-output-console","text":"Run configuration select box -> Edit Configurations... -> Select the configuration -> check the Emulate terminal in output console","title":"Enable Progress Bars in output console"},{"location":"Programming/Python/Python%20Workflow/#project-configuration","text":"configure the correct test suite in File -> Settings -> Tools -> Python Integrated Tools -> testing","title":"Project Configuration"},{"location":"Programming/Python/Python%20Workflow/#known-problems-solutions","text":"","title":"Known problems &amp; solutions"},{"location":"Programming/Python/Python%20Workflow/#non-deterministic-output-in-the-run-window","text":"Problem: It can happen that the output printing/logging can be reordered randomly (not matching the order of calls in the source, neither the system console output). Solution: Edit Configurations... -> select configuration for the script -> check Emulate terminal in output console .","title":"Non deterministic output in the run window"},{"location":"Programming/Python/Python%20Workflow/#pycharm-does-not-recognize-a-locally-installed-package","text":"It can happen that a locally installed package ( -e ) is not recognized by Pycharm. This can be solved by adding the path to the package to the interpreter paths: 1. File -> Settings -> Project: <project name> -> Python Interpreter 1. Click on the arrow next to the interpreter name and choose Show All... 1. Click on the desired interpreter and click on the filetree icon on the top of the window 1. Add the path to the package to the list of paths","title":"Pycharm does not recognize a locally installed package"},{"location":"Programming/Python/Python%20Workflow/#jupyter","text":"Jupyter can be used both in Pycharm and in a web browser.","title":"Jupyter"},{"location":"Programming/Python/Python%20Workflow/#jupyter-in-pycharm","text":"The key for the effectivity of Jupyter in Pycharm is using the command mode . To enter the command mode, press Esc . To enter the edit mode, pres enter.","title":"Jupyter in Pycharm"},{"location":"Programming/Python/Python%20Workflow/#command-mode-shortcuts","text":"m : change cell type to markdown Ctrl + C : copy cell Ctrl + V : paste cell Ctrl + Shift + Up : move cell up Ctrl + Shift + Down : move cell down Ctrl + - : split cell","title":"Command mode shortcuts"},{"location":"Programming/Python/Python%20Workflow/#web-browser-configuration","text":"","title":"Web Browser Configuration"},{"location":"Programming/Python/Python%20Workflow/#install-extension-manager-with-basic-extensions","text":"Best use the official installation guide . The extensions then can be toggled on in the Nbextensions tab in the jupyter homepage. Be sure to unselect the disable configuration for nbextensions without explicit compatibility (they may break your notebook environment, but can be useful to show for nbextension development) checkbox, otherwise, all extensions will be disabled.","title":"Install Extension Manager with basic extensions"},{"location":"Programming/Python/Python%20Workflow/#debugging","text":"Pycharm contains a good debuger for python code. However, it cannot step into most standard library functions, as those are native, implemented in C/C++. For that, we need mixed python/native debugging.","title":"Debugging"},{"location":"Programming/Python/Python%20Workflow/#testing","text":"To run pytest, simply go to the folder and run pytest . Arguments: - -x : stop on first failure","title":"Testing"},{"location":"Programming/Python/Python%20Workflow/#mixed-python-native-debugging","text":"In theory, there are two ways how to debug python native code: - use a tool that can step from python to C++ (only Visual Studio offers this possibility AFAIK) - use two debuggers, start with a python debugger, and attach a native debugger to the python process. This way, the debuggers can be independent. However, one needs to put a breakpoint in the native code, and for that, we need to know the location of the code that will be executed from python (non-trivial for standard library)","title":"Mixed Python-native debugging"},{"location":"Programming/Python/Python%20Workflow/#python-native-debugger-in-visual-studio","text":"First check and install the requirements: - Python 3.5-3.9, including debugging symbols - Python 3.10 is not compatible yet - in Visual Studio, a Python development component needs to be installed, including Python native development tools Then, configure the Visual studio: - Go to Tools -> Options -> Python -> Debugging and check: Enable debugging of the Python standard library . Finally, create a new Python project (either clean, or from existing code) and configure it: - use the interpreter directly, do not create a virtual environment - enable native code debugging: 1. Go to project properties -> Debug 2. Check enable native code debugging - use the -i flag to always see the debug console, even if the program ends without breaking 1. Go to project properties -> Debug 2. Add the -i flag to the Interpreter Arguments field","title":"Python-native debugger in Visual Studio"},{"location":"Programming/Python/Python%20Workflow/#known-issues","text":"The reference assemblies for .NETFramework,Version=v4.7.2 were not found. -> Install this component using the visual studio installer","title":"Known issues"},{"location":"Programming/Python/Python%20Workflow/#other-sources","text":"Microsoft official documentation Python tools for Visual Studio GitHub page","title":"Other Sources"},{"location":"Programming/Python/Python%20Workflow/#releasing-libraries-to-pypi","text":"","title":"Releasing libraries to PyPi"},{"location":"Programming/Python/Python%20Workflow/#steps","text":"add license headers: https://github.com/johann-petrak/licenseheaders check that setup.py contains all requirements - pipreqs release update the min version in dependencies","title":"Steps"},{"location":"Programming/Python/Python%20Workflow/#release","text":"raise version run sdist upload to pypi: twine upload dist/*","title":"Release"},{"location":"Programming/Web/CSS%20Manual/","text":"Layout options \u00b6 Grid Layout \u00b6 For complex pages. See the tutorial . Flex Layout \u00b6 For simpler pages. See the tutorial . Note that setting max_width: 100% for child elements of flex items does not work frequenly, so it's better to specify max_witdth ( SO ). Oldschool Layout \u00b6 Oldschool layout use floats. Very Oldschool Layout \u00b6 With tables... Practical Selectors \u00b6 Last Child of a Specific Parent \u00b6 #parent > :last-child Written with StackEdit .","title":"CSS Manual"},{"location":"Programming/Web/CSS%20Manual/#layout-options","text":"","title":"Layout options"},{"location":"Programming/Web/CSS%20Manual/#grid-layout","text":"For complex pages. See the tutorial .","title":"Grid Layout"},{"location":"Programming/Web/CSS%20Manual/#flex-layout","text":"For simpler pages. See the tutorial . Note that setting max_width: 100% for child elements of flex items does not work frequenly, so it's better to specify max_witdth ( SO ).","title":"Flex Layout"},{"location":"Programming/Web/CSS%20Manual/#oldschool-layout","text":"Oldschool layout use floats.","title":"Oldschool Layout"},{"location":"Programming/Web/CSS%20Manual/#very-oldschool-layout","text":"With tables...","title":"Very Oldschool Layout"},{"location":"Programming/Web/CSS%20Manual/#practical-selectors","text":"","title":"Practical Selectors"},{"location":"Programming/Web/CSS%20Manual/#last-child-of-a-specific-parent","text":"#parent > :last-child Written with StackEdit .","title":"Last Child of a Specific Parent"},{"location":"Programming/Web/JavaScript/","text":"","title":"JavaScript"},{"location":"Programming/Web/Jekyll/","text":"Local testing \u00b6 Install Ruby Download dependencies using bundle command Run bundle exec jekyll serve to start local server Directory structure \u00b6 Jekyll has a predefined directory structure. The important directories are: - _posts : Contains all the posts. The file name should be in the format YYYY-MM-DD-title.md . - _pages : Contains all the pages. The file name should be in the format title.md . - _config.yml : Contains the configuration of the site. Configuration \u00b6 The configuration can be set in _config.yml file. Impotant options are: Markdown \u00b6 Code blocks \u00b6 Markdown code blocks are supported by Jekyll. The syntax is: ```language code \\``` Note that for Jekyll, the language name is case sensitive . For example, java is correct, but Java is not. This is in contrast to GitHub markdown, where the language name is case insensitive. Plugins \u00b6 Jekyll functionality can be extended using plugins. Plugins are Ruby gems, which should be added to the Gemfile and also to the _config.yml file. Note that GitHub Pages supports only a limited number of plugins . The list of supported plugins can be found here .","title":"Jekyll"},{"location":"Programming/Web/Jekyll/#local-testing","text":"Install Ruby Download dependencies using bundle command Run bundle exec jekyll serve to start local server","title":"Local testing"},{"location":"Programming/Web/Jekyll/#directory-structure","text":"Jekyll has a predefined directory structure. The important directories are: - _posts : Contains all the posts. The file name should be in the format YYYY-MM-DD-title.md . - _pages : Contains all the pages. The file name should be in the format title.md . - _config.yml : Contains the configuration of the site.","title":"Directory structure"},{"location":"Programming/Web/Jekyll/#configuration","text":"The configuration can be set in _config.yml file. Impotant options are:","title":"Configuration"},{"location":"Programming/Web/Jekyll/#markdown","text":"","title":"Markdown"},{"location":"Programming/Web/Jekyll/#code-blocks","text":"Markdown code blocks are supported by Jekyll. The syntax is: ```language code \\``` Note that for Jekyll, the language name is case sensitive . For example, java is correct, but Java is not. This is in contrast to GitHub markdown, where the language name is case insensitive.","title":"Code blocks"},{"location":"Programming/Web/Jekyll/#plugins","text":"Jekyll functionality can be extended using plugins. Plugins are Ruby gems, which should be added to the Gemfile and also to the _config.yml file. Note that GitHub Pages supports only a limited number of plugins . The list of supported plugins can be found here .","title":"Plugins"},{"location":"Programming/Web/htaccess/","text":"kakpsatweb","title":"htaccess"},{"location":"Programming/Web/mkdocs/","text":"update steps: 1. generate navigation: python .\\generate_nav.py 1. generate site: mkdocs build 1. deploy: mkdocs gh-deploy`","title":"mkdocs"},{"location":"Ubuntu/Linux%20Manual/","text":"Usefull Comands \u00b6 Check Ubuntu Version \u00b6 lsb_release -a Printing used ports \u00b6 sudo lsof -i -P Checking GLIBC version \u00b6 ldd --version Exit codes \u00b6 Exit code of the last command: $? Common exit codes for C++ programs Show path to executable \u00b6 Use the which command: which <executable> Unpack file \u00b6 Foe unpacking, you can use the tar -f <file> command. The most used options are: - x : extract Environment Variables \u00b6 The environment variables are introduced with the export command: export <variable>=<value> without export, the variable is just a local shell variable: <variable>=<value> # local variable We will demonstrate the work with environment variables on the PATH example. If you have a program in a custom location, adding it to $PATH permanently and be able to run it under all circumstances is not an easy task on linux. Standard procedure is to add a system variable: 1. Create a dedicated .sh file in /etc/profile.d. for your configuration (config for each app should be stored in a separate file). 2. the file should contain: export PATH=$PATH:YOURPATH 1. exit nano and save the file: ctrl+x and y 3. logout and login again to load the newly added varibales - for WSL close the console and reopen it, it is not necessary to restart the WSL ( click here for detailed description ) Enable Variable with sudo \u00b6 To enable the variable even if you use sudo , you need to edit sudo config using sudo visudo and: 1. exclude PATH from variable being reset when running sudo : Defaults env_keep += \"PATH\" 2. disable the safe path mechanism for sudo , i.e., comment the line: Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin 3. logout and login again to load the new added varibales Enable the Variable Outside Bash \u00b6 If you need the variable outside bash, the above-mentioned approach won\u2019t work. Currently, I do not know about any general solution for variables. The solution below, unfortunately, work only for PAM shells (see this SO answer why). Add the variable to /etc/environment . Note that it is not a script file, so you can use only simple variable assignments. Enable Variable on a Device Without Root Access \u00b6 Without root access, we can only edit the user config files. put the necessary config into: - ~/.bash_profile if it already exists - or to ~/.profile Note that the .profile file is ignored when the .bash_profile file exists. Remove Windows $PATH from WSL \u00b6 By default, the PATH environment variable from Windows is inluded in the PATH variable in Ubuntu running in WSL. This can lead to conflicts, when the same executables are used in both systems (e.g., vcpkg, cmake). To turn of the Windows PATH inclusion: 1. open /etc/wsl.conf 1. add the fllowing code: [interop] appendWindowsPath = false restart WSL File System \u00b6 copy file \u00b6 The cp command is used to copy files: cp <source> <destination> . The most used options are: - -r , -R : copy recursively - -v : verbose - -f : force - -p : preserve permissions and timestamps - -a : same as -p -R plus some other options For more sophisticated copying, use rsync : rsync <source> <destination> . The most used options are: - -h : human readable - -a : archive mode, equivalent to -rlptgoD - --progress : show progress Remove file \u00b6 The rm command is used to remove files. The most used options are: - -r , -R : remove recursively To remove all files in a directory, you can use Access rights \u00b6 The Linux access rights use the same system for files and folders. The access rights are divided into three groups, from left to right: - owner - group - other Each group has three possible access rights: - r : read - w : write - x : execute Important aspects: - to access a directory, the user has to have the x right on the directory. - to access a file, the user has to have the x right on all folders in the path to the file. Compute directory size \u00b6 To compute the size of a directory, use the du command: du <path> . The most used options are: - -h : human readable - -s : summarize Find files \u00b6 To find files, use the find command: find <where> <options> <params> . The most used options are: - -name : find by name. This option should be followed by a file name pattern. - -path : find by path. This option should be followed by a path pattern. Network \u00b6 netstat \u00b6 The netstat command is the basic command to monitor the networ. It displays the TCP connections. It is available both on Linux and on Windows, although the interface differs. Important parameters: - -n : do not translate IP and ports into human readable names - -a : show all connections. Without this, some connections can be skipped. SSH \u00b6 The SSH is a protocol for secure communication between two computers. The basic usage is to connect to a remote computer: ssh <username>@<address> To close the connection, type exit to the console and press enter. If we do not want to establish a connection, but just run a single command, we can add the command at the end of the ssh command: ssh <username>@<address> <command> SSH Tunneling \u00b6 An SSH tunnel can be created by the ssh command. The usuall syntax is following: ssh -L <local port>:<remote machine>:<remote port> <ssh server username>@<ssh server address> The -L argument stands for local port forwarding , i.e., we forward a local port to some remote port. Example: ssh -L 1111:localhost:5432 fiedler@its.fel.cvut.cz The local port (here 1111 ) is arbitrary, we can use any free port. The aplications using the tunnel should then be configured as: - host= localhost - port:= 1111 The remote machine is the address of the machine we want to access relative to the ssh server . If the ssh server is running on the computer we want to access through the tunnel, we can use localhost . Analogously, the remote port is the port we wan to use on the remote machine (here 5432 is a PostgreSQL db port). The ssh server username and ssh server address are then the username/address of the remote machine. On top of that, we need password or priveta key to validate our identity. Note that the credential here are the credential for the server, not the credentials of the service we are accessing through the ssh connection. Those credentials has to be usually supplied by the application accessing the service through the ssh tunnel. The connection can be canceled any time byping exit to the console. More information Debugging a SSH tunnel \u00b6 This guide suppose that the tunnel creation comman run without any error message. 1. If the tunnel seems to not work, first use a command line tool to be sure: - web browser for HTTP tunnels (remote port 80) - psql for postgeSQL tunnels (remote port 5432) - telnet for telnet tunnels (reote port 23) 2. If the test is unsucessful, try to kill all ssh connections to the server by shutting down all applications with ssh connections tunnels, untill there will be only one connection at the server (the console). The number of ssh connections can be checked with: sudo netstat -nap | grep :22 Enabnling SSH Access on Server \u00b6 install openssh: sudo apt update sudo apt install openssh-server configure access password: open /etc/ssh/sshd_config set PasswordAuothentication yes after restrat you can log in with the user and password used in Ubuntu keys: TODO restart the ssh server: sudo service ssh restart WSL configuration \u00b6 port 22 can be used on Windows, so change the port to 2222 in sshd_config when loging from Windows use 127.0.0.1 as a host Debugging \u00b6 1n from now e as a t euin. check the ssh status with: s th service ssh status 2. check the ssh porot with sudso netstat t-tpln SSH Agent \u00b6 Normally, the SSH client process runs only while the SSH session is active, then, it is terminated. That means that we have to reenter the passphrase for the private key every time we want to connect to the server. To overcome this limitation, we can use the SSH agent programe. An SSH agent is a process for storing decrypted SSH keys in memory. This means that we have to enter the passphrase only once per each OS login. The agent can be configured to automatically run on OS startup. The default SSH agent is ssh-agent , the rest of the section is dedicated to this agent. Listing keys \u00b6 To list the keys stored in the agent, run: ssh-add -l Adding keys \u00b6 To add a key to the agent, run: ssh-add <path to key> Debuging \u00b6 If the agent is running and the key is listed, the first thing to try is to connect via ssh to see whether it is an agent/ssh issue or an issue of the program using the SSH (like git, IDE, file manager...) Configuration \u00b6 For configuration, we can use the git config command. There are three levels of configuration: - system : the configuration is applied to all users on the system. This configuration is set during the installation of git. - global : the configuration is applied to all repositories of the current user. This configuration is set by the --global parameter. - local : the configuration is applied only to the current repository. This configuration is set by the --local parameter. To list the configuration, use the -l / --list parameter of the git config command. To list the configuration for a specific level, use the --system , --global , --local parameters. To see the default value of a configuration, search in the git config documentation . known_hosts file \u00b6 To know that a connection leads to the desired server and not to some impersonator, the server sends its public key to the client. The client then checks the public key against the list of keys previously set as valid. This list is stored in the .ssh/known_hosts file. The format of the file is: <server address> <key type> <key> Each line contains one server record. What is confusing here is that each server can have multiple records, due to: - different key type (e.g., RSA, ECDSA) - key for both host name and IP address (e.g., github.com and 140.82.121.3 ) It is important to delete/updete all of these recorsds in case the server change its keys. More info is at this SO answer . Screen: executing a long running process over SSH \u00b6 When the SSH connection to a server is disconnected (either manually, or by network failure or timeout), the process running in the console is canceled. To overcome this limitation, we can use the screen command, which is especially usefull for long running processes. A typical workflow can look like this: 1. execute screen to start the screen session 2. run the long running command 3. disconnect 4. connect to the server again 5. run screen -r to recconect to the session and see the results of the command. Sometimes, the server does not detect the connection failure and do not allow you to resume the session (step 5). In this way, we need to find the screen session ID and perform a detach and atach: 6. screen -ls 7. read the ID from the output and exec screen -rd <ID> Copying files over SSH using scp \u00b6 The scp command is used to copy files over SSH. The syntax is: scp <source> <destination> The <source> and <destination> can be either local or remote. The remote files are specified using the <username>@<address>:<path> syntax. Note that if the remote path contains spaces, double quoting is necessary , one for local and one for remote: scp <source> \"<username>@<address>:'<path with spaces>'\" Problems \u00b6 protocol error: filename does not match request : This error is triggered if the path contains unexpected characters. Sometimes, it can be triggered even for correct path, if the local console does not match the remote console. In that case, the solution is to use the -T parameter to disable the security check. WSL configuration \u00b6 port 22 can be used on Windows, so change port to 2222 in sshd_config when loging from Windows use 127.0.0.1 as a host Debugging \u00b6 check the ssh status with: service ssh status check the ssh port with sudo netstat -tpln Bash \u00b6 General Remarks \u00b6 It's important to use Linux newlines, otherwise, bash scripts will fail with unexpected character error Empty constructs are not allowed, i.e, empty function or loop results in an error brackets needs spaces around them, otherwise, there will be a syntax error Working with I/O \u00b6 Output Forwarding \u00b6 Output forwarding is a process of redirecting the output of a command to an input of another command. The operator for that is the pipe | . The syntax is: <command 1> | <command 2> Note that the content of the pipe cannot be examined, the process on the right hand side consume it. Therefore, it is not possible to simply branch on pipe content while use it in the subsequent process. Output Redirection \u00b6 Output redirection is a process of redirecting the output (stdout, stderr,..) from the console to a file. The syntax is: <command> <operator> <file> The possible operators and their effects are listed in the table (full explanation on SO ) below: Operator Stdout Stderr Mode (in file) > file console overwrite >> file console append &> file file overwrite &>> file file append 2> console file overwrite 2>> console file append \\| tee both console overwrite \\| tee -a both console append \\|& tee both both overwrite \\|& tee -a both both append tee is actully a command, not an operator. It is used as follows: <command> | tee <file> We can also use tee to forward the output of a command to multiple commands: <command 1> | tee >(<command 2>) | <command 3> This forward <command 1> to both <command 2> and <command 3> . Use bash variables as input \u00b6 Bash variables can be used as input of a command. Syntax: <command> <<< $<variable>` Command Substitution \u00b6 When we need to use an output of a command instead of a constant or variable, we have to use command substtution: `<command>` # or equivalently $(<command>) e.g.: echo resut is: `cut -d, -f 7` # or equivalently echo resut is: $(cut -d, -f 7) Bash Script Arguments \u00b6 We refer the arguments of a bash script as - $0 - the name of the script - $1..$n - the arguments of the script - $@ - all the arguments of the script Conditions \u00b6 In general, condition in bash has the following syntax: if <condition> then <command> else <command> fi The condition above uses the exit code of a command to determine the logical value. When testing that the command succeeded, we can use it like: if <command> then ... fi Example: if grep -q \"$text\" $file then ... fi But if we want to use some arbitrary value (e. g. the return value of a command), or comparisons in the condition (simmilar to programming languages), we have to use the test command: if test <comparison> then ... fi The test command is also invoked if we use square brackets: if [ <comparison> ] then ... fi Example: if [ $var = 1 ] then ... fi The test command expects to compare to values. If we want to compare a result of some command, we need to usethe command substtitution. Loops \u00b6 while condition do <command1> <command2> ... done Forward to loop \u00b6 We can forward an input into while loop using | as usuall. Additionally, it is possible to read from file directly by adding < to the end like this: while condition do <command1> <command2> ... done < <input> The same goes for the output, i.e., we can forward th outut of a loop with | . Strings literals \u00b6 String literals can be easily defined as: str=\"string literal\" # or equivalently str='string literal' If we use double quotes, the variables are expanded, e.g., echo \"Hello $USER\" will print Hello <username> . The problem arises when we want to use double quotes in the string literal containing variables, e.g., normal string \"quotted string\" $myvar . In this case, we have to use quite cumbersome syntax: a = \"normal string \"\\\"\"quotted string\"\\\" # or equivalently a = \"normal string \"'\"'\"quotted string\"'\"' # Managing packages with `apt` and `dpkg` To **update the list** of possible updates: ```bash sudo apt update To perform the update : sudo apt upgrade To find the location of an installed package : dpkg -L <package> To search for a package: apt-cache search <package> Changing package repositories \u00b6 If the downolad speed is not satisfactory, we can change the repositories. To find the fastest repository from the list of nearby repositories, run: curl -s http://mirrors.ubuntu.com/mirrors.txt | xargs -n1 -I {} sh -c 'echo `curl -r 0-10240000 -s -w %{speed_download} -o /dev/null {}/ls-lR.gz` {}' | sort -g -r The number in the leftmost column indicates the bandwidth in bytes (larger number is better). To change the repositories to the best mirror, we need to replace the mirror in etc/apt/source.list . We can do it manually, however, to prevent the mistakes, it is better to use a dedicated python script: apt-mirror-updater . Steps: 1. install the python script: sudo pip install apt-mirror-updater 1. backup the old file: sudo cp sources.list sources.list.bak 1. change the mirror with the script: apt-mirror-updater -c <mirror URL> Note that the apt-mirror-updater script can also measure the bandwidth, however, the result does not seem to be reliable. String Processing \u00b6 Word count with wc \u00b6 The wc command counts words, lines, and characters. What is counted is determined by the parameters: - -w : words - -l : lines - -c : characters String mofification with sed \u00b6 Sed is a multi purpose command for string modification. It can search and replace in string. The syntax is folowing: sed s/<search>/<replace>/[<occurance>] Example: s/$/,3/ Any regex can be used as <search> . Some characters (e.g. | ) must be escaped with backslash and used together with the -E parameter. This replace the nd of the line with string \",3\" . Note that there is a slash at the end, despite we use the default option for occurance. Also, it can delete lines containing string using: sed /<pattern>/d cut \u00b6 Cut is a useful command for data with delimiters. Usage: cut -f <columns> Where columns are splited by comma, e.g., 2,4,7 . If we need to specify delimiters, we use the -d parameter: cut -d, -f 1,5 AWK \u00b6 AWK is a powerful tool for text processing. It is a programming language, so it can be used for more complex tasks than sed or cut . The basic syntax is: awk '<pattern> {<action>}' Where <pattern> is a regex and <action> is a command. The <action> is executed only if the line matches the <pattern> . Pattern \u00b6 In the awk , / is used as a delimiter of the regex pattern. Action \u00b6 The <action> can be a sequence of commands, separated by ; . We can use column values by using special column variables: - $0 : the whole line - $1 : the first column ... Trim string \u00b6 <command with string output> | xargs Processes \u00b6 for checking all processes, we can use htop for checking a specific process, we can use ps to kill a process, we can use kill to kill a process by name, we can use pkill to get information about a process selected by name, we can use pgrep pkill \u00b6 The pkill command kills a process by name. The syntax is: pkill <process name> important parameters: - -f : match the whole command line, not only the process name - -9 : force kill Process Info \u00b6 Installing Java \u00b6 Oracle JDK \u00b6 Go to the download page, the link to the dowload page for current version of java is on the main JDK page . Click on the debian package, accept the license, and download it. If installing on system without GUI, copy now (after accepting the license) the target link and dowload the debian package with wget : wget --header \"Cookie: oraclelicense=accept-securebackup-cookie\" <COPIED LINK> . More info on SO . Install the package with sudo apt-get install <PATH TO DOWNLOADED .deb> if there is a problem with the isntallation, check the file integritiy with: sha256 <PATH TO DOWNLOADED .deb> . It should match with the checksums refered on the download page. If not cheksums do not match, go back to download step. In case there is another version of Java alreadz install, we need to overwrite it using the update-alternatives command: sudo update-alternatives --install /usr/bin/java java <PATH TO JAVA> <PRIORITY> . Example: sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk-16/bin/java 2 To check the priorities, call update-alternatives --query java . The newly installed JDK should have the highest priority. Python \u00b6 Python has to be executed with python3 by default, instead of python . Other usefull commands \u00b6 Selecting lines from file \u00b6 The head and tail commands are for that, printing the top and bottom 10 lines, respectively. Skip the header \u00b6 tail -n +2 Print lines from to \u00b6 tail -n +<from line> | head -n <number of lines> Progress bar \u00b6 the progress bar can be printed using the pv command. pv <file> | <other comands> # or <other comands> | pv | <other comands> Free disk space \u00b6 df -h piping parameters using xargs \u00b6 The xargs command transfers the output of one command into call of another command with the output of the first command as parameters of the second command. This is usefull when the second command does not accept the output of the first command as input, but accepts the output as parameters. Example: ls | xargs rm # remove all files in the current directory Upgrade \u00b6 First run the update of the current version . Then optionaly backup the WSL run sudo do-release-upgrade WSL backup \u00b6 check the WSL distro name: wsl -l -v shutdown WSL: wsl --shutdown backup the distro: wsl --export <disto name> <backup folder path>/<backup name>.tar","title":"Linux Manual"},{"location":"Ubuntu/Linux%20Manual/#usefull-comands","text":"","title":"Usefull Comands"},{"location":"Ubuntu/Linux%20Manual/#check-ubuntu-version","text":"lsb_release -a","title":"Check Ubuntu Version"},{"location":"Ubuntu/Linux%20Manual/#printing-used-ports","text":"sudo lsof -i -P","title":"Printing used ports"},{"location":"Ubuntu/Linux%20Manual/#checking-glibc-version","text":"ldd --version","title":"Checking GLIBC version"},{"location":"Ubuntu/Linux%20Manual/#exit-codes","text":"Exit code of the last command: $? Common exit codes for C++ programs","title":"Exit codes"},{"location":"Ubuntu/Linux%20Manual/#show-path-to-executable","text":"Use the which command: which <executable>","title":"Show path to executable"},{"location":"Ubuntu/Linux%20Manual/#unpack-file","text":"Foe unpacking, you can use the tar -f <file> command. The most used options are: - x : extract","title":"Unpack file"},{"location":"Ubuntu/Linux%20Manual/#environment-variables","text":"The environment variables are introduced with the export command: export <variable>=<value> without export, the variable is just a local shell variable: <variable>=<value> # local variable We will demonstrate the work with environment variables on the PATH example. If you have a program in a custom location, adding it to $PATH permanently and be able to run it under all circumstances is not an easy task on linux. Standard procedure is to add a system variable: 1. Create a dedicated .sh file in /etc/profile.d. for your configuration (config for each app should be stored in a separate file). 2. the file should contain: export PATH=$PATH:YOURPATH 1. exit nano and save the file: ctrl+x and y 3. logout and login again to load the newly added varibales - for WSL close the console and reopen it, it is not necessary to restart the WSL ( click here for detailed description )","title":"Environment Variables"},{"location":"Ubuntu/Linux%20Manual/#enable-variable-with-sudo","text":"To enable the variable even if you use sudo , you need to edit sudo config using sudo visudo and: 1. exclude PATH from variable being reset when running sudo : Defaults env_keep += \"PATH\" 2. disable the safe path mechanism for sudo , i.e., comment the line: Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin 3. logout and login again to load the new added varibales","title":"Enable Variable with sudo"},{"location":"Ubuntu/Linux%20Manual/#enable-the-variable-outside-bash","text":"If you need the variable outside bash, the above-mentioned approach won\u2019t work. Currently, I do not know about any general solution for variables. The solution below, unfortunately, work only for PAM shells (see this SO answer why). Add the variable to /etc/environment . Note that it is not a script file, so you can use only simple variable assignments.","title":"Enable the Variable Outside Bash"},{"location":"Ubuntu/Linux%20Manual/#enable-variable-on-a-device-without-root-access","text":"Without root access, we can only edit the user config files. put the necessary config into: - ~/.bash_profile if it already exists - or to ~/.profile Note that the .profile file is ignored when the .bash_profile file exists.","title":"Enable Variable on a Device Without Root Access"},{"location":"Ubuntu/Linux%20Manual/#remove-windows-path-from-wsl","text":"By default, the PATH environment variable from Windows is inluded in the PATH variable in Ubuntu running in WSL. This can lead to conflicts, when the same executables are used in both systems (e.g., vcpkg, cmake). To turn of the Windows PATH inclusion: 1. open /etc/wsl.conf 1. add the fllowing code: [interop] appendWindowsPath = false restart WSL","title":"Remove Windows $PATH from WSL"},{"location":"Ubuntu/Linux%20Manual/#file-system","text":"","title":"File System"},{"location":"Ubuntu/Linux%20Manual/#copy-file","text":"The cp command is used to copy files: cp <source> <destination> . The most used options are: - -r , -R : copy recursively - -v : verbose - -f : force - -p : preserve permissions and timestamps - -a : same as -p -R plus some other options For more sophisticated copying, use rsync : rsync <source> <destination> . The most used options are: - -h : human readable - -a : archive mode, equivalent to -rlptgoD - --progress : show progress","title":"copy file"},{"location":"Ubuntu/Linux%20Manual/#remove-file","text":"The rm command is used to remove files. The most used options are: - -r , -R : remove recursively To remove all files in a directory, you can use","title":"Remove file"},{"location":"Ubuntu/Linux%20Manual/#access-rights","text":"The Linux access rights use the same system for files and folders. The access rights are divided into three groups, from left to right: - owner - group - other Each group has three possible access rights: - r : read - w : write - x : execute Important aspects: - to access a directory, the user has to have the x right on the directory. - to access a file, the user has to have the x right on all folders in the path to the file.","title":"Access rights"},{"location":"Ubuntu/Linux%20Manual/#compute-directory-size","text":"To compute the size of a directory, use the du command: du <path> . The most used options are: - -h : human readable - -s : summarize","title":"Compute directory size"},{"location":"Ubuntu/Linux%20Manual/#find-files","text":"To find files, use the find command: find <where> <options> <params> . The most used options are: - -name : find by name. This option should be followed by a file name pattern. - -path : find by path. This option should be followed by a path pattern.","title":"Find files"},{"location":"Ubuntu/Linux%20Manual/#network","text":"","title":"Network"},{"location":"Ubuntu/Linux%20Manual/#netstat","text":"The netstat command is the basic command to monitor the networ. It displays the TCP connections. It is available both on Linux and on Windows, although the interface differs. Important parameters: - -n : do not translate IP and ports into human readable names - -a : show all connections. Without this, some connections can be skipped.","title":"netstat"},{"location":"Ubuntu/Linux%20Manual/#ssh","text":"The SSH is a protocol for secure communication between two computers. The basic usage is to connect to a remote computer: ssh <username>@<address> To close the connection, type exit to the console and press enter. If we do not want to establish a connection, but just run a single command, we can add the command at the end of the ssh command: ssh <username>@<address> <command>","title":"SSH"},{"location":"Ubuntu/Linux%20Manual/#ssh-tunneling","text":"An SSH tunnel can be created by the ssh command. The usuall syntax is following: ssh -L <local port>:<remote machine>:<remote port> <ssh server username>@<ssh server address> The -L argument stands for local port forwarding , i.e., we forward a local port to some remote port. Example: ssh -L 1111:localhost:5432 fiedler@its.fel.cvut.cz The local port (here 1111 ) is arbitrary, we can use any free port. The aplications using the tunnel should then be configured as: - host= localhost - port:= 1111 The remote machine is the address of the machine we want to access relative to the ssh server . If the ssh server is running on the computer we want to access through the tunnel, we can use localhost . Analogously, the remote port is the port we wan to use on the remote machine (here 5432 is a PostgreSQL db port). The ssh server username and ssh server address are then the username/address of the remote machine. On top of that, we need password or priveta key to validate our identity. Note that the credential here are the credential for the server, not the credentials of the service we are accessing through the ssh connection. Those credentials has to be usually supplied by the application accessing the service through the ssh tunnel. The connection can be canceled any time byping exit to the console. More information","title":"SSH Tunneling"},{"location":"Ubuntu/Linux%20Manual/#debugging-a-ssh-tunnel","text":"This guide suppose that the tunnel creation comman run without any error message. 1. If the tunnel seems to not work, first use a command line tool to be sure: - web browser for HTTP tunnels (remote port 80) - psql for postgeSQL tunnels (remote port 5432) - telnet for telnet tunnels (reote port 23) 2. If the test is unsucessful, try to kill all ssh connections to the server by shutting down all applications with ssh connections tunnels, untill there will be only one connection at the server (the console). The number of ssh connections can be checked with: sudo netstat -nap | grep :22","title":"Debugging a SSH tunnel"},{"location":"Ubuntu/Linux%20Manual/#enabnling-ssh-access-on-server","text":"install openssh: sudo apt update sudo apt install openssh-server configure access password: open /etc/ssh/sshd_config set PasswordAuothentication yes after restrat you can log in with the user and password used in Ubuntu keys: TODO restart the ssh server: sudo service ssh restart","title":"Enabnling SSH Access on Server"},{"location":"Ubuntu/Linux%20Manual/#wsl-configuration","text":"port 22 can be used on Windows, so change the port to 2222 in sshd_config when loging from Windows use 127.0.0.1 as a host","title":"WSL configuration"},{"location":"Ubuntu/Linux%20Manual/#debugging","text":"1n from now e as a t euin. check the ssh status with: s th service ssh status 2. check the ssh porot with sudso netstat t-tpln","title":"Debugging"},{"location":"Ubuntu/Linux%20Manual/#ssh-agent","text":"Normally, the SSH client process runs only while the SSH session is active, then, it is terminated. That means that we have to reenter the passphrase for the private key every time we want to connect to the server. To overcome this limitation, we can use the SSH agent programe. An SSH agent is a process for storing decrypted SSH keys in memory. This means that we have to enter the passphrase only once per each OS login. The agent can be configured to automatically run on OS startup. The default SSH agent is ssh-agent , the rest of the section is dedicated to this agent.","title":"SSH Agent"},{"location":"Ubuntu/Linux%20Manual/#listing-keys","text":"To list the keys stored in the agent, run: ssh-add -l","title":"Listing keys"},{"location":"Ubuntu/Linux%20Manual/#adding-keys","text":"To add a key to the agent, run: ssh-add <path to key>","title":"Adding keys"},{"location":"Ubuntu/Linux%20Manual/#debuging","text":"If the agent is running and the key is listed, the first thing to try is to connect via ssh to see whether it is an agent/ssh issue or an issue of the program using the SSH (like git, IDE, file manager...)","title":"Debuging"},{"location":"Ubuntu/Linux%20Manual/#configuration","text":"For configuration, we can use the git config command. There are three levels of configuration: - system : the configuration is applied to all users on the system. This configuration is set during the installation of git. - global : the configuration is applied to all repositories of the current user. This configuration is set by the --global parameter. - local : the configuration is applied only to the current repository. This configuration is set by the --local parameter. To list the configuration, use the -l / --list parameter of the git config command. To list the configuration for a specific level, use the --system , --global , --local parameters. To see the default value of a configuration, search in the git config documentation .","title":"Configuration"},{"location":"Ubuntu/Linux%20Manual/#known_hosts-file","text":"To know that a connection leads to the desired server and not to some impersonator, the server sends its public key to the client. The client then checks the public key against the list of keys previously set as valid. This list is stored in the .ssh/known_hosts file. The format of the file is: <server address> <key type> <key> Each line contains one server record. What is confusing here is that each server can have multiple records, due to: - different key type (e.g., RSA, ECDSA) - key for both host name and IP address (e.g., github.com and 140.82.121.3 ) It is important to delete/updete all of these recorsds in case the server change its keys. More info is at this SO answer .","title":"known_hosts file"},{"location":"Ubuntu/Linux%20Manual/#screen-executing-a-long-running-process-over-ssh","text":"When the SSH connection to a server is disconnected (either manually, or by network failure or timeout), the process running in the console is canceled. To overcome this limitation, we can use the screen command, which is especially usefull for long running processes. A typical workflow can look like this: 1. execute screen to start the screen session 2. run the long running command 3. disconnect 4. connect to the server again 5. run screen -r to recconect to the session and see the results of the command. Sometimes, the server does not detect the connection failure and do not allow you to resume the session (step 5). In this way, we need to find the screen session ID and perform a detach and atach: 6. screen -ls 7. read the ID from the output and exec screen -rd <ID>","title":"Screen: executing a long running process over SSH"},{"location":"Ubuntu/Linux%20Manual/#copying-files-over-ssh-using-scp","text":"The scp command is used to copy files over SSH. The syntax is: scp <source> <destination> The <source> and <destination> can be either local or remote. The remote files are specified using the <username>@<address>:<path> syntax. Note that if the remote path contains spaces, double quoting is necessary , one for local and one for remote: scp <source> \"<username>@<address>:'<path with spaces>'\"","title":"Copying files over SSH using scp"},{"location":"Ubuntu/Linux%20Manual/#problems","text":"protocol error: filename does not match request : This error is triggered if the path contains unexpected characters. Sometimes, it can be triggered even for correct path, if the local console does not match the remote console. In that case, the solution is to use the -T parameter to disable the security check.","title":"Problems"},{"location":"Ubuntu/Linux%20Manual/#wsl-configuration_1","text":"port 22 can be used on Windows, so change port to 2222 in sshd_config when loging from Windows use 127.0.0.1 as a host","title":"WSL configuration"},{"location":"Ubuntu/Linux%20Manual/#debugging_1","text":"check the ssh status with: service ssh status check the ssh port with sudo netstat -tpln","title":"Debugging"},{"location":"Ubuntu/Linux%20Manual/#bash","text":"","title":"Bash"},{"location":"Ubuntu/Linux%20Manual/#general-remarks","text":"It's important to use Linux newlines, otherwise, bash scripts will fail with unexpected character error Empty constructs are not allowed, i.e, empty function or loop results in an error brackets needs spaces around them, otherwise, there will be a syntax error","title":"General Remarks"},{"location":"Ubuntu/Linux%20Manual/#working-with-io","text":"","title":"Working with I/O"},{"location":"Ubuntu/Linux%20Manual/#output-forwarding","text":"Output forwarding is a process of redirecting the output of a command to an input of another command. The operator for that is the pipe | . The syntax is: <command 1> | <command 2> Note that the content of the pipe cannot be examined, the process on the right hand side consume it. Therefore, it is not possible to simply branch on pipe content while use it in the subsequent process.","title":"Output Forwarding"},{"location":"Ubuntu/Linux%20Manual/#output-redirection","text":"Output redirection is a process of redirecting the output (stdout, stderr,..) from the console to a file. The syntax is: <command> <operator> <file> The possible operators and their effects are listed in the table (full explanation on SO ) below: Operator Stdout Stderr Mode (in file) > file console overwrite >> file console append &> file file overwrite &>> file file append 2> console file overwrite 2>> console file append \\| tee both console overwrite \\| tee -a both console append \\|& tee both both overwrite \\|& tee -a both both append tee is actully a command, not an operator. It is used as follows: <command> | tee <file> We can also use tee to forward the output of a command to multiple commands: <command 1> | tee >(<command 2>) | <command 3> This forward <command 1> to both <command 2> and <command 3> .","title":"Output Redirection"},{"location":"Ubuntu/Linux%20Manual/#use-bash-variables-as-input","text":"Bash variables can be used as input of a command. Syntax: <command> <<< $<variable>`","title":"Use bash variables as input"},{"location":"Ubuntu/Linux%20Manual/#command-substitution","text":"When we need to use an output of a command instead of a constant or variable, we have to use command substtution: `<command>` # or equivalently $(<command>) e.g.: echo resut is: `cut -d, -f 7` # or equivalently echo resut is: $(cut -d, -f 7)","title":"Command Substitution"},{"location":"Ubuntu/Linux%20Manual/#bash-script-arguments","text":"We refer the arguments of a bash script as - $0 - the name of the script - $1..$n - the arguments of the script - $@ - all the arguments of the script","title":"Bash Script Arguments"},{"location":"Ubuntu/Linux%20Manual/#conditions","text":"In general, condition in bash has the following syntax: if <condition> then <command> else <command> fi The condition above uses the exit code of a command to determine the logical value. When testing that the command succeeded, we can use it like: if <command> then ... fi Example: if grep -q \"$text\" $file then ... fi But if we want to use some arbitrary value (e. g. the return value of a command), or comparisons in the condition (simmilar to programming languages), we have to use the test command: if test <comparison> then ... fi The test command is also invoked if we use square brackets: if [ <comparison> ] then ... fi Example: if [ $var = 1 ] then ... fi The test command expects to compare to values. If we want to compare a result of some command, we need to usethe command substtitution.","title":"Conditions"},{"location":"Ubuntu/Linux%20Manual/#loops","text":"while condition do <command1> <command2> ... done","title":"Loops"},{"location":"Ubuntu/Linux%20Manual/#forward-to-loop","text":"We can forward an input into while loop using | as usuall. Additionally, it is possible to read from file directly by adding < to the end like this: while condition do <command1> <command2> ... done < <input> The same goes for the output, i.e., we can forward th outut of a loop with | .","title":"Forward to loop"},{"location":"Ubuntu/Linux%20Manual/#strings-literals","text":"String literals can be easily defined as: str=\"string literal\" # or equivalently str='string literal' If we use double quotes, the variables are expanded, e.g., echo \"Hello $USER\" will print Hello <username> . The problem arises when we want to use double quotes in the string literal containing variables, e.g., normal string \"quotted string\" $myvar . In this case, we have to use quite cumbersome syntax: a = \"normal string \"\\\"\"quotted string\"\\\" # or equivalently a = \"normal string \"'\"'\"quotted string\"'\"' # Managing packages with `apt` and `dpkg` To **update the list** of possible updates: ```bash sudo apt update To perform the update : sudo apt upgrade To find the location of an installed package : dpkg -L <package> To search for a package: apt-cache search <package>","title":"Strings literals"},{"location":"Ubuntu/Linux%20Manual/#changing-package-repositories","text":"If the downolad speed is not satisfactory, we can change the repositories. To find the fastest repository from the list of nearby repositories, run: curl -s http://mirrors.ubuntu.com/mirrors.txt | xargs -n1 -I {} sh -c 'echo `curl -r 0-10240000 -s -w %{speed_download} -o /dev/null {}/ls-lR.gz` {}' | sort -g -r The number in the leftmost column indicates the bandwidth in bytes (larger number is better). To change the repositories to the best mirror, we need to replace the mirror in etc/apt/source.list . We can do it manually, however, to prevent the mistakes, it is better to use a dedicated python script: apt-mirror-updater . Steps: 1. install the python script: sudo pip install apt-mirror-updater 1. backup the old file: sudo cp sources.list sources.list.bak 1. change the mirror with the script: apt-mirror-updater -c <mirror URL> Note that the apt-mirror-updater script can also measure the bandwidth, however, the result does not seem to be reliable.","title":"Changing package repositories"},{"location":"Ubuntu/Linux%20Manual/#string-processing","text":"","title":"String Processing"},{"location":"Ubuntu/Linux%20Manual/#word-count-with-wc","text":"The wc command counts words, lines, and characters. What is counted is determined by the parameters: - -w : words - -l : lines - -c : characters","title":"Word count with wc"},{"location":"Ubuntu/Linux%20Manual/#string-mofification-with-sed","text":"Sed is a multi purpose command for string modification. It can search and replace in string. The syntax is folowing: sed s/<search>/<replace>/[<occurance>] Example: s/$/,3/ Any regex can be used as <search> . Some characters (e.g. | ) must be escaped with backslash and used together with the -E parameter. This replace the nd of the line with string \",3\" . Note that there is a slash at the end, despite we use the default option for occurance. Also, it can delete lines containing string using: sed /<pattern>/d","title":"String mofification with sed"},{"location":"Ubuntu/Linux%20Manual/#cut","text":"Cut is a useful command for data with delimiters. Usage: cut -f <columns> Where columns are splited by comma, e.g., 2,4,7 . If we need to specify delimiters, we use the -d parameter: cut -d, -f 1,5","title":"cut"},{"location":"Ubuntu/Linux%20Manual/#awk","text":"AWK is a powerful tool for text processing. It is a programming language, so it can be used for more complex tasks than sed or cut . The basic syntax is: awk '<pattern> {<action>}' Where <pattern> is a regex and <action> is a command. The <action> is executed only if the line matches the <pattern> .","title":"AWK"},{"location":"Ubuntu/Linux%20Manual/#pattern","text":"In the awk , / is used as a delimiter of the regex pattern.","title":"Pattern"},{"location":"Ubuntu/Linux%20Manual/#action","text":"The <action> can be a sequence of commands, separated by ; . We can use column values by using special column variables: - $0 : the whole line - $1 : the first column ...","title":"Action"},{"location":"Ubuntu/Linux%20Manual/#trim-string","text":"<command with string output> | xargs","title":"Trim string"},{"location":"Ubuntu/Linux%20Manual/#processes","text":"for checking all processes, we can use htop for checking a specific process, we can use ps to kill a process, we can use kill to kill a process by name, we can use pkill to get information about a process selected by name, we can use pgrep","title":"Processes"},{"location":"Ubuntu/Linux%20Manual/#pkill","text":"The pkill command kills a process by name. The syntax is: pkill <process name> important parameters: - -f : match the whole command line, not only the process name - -9 : force kill","title":"pkill"},{"location":"Ubuntu/Linux%20Manual/#process-info","text":"","title":"Process Info"},{"location":"Ubuntu/Linux%20Manual/#installing-java","text":"","title":"Installing Java"},{"location":"Ubuntu/Linux%20Manual/#oracle-jdk","text":"Go to the download page, the link to the dowload page for current version of java is on the main JDK page . Click on the debian package, accept the license, and download it. If installing on system without GUI, copy now (after accepting the license) the target link and dowload the debian package with wget : wget --header \"Cookie: oraclelicense=accept-securebackup-cookie\" <COPIED LINK> . More info on SO . Install the package with sudo apt-get install <PATH TO DOWNLOADED .deb> if there is a problem with the isntallation, check the file integritiy with: sha256 <PATH TO DOWNLOADED .deb> . It should match with the checksums refered on the download page. If not cheksums do not match, go back to download step. In case there is another version of Java alreadz install, we need to overwrite it using the update-alternatives command: sudo update-alternatives --install /usr/bin/java java <PATH TO JAVA> <PRIORITY> . Example: sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk-16/bin/java 2 To check the priorities, call update-alternatives --query java . The newly installed JDK should have the highest priority.","title":"Oracle JDK"},{"location":"Ubuntu/Linux%20Manual/#python","text":"Python has to be executed with python3 by default, instead of python .","title":"Python"},{"location":"Ubuntu/Linux%20Manual/#other-usefull-commands","text":"","title":"Other usefull commands"},{"location":"Ubuntu/Linux%20Manual/#selecting-lines-from-file","text":"The head and tail commands are for that, printing the top and bottom 10 lines, respectively.","title":"Selecting lines from file"},{"location":"Ubuntu/Linux%20Manual/#skip-the-header","text":"tail -n +2","title":"Skip the header"},{"location":"Ubuntu/Linux%20Manual/#print-lines-from-to","text":"tail -n +<from line> | head -n <number of lines>","title":"Print lines from to"},{"location":"Ubuntu/Linux%20Manual/#progress-bar","text":"the progress bar can be printed using the pv command. pv <file> | <other comands> # or <other comands> | pv | <other comands>","title":"Progress bar"},{"location":"Ubuntu/Linux%20Manual/#free-disk-space","text":"df -h","title":"Free disk space"},{"location":"Ubuntu/Linux%20Manual/#piping-parameters-using-xargs","text":"The xargs command transfers the output of one command into call of another command with the output of the first command as parameters of the second command. This is usefull when the second command does not accept the output of the first command as input, but accepts the output as parameters. Example: ls | xargs rm # remove all files in the current directory","title":"piping parameters using xargs"},{"location":"Ubuntu/Linux%20Manual/#upgrade","text":"First run the update of the current version . Then optionaly backup the WSL run sudo do-release-upgrade","title":"Upgrade"},{"location":"Ubuntu/Linux%20Manual/#wsl-backup","text":"check the WSL distro name: wsl -l -v shutdown WSL: wsl --shutdown backup the distro: wsl --export <disto name> <backup folder path>/<backup name>.tar","title":"WSL backup"},{"location":"Windows/Powershell%20Manual/","text":"PowerShell is the new command line interface for Windows that replaces the old command prompt. It is superior in almost every aspect so it is recommended to use it instead of the old command prompt. The PowerShell script files have the .ps1 extension. Important Aspects \u00b6 New PowerShell \u00b6 The PowerShell Integrated in Windows is version 5. You can recognize it by the iconical blue background color. This old version has some important limitations (e. g. it cannot pass arguments containing arguments with spaces ). Therefore, it is best to install the new PowerShell first. Quick Edit / Insert Mode \u00b6 PowerShell enables copy/pase of commands. The downside is that every time you click inside PowerShell, the execution (if PowerShell is currently executing somethig) stops. To resume the execution, hit enter . Arguments starting with - \u00b6 If a program argument starts with - , and contains . it needs to be wrapped by ' . Otherwise, the argument will be split on the dot. Example: In sommand prompt or on Linux: mvn exec:java -Dexec.mainClass=com.example.MyExample -Dfile.encoding=UTF-8 In Powershell, this needs to be converted to: mvn exec:java '-Dexec.mainClass=com.example.MyExample' '-Dfile.encoding=UTF-8' Escaping \" and ' in Arguments \u00b6 Double quotes \" contained in arguments can be preserved by escaping with backslash: \\\" . Example for that can be passing an argument list to some executable: 'args=\\\"arg1 arg2\\\"' Single quotes ' are esceped by duble single quote: '' . Example can be passing a list of args, where some of them contains space: 'args=\\\"''arg1 with space'' arg2\\\"' No Output for EXE file \u00b6 Some errors are unfortunatelly not reported by powershell (e.g. missing dll ). The solution is to run such program in cmd, which reports the error. Control Structures \u00b6 Cycles \u00b6 foreach \u00b6 The foreach cycle iterates over a collection. The syntax is: foreach ($item in $collection) { # do something with $item } String Manipulation \u00b6 Replace \u00b6 The Replace method replaces all occurences of a string with another string. The syntax is: $myString.Replace(\"oldString\", \"newString\") Select-String \u00b6 The Select-String is the grep equivalent for PowerShell. The alias for the comand is sls . Parameters: - -Content <before>[, <after>] : Select also <before> lines before the matched line and <after> lines after the matched line Arrays \u00b6 To creante an array, use the @() operator: a = @() To add an element to an array, use the += operator: a += 1 . The arrays can be iterated over using the foreach loop. Network \u00b6 netstat \u00b6 The netstat is the basic network monitoring program. It displays TCP connections. It originated on Linux, so the usage and parameters are described in the Linux manual. Below, we discuss the differencies in the command interface and behavior of the Windows version of the command. Winndows manual . Lot of kubernetes entries in the log \u00b6 By default, the netstat command translates IPs into names. Unfortunatelly, on Windows, it also uses the information from the hosts file ( C:\\Windows\\System32\\drivers\\etc\\hosts ). This is a problem, because some services, like Docker, can use the hosts file to redirect some adddress to localhost. Therefore, at the end, all localhost entries are named kubernetes. Solution options: - use the -n parameter for netstat or - remove the redirection to localhost in the hosts file Display executable for connection \u00b6 To display executable for all connection, just use the -b swith. For filtering out only some, you have to use the -Context parameter in the Select-String command, as the executable is printed one line below the connection: netstat -b | sls <search pattern> -Context 0,1 Usefull Commands \u00b6 Print system variable \u00b6 echo $env:PATH File Encoding Conversion \u00b6 gc [INPUT PATH] | Out-File -en [ENCODING] [OUTPUT PATH] example: gc \"C:\\AIC data\\Roadmaptools big data test/map-filtered.osm\" | Out-File -en utf8 \"C:\\AIC data\\Roadmaptools big data test/map-filtered-utf8.osm\" NOTE: it is not very fast :) Delete all files with a certain extension \u00b6 ls *.extension -Recurse | foreach {rm $_} to try it, add the -WhatIf parameter to rm Batch rename \u00b6 Example dir . | % { $newName = $_.Name -replace '^DSC_0(.*)', 'DSC_1$1'; rename-item -newname $newName -literalPath $_.Fullname -whatif} Count Lines in large file \u00b6 switch -File FILE { default { ++$count } } Get Help \u00b6 Mirosoft documentation To get help about a command, use the Get-Help (alias man ) command. Example: Get-Help Get-ChildItem If the output is the list of articles, it means that there is no help for the command. Translate alias to command \u00b6 To translate an alias to a command, use the Get-Alias command. Example: Get-Alias ls # returns Get-ChildItem","title":"Powershell Manual"},{"location":"Windows/Powershell%20Manual/#important-aspects","text":"","title":"Important Aspects"},{"location":"Windows/Powershell%20Manual/#new-powershell","text":"The PowerShell Integrated in Windows is version 5. You can recognize it by the iconical blue background color. This old version has some important limitations (e. g. it cannot pass arguments containing arguments with spaces ). Therefore, it is best to install the new PowerShell first.","title":"New PowerShell"},{"location":"Windows/Powershell%20Manual/#quick-edit-insert-mode","text":"PowerShell enables copy/pase of commands. The downside is that every time you click inside PowerShell, the execution (if PowerShell is currently executing somethig) stops. To resume the execution, hit enter .","title":"Quick Edit / Insert Mode"},{"location":"Windows/Powershell%20Manual/#arguments-starting-with-","text":"If a program argument starts with - , and contains . it needs to be wrapped by ' . Otherwise, the argument will be split on the dot. Example: In sommand prompt or on Linux: mvn exec:java -Dexec.mainClass=com.example.MyExample -Dfile.encoding=UTF-8 In Powershell, this needs to be converted to: mvn exec:java '-Dexec.mainClass=com.example.MyExample' '-Dfile.encoding=UTF-8'","title":"Arguments starting with -"},{"location":"Windows/Powershell%20Manual/#escaping-and-in-arguments","text":"Double quotes \" contained in arguments can be preserved by escaping with backslash: \\\" . Example for that can be passing an argument list to some executable: 'args=\\\"arg1 arg2\\\"' Single quotes ' are esceped by duble single quote: '' . Example can be passing a list of args, where some of them contains space: 'args=\\\"''arg1 with space'' arg2\\\"'","title":"Escaping \" and ' in Arguments"},{"location":"Windows/Powershell%20Manual/#no-output-for-exe-file","text":"Some errors are unfortunatelly not reported by powershell (e.g. missing dll ). The solution is to run such program in cmd, which reports the error.","title":"No Output for EXE file"},{"location":"Windows/Powershell%20Manual/#control-structures","text":"","title":"Control Structures"},{"location":"Windows/Powershell%20Manual/#cycles","text":"","title":"Cycles"},{"location":"Windows/Powershell%20Manual/#foreach","text":"The foreach cycle iterates over a collection. The syntax is: foreach ($item in $collection) { # do something with $item }","title":"foreach"},{"location":"Windows/Powershell%20Manual/#string-manipulation","text":"","title":"String Manipulation"},{"location":"Windows/Powershell%20Manual/#replace","text":"The Replace method replaces all occurences of a string with another string. The syntax is: $myString.Replace(\"oldString\", \"newString\")","title":"Replace"},{"location":"Windows/Powershell%20Manual/#select-string","text":"The Select-String is the grep equivalent for PowerShell. The alias for the comand is sls . Parameters: - -Content <before>[, <after>] : Select also <before> lines before the matched line and <after> lines after the matched line","title":"Select-String"},{"location":"Windows/Powershell%20Manual/#arrays","text":"To creante an array, use the @() operator: a = @() To add an element to an array, use the += operator: a += 1 . The arrays can be iterated over using the foreach loop.","title":"Arrays"},{"location":"Windows/Powershell%20Manual/#network","text":"","title":"Network"},{"location":"Windows/Powershell%20Manual/#netstat","text":"The netstat is the basic network monitoring program. It displays TCP connections. It originated on Linux, so the usage and parameters are described in the Linux manual. Below, we discuss the differencies in the command interface and behavior of the Windows version of the command. Winndows manual .","title":"netstat"},{"location":"Windows/Powershell%20Manual/#lot-of-kubernetes-entries-in-the-log","text":"By default, the netstat command translates IPs into names. Unfortunatelly, on Windows, it also uses the information from the hosts file ( C:\\Windows\\System32\\drivers\\etc\\hosts ). This is a problem, because some services, like Docker, can use the hosts file to redirect some adddress to localhost. Therefore, at the end, all localhost entries are named kubernetes. Solution options: - use the -n parameter for netstat or - remove the redirection to localhost in the hosts file","title":"Lot of kubernetes entries in the log"},{"location":"Windows/Powershell%20Manual/#display-executable-for-connection","text":"To display executable for all connection, just use the -b swith. For filtering out only some, you have to use the -Context parameter in the Select-String command, as the executable is printed one line below the connection: netstat -b | sls <search pattern> -Context 0,1","title":"Display executable for connection"},{"location":"Windows/Powershell%20Manual/#usefull-commands","text":"","title":"Usefull Commands"},{"location":"Windows/Powershell%20Manual/#print-system-variable","text":"echo $env:PATH","title":"Print system variable"},{"location":"Windows/Powershell%20Manual/#file-encoding-conversion","text":"gc [INPUT PATH] | Out-File -en [ENCODING] [OUTPUT PATH] example: gc \"C:\\AIC data\\Roadmaptools big data test/map-filtered.osm\" | Out-File -en utf8 \"C:\\AIC data\\Roadmaptools big data test/map-filtered-utf8.osm\" NOTE: it is not very fast :)","title":"File Encoding Conversion"},{"location":"Windows/Powershell%20Manual/#delete-all-files-with-a-certain-extension","text":"ls *.extension -Recurse | foreach {rm $_} to try it, add the -WhatIf parameter to rm","title":"Delete all files with a certain extension"},{"location":"Windows/Powershell%20Manual/#batch-rename","text":"Example dir . | % { $newName = $_.Name -replace '^DSC_0(.*)', 'DSC_1$1'; rename-item -newname $newName -literalPath $_.Fullname -whatif}","title":"Batch rename"},{"location":"Windows/Powershell%20Manual/#count-lines-in-large-file","text":"switch -File FILE { default { ++$count } }","title":"Count Lines in large file"},{"location":"Windows/Powershell%20Manual/#get-help","text":"Mirosoft documentation To get help about a command, use the Get-Help (alias man ) command. Example: Get-Help Get-ChildItem If the output is the list of articles, it means that there is no help for the command.","title":"Get Help"},{"location":"Windows/Powershell%20Manual/#translate-alias-to-command","text":"To translate an alias to a command, use the Get-Alias command. Example: Get-Alias ls # returns Get-ChildItem","title":"Translate alias to command"},{"location":"Windows/VSCode/","text":"Settings \u00b6 Language specific settings \u00b6 Almost all settings can be set specifically to some language. To do that: 1. in settings, next to the filter box, click on the filter icon and select language 2. select the language 3. find the setting either manually or by adding more filters 4. change the setting To be sure that the setting is applied only to the selected language, look at the panel under the search box. Instead of User , Workspace , there should be User[<language>] , Workspace[<language>] .","title":"VSCode"},{"location":"Windows/VSCode/#settings","text":"","title":"Settings"},{"location":"Windows/VSCode/#language-specific-settings","text":"Almost all settings can be set specifically to some language. To do that: 1. in settings, next to the filter box, click on the filter icon and select language 2. select the language 3. find the setting either manually or by adding more filters 4. change the setting To be sure that the setting is applied only to the selected language, look at the panel under the search box. Instead of User , Workspace , there should be User[<language>] , Workspace[<language>] .","title":"Language specific settings"},{"location":"Windows/Windows%20Manual/","text":"General Guides \u00b6 For PowerShell solutions/guides, check the PoweShell manual . Keyboard Shortcuts \u00b6 Alt + Shift : change input language Win + Space : change keyboard input method Wireless Network \u00b6 Problem: Can't connect to this network \u00b6 Solution: Forget the connection and connect to the network manually Connect to a Network Manually \u00b6 Control Panel -> Network and Internet -> Network and Sharing Center Set up a new connection or network Manually connect to a wireless network Fill the credentials: Network name: SSID Security type: dpends, try WPA2 personal Security key: password Click next Close the dialog Click the wifi icon in the taskbar and connect to the network There are various usefull comands. For most of the commands, you need to open PowerShell as admin. Various Commands Related to the Wifi \u00b6 Show All Network Profiles \u00b6 This command show network configurations stored on the device. netsh wlan show profile Various Wifi Reports in HTML \u00b6 netsh wlan show wlanreport Changing the input method \u00b6 It is possible to let the system have a different input method for each app. It is not possible however, to remember the input method (after app/OS restart). Troubleshooting \u00b6 Nothing happens after clicking on the input method in the taskbar (windows 10) \u00b6 restrat the computer :) Bluetooth \u00b6 Troiubleshooting \u00b6 Cannot connect to the device \u00b6 Try to remove the device and pair it with the PC again If it does not help, proceeed to the next section (even if the pairing is successfull) Cannot pair with the device \u00b6 Turn off the device and unplug it from the electricity/remove batteries. Then plug it back after ~10 seconds, power it of, and try to pair with it again. Bluetooth Command Line Tools \u00b6 https://bluetoothinstaller.com/bluetooth-command-line-tools Bluetooth Command Line Tools is a set off tools that enables command line interaction with blootooth services. Basic usage: - discover and list available devices: btdiscovery -s Filesytem \u00b6 Standard folder structure \u00b6 In Windows, the standard folder structure is completely different for system and user instalation. Details are listed below, but the main difference is that the system instalations are stored in a single root folder for each application (similarly to Android), while the user instalations' files are distributed among multiple folders, depending on the type of the file (similarly to Linux). User home folder \u00b6 The user home folder is located in C:\\Users\\<username> by default. It is aliased as %userprofile% . System instalation folders \u00b6 If an application is installed for all users, all its files are usually installed in a single folder per application. The location of the folder depends on the type of the application: - C:\\Program Files : 64-bit applications - C:\\Program Files (x86) : 32-bit applications If the application needs to store some data, they are usually stored in the C:\\ProgramData (aliased as %programdata% ) folder. User instalation folders \u00b6 User instalations are stored in multiple folders, depending on the type of the file. All these folders are located in the user's home folder, which is C:\\Users\\<username> by default. The folders are: - ~\\AppData\\Local : Program data and sometimes also executables - ~\\AppData\\Local\\Promgrams : program files and executables - ~\\AppData\\LocalLow : - ~\\AppData\\Roaming (aliased as %appdata% ): Start Menu folder \u00b6 The user specific shortcuts are stored in: %appdata%\\Microsoft\\Windows\\Start Menu\\Programs . The system wide shortcuts are stored in: %programdata%\\Microsoft\\Windows\\Start Menu\\Programs . Read Only Files and Folders \u00b6 An ancient form of file protection on Windows is the read only flag that can be set on files and folders. It is not a real protection, as it can be easily removed by the user, but it can be used to prevent accidental changes. Most of the programs can ignore this flag and work with the file anyway. However, some programs (e.g. Python) can have problems with it. Sugarsync \u00b6 Quick orientation in the desktop app: - for file changes, check left menu -> Activity - for deleted files, check left menu -> Deleted Items Solving sync problems \u00b6 check if the file is updated in cloud using web browser if not, check the activity log on the computer with the updated file if the change is not in the log, a simple hack can help: copy the file outside SugarSync folder and back. Useful Commands \u00b6 Get Motherboard Info \u00b6 wmic baseboard get product,Manufacturer,version,serialnumber Copy multiple files/dirs \u00b6 robocopy is the best command for that. Usefull params: - /e : Copy subdirectories, including empty ones - /b : Copy in the backup mode, so that even files with a different owner can be copied - /xc : Excludes changed files. - /xn : Excludes newer files. - /xo : Excludes older files. - /r:<n> : Specifies the number of retries on failed copies. The default value of n is 1,000,000 (one million retries). - /w:<n> : Specifies the wait time between retries, in seconds. The default value of n is 30 (wait time 30 seconds). Installation \u00b6 Windows 11 \u00b6 Windows 11 can be installed only as an update of Windows 10. Windows 10 \u00b6 Can be installed from bootable USB created by a tool downloaded from the official Miccosoft website. Single image for all Windows versions, a particular version is choosen based on the license key. Steps: 1. Download the install tool from Microsoft 2. Create a bootable USB 3. Start the installation 4. Fill in the licence key we couldn\u2019t create a partition or locate an existing one \u00b6 Ensure that the boot priority of the drive where the Windows should be installed is right behind the installation USB priority. Diskpart \u00b6 Diskpart is a useful command line tool for work with diska, partitions, etc. Find out wheteher a disk is MBR or GPT \u00b6 Open Command Promp from the Windows 10 USB \u00b6 Insert the USB stick Wait till the first installation screen shift + F10 Firewall \u00b6 Generate firewall logs \u00b6 Go to Windows firewall and select properties on the right At the top, choose the profile corresponding to the current network profile In the logging section, click to customizze set both DROP and ACCEPT to yes Do not forgot to turn of the logging after the investigation! SSH \u00b6 For ssh, we can use the standard ssh commannd available in Windows (check Linux manual for more info). If the command is not available, it can be installed in Apps & Features -> Optional features . One thing that differs from Linux is that the Windows ssh does not support the <addres>:<port> syntax. To specify the port, it is necessary to use the -p parameter For more features, we can use other programs - KiTTY for credentials storage, automatic reconection, etc. - WinSCP for file manipulation KiTTY \u00b6 It is best to use the portable version, so that nothing is stored in the Windows registry. Configurtation: - copy the PuTTY credentials : .\\kitty_portable-0.76.1.3.exe -convert-dir - auto reconnect: Connection -> auto reconnect on connection failure and auto reconnect on system wakeup WinSCP \u00b6 WinSCP is a graphical tool for file manipulation. Ii can be used both for local and remote files, and it supports various protocols (FTP, SFTP, SCP, WebDAV, etc.). Adding a new connection \u00b6 There is a simple New Site button on the left, which opens a straightforward dialog. The only complicated thing can be the SSH key. To add it, click on the Advanced button and go to the SSH -> Authentication tab. There, we can select the private key file. Bookmarks \u00b6 To add bookmarks, go to Local / Remote -> Add Path to Bookmarks or press Ctrl + B . To open a bookmark, go to Local / Remote -> Go To -> Open Drirectory/bookmark or press Ctrl + O . SSH key agent \u00b6 To enable the ssh-agent on Windows, one extra step is needed: we need to start the ssh-agent service. To do that, open the services manager and start the OpenSSH Authentication Agent service. Git \u00b6 Most of the git functionality is the same as in Linux, so check the Linux manual for more info. However, there are some important differences mostly resulting from the fact that Git is not a native Windows application, but it runs in MinGW. Git on Windows and SSH \u00b6 As the git on Windows runs in MinGW, it does not use the Windows SSH command. That can be problematic if we want to debug the SSH connection using the env variable or configuring an ssh key agent. To force git to use the Windows SSH, we need to set the sshCommand config variable to the path to the Windows SSH: git config --global core.sshCommand C:/Windows/System32/OpenSSH/ssh.exe Problems \u00b6 Folder Sharing Problems \u00b6 Note that updated Windows 10 disabled anonymous sharing , so password protected sharing has to be turned on. To login, use the credentials for the computer with the shared folder . Below is a list of possible problems, together with solutions. The user name or password is incorrect \u00b6 Check whether the computer can be seen in the network. If not, resolve this issue first. quick check by running net view <IP address> Check that you are using the right username. You need to use the username and password of the computer you are connecting to . Check that the user name is correct by running net user on the target computer Check that the folder is shared with you in: right click on the folder -> Properties -> Sharing -> Advanced Sharing... -> Permisions . Note that your full name can be there instead of your username, which is OK. Check that you are using the right password. You have to use the password associated with your microsoft account. Note that it can differ from the password (PIN) you are using to log in to the computer! check it on the command line: net use * \\\\<IP address>\\<drive letter>$ /use:<username> <password> Folder right and ownership cannot be read \u00b6 Try to clear the windows filecache (CCcleaner or restart) Computer does not see itself in Network section in File Explorer \u00b6 Solution to this problem is to restart the service called Function Discovery Resource Publication . Either restart it in Computer Management -> Services, or by: net stop FDResPub net start FDResPub PC wakes up or cannot enter sleep \u00b6 1 Find the source \u00b6 Using the Event viwer 1. open the event viewer 2. go to windows logs -> system 3. In case of wake up 1. inspect the logs when the wake up happened and search for the Information log with the message \"The system has returned from a low power state.\" 2. There is a wake up source in the end of the log message. If the soure is Unknown go to the next section 4. In case of not entering sleep 1. Search for the any kernel power event 2. If there is an event stating: The system is entering connected standby , it means that the modern fake sleep is present in the system, replacing the real sleep mode. Using command line (admin): 1. Try powercfg -lastwake 2. If the results are not know, try to call powercfg -devicequery wake_armed to get the list of devices that can wake the computer 2 Solve the problem \u00b6 Device waken up by network adapter \u00b6 Open device manager and search for the specific network adapter right click -> Properties -> Power Management Check Only allow a magic packet to wake up the computer The real sleep mode is not available on the system \u00b6 If this is the case, use the hibernate mode instead. To add it to the start menu: 1. go to Control panel -> Hardware and sound -> Power options 2. click on the left panel to Choose what the power buttons does 3. click on Change settings that are currently unavailable 4. check the hibernate checkbox below Camera problem \u00b6 Symptoms: the screen is blank, black, single color, in all apps and there are no problems reported in device manager Cause: it can be caused by some external cameras (now disconnected) that are still selected in the apps using the camera. Go Solution: Go to the app setting and select the correct camera Phone app cannot see the connected cell phone \u00b6 It can be due to the fucked up Windows N edition. Just install the normal edition. vmmem process uses a lot of CPU \u00b6 This process represents all virtual systems. One cultprit is therefore WSL. Try to shutdown the WSL using wsl --shutdown","title":"Windows Manual"},{"location":"Windows/Windows%20Manual/#general-guides","text":"For PowerShell solutions/guides, check the PoweShell manual .","title":"General Guides"},{"location":"Windows/Windows%20Manual/#keyboard-shortcuts","text":"Alt + Shift : change input language Win + Space : change keyboard input method","title":"Keyboard Shortcuts"},{"location":"Windows/Windows%20Manual/#wireless-network","text":"","title":"Wireless Network"},{"location":"Windows/Windows%20Manual/#problem-cant-connect-to-this-network","text":"Solution: Forget the connection and connect to the network manually","title":"Problem: Can't connect to this network"},{"location":"Windows/Windows%20Manual/#connect-to-a-network-manually","text":"Control Panel -> Network and Internet -> Network and Sharing Center Set up a new connection or network Manually connect to a wireless network Fill the credentials: Network name: SSID Security type: dpends, try WPA2 personal Security key: password Click next Close the dialog Click the wifi icon in the taskbar and connect to the network There are various usefull comands. For most of the commands, you need to open PowerShell as admin.","title":"Connect to a Network Manually"},{"location":"Windows/Windows%20Manual/#various-commands-related-to-the-wifi","text":"","title":"Various Commands Related to the Wifi"},{"location":"Windows/Windows%20Manual/#show-all-network-profiles","text":"This command show network configurations stored on the device. netsh wlan show profile","title":"Show All Network Profiles"},{"location":"Windows/Windows%20Manual/#various-wifi-reports-in-html","text":"netsh wlan show wlanreport","title":"Various Wifi Reports in HTML"},{"location":"Windows/Windows%20Manual/#changing-the-input-method","text":"It is possible to let the system have a different input method for each app. It is not possible however, to remember the input method (after app/OS restart).","title":"Changing the input method"},{"location":"Windows/Windows%20Manual/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Windows/Windows%20Manual/#nothing-happens-after-clicking-on-the-input-method-in-the-taskbar-windows-10","text":"restrat the computer :)","title":"Nothing happens after clicking on the input method in the taskbar (windows 10)"},{"location":"Windows/Windows%20Manual/#bluetooth","text":"","title":"Bluetooth"},{"location":"Windows/Windows%20Manual/#troiubleshooting","text":"","title":"Troiubleshooting"},{"location":"Windows/Windows%20Manual/#cannot-connect-to-the-device","text":"Try to remove the device and pair it with the PC again If it does not help, proceeed to the next section (even if the pairing is successfull)","title":"Cannot connect to the device"},{"location":"Windows/Windows%20Manual/#cannot-pair-with-the-device","text":"Turn off the device and unplug it from the electricity/remove batteries. Then plug it back after ~10 seconds, power it of, and try to pair with it again.","title":"Cannot pair with the device"},{"location":"Windows/Windows%20Manual/#bluetooth-command-line-tools","text":"https://bluetoothinstaller.com/bluetooth-command-line-tools Bluetooth Command Line Tools is a set off tools that enables command line interaction with blootooth services. Basic usage: - discover and list available devices: btdiscovery -s","title":"Bluetooth Command Line Tools"},{"location":"Windows/Windows%20Manual/#filesytem","text":"","title":"Filesytem"},{"location":"Windows/Windows%20Manual/#standard-folder-structure","text":"In Windows, the standard folder structure is completely different for system and user instalation. Details are listed below, but the main difference is that the system instalations are stored in a single root folder for each application (similarly to Android), while the user instalations' files are distributed among multiple folders, depending on the type of the file (similarly to Linux).","title":"Standard folder structure"},{"location":"Windows/Windows%20Manual/#user-home-folder","text":"The user home folder is located in C:\\Users\\<username> by default. It is aliased as %userprofile% .","title":"User home folder"},{"location":"Windows/Windows%20Manual/#system-instalation-folders","text":"If an application is installed for all users, all its files are usually installed in a single folder per application. The location of the folder depends on the type of the application: - C:\\Program Files : 64-bit applications - C:\\Program Files (x86) : 32-bit applications If the application needs to store some data, they are usually stored in the C:\\ProgramData (aliased as %programdata% ) folder.","title":"System instalation folders"},{"location":"Windows/Windows%20Manual/#user-instalation-folders","text":"User instalations are stored in multiple folders, depending on the type of the file. All these folders are located in the user's home folder, which is C:\\Users\\<username> by default. The folders are: - ~\\AppData\\Local : Program data and sometimes also executables - ~\\AppData\\Local\\Promgrams : program files and executables - ~\\AppData\\LocalLow : - ~\\AppData\\Roaming (aliased as %appdata% ):","title":"User instalation folders"},{"location":"Windows/Windows%20Manual/#start-menu-folder","text":"The user specific shortcuts are stored in: %appdata%\\Microsoft\\Windows\\Start Menu\\Programs . The system wide shortcuts are stored in: %programdata%\\Microsoft\\Windows\\Start Menu\\Programs .","title":"Start Menu folder"},{"location":"Windows/Windows%20Manual/#read-only-files-and-folders","text":"An ancient form of file protection on Windows is the read only flag that can be set on files and folders. It is not a real protection, as it can be easily removed by the user, but it can be used to prevent accidental changes. Most of the programs can ignore this flag and work with the file anyway. However, some programs (e.g. Python) can have problems with it.","title":"Read Only Files and Folders"},{"location":"Windows/Windows%20Manual/#sugarsync","text":"Quick orientation in the desktop app: - for file changes, check left menu -> Activity - for deleted files, check left menu -> Deleted Items","title":"Sugarsync"},{"location":"Windows/Windows%20Manual/#solving-sync-problems","text":"check if the file is updated in cloud using web browser if not, check the activity log on the computer with the updated file if the change is not in the log, a simple hack can help: copy the file outside SugarSync folder and back.","title":"Solving sync problems"},{"location":"Windows/Windows%20Manual/#useful-commands","text":"","title":"Useful Commands"},{"location":"Windows/Windows%20Manual/#get-motherboard-info","text":"wmic baseboard get product,Manufacturer,version,serialnumber","title":"Get Motherboard Info"},{"location":"Windows/Windows%20Manual/#copy-multiple-filesdirs","text":"robocopy is the best command for that. Usefull params: - /e : Copy subdirectories, including empty ones - /b : Copy in the backup mode, so that even files with a different owner can be copied - /xc : Excludes changed files. - /xn : Excludes newer files. - /xo : Excludes older files. - /r:<n> : Specifies the number of retries on failed copies. The default value of n is 1,000,000 (one million retries). - /w:<n> : Specifies the wait time between retries, in seconds. The default value of n is 30 (wait time 30 seconds).","title":"Copy multiple files/dirs"},{"location":"Windows/Windows%20Manual/#installation","text":"","title":"Installation"},{"location":"Windows/Windows%20Manual/#windows-11","text":"Windows 11 can be installed only as an update of Windows 10.","title":"Windows 11"},{"location":"Windows/Windows%20Manual/#windows-10","text":"Can be installed from bootable USB created by a tool downloaded from the official Miccosoft website. Single image for all Windows versions, a particular version is choosen based on the license key. Steps: 1. Download the install tool from Microsoft 2. Create a bootable USB 3. Start the installation 4. Fill in the licence key","title":"Windows 10"},{"location":"Windows/Windows%20Manual/#we-couldnt-create-a-partition-or-locate-an-existing-one","text":"Ensure that the boot priority of the drive where the Windows should be installed is right behind the installation USB priority.","title":"we couldn\u2019t create a partition or locate an existing one"},{"location":"Windows/Windows%20Manual/#diskpart","text":"Diskpart is a useful command line tool for work with diska, partitions, etc.","title":"Diskpart"},{"location":"Windows/Windows%20Manual/#find-out-wheteher-a-disk-is-mbr-or-gpt","text":"","title":"Find out wheteher a disk is MBR or GPT"},{"location":"Windows/Windows%20Manual/#open-command-promp-from-the-windows-10-usb","text":"Insert the USB stick Wait till the first installation screen shift + F10","title":"Open Command Promp from the Windows 10 USB"},{"location":"Windows/Windows%20Manual/#firewall","text":"","title":"Firewall"},{"location":"Windows/Windows%20Manual/#generate-firewall-logs","text":"Go to Windows firewall and select properties on the right At the top, choose the profile corresponding to the current network profile In the logging section, click to customizze set both DROP and ACCEPT to yes Do not forgot to turn of the logging after the investigation!","title":"Generate firewall logs"},{"location":"Windows/Windows%20Manual/#ssh","text":"For ssh, we can use the standard ssh commannd available in Windows (check Linux manual for more info). If the command is not available, it can be installed in Apps & Features -> Optional features . One thing that differs from Linux is that the Windows ssh does not support the <addres>:<port> syntax. To specify the port, it is necessary to use the -p parameter For more features, we can use other programs - KiTTY for credentials storage, automatic reconection, etc. - WinSCP for file manipulation","title":"SSH"},{"location":"Windows/Windows%20Manual/#kitty","text":"It is best to use the portable version, so that nothing is stored in the Windows registry. Configurtation: - copy the PuTTY credentials : .\\kitty_portable-0.76.1.3.exe -convert-dir - auto reconnect: Connection -> auto reconnect on connection failure and auto reconnect on system wakeup","title":"KiTTY"},{"location":"Windows/Windows%20Manual/#winscp","text":"WinSCP is a graphical tool for file manipulation. Ii can be used both for local and remote files, and it supports various protocols (FTP, SFTP, SCP, WebDAV, etc.).","title":"WinSCP"},{"location":"Windows/Windows%20Manual/#adding-a-new-connection","text":"There is a simple New Site button on the left, which opens a straightforward dialog. The only complicated thing can be the SSH key. To add it, click on the Advanced button and go to the SSH -> Authentication tab. There, we can select the private key file.","title":"Adding a new connection"},{"location":"Windows/Windows%20Manual/#bookmarks","text":"To add bookmarks, go to Local / Remote -> Add Path to Bookmarks or press Ctrl + B . To open a bookmark, go to Local / Remote -> Go To -> Open Drirectory/bookmark or press Ctrl + O .","title":"Bookmarks"},{"location":"Windows/Windows%20Manual/#ssh-key-agent","text":"To enable the ssh-agent on Windows, one extra step is needed: we need to start the ssh-agent service. To do that, open the services manager and start the OpenSSH Authentication Agent service.","title":"SSH key agent"},{"location":"Windows/Windows%20Manual/#git","text":"Most of the git functionality is the same as in Linux, so check the Linux manual for more info. However, there are some important differences mostly resulting from the fact that Git is not a native Windows application, but it runs in MinGW.","title":"Git"},{"location":"Windows/Windows%20Manual/#git-on-windows-and-ssh","text":"As the git on Windows runs in MinGW, it does not use the Windows SSH command. That can be problematic if we want to debug the SSH connection using the env variable or configuring an ssh key agent. To force git to use the Windows SSH, we need to set the sshCommand config variable to the path to the Windows SSH: git config --global core.sshCommand C:/Windows/System32/OpenSSH/ssh.exe","title":"Git on Windows and SSH"},{"location":"Windows/Windows%20Manual/#problems","text":"","title":"Problems"},{"location":"Windows/Windows%20Manual/#folder-sharing-problems","text":"Note that updated Windows 10 disabled anonymous sharing , so password protected sharing has to be turned on. To login, use the credentials for the computer with the shared folder . Below is a list of possible problems, together with solutions.","title":"Folder Sharing Problems"},{"location":"Windows/Windows%20Manual/#the-user-name-or-password-is-incorrect","text":"Check whether the computer can be seen in the network. If not, resolve this issue first. quick check by running net view <IP address> Check that you are using the right username. You need to use the username and password of the computer you are connecting to . Check that the user name is correct by running net user on the target computer Check that the folder is shared with you in: right click on the folder -> Properties -> Sharing -> Advanced Sharing... -> Permisions . Note that your full name can be there instead of your username, which is OK. Check that you are using the right password. You have to use the password associated with your microsoft account. Note that it can differ from the password (PIN) you are using to log in to the computer! check it on the command line: net use * \\\\<IP address>\\<drive letter>$ /use:<username> <password>","title":"The user name or password is incorrect"},{"location":"Windows/Windows%20Manual/#folder-right-and-ownership-cannot-be-read","text":"Try to clear the windows filecache (CCcleaner or restart)","title":"Folder right and ownership cannot be read"},{"location":"Windows/Windows%20Manual/#computer-does-not-see-itself-in-network-section-in-file-explorer","text":"Solution to this problem is to restart the service called Function Discovery Resource Publication . Either restart it in Computer Management -> Services, or by: net stop FDResPub net start FDResPub","title":"Computer does not see itself in Network section in File Explorer"},{"location":"Windows/Windows%20Manual/#pc-wakes-up-or-cannot-enter-sleep","text":"","title":"PC wakes up or cannot enter sleep"},{"location":"Windows/Windows%20Manual/#1-find-the-source","text":"Using the Event viwer 1. open the event viewer 2. go to windows logs -> system 3. In case of wake up 1. inspect the logs when the wake up happened and search for the Information log with the message \"The system has returned from a low power state.\" 2. There is a wake up source in the end of the log message. If the soure is Unknown go to the next section 4. In case of not entering sleep 1. Search for the any kernel power event 2. If there is an event stating: The system is entering connected standby , it means that the modern fake sleep is present in the system, replacing the real sleep mode. Using command line (admin): 1. Try powercfg -lastwake 2. If the results are not know, try to call powercfg -devicequery wake_armed to get the list of devices that can wake the computer","title":"1 Find the source"},{"location":"Windows/Windows%20Manual/#2-solve-the-problem","text":"","title":"2 Solve the problem"},{"location":"Windows/Windows%20Manual/#device-waken-up-by-network-adapter","text":"Open device manager and search for the specific network adapter right click -> Properties -> Power Management Check Only allow a magic packet to wake up the computer","title":"Device waken up by network adapter"},{"location":"Windows/Windows%20Manual/#the-real-sleep-mode-is-not-available-on-the-system","text":"If this is the case, use the hibernate mode instead. To add it to the start menu: 1. go to Control panel -> Hardware and sound -> Power options 2. click on the left panel to Choose what the power buttons does 3. click on Change settings that are currently unavailable 4. check the hibernate checkbox below","title":"The real sleep mode is not available on the system"},{"location":"Windows/Windows%20Manual/#camera-problem","text":"Symptoms: the screen is blank, black, single color, in all apps and there are no problems reported in device manager Cause: it can be caused by some external cameras (now disconnected) that are still selected in the apps using the camera. Go Solution: Go to the app setting and select the correct camera","title":"Camera problem"},{"location":"Windows/Windows%20Manual/#phone-app-cannot-see-the-connected-cell-phone","text":"It can be due to the fucked up Windows N edition. Just install the normal edition.","title":"Phone app cannot see the connected cell phone"},{"location":"Windows/Windows%20Manual/#vmmem-process-uses-a-lot-of-cpu","text":"This process represents all virtual systems. One cultprit is therefore WSL. Try to shutdown the WSL using wsl --shutdown","title":"vmmem process uses a lot of CPU"}]}