{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the manuals pages!","title":"Home"},{"location":"FFMpeg/","text":"First download binaries for Windows . Converting a Video to Gif Image Example: ffmpeg -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=512:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -ss 00:00:03 -to 00:00:06 simod_showcase.gif To change the speed, we can use an -itsscale inut option: ffmpeg -itsscale 0.2 -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=838:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -ss 00:00:00 -to 00:00:15 simod_showcase.gif Detailed description on SE Written with StackEdit .","title":"FFMpeg"},{"location":"FFMpeg/#converting-a-video-to-gif-image","text":"Example: ffmpeg -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=512:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -ss 00:00:03 -to 00:00:06 simod_showcase.gif To change the speed, we can use an -itsscale inut option: ffmpeg -itsscale 0.2 -i .\\screen_recording1618479593603.mp4 -r 15 -vf \"scale=838:-1,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -ss 00:00:00 -to 00:00:15 simod_showcase.gif Detailed description on SE Written with StackEdit .","title":"Converting a Video to Gif Image"},{"location":"Overpass%20Manual/","text":"Sources wiki/Overpass QL Strucutre Every statement ents with ; . Sets Almost all statements works with sets of data. Unless specified, we work with a default set ( _ ). To wrrito to a specific set, we can use ->. operator: <statement>->.<set> writes the result of the <statement> to the . The default set can be ommited: <statement> is equal to <statement>->._ . out statement All queries should contain an out statement that determines the output format. - out is used for data only request - out geom returns data with all elements associated with their geometry. Note that while the output format can be specified, we cannot filter the output (e.g., we cannot filter the ralation members ). Area specification We select an area by name as: area[\"name\"=\"Letkov\"]; And then we used it as: node(area); If we need more areas in a query, we can store them in variable: area[\"name\"=\"Letkov\"]->.let; And: node(area.let); Important note: If an area needs to be used repetadly, it has to be named, as the value of area ( area._ ) is replaced by the output of any statement. Filtering filters are specified in brackets: rel[\"admin_level\"=8]; It is also possible to use a regex filtering, we just neeed to replace = with ~ : rel[\"admin_level\"~\".*\"]; Selecting Multiple Data Sets Implicitely, all filters are aplied to a default dataset called _ and also written to it. Therefore, we cannot do: rel[\"admin_level\"~\".*\"]; node; because we are basically selecting nodes from a set of relations. Solutions: 1) Union statement 2) Named sets Union Utatement Union statement is surounded by parantheses. We can achieve the intended behaviour by: ( rel[\"admin_level\"~\".*\"]; node; ); Select Area Boundary Sometimes, it is usefull to check the boundary of the named area. However, the administrative level differ for various areas cities, countries, so the admin_level relation cannot be used for that. Luckilly, there is an option to use the pivot filter. Example: area[\"name\"=\"Praha\"]; node(pivot); out geom; Discover the full name of an area If we want to know the full name of the area while the above query return multiple results, we can do that in openstreet map: 1. Move the map to see the area 2. Click the button with cusor and question mark to select the exploration tool 3. Click inside the area 4. Scroll down to area relations 5. Click on the proper region 6. The name property is what we are looking for Filter areas with duplicite names Sometimes, even the full name of the area is not specific enough. In that case, we can use two approaches: - select the area by the area relation id - specify the area by the higher level area (state, country) Select area by ID select the requested area copy the id and add 3 600 000 000 to it (start with 36 and fill zerose till there are 10 digits in total) replace area[\"name\"=\"<NAME>] with area(<ID>) . Note that round brackets are used instead of square brackets! Specify area with higher level area In this exaple, we select the Coo County, Illinois. area[name=\"Illinois\"]; rel[name=\"Cook County\"](area); map_to_area; node(pivot); out geom; more info Get historical data To get historical data, prepend the query with a date statement. Example: [date:\"2016-01-01T00:00:00Z\"]; area[name=\"City of New York\"]; node(pivot); out geom;","title":"Sources"},{"location":"Overpass%20Manual/#sources","text":"wiki/Overpass QL","title":"Sources"},{"location":"Overpass%20Manual/#strucutre","text":"Every statement ents with ; .","title":"Strucutre"},{"location":"Overpass%20Manual/#sets","text":"Almost all statements works with sets of data. Unless specified, we work with a default set ( _ ). To wrrito to a specific set, we can use ->. operator: <statement>->.<set> writes the result of the <statement> to the . The default set can be ommited: <statement> is equal to <statement>->._ .","title":"Sets"},{"location":"Overpass%20Manual/#out-statement","text":"All queries should contain an out statement that determines the output format. - out is used for data only request - out geom returns data with all elements associated with their geometry. Note that while the output format can be specified, we cannot filter the output (e.g., we cannot filter the ralation members ).","title":"out statement"},{"location":"Overpass%20Manual/#area-specification","text":"We select an area by name as: area[\"name\"=\"Letkov\"]; And then we used it as: node(area); If we need more areas in a query, we can store them in variable: area[\"name\"=\"Letkov\"]->.let; And: node(area.let); Important note: If an area needs to be used repetadly, it has to be named, as the value of area ( area._ ) is replaced by the output of any statement.","title":"Area specification"},{"location":"Overpass%20Manual/#filtering","text":"filters are specified in brackets: rel[\"admin_level\"=8]; It is also possible to use a regex filtering, we just neeed to replace = with ~ : rel[\"admin_level\"~\".*\"];","title":"Filtering"},{"location":"Overpass%20Manual/#selecting-multiple-data-sets","text":"Implicitely, all filters are aplied to a default dataset called _ and also written to it. Therefore, we cannot do: rel[\"admin_level\"~\".*\"]; node; because we are basically selecting nodes from a set of relations. Solutions: 1) Union statement 2) Named sets","title":"Selecting Multiple Data Sets"},{"location":"Overpass%20Manual/#union-utatement","text":"Union statement is surounded by parantheses. We can achieve the intended behaviour by: ( rel[\"admin_level\"~\".*\"]; node; );","title":"Union Utatement"},{"location":"Overpass%20Manual/#select-area-boundary","text":"Sometimes, it is usefull to check the boundary of the named area. However, the administrative level differ for various areas cities, countries, so the admin_level relation cannot be used for that. Luckilly, there is an option to use the pivot filter. Example: area[\"name\"=\"Praha\"]; node(pivot); out geom;","title":"Select Area Boundary"},{"location":"Overpass%20Manual/#discover-the-full-name-of-an-area","text":"If we want to know the full name of the area while the above query return multiple results, we can do that in openstreet map: 1. Move the map to see the area 2. Click the button with cusor and question mark to select the exploration tool 3. Click inside the area 4. Scroll down to area relations 5. Click on the proper region 6. The name property is what we are looking for","title":"Discover the full name of an area"},{"location":"Overpass%20Manual/#filter-areas-with-duplicite-names","text":"Sometimes, even the full name of the area is not specific enough. In that case, we can use two approaches: - select the area by the area relation id - specify the area by the higher level area (state, country)","title":"Filter areas with duplicite names"},{"location":"Overpass%20Manual/#select-area-by-id","text":"select the requested area copy the id and add 3 600 000 000 to it (start with 36 and fill zerose till there are 10 digits in total) replace area[\"name\"=\"<NAME>] with area(<ID>) . Note that round brackets are used instead of square brackets!","title":"Select area by ID"},{"location":"Overpass%20Manual/#specify-area-with-higher-level-area","text":"In this exaple, we select the Coo County, Illinois. area[name=\"Illinois\"]; rel[name=\"Cook County\"](area); map_to_area; node(pivot); out geom; more info","title":"Specify area with higher level area"},{"location":"Overpass%20Manual/#get-historical-data","text":"To get historical data, prepend the query with a date statement. Example: [date:\"2016-01-01T00:00:00Z\"]; area[name=\"City of New York\"]; node(pivot); out geom;","title":"Get historical data"},{"location":"QGIS/","text":"Adding layers Layer -> Add Layer Count the number of features in area select the right layer on the top panel, third row, select the tool Select Features by area or single click select features Open the attribute table for layer. In the heder, there should be the number of selected features Postgis Layers Invalid layer The layer can be marked as invalid because of a missing or invalid index column. Each Postgis layer needs an id column consisting of unique integer values .","title":"Adding layers"},{"location":"QGIS/#adding-layers","text":"Layer -> Add Layer","title":"Adding layers"},{"location":"QGIS/#count-the-number-of-features-in-area","text":"select the right layer on the top panel, third row, select the tool Select Features by area or single click select features Open the attribute table for layer. In the heder, there should be the number of selected features","title":"Count the number of features in area"},{"location":"QGIS/#postgis-layers","text":"","title":"Postgis Layers"},{"location":"QGIS/#invalid-layer","text":"The layer can be marked as invalid because of a missing or invalid index column. Each Postgis layer needs an id column consisting of unique integer values .","title":"Invalid layer"},{"location":"RCI%20cluster/","text":"General To get access to the RCI cluster: https://docs.google.com/forms/d/e/1FAIpQLSewws_V6-D567fkp6QZmr0GQlkzQrEoB6QquAgQkZu8so818Q/viewform Official instructions: https://login.rci.cvut.cz/wiki/how_to_start Usual command Usage You can watch your jobs with squeue -u username . You test/debug your program when running it with srun command you usually don\u2019t have to allocate resources when testing To start an interactive shell, run: srun -p cpufast --pty bash -i Set your main script file (sh) executable via chmod +x <filename> command Test your script in console Cancel the job: scancel <JOB ID> You run your job with sbatch command with allocated resources Example sbatch: sbatch --mem=30G -t 60 -n30 -o /home/fiedlda1/Amodsim/log/ih.log /home/fiedlda1/Amodsim/rci_launchers/ih.sh How to clone projects Usually, you need to clone some of the SUM projects to start working on the RCI cluster. To do that: 1. Copy your key to ~/.ssh/ 2. Set file permissions to your ssh key safely 3. Modify ~/.ssh/config IdentityFile to point to your key 4. Clone your project Specifics for java projects Clone project on RCI cluster Download binary maven from: http://maven.apache.org/download.cgi and export it to your home folder on the RCI cluster. Prepare your bash script Add #!/bin/bash to the first line Change the environment variable PATH for your maven location with this command: PATH=$PATH:/home/$USER/apache-maven-3.6.1/bin/ Load all required software via ml command, definitely ml Java Build, compile, run your project via mvn commands Set your file executable via chmod +x filename command Example run command: mvn exec:exec -Dexec.executable=java -Dexec.args=\u2019-classpath %classpath -Xmx30g cz.cvut.fel.aic.amodsim.OnDemandVehiclesSimulation /home/kholkolg/amod-to-agentpolis/local_config_files/olga_vga_RCI.cfg\u2019 -Dfile.encoding=UTF-8 Example - Bash script for amodsim project: Run your script with srun, sbatch etc. commands, I recommend first use srun, to check everything is set up ok and then use sbatch command, because if computational nodes are busy, your job will be added to the queue and you can do other work. Specifics for python projects First load the appropriate version of python, e. G.: ml Python/3.6.6-foss-2018b You can\u2019t just install the packages with sudo , you have to install them to the user space instead: Run pip install --user packagename Specifics for C++ projects As linux binaries are usually not portable. They are not compatible with older linux versions due to the infamous glibc incompatibility. There are three solutions to this problem: Method Setup Program Upgrade Compile the code on the RCI Setup the compilation on RCI. Copy the source code to RCI and recompile after every change Use a Singularity container learn with singularity, create the container Generate new container and copy it to the RCI Build a compatible binary using a modified toolchain learn with a toolchain generator, configure and generate the right toolchain Copy the updated binary Specific for projects with gurobi Load Gurobi with ml Gurobi or ml Gurobi/8.1.1-foss-2018b-Python-3.6.6 for a specific version Be aware that this operation can reload other packages Gurobi and Java It is necessary to install Gurobi to maven: mvn install:install-file -Dfile=/mnt/appl/software/Gurobi/9.0.3-GCCcore-8.3.0-Python-3.7.4/lib/gurobi.jar -DgroupId=com.gurobi -DartifactId=gurobi -Dversion=1.0 -Dpackaging=jar Gurobi and C++ As RCI use Linux as OS, we need to compile the Gurobi C++ libs with the same compiler as the one we use for compilation of our code (see C++ Workflow for more details). Note that this is necessary even if the Gurobi seems to be compiled with the same copiler we use for compilation . Unlike in Linux installation we controll, we cannot build the C++ lib in the Gurobi installation folder. To make the Linking work, foloow these steps: 1. copy the src dir from the RCI Gurobi module located at mnt/appl/software/Gurobi/<desired version> to our home 2. run make located in src/build 3. copy the libgurobi_c++.a to the lib subfolder of your project 4. configure the searching for the C++ lib in FindGUROVI.cmake file: find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED ) if the CMake cache is already generated, delete it. Generate the CMake cache and build the project Commands For command description, see Slurm manual. Main params - --mem=<required memory> - <required memory> is in megabytes by default, for gigabytes, we need to add G - -t <time> - Time in minutes Task Explanations srun --pty bash -i - --pty runs the first task and close output and error stream for everything except the first task - bash : what we want to run - -i Input setting, here followed by no param indicating that the input stream is closed osed","title":"General"},{"location":"RCI%20cluster/#general","text":"To get access to the RCI cluster: https://docs.google.com/forms/d/e/1FAIpQLSewws_V6-D567fkp6QZmr0GQlkzQrEoB6QquAgQkZu8so818Q/viewform Official instructions: https://login.rci.cvut.cz/wiki/how_to_start","title":"General"},{"location":"RCI%20cluster/#usual-command-usage","text":"You can watch your jobs with squeue -u username . You test/debug your program when running it with srun command you usually don\u2019t have to allocate resources when testing To start an interactive shell, run: srun -p cpufast --pty bash -i Set your main script file (sh) executable via chmod +x <filename> command Test your script in console Cancel the job: scancel <JOB ID> You run your job with sbatch command with allocated resources Example sbatch: sbatch --mem=30G -t 60 -n30 -o /home/fiedlda1/Amodsim/log/ih.log /home/fiedlda1/Amodsim/rci_launchers/ih.sh","title":"Usual command Usage"},{"location":"RCI%20cluster/#how-to-clone-projects","text":"Usually, you need to clone some of the SUM projects to start working on the RCI cluster. To do that: 1. Copy your key to ~/.ssh/ 2. Set file permissions to your ssh key safely 3. Modify ~/.ssh/config IdentityFile to point to your key 4. Clone your project","title":"How to clone projects"},{"location":"RCI%20cluster/#specifics-for-java-projects","text":"Clone project on RCI cluster Download binary maven from: http://maven.apache.org/download.cgi and export it to your home folder on the RCI cluster. Prepare your bash script Add #!/bin/bash to the first line Change the environment variable PATH for your maven location with this command: PATH=$PATH:/home/$USER/apache-maven-3.6.1/bin/ Load all required software via ml command, definitely ml Java Build, compile, run your project via mvn commands Set your file executable via chmod +x filename command Example run command: mvn exec:exec -Dexec.executable=java -Dexec.args=\u2019-classpath %classpath -Xmx30g cz.cvut.fel.aic.amodsim.OnDemandVehiclesSimulation /home/kholkolg/amod-to-agentpolis/local_config_files/olga_vga_RCI.cfg\u2019 -Dfile.encoding=UTF-8 Example - Bash script for amodsim project: Run your script with srun, sbatch etc. commands, I recommend first use srun, to check everything is set up ok and then use sbatch command, because if computational nodes are busy, your job will be added to the queue and you can do other work.","title":"Specifics for java projects"},{"location":"RCI%20cluster/#specifics-for-python-projects","text":"First load the appropriate version of python, e. G.: ml Python/3.6.6-foss-2018b You can\u2019t just install the packages with sudo , you have to install them to the user space instead: Run pip install --user packagename","title":"Specifics for python projects"},{"location":"RCI%20cluster/#specifics-for-c-projects","text":"As linux binaries are usually not portable. They are not compatible with older linux versions due to the infamous glibc incompatibility. There are three solutions to this problem: Method Setup Program Upgrade Compile the code on the RCI Setup the compilation on RCI. Copy the source code to RCI and recompile after every change Use a Singularity container learn with singularity, create the container Generate new container and copy it to the RCI Build a compatible binary using a modified toolchain learn with a toolchain generator, configure and generate the right toolchain Copy the updated binary","title":"Specifics for C++ projects"},{"location":"RCI%20cluster/#specific-for-projects-with-gurobi","text":"Load Gurobi with ml Gurobi or ml Gurobi/8.1.1-foss-2018b-Python-3.6.6 for a specific version Be aware that this operation can reload other packages","title":"Specific for projects with gurobi"},{"location":"RCI%20cluster/#gurobi-and-java","text":"It is necessary to install Gurobi to maven: mvn install:install-file -Dfile=/mnt/appl/software/Gurobi/9.0.3-GCCcore-8.3.0-Python-3.7.4/lib/gurobi.jar -DgroupId=com.gurobi -DartifactId=gurobi -Dversion=1.0 -Dpackaging=jar","title":"Gurobi and Java"},{"location":"RCI%20cluster/#gurobi-and-c","text":"As RCI use Linux as OS, we need to compile the Gurobi C++ libs with the same compiler as the one we use for compilation of our code (see C++ Workflow for more details). Note that this is necessary even if the Gurobi seems to be compiled with the same copiler we use for compilation . Unlike in Linux installation we controll, we cannot build the C++ lib in the Gurobi installation folder. To make the Linking work, foloow these steps: 1. copy the src dir from the RCI Gurobi module located at mnt/appl/software/Gurobi/<desired version> to our home 2. run make located in src/build 3. copy the libgurobi_c++.a to the lib subfolder of your project 4. configure the searching for the C++ lib in FindGUROVI.cmake file: find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED ) if the CMake cache is already generated, delete it. Generate the CMake cache and build the project","title":"Gurobi and C++"},{"location":"RCI%20cluster/#commands","text":"For command description, see Slurm manual. Main params - --mem=<required memory> - <required memory> is in megabytes by default, for gigabytes, we need to add G - -t <time> - Time in minutes","title":"Commands"},{"location":"RCI%20cluster/#task-explanations","text":"srun --pty bash -i - --pty runs the first task and close output and error stream for everything except the first task - bash : what we want to run - -i Input setting, here followed by no param indicating that the input stream is closed osed","title":"Task Explanations"},{"location":"LaTeX/Latex%20manual/","text":"Floats The following environments are floats: - figure - table - algorithm Placement Any float takes the position as a first argument. The following positions are available: - h : here - t : top - b : bottom - p : special dedicated page per float - ! : ignore nice positioning and put it according to the float specifiers The placement algorithm then iterate pages starting from the page where the float is placed. For each page, it tries to find a place for the float according to the float specifiers (in the same order as they appear in the float position argument). In case of success, the procedure stops and place the float. If the procedure fails for all pages, the float is placed at the end. Sources: - LaTeX Wikibook - Overleaf Default placement The default placement differs between environments and also classes. For example for article class, the default placement for figure and table is tbp ( see SO ). Tables The float environment for tables is table . However, the rows and columns are wrapped in another environment, usually tabular . The usual way to create a table is: \\begin{table}[h] \\centering % center the content of the table environment \\begin{tabular}{|c|c|} ... rows and columns \\end{tabular} \\caption{My table} \\label{tab:my_table} \\end{table} Column types The column types are specified in the argument of the tabular environment. The following column types are available: - l : left aligned - c : centered - r : right aligned - p{width} : paragraph column with specified width Bibliography There are two bibliography management systems in LaTeX: the older bibtex and newer and more powerfull biblatex. Unfortunatelly, the two systems are not compatible. This means that for each of them, we need to: - have the bibliography file in the right format - have the style in the right format ( .bst for bibtex, .bbx for biblatex) By default, we should use biblatex. Howevver, there are some circumstances where we need to use bibtex, for example, if we need to use a style that is not available for biblatex (as there is no conversion tool ). The styles available for biblatex are listed on CTAN . Biblatex Basic setup: \\usepackage[style=numeric]{biblatex} ... \\addbibresource{bibliography.bib} ... \\printbibliography The style parameter is optional. The styles available for biblatex are listed on CTAN . Bibtex Basic setup: \\bibliographystyle{plain} ... \\bibliography{bibliography} Note that we do not to use any package for bibtex. Also note, that the \\bibliographystyle command is mandatory . Finally, we do not need to specify the extension of the bibliography file. Common problems Ugly font in pdf This can be cause by the missing vector fonts. If the vector fonts are missing, the bitmap fonts are used instead. 1. To check if this is the cause, zoom in on the pdf. If the text is blurry, the bitmap fonts are used. 1. To fix this, install the vector fonts. - On Windows, install the cm-super package through MikTeX.","title":"Floats"},{"location":"LaTeX/Latex%20manual/#floats","text":"The following environments are floats: - figure - table - algorithm","title":"Floats"},{"location":"LaTeX/Latex%20manual/#placement","text":"Any float takes the position as a first argument. The following positions are available: - h : here - t : top - b : bottom - p : special dedicated page per float - ! : ignore nice positioning and put it according to the float specifiers The placement algorithm then iterate pages starting from the page where the float is placed. For each page, it tries to find a place for the float according to the float specifiers (in the same order as they appear in the float position argument). In case of success, the procedure stops and place the float. If the procedure fails for all pages, the float is placed at the end. Sources: - LaTeX Wikibook - Overleaf","title":"Placement"},{"location":"LaTeX/Latex%20manual/#default-placement","text":"The default placement differs between environments and also classes. For example for article class, the default placement for figure and table is tbp ( see SO ).","title":"Default placement"},{"location":"LaTeX/Latex%20manual/#tables","text":"The float environment for tables is table . However, the rows and columns are wrapped in another environment, usually tabular . The usual way to create a table is: \\begin{table}[h] \\centering % center the content of the table environment \\begin{tabular}{|c|c|} ... rows and columns \\end{tabular} \\caption{My table} \\label{tab:my_table} \\end{table}","title":"Tables"},{"location":"LaTeX/Latex%20manual/#column-types","text":"The column types are specified in the argument of the tabular environment. The following column types are available: - l : left aligned - c : centered - r : right aligned - p{width} : paragraph column with specified width","title":"Column types"},{"location":"LaTeX/Latex%20manual/#bibliography","text":"There are two bibliography management systems in LaTeX: the older bibtex and newer and more powerfull biblatex. Unfortunatelly, the two systems are not compatible. This means that for each of them, we need to: - have the bibliography file in the right format - have the style in the right format ( .bst for bibtex, .bbx for biblatex) By default, we should use biblatex. Howevver, there are some circumstances where we need to use bibtex, for example, if we need to use a style that is not available for biblatex (as there is no conversion tool ). The styles available for biblatex are listed on CTAN .","title":"Bibliography"},{"location":"LaTeX/Latex%20manual/#biblatex","text":"Basic setup: \\usepackage[style=numeric]{biblatex} ... \\addbibresource{bibliography.bib} ... \\printbibliography The style parameter is optional. The styles available for biblatex are listed on CTAN .","title":"Biblatex"},{"location":"LaTeX/Latex%20manual/#bibtex","text":"Basic setup: \\bibliographystyle{plain} ... \\bibliography{bibliography} Note that we do not to use any package for bibtex. Also note, that the \\bibliographystyle command is mandatory . Finally, we do not need to specify the extension of the bibliography file.","title":"Bibtex"},{"location":"LaTeX/Latex%20manual/#common-problems","text":"","title":"Common problems"},{"location":"LaTeX/Latex%20manual/#ugly-font-in-pdf","text":"This can be cause by the missing vector fonts. If the vector fonts are missing, the bitmap fonts are used instead. 1. To check if this is the cause, zoom in on the pdf. If the text is blurry, the bitmap fonts are used. 1. To fix this, install the vector fonts. - On Windows, install the cm-super package through MikTeX.","title":"Ugly font in pdf"},{"location":"Programming/Common/","text":"Keymap Copy : Ctrl + C Cut : Ctrl + X Paste : Ctrl + V Toggle comment : Ctrl + Q Search in file : Ctrl + S Sellect all : Ctrl + A Format selection : Ctrl + F Format File : Ctrl + Shift + F Refacor->Rename : Ctrl + R Text transform : Ctrl + T + U : to upper case Testing private methods An urgent need to test privete method accompanied with a lack of knowledge of how to do it is a common problem. In almost all programming languages, the testing of private methods is obsturcted by the language itself, i.e., the test frameworks does not have a special access to private methods. In this section we disscuss the usuall solutions to this problem. These implementation is specific to a particular language, but the general ideas are the same. The possible approaches are: - Makeing the method public : Only recommended if the method should be exposed, i.e., its functionality is not limited to the class itself. - Move the method to a different class : Maybe, the method is correcly marked as private in the current context, but it can also be extracted to its own class, where it will become the main method of the class. This applies to methods that can be used in other contexts, or for methods contained in large classes. - Mark the method as internal and make it public : This is a strategy that can be always applied with minimum effort. Various ways how to signalize that the method is intended for internal use are: - Naming convention : The method name can start with an underscore, e.g., _my_method . - Documentation : The comments can contain a warning that the method is intended for internal use. - Namespace : The method can be placed in a namespace that signals that it is intended for internal use, e.g., internal::my_method . - Special access : We can use special language-dependant tools that can provide a special access to private methods: - in C++ the friend keyword can be used to grant access to a class to another class. - In Java, the @VisibleForTesting annotation can be used to mark a method as visible for testing. - In Python, the __test__ attribute can be used to mark a method as visible for testing.","title":"Keymap"},{"location":"Programming/Common/#keymap","text":"Copy : Ctrl + C Cut : Ctrl + X Paste : Ctrl + V Toggle comment : Ctrl + Q Search in file : Ctrl + S Sellect all : Ctrl + A Format selection : Ctrl + F Format File : Ctrl + Shift + F Refacor->Rename : Ctrl + R Text transform : Ctrl + T + U : to upper case","title":"Keymap"},{"location":"Programming/Common/#testing-private-methods","text":"An urgent need to test privete method accompanied with a lack of knowledge of how to do it is a common problem. In almost all programming languages, the testing of private methods is obsturcted by the language itself, i.e., the test frameworks does not have a special access to private methods. In this section we disscuss the usuall solutions to this problem. These implementation is specific to a particular language, but the general ideas are the same. The possible approaches are: - Makeing the method public : Only recommended if the method should be exposed, i.e., its functionality is not limited to the class itself. - Move the method to a different class : Maybe, the method is correcly marked as private in the current context, but it can also be extracted to its own class, where it will become the main method of the class. This applies to methods that can be used in other contexts, or for methods contained in large classes. - Mark the method as internal and make it public : This is a strategy that can be always applied with minimum effort. Various ways how to signalize that the method is intended for internal use are: - Naming convention : The method name can start with an underscore, e.g., _my_method . - Documentation : The comments can contain a warning that the method is intended for internal use. - Namespace : The method can be placed in a namespace that signals that it is intended for internal use, e.g., internal::my_method . - Special access : We can use special language-dependant tools that can provide a special access to private methods: - in C++ the friend keyword can be used to grant access to a class to another class. - In Java, the @VisibleForTesting annotation can be used to mark a method as visible for testing. - In Python, the __test__ attribute can be used to mark a method as visible for testing.","title":"Testing private methods"},{"location":"Programming/Google%20API%20and%20Apps%20Script/","text":"Comments API reference Anchored Comments According to my experiments and SO . It is not possible to add achore comments to google docs. Also, it is not possible to get any readable anchors from existing comments. The only way to extract comments with anchors is to export the document as Microsoft Word. Written with StackEdit .","title":"Comments"},{"location":"Programming/Google%20API%20and%20Apps%20Script/#comments","text":"API reference","title":"Comments"},{"location":"Programming/Google%20API%20and%20Apps%20Script/#anchored-comments","text":"According to my experiments and SO . It is not possible to add achore comments to google docs. Also, it is not possible to get any readable anchors from existing comments. The only way to extract comments with anchors is to export the document as Microsoft Word. Written with StackEdit .","title":"Anchored Comments"},{"location":"Programming/Gurobi/","text":"Parallel Execution The gurobi solver solves a problem in parallel by default, trying multiple solution methods at the same time (see the official description ). It is also possible to run multiple problems in parallel ( source ), but each problem should be run in its own gurobi environment. Also, each environment should be configured to use only a single thread (e.g., in C++: env.set(GRB_IntParam_Threads, 1); ). The problem with this approach is that the CPU is usually not the bottleneck of the computation, the bottleneck is the memory ( source ). Therefore, solving multiple problems in parallel does not guarantee any speed up, it could be actually slower. The performance could be most likely improved when running the problems in parallel on multiple machines (not multiple cores of the same machine). Some advised to use MPI for that. Written with StackEdit .","title":"Parallel Execution"},{"location":"Programming/Gurobi/#parallel-execution","text":"The gurobi solver solves a problem in parallel by default, trying multiple solution methods at the same time (see the official description ). It is also possible to run multiple problems in parallel ( source ), but each problem should be run in its own gurobi environment. Also, each environment should be configured to use only a single thread (e.g., in C++: env.set(GRB_IntParam_Threads, 1); ). The problem with this approach is that the CPU is usually not the bottleneck of the computation, the bottleneck is the memory ( source ). Therefore, solving multiple problems in parallel does not guarantee any speed up, it could be actually slower. The performance could be most likely improved when running the problems in parallel on multiple machines (not multiple cores of the same machine). Some advised to use MPI for that. Written with StackEdit .","title":"Parallel Execution"},{"location":"Programming/JSON/","text":"Jackson Usefull Annotations @JsonIncludeProperties : Ignore all properties except listed @JsonProperty(\"my_name\") : Custom name of the JSON key @JsonIgnore : Ignore the json property below Wrapping the Obejct in Another JSON Object To do that, use these annotations above the class. @JsonTypeName(value = \"action\") @JsonTypeInfo(include=As.WRAPPER_OBJECT, use=Id.NAME) If you do not care about the name, you can skip the @JsonTypeName annotation. Written with StackEdit .","title":"Jackson"},{"location":"Programming/JSON/#jackson","text":"","title":"Jackson"},{"location":"Programming/JSON/#usefull-annotations","text":"@JsonIncludeProperties : Ignore all properties except listed @JsonProperty(\"my_name\") : Custom name of the JSON key @JsonIgnore : Ignore the json property below","title":"Usefull Annotations"},{"location":"Programming/JSON/#wrapping-the-obejct-in-another-json-object","text":"To do that, use these annotations above the class. @JsonTypeName(value = \"action\") @JsonTypeInfo(include=As.WRAPPER_OBJECT, use=Id.NAME) If you do not care about the name, you can skip the @JsonTypeName annotation. Written with StackEdit .","title":"Wrapping the Obejct in Another JSON Object"},{"location":"Programming/PostgreSQL%20Manual/","text":"Data types official documentation Date official documentation date : for dates time for time timestmp for both date and time interval Select a part of date/time/timestamp If we want just a part of a date, time, or timestamp, we can use the extract function. Example: SELECT extract(hour FROM <date column name>) FROM... Other parts can be extracted too. To extract day of week , we can use isodow (assigns 1 to Monday and 7 to Sunday). Auto incrementing columns In PostgreSQL, sequences are used for auto-incrementing columns. When you are creating a new db table or adding a new column, the process of creating a new sequence can be automated by choosing an identity or a serial column type. When updating an aexisting column, a manual intervention is required: 1. change the column to some numerical datatype 2. create the sequence: CREATE SEQUENCE <SEQUENCE NAME> OWNED BY <TABLE NAME>.<COLUMN NAME>; adjust the value of the sequence: SELECT setval(pg_get_serial_sequence('<TABLE NAME>', '<COLUMN NAME>'), max(<COLUMN NAME>)) FROM <TABLE NAME>; set the column to be incremented by the sequence: ALTER TABLE <TABLE NAME> ALTER COLUMN <COLUMN NAME> SET DEFAULT nextval('<SEQUENCE NAME>'); Strings There are many string function available, including the format function that works similarly to the C format function. For all functions, check the documentation . Arrays array functions and operators To compute array length , use array_length(contracted_vertices, 1) where 1 stands for the first dimension. To cretea an array literal , we use single quatation and curly brackets: '{1, 2, 3}' . To check that some value match at least some member of the array, we use ANY : SELECT ... FROM tab WHERE tab.a = ANY(<array>) Working with the array members individualy For using the members of an array in the SELECT or JOIN , we have to first split the array using the unnest function. This function transforms the result set to a form where there is a separate row for each member of the array (a kind of inverse operation to group by). If we want to also keep the array index, we can use the WITH ORDINALITY expression, as shown in the manual or on SO . hstore A specific feature of PostgreSQL is the hstore column type. It enables to store structured data in a single column. It can be used to dump variables that we do not plan to utilize in the database (i.e., in the SELECT, JOIN statements) frequently. When we, exceptionally, want to access a variable from a hstore column, we can use the following syntax: SELECT <COLUMN NAME>-><VARIABLE NAME> AS ... Selecting rows for deletion based on data from another table If we want to delete rows from a table based on some condition on data from another table, we can use the DELETE statement with a USING clause. Example: DELETE FROM nodes_ways_speeds USING nodes_ways WHERE nodes_ways_speeds.to_node_ways_id = nodes_ways.id AND nodes_ways.area IN (5,6) Handeling duplicates in the INSERT statement To handle duplicates on INSERT , PostgreSQL provides the ON CONFLICT clause (see the INSERT documentation). The options are: - DO NOTHING : do nothing - DO UPDATE SET <column name> = <value> : update the column to the given value Procedures and functions Calling a procedure to exectue a stored procedure, use\" CALL <procedure name>(<procedure arguments>) Unlike in programing languages, there is no implicit type cast of the program arguments, including literals. Therefore, we need to cast all parameters explicitely, as in the following example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments(target_area_id smallint) ... CALL compute_speeds_for_segments(1::smallint); Creating a procedure documentation The syntax is as follows: CREATE PROCEDURE <name> (<params>) LANGUAGE <language name> <procedure body> , while <procedure body> can be both a delimited string: AS $$ <sql statements> $$ OR an active SQL body (since PostgreSQL 14): BEGIN ATOMIC <sql statements> END There are some differences between those syntaxes (e.g., the second one works only for SQL and is evaluated/checked for validity at the time of creation), bot in most cases, they are interchangable. For more details, check the manual . Variables In PostgreSQL, all variables must be declared before assignmant in the DECLARE block which is before the sql body of the function/procedure/ DO . The syntax is: <variable name> <variable type>[ = <varianle value>]; Example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments() LANGUAGE plpgsql AS $$ DECLARE dataset_quality smallint = 1; BEGIN ... The, the value of a variable can be change using the classical assignment syntax: <variable name> = <varianle value>; Be carful to not use a variable name equal to the name of some of the columns used in the same context, which results in a name clash. Assigning a value to a variable using SQL There are two options how to assign a value to a variable using SQL: - using the INTO clause in the SELECT statement - using a SELECT statement as rvlaue of the assignment Example with INTO : SELECT name INTO customer_name FROM customers WHERE id = 1 EXAMPLE with SELECT as rvalue: customer_name = (SELECT name FROM customers WHERE id = 1) Functions Functions in PostgreSQL have a similar syntax to procedures. Unlike for procedures, we need to specify a return type for function, either as an OUT / INOUT parameter, or using the RETURNS clause. To return tabular data, we use TABLE return type: RETURNS TABLE(<param 1 name> <param 1 type>, ..., <param n name> <param n type>) To select the result of a function with the return type above, call: SELECT * FROM <function signature> RETURN NEXT and RETURN QUERY Sometimes, we need to do some cleanup after selecting the rows to be returned from function, or we need to build the result in a loop. In classical programming languages, we use variables for this purpose. In PG/plSQL, we can also use the RETURN NEXT and RETURN QUERY constructs. These constructs prepare the result, and does not return from the function . Instead, use an empty RETURN to return from the function. Example: RETURN QUERY SELECT ...; DROP TABLE target_ways; RETURN; Note that for these constructs, the return type needs to be a table or setof type. The RETURN QUERY cannot be used for returning a single value even if the query returns a single value. If we have a single value return type and need to do some postprocessing between selecting the value and returning from the function, we have to use a variable instead. Deciding the language For simple statements, we can use the SQL language. We need the PL/pgSQL if: - we need to use variables or control statements specific to PL/pgSQL - we need to use temporary tables, as the SQL language fails to recognize them if they are created inside the function/procedure Conditional filters based on the value of a variable or input parameter To add some filter to the WHERE or ON clause of a query based on the value of a variable or input parameter, we can use the following technique: 1. set the variable or input parameter to NULL if we do not want to apply the filter 2. in the filter test for disjunction of NULL or the filter condition Example: SELECT ... FROM ... WHERE param IS NULL OR some_column = param Temporary tables in PostgreSQL To use a result set efficiently in a function or procedure, we often use temporary tables. Unlike in other relational database systems, in PostgreSQl, the lifetime of a temporary table is bound to a session. Therefoe, if we call a function that creates a tempoary table multiple times in a single session, we encounter an error, because the table already exists. To tackle this problem, we need to delete all temporary tables manually. Luckily, there is a special DISCARD command that can be used to dtop all temporary tables at once: DISCARD TEMPORARY; DO command The DO command can be used to execude an anonymus code block in any of the languages suported by PostgreSQL. It behaves like a function with no parameters and no return value. Syntax: DO [LANGUAGE <lang name>] <code> The default language is plpgsql . Example: DO $$ BEGIN RAISE NOTICE 'Hello world'; END $$ Window functions PostgreSQL supports an extended syntax for window functions . We can use it for example to retrieve the value of a column that has maximum value in another column, as demonstrated in an SO answer . PostGis Geometry columns Postgis features can be utilized with geometry and geography column types. To add a new geometry column: ADD COLUMN <COLUUMN NAME> geometry(<GEOMETRY TYPE>, <SRID>) Spatial Indexing Documentation Analogously to standard SQL column indicis, there are spatial indices in PostGIS. The only difference is that we need to add the USING GIST at the end of the CREATE INDEX statement: CREATE INDEX nodes_geom_idx ON nodes USING GIST (geom); Converting between geometry types There are dedicated functions whcich we can use to convert between geometry types: - ST_Multi : converts geometries to their multi-variant, e.g., LineString to MultiLineString . Compute area surrounding geometries If we are ok with a convex envelope of the geometries, we can simply use the St_ConvexHull functon. Howeever, if we need the exact shape, We have to use the St_ConcaveHull function which computes the concave hull of a geometry. The St_ConcaveHull function takes an additional parameter param_pctconvex which determines how concave is the result: 0 means strictly concave, while 1 means convex hull. Note that while lowere values leads to more acurate results, the computation is much slower. There is also another parameter param_allow_holes which determines whether holes in the object are permited (default false). Split area to polygons based on points The split of spece into polygons based on a set of points is called Voronoi diagram . In PostGIS, we have a [ ST_Voronoi_Polygons](https://postgis.net/docs/ST_VoronoiPolygons.html) for that. To obtain a set of polygons from a set of points, it is necessary to 1. Aggregate the rows ( ST_Collect ) 2. Compute the polygon geometry ( ST_Voronoi_Polygons ) 3. Disaggregate the geometry into individual polygons ( ST_Dump`) Also, there is an important aspect of how far the polygons will reach outside of the points. By default, it enlarge the area determined by the points by about 50%. If we need a larger area, we can use the extend_to parameter. If we need a smaller area, however, we need to compute the intersection with this smaller area afterwards manually. Full example: SELECT st_intersection( (st_dump( st_voronoipolygons(st_collect(<GEOMETRY COLUMN>)) )).geom, (<select clause for area of desired voronoi polygons>) ) AS geom FROM ... If we need to join the polygons to the original points, we need to do it manually (e.g. by JOIN ... ON ST_Within(<POINT COLUMN>, <VORONOI POLYGONS GEO DUMP>) ). Other Useful Functions [ ST_Within ] ST_Within (A, B) if A is completly inside B ST_Intersects ST_Intersects(g1, g2) if g1 and g2 have at least one point in common. ST_Transform : ST_Transform(g, srid) transforms geometry g to a projection defined by the srid and returns the result as a geometry. ST_Buffer : ST_Buffer(g, radius) computes a geometry that is an extension of g by radius to all directions St_Collect aggregates data into single geometry. It is usually apllied to a geometry column in an SQL selection. ST_Union ST_Area ST_Equals ST_MakeLine : make line between two points ST_SetSRID : sets the SRID of the geometry and returns the result as a new geometry. ST_MakePoint : creates a point geometry from the given coordinates. PgRouting PgRouting is a PostgreSQL extension focused on graph/network manpulation. It contains functions for: - finding the strongly connected components: `pgr_strongComponents - graph contraction/simplification Finding strongly connected components The function pgr_strongComponents finds the strongly connected components of a graph. The only parameter of the script is a query that should return edge data in the folowing format: - id , - source , - target , - cost , - reverse_cost . The first three parameters are obvious. The cost parameter does not have any effect. You should provide a negative reverse_cost , othervise, the edge will be considered as bidirectional! PL/pgSQL PL/pgSQL is a procedural language available in PostgreSQL databases. It can be used inside: - functions - procedures - DO command Branching PL/pgSQL has the following branching: IF <condition> THEN ... [ELSEIF ... ] [ELSE ... ] END IF Logging Basic logging can be done using the RAISE command: RAISE NOTICE 'Some message'; We can add parameters by using the % placeholder: RAISE NOTICE 'Some message %', <variable or SQL command>; For more, see the documentation . Query diagnostics Various information about the last query can be obtained using the GET DIAGNOSTIC command. For example, the number of rows affected by the last query can be obtained using the ROW_COUNT parameter: GET DIAGNOSTIC <variable> = ROW_COUNT; The result is stored in the variable <variable> . Note that this constract is not available in the SQL queries but only in a PL/pgSQL block. For other diagnostic fields, see the documentation . psql psql is a basic command line utitiltyfor manipulating postgres database. To connect: psql -d <db name> Then the psql commands can be executed. To execute command immediatelly, use the -c parameter: psql -d <db name> -c \"<command>\" Do not forget to quote the command. Importing data from csv The prefered mathod depends on the characte of the data: - data exactly match the table in the database: use psql COPY command - data do not match the table, but they are small: 1. load the data with pandas 2. process the data as needed 3. use pandas.to_sql to upload the data - data do not match the table and they are large: 1. preprocess the data with bach commands 2. use psql COPY to upload the data - data do not match the table and they are large and dirty : use the file_fdw module: 1. create a table for SQL mapping with tolerant column types (e.g., text for problematic columns) 2. select from the mapping to the real table psql COPY command The COPY command can be used to copy the input into a database table. A subset of database column can be selected, but that is not true for the input, i.e, all input columns have to be used. If a subset of input columns needs to be used, or some columns requires processing, you need to perform some preprocessing. COPY manual Importing data from a shapefile There are multiple options: - shp2psql : simple tool that creates sql from a shapefile - easy start, almost no configuration - always imports all data from shapefile, cannot be configured to skip columns - included in the Postgres instalation - ogr2ogr - needs to be installed, part of GDAL - QGIS - The db manger can be used to export data from QGIS - data can be viewed before import - only suitable for creating new table, not for appending to an existing one Importing data from GeoJSON For a single geometry stored in a GeoJSON file, the function ST_GeomFromGeoJSON can be used. - just copy the geometry part of the file - change the geometry type to match the type in db - don't forget to surround the object with curly brackets For the whole document, the ogr2ogr tool can be used. Lost Password to the Postgres Server The password for the db superuser is stored in db postgres . In order to log there and change it, the whole authentification has to be turned off, and then we can proceed with changing the password. Steps: 1. find the pg_hba.conf file - usually located in C:\\Program Files\\PostgreSQL\\13\\data 2. backup the file and replace all occurances of scram-sha-256 in the file with trust 3. restart the posgreSQL service - in the Windows service management, there should be a service for postgresql running 4. change the password for the superuser 1. psql -U postgres 2. ALTER USER postgres WITH password 'yourpassword'; (do not forget the semicolon at the end!) 5. restore the pg_hba.conf file from backup 6. restart the postgreSQL service again 7. test if the new password works DataGrip Import Formats DataGrip can handle imports only from separator baset files (csv, tsv). View Geometry In the reuslt window/tab, click on the gear wheel -> Show GeoView . However, the geoviewer has a fixed WGS84 projection, so you have to project the result to this projection first . Create a spatial Index There is currently no GUI tool for that in DataGrip. Just add a normal index and modify the auto generated statement by changing <column> to USING GIST(<column>) at the end of the statement. Filter out store procedures right-click on routines click open table sort by type Creating functions and procedures There is no UI available currently, use Navicat or console Duplicate table Drag the table in the database explorer and drop it to the location you want it to copy to. Navicat Cannot connect to db Symptoms: - cant connect to db server: Could not connect - after editing the connection and trying to save it (ok button): connection is being used Try: 1. close navicat 2. open navicat, edit connection 1. click test connection 1. click ok, and start the connection by double click PgAdmin The best way to install the PgAdmin is to use the EDB PostgreSQL installer and uncheck the database installation during the installation configuration. This way, we also install useful tools like psql Diagrams To create diagram from an existing database: right click on the database -> Generate ERD Kill a hanging query To kill a hanging query, we need to complete two steps: 1. identify the query PID 1. kill the query To identify the PID of the problematic query, we can use tool such as pg_activity : sudo -u postgres pg_activity -U postgres To kill the query, run: SELECT pg_cancel_backend(<PID>) Troubleshooting If the db tools are unresponsive on certain tasks/queries, check if the table needed for those queries is not locke by some problematic query. Select PostgreSQL version SELECT version() Tried to send an out-of-range integer as a 2-byte value This error is caused by a too large number of values in the insert statement. The maximum index is a 2-byte number (max value: 32767). The solution is to split the insert statement into smaller bulks.","title":"Data types"},{"location":"Programming/PostgreSQL%20Manual/#data-types","text":"official documentation","title":"Data types"},{"location":"Programming/PostgreSQL%20Manual/#date","text":"official documentation date : for dates time for time timestmp for both date and time interval","title":"Date"},{"location":"Programming/PostgreSQL%20Manual/#select-a-part-of-datetimetimestamp","text":"If we want just a part of a date, time, or timestamp, we can use the extract function. Example: SELECT extract(hour FROM <date column name>) FROM... Other parts can be extracted too. To extract day of week , we can use isodow (assigns 1 to Monday and 7 to Sunday).","title":"Select a part of date/time/timestamp"},{"location":"Programming/PostgreSQL%20Manual/#auto-incrementing-columns","text":"In PostgreSQL, sequences are used for auto-incrementing columns. When you are creating a new db table or adding a new column, the process of creating a new sequence can be automated by choosing an identity or a serial column type. When updating an aexisting column, a manual intervention is required: 1. change the column to some numerical datatype 2. create the sequence: CREATE SEQUENCE <SEQUENCE NAME> OWNED BY <TABLE NAME>.<COLUMN NAME>; adjust the value of the sequence: SELECT setval(pg_get_serial_sequence('<TABLE NAME>', '<COLUMN NAME>'), max(<COLUMN NAME>)) FROM <TABLE NAME>; set the column to be incremented by the sequence: ALTER TABLE <TABLE NAME> ALTER COLUMN <COLUMN NAME> SET DEFAULT nextval('<SEQUENCE NAME>');","title":"Auto incrementing columns"},{"location":"Programming/PostgreSQL%20Manual/#strings","text":"There are many string function available, including the format function that works similarly to the C format function. For all functions, check the documentation .","title":"Strings"},{"location":"Programming/PostgreSQL%20Manual/#arrays","text":"array functions and operators To compute array length , use array_length(contracted_vertices, 1) where 1 stands for the first dimension. To cretea an array literal , we use single quatation and curly brackets: '{1, 2, 3}' . To check that some value match at least some member of the array, we use ANY : SELECT ... FROM tab WHERE tab.a = ANY(<array>)","title":"Arrays"},{"location":"Programming/PostgreSQL%20Manual/#working-with-the-array-members-individualy","text":"For using the members of an array in the SELECT or JOIN , we have to first split the array using the unnest function. This function transforms the result set to a form where there is a separate row for each member of the array (a kind of inverse operation to group by). If we want to also keep the array index, we can use the WITH ORDINALITY expression, as shown in the manual or on SO .","title":"Working with the array members individualy"},{"location":"Programming/PostgreSQL%20Manual/#hstore","text":"A specific feature of PostgreSQL is the hstore column type. It enables to store structured data in a single column. It can be used to dump variables that we do not plan to utilize in the database (i.e., in the SELECT, JOIN statements) frequently. When we, exceptionally, want to access a variable from a hstore column, we can use the following syntax: SELECT <COLUMN NAME>-><VARIABLE NAME> AS ...","title":"hstore"},{"location":"Programming/PostgreSQL%20Manual/#selecting-rows-for-deletion-based-on-data-from-another-table","text":"If we want to delete rows from a table based on some condition on data from another table, we can use the DELETE statement with a USING clause. Example: DELETE FROM nodes_ways_speeds USING nodes_ways WHERE nodes_ways_speeds.to_node_ways_id = nodes_ways.id AND nodes_ways.area IN (5,6)","title":"Selecting rows for deletion based on data from another table"},{"location":"Programming/PostgreSQL%20Manual/#handeling-duplicates-in-the-insert-statement","text":"To handle duplicates on INSERT , PostgreSQL provides the ON CONFLICT clause (see the INSERT documentation). The options are: - DO NOTHING : do nothing - DO UPDATE SET <column name> = <value> : update the column to the given value","title":"Handeling  duplicates in the INSERT statement"},{"location":"Programming/PostgreSQL%20Manual/#procedures-and-functions","text":"","title":"Procedures and functions"},{"location":"Programming/PostgreSQL%20Manual/#calling-a-procedure","text":"to exectue a stored procedure, use\" CALL <procedure name>(<procedure arguments>) Unlike in programing languages, there is no implicit type cast of the program arguments, including literals. Therefore, we need to cast all parameters explicitely, as in the following example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments(target_area_id smallint) ... CALL compute_speeds_for_segments(1::smallint);","title":"Calling a procedure"},{"location":"Programming/PostgreSQL%20Manual/#creating-a-procedure","text":"documentation The syntax is as follows: CREATE PROCEDURE <name> (<params>) LANGUAGE <language name> <procedure body> , while <procedure body> can be both a delimited string: AS $$ <sql statements> $$ OR an active SQL body (since PostgreSQL 14): BEGIN ATOMIC <sql statements> END There are some differences between those syntaxes (e.g., the second one works only for SQL and is evaluated/checked for validity at the time of creation), bot in most cases, they are interchangable. For more details, check the manual .","title":"Creating a procedure"},{"location":"Programming/PostgreSQL%20Manual/#variables","text":"In PostgreSQL, all variables must be declared before assignmant in the DECLARE block which is before the sql body of the function/procedure/ DO . The syntax is: <variable name> <variable type>[ = <varianle value>]; Example: CREATE OR REPLACE PROCEDURE compute_speeds_for_segments() LANGUAGE plpgsql AS $$ DECLARE dataset_quality smallint = 1; BEGIN ... The, the value of a variable can be change using the classical assignment syntax: <variable name> = <varianle value>; Be carful to not use a variable name equal to the name of some of the columns used in the same context, which results in a name clash.","title":"Variables"},{"location":"Programming/PostgreSQL%20Manual/#assigning-a-value-to-a-variable-using-sql","text":"There are two options how to assign a value to a variable using SQL: - using the INTO clause in the SELECT statement - using a SELECT statement as rvlaue of the assignment Example with INTO : SELECT name INTO customer_name FROM customers WHERE id = 1 EXAMPLE with SELECT as rvalue: customer_name = (SELECT name FROM customers WHERE id = 1)","title":"Assigning a value to a variable using SQL"},{"location":"Programming/PostgreSQL%20Manual/#functions","text":"Functions in PostgreSQL have a similar syntax to procedures. Unlike for procedures, we need to specify a return type for function, either as an OUT / INOUT parameter, or using the RETURNS clause. To return tabular data, we use TABLE return type: RETURNS TABLE(<param 1 name> <param 1 type>, ..., <param n name> <param n type>) To select the result of a function with the return type above, call: SELECT * FROM <function signature>","title":"Functions"},{"location":"Programming/PostgreSQL%20Manual/#return-next-and-return-query","text":"Sometimes, we need to do some cleanup after selecting the rows to be returned from function, or we need to build the result in a loop. In classical programming languages, we use variables for this purpose. In PG/plSQL, we can also use the RETURN NEXT and RETURN QUERY constructs. These constructs prepare the result, and does not return from the function . Instead, use an empty RETURN to return from the function. Example: RETURN QUERY SELECT ...; DROP TABLE target_ways; RETURN; Note that for these constructs, the return type needs to be a table or setof type. The RETURN QUERY cannot be used for returning a single value even if the query returns a single value. If we have a single value return type and need to do some postprocessing between selecting the value and returning from the function, we have to use a variable instead.","title":"RETURN NEXT and RETURN QUERY"},{"location":"Programming/PostgreSQL%20Manual/#deciding-the-language","text":"For simple statements, we can use the SQL language. We need the PL/pgSQL if: - we need to use variables or control statements specific to PL/pgSQL - we need to use temporary tables, as the SQL language fails to recognize them if they are created inside the function/procedure","title":"Deciding the language"},{"location":"Programming/PostgreSQL%20Manual/#conditional-filters-based-on-the-value-of-a-variable-or-input-parameter","text":"To add some filter to the WHERE or ON clause of a query based on the value of a variable or input parameter, we can use the following technique: 1. set the variable or input parameter to NULL if we do not want to apply the filter 2. in the filter test for disjunction of NULL or the filter condition Example: SELECT ... FROM ... WHERE param IS NULL OR some_column = param","title":"Conditional filters based on the value of a variable or input parameter"},{"location":"Programming/PostgreSQL%20Manual/#temporary-tables-in-postgresql","text":"To use a result set efficiently in a function or procedure, we often use temporary tables. Unlike in other relational database systems, in PostgreSQl, the lifetime of a temporary table is bound to a session. Therefoe, if we call a function that creates a tempoary table multiple times in a single session, we encounter an error, because the table already exists. To tackle this problem, we need to delete all temporary tables manually. Luckily, there is a special DISCARD command that can be used to dtop all temporary tables at once: DISCARD TEMPORARY;","title":"Temporary tables in PostgreSQL"},{"location":"Programming/PostgreSQL%20Manual/#do-command","text":"The DO command can be used to execude an anonymus code block in any of the languages suported by PostgreSQL. It behaves like a function with no parameters and no return value. Syntax: DO [LANGUAGE <lang name>] <code> The default language is plpgsql . Example: DO $$ BEGIN RAISE NOTICE 'Hello world'; END $$","title":"DO command"},{"location":"Programming/PostgreSQL%20Manual/#window-functions","text":"PostgreSQL supports an extended syntax for window functions . We can use it for example to retrieve the value of a column that has maximum value in another column, as demonstrated in an SO answer .","title":"Window functions"},{"location":"Programming/PostgreSQL%20Manual/#postgis","text":"","title":"PostGis"},{"location":"Programming/PostgreSQL%20Manual/#geometry-columns","text":"Postgis features can be utilized with geometry and geography column types. To add a new geometry column: ADD COLUMN <COLUUMN NAME> geometry(<GEOMETRY TYPE>, <SRID>)","title":"Geometry columns"},{"location":"Programming/PostgreSQL%20Manual/#spatial-indexing","text":"Documentation Analogously to standard SQL column indicis, there are spatial indices in PostGIS. The only difference is that we need to add the USING GIST at the end of the CREATE INDEX statement: CREATE INDEX nodes_geom_idx ON nodes USING GIST (geom);","title":"Spatial Indexing"},{"location":"Programming/PostgreSQL%20Manual/#converting-between-geometry-types","text":"There are dedicated functions whcich we can use to convert between geometry types: - ST_Multi : converts geometries to their multi-variant, e.g., LineString to MultiLineString .","title":"Converting between geometry types"},{"location":"Programming/PostgreSQL%20Manual/#compute-area-surrounding-geometries","text":"If we are ok with a convex envelope of the geometries, we can simply use the St_ConvexHull functon. Howeever, if we need the exact shape, We have to use the St_ConcaveHull function which computes the concave hull of a geometry. The St_ConcaveHull function takes an additional parameter param_pctconvex which determines how concave is the result: 0 means strictly concave, while 1 means convex hull. Note that while lowere values leads to more acurate results, the computation is much slower. There is also another parameter param_allow_holes which determines whether holes in the object are permited (default false).","title":"Compute area surrounding geometries"},{"location":"Programming/PostgreSQL%20Manual/#split-area-to-polygons-based-on-points","text":"The split of spece into polygons based on a set of points is called Voronoi diagram . In PostGIS, we have a [ ST_Voronoi_Polygons](https://postgis.net/docs/ST_VoronoiPolygons.html) for that. To obtain a set of polygons from a set of points, it is necessary to 1. Aggregate the rows ( ST_Collect ) 2. Compute the polygon geometry ( ST_Voronoi_Polygons ) 3. Disaggregate the geometry into individual polygons ( ST_Dump`) Also, there is an important aspect of how far the polygons will reach outside of the points. By default, it enlarge the area determined by the points by about 50%. If we need a larger area, we can use the extend_to parameter. If we need a smaller area, however, we need to compute the intersection with this smaller area afterwards manually. Full example: SELECT st_intersection( (st_dump( st_voronoipolygons(st_collect(<GEOMETRY COLUMN>)) )).geom, (<select clause for area of desired voronoi polygons>) ) AS geom FROM ... If we need to join the polygons to the original points, we need to do it manually (e.g. by JOIN ... ON ST_Within(<POINT COLUMN>, <VORONOI POLYGONS GEO DUMP>) ).","title":"Split area to polygons based on points"},{"location":"Programming/PostgreSQL%20Manual/#other-useful-functions","text":"[ ST_Within ] ST_Within (A, B) if A is completly inside B ST_Intersects ST_Intersects(g1, g2) if g1 and g2 have at least one point in common. ST_Transform : ST_Transform(g, srid) transforms geometry g to a projection defined by the srid and returns the result as a geometry. ST_Buffer : ST_Buffer(g, radius) computes a geometry that is an extension of g by radius to all directions St_Collect aggregates data into single geometry. It is usually apllied to a geometry column in an SQL selection. ST_Union ST_Area ST_Equals ST_MakeLine : make line between two points ST_SetSRID : sets the SRID of the geometry and returns the result as a new geometry. ST_MakePoint : creates a point geometry from the given coordinates.","title":"Other Useful Functions"},{"location":"Programming/PostgreSQL%20Manual/#pgrouting","text":"PgRouting is a PostgreSQL extension focused on graph/network manpulation. It contains functions for: - finding the strongly connected components: `pgr_strongComponents - graph contraction/simplification","title":"PgRouting"},{"location":"Programming/PostgreSQL%20Manual/#finding-strongly-connected-components","text":"The function pgr_strongComponents finds the strongly connected components of a graph. The only parameter of the script is a query that should return edge data in the folowing format: - id , - source , - target , - cost , - reverse_cost . The first three parameters are obvious. The cost parameter does not have any effect. You should provide a negative reverse_cost , othervise, the edge will be considered as bidirectional!","title":"Finding strongly connected components"},{"location":"Programming/PostgreSQL%20Manual/#plpgsql","text":"PL/pgSQL is a procedural language available in PostgreSQL databases. It can be used inside: - functions - procedures - DO command","title":"PL/pgSQL"},{"location":"Programming/PostgreSQL%20Manual/#branching","text":"PL/pgSQL has the following branching: IF <condition> THEN ... [ELSEIF ... ] [ELSE ... ] END IF","title":"Branching"},{"location":"Programming/PostgreSQL%20Manual/#logging","text":"Basic logging can be done using the RAISE command: RAISE NOTICE 'Some message'; We can add parameters by using the % placeholder: RAISE NOTICE 'Some message %', <variable or SQL command>; For more, see the documentation .","title":"Logging"},{"location":"Programming/PostgreSQL%20Manual/#query-diagnostics","text":"Various information about the last query can be obtained using the GET DIAGNOSTIC command. For example, the number of rows affected by the last query can be obtained using the ROW_COUNT parameter: GET DIAGNOSTIC <variable> = ROW_COUNT; The result is stored in the variable <variable> . Note that this constract is not available in the SQL queries but only in a PL/pgSQL block. For other diagnostic fields, see the documentation .","title":"Query diagnostics"},{"location":"Programming/PostgreSQL%20Manual/#psql","text":"psql is a basic command line utitiltyfor manipulating postgres database. To connect: psql -d <db name> Then the psql commands can be executed. To execute command immediatelly, use the -c parameter: psql -d <db name> -c \"<command>\" Do not forget to quote the command.","title":"psql"},{"location":"Programming/PostgreSQL%20Manual/#importing-data-from-csv","text":"The prefered mathod depends on the characte of the data: - data exactly match the table in the database: use psql COPY command - data do not match the table, but they are small: 1. load the data with pandas 2. process the data as needed 3. use pandas.to_sql to upload the data - data do not match the table and they are large: 1. preprocess the data with bach commands 2. use psql COPY to upload the data - data do not match the table and they are large and dirty : use the file_fdw module: 1. create a table for SQL mapping with tolerant column types (e.g., text for problematic columns) 2. select from the mapping to the real table","title":"Importing data from csv"},{"location":"Programming/PostgreSQL%20Manual/#psql-copy-command","text":"The COPY command can be used to copy the input into a database table. A subset of database column can be selected, but that is not true for the input, i.e, all input columns have to be used. If a subset of input columns needs to be used, or some columns requires processing, you need to perform some preprocessing. COPY manual","title":"psql COPY command"},{"location":"Programming/PostgreSQL%20Manual/#importing-data-from-a-shapefile","text":"There are multiple options: - shp2psql : simple tool that creates sql from a shapefile - easy start, almost no configuration - always imports all data from shapefile, cannot be configured to skip columns - included in the Postgres instalation - ogr2ogr - needs to be installed, part of GDAL - QGIS - The db manger can be used to export data from QGIS - data can be viewed before import - only suitable for creating new table, not for appending to an existing one","title":"Importing data from a shapefile"},{"location":"Programming/PostgreSQL%20Manual/#importing-data-from-geojson","text":"For a single geometry stored in a GeoJSON file, the function ST_GeomFromGeoJSON can be used. - just copy the geometry part of the file - change the geometry type to match the type in db - don't forget to surround the object with curly brackets For the whole document, the ogr2ogr tool can be used.","title":"Importing data from GeoJSON"},{"location":"Programming/PostgreSQL%20Manual/#lost-password-to-the-postgres-server","text":"The password for the db superuser is stored in db postgres . In order to log there and change it, the whole authentification has to be turned off, and then we can proceed with changing the password. Steps: 1. find the pg_hba.conf file - usually located in C:\\Program Files\\PostgreSQL\\13\\data 2. backup the file and replace all occurances of scram-sha-256 in the file with trust 3. restart the posgreSQL service - in the Windows service management, there should be a service for postgresql running 4. change the password for the superuser 1. psql -U postgres 2. ALTER USER postgres WITH password 'yourpassword'; (do not forget the semicolon at the end!) 5. restore the pg_hba.conf file from backup 6. restart the postgreSQL service again 7. test if the new password works","title":"Lost Password to the Postgres Server"},{"location":"Programming/PostgreSQL%20Manual/#datagrip","text":"","title":"DataGrip"},{"location":"Programming/PostgreSQL%20Manual/#import-formats","text":"DataGrip can handle imports only from separator baset files (csv, tsv).","title":"Import Formats"},{"location":"Programming/PostgreSQL%20Manual/#view-geometry","text":"In the reuslt window/tab, click on the gear wheel -> Show GeoView . However, the geoviewer has a fixed WGS84 projection, so you have to project the result to this projection first .","title":"View Geometry"},{"location":"Programming/PostgreSQL%20Manual/#create-a-spatial-index","text":"There is currently no GUI tool for that in DataGrip. Just add a normal index and modify the auto generated statement by changing <column> to USING GIST(<column>) at the end of the statement.","title":"Create a spatial Index"},{"location":"Programming/PostgreSQL%20Manual/#filter-out-store-procedures","text":"right-click on routines click open table sort by type","title":"Filter out store procedures"},{"location":"Programming/PostgreSQL%20Manual/#creating-functions-and-procedures","text":"There is no UI available currently, use Navicat or console","title":"Creating functions and procedures"},{"location":"Programming/PostgreSQL%20Manual/#duplicate-table","text":"Drag the table in the database explorer and drop it to the location you want it to copy to.","title":"Duplicate table"},{"location":"Programming/PostgreSQL%20Manual/#navicat","text":"","title":"Navicat"},{"location":"Programming/PostgreSQL%20Manual/#cannot-connect-to-db","text":"Symptoms: - cant connect to db server: Could not connect - after editing the connection and trying to save it (ok button): connection is being used Try: 1. close navicat 2. open navicat, edit connection 1. click test connection 1. click ok, and start the connection by double click","title":"Cannot connect to db"},{"location":"Programming/PostgreSQL%20Manual/#pgadmin","text":"The best way to install the PgAdmin is to use the EDB PostgreSQL installer and uncheck the database installation during the installation configuration. This way, we also install useful tools like psql","title":"PgAdmin"},{"location":"Programming/PostgreSQL%20Manual/#diagrams","text":"To create diagram from an existing database: right click on the database -> Generate ERD","title":"Diagrams"},{"location":"Programming/PostgreSQL%20Manual/#kill-a-hanging-query","text":"To kill a hanging query, we need to complete two steps: 1. identify the query PID 1. kill the query To identify the PID of the problematic query, we can use tool such as pg_activity : sudo -u postgres pg_activity -U postgres To kill the query, run: SELECT pg_cancel_backend(<PID>)","title":"Kill a hanging query"},{"location":"Programming/PostgreSQL%20Manual/#troubleshooting","text":"If the db tools are unresponsive on certain tasks/queries, check if the table needed for those queries is not locke by some problematic query.","title":"Troubleshooting"},{"location":"Programming/PostgreSQL%20Manual/#select-postgresql-version","text":"SELECT version()","title":"Select PostgreSQL version"},{"location":"Programming/PostgreSQL%20Manual/#tried-to-send-an-out-of-range-integer-as-a-2-byte-value","text":"This error is caused by a too large number of values in the insert statement. The maximum index is a 2-byte number (max value: 32767). The solution is to split the insert statement into smaller bulks.","title":"Tried to send an out-of-range integer as a 2-byte value"},{"location":"Programming/Regex/","text":"Symbol meaning . any character [xyz] one of these characters [c-j] any character between c and j . We can combine this and the previou syntax, e.g.: [az0-4jsd] . Note that the minus sign is interpreted as a range only if it is between two characters. [^c-g] ^ means negation: anything except the following set of characters. Note that the negation( ^ ) sign needs to be the first character in the bracket. \\ : escape character. | means OR. It has the lowest precedence, so it is evaluated last. ? lazy quantifier. It will try to match as few characters as possible (i.e., previous pattern will try to match only till the next patern matches). ?= positive lookahead. It will try to match the pattern, but it will not consume it. ?! negative lookahead. It is useful when we want to match a pattern, but we don't want to consume it. ?R recursive pattern. Quantifiers * : zero or more + : one or more ? zero or one {<from>, <to>} between from and to times. If to is omitted, it means infinity. If from is omitted, it means zero. If there is only one number, it means exact count. If both are omitted, it means one. Anchors ^x must start with x x$ must end with x Groups () capture group. We can refer to it later, either in the regex, or in the result of the match, depending on the programming language. The nubering starts from 1, the 0 group is usually the whole match. in the regex we refer to group using \\1 , \\2 , etc. (?:) non-caputing group. It is useful when we want to use the quantifiers on a group, but we don't want to capture it. Examples Any whitespace /[\\x{00a0}\\s]/u Non-breaking space ((?!&nbsp;)[^\\s\\x{00a0}]) Transform Google sheet to latex table naj\u00edt ([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n nahradit \\1 & \\2 & \\3 & \\4 \\\\\\\\\\r\\n CSV to Latex Search: ([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,\\r\\n]*)\\n Replace \\1 & \\2 & \\3 & \\4 & \\5 & \\6 & \\7 \\\\\\\\\\r\\n Name regex ([A\u00c1BC\u010cDEFGHIJKLMNOPQR\u0158S\u0160TU\u00daVWXYZ\u017d]{1}[a\u00e1bc\u010dd\u010fe\u00e9\u011bfghchi\u00edjklmn\u0148o\u00f3pqr\u0159s\u0161t\u0165u\u00fa\u016fvwxy\u00fdz\u017ew]+ *){2,3} Archive vasdomovnik naj\u00edt [0-9]+[ ]*([^\\r\\n]*)[\\r\\n]+ nahradit '\\1' , sloupce na pole php naj\u00edt ([^\\r\\n]*)([\\r\\n])+ nahradit '\\1' => ''\\2, Pogamut - jen logy od jednoho bota \\(TeamCTF[^2][^\\r\\n]*\\r\\n nahradit za pr\u00e1zdno","title":"Symbol meaning"},{"location":"Programming/Regex/#symbol-meaning","text":". any character [xyz] one of these characters [c-j] any character between c and j . We can combine this and the previou syntax, e.g.: [az0-4jsd] . Note that the minus sign is interpreted as a range only if it is between two characters. [^c-g] ^ means negation: anything except the following set of characters. Note that the negation( ^ ) sign needs to be the first character in the bracket. \\ : escape character. | means OR. It has the lowest precedence, so it is evaluated last. ? lazy quantifier. It will try to match as few characters as possible (i.e., previous pattern will try to match only till the next patern matches). ?= positive lookahead. It will try to match the pattern, but it will not consume it. ?! negative lookahead. It is useful when we want to match a pattern, but we don't want to consume it. ?R recursive pattern.","title":"Symbol meaning"},{"location":"Programming/Regex/#quantifiers","text":"* : zero or more + : one or more ? zero or one {<from>, <to>} between from and to times. If to is omitted, it means infinity. If from is omitted, it means zero. If there is only one number, it means exact count. If both are omitted, it means one.","title":"Quantifiers"},{"location":"Programming/Regex/#anchors","text":"^x must start with x x$ must end with x","title":"Anchors"},{"location":"Programming/Regex/#groups","text":"() capture group. We can refer to it later, either in the regex, or in the result of the match, depending on the programming language. The nubering starts from 1, the 0 group is usually the whole match. in the regex we refer to group using \\1 , \\2 , etc. (?:) non-caputing group. It is useful when we want to use the quantifiers on a group, but we don't want to capture it.","title":"Groups"},{"location":"Programming/Regex/#examples","text":"","title":"Examples"},{"location":"Programming/Regex/#any-whitespace","text":"/[\\x{00a0}\\s]/u","title":"Any whitespace"},{"location":"Programming/Regex/#non-breaking-space","text":"((?!&nbsp;)[^\\s\\x{00a0}])","title":"Non-breaking space"},{"location":"Programming/Regex/#transform-google-sheet-to-latex-table","text":"naj\u00edt ([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n([^\\r\\n]*)\\r\\n nahradit \\1 & \\2 & \\3 & \\4 \\\\\\\\\\r\\n","title":"Transform Google sheet to latex table"},{"location":"Programming/Regex/#csv-to-latex","text":"Search: ([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,]*),([^,\\r\\n]*)\\n Replace \\1 & \\2 & \\3 & \\4 & \\5 & \\6 & \\7 \\\\\\\\\\r\\n","title":"CSV to Latex"},{"location":"Programming/Regex/#name-regex","text":"([A\u00c1BC\u010cDEFGHIJKLMNOPQR\u0158S\u0160TU\u00daVWXYZ\u017d]{1}[a\u00e1bc\u010dd\u010fe\u00e9\u011bfghchi\u00edjklmn\u0148o\u00f3pqr\u0159s\u0161t\u0165u\u00fa\u016fvwxy\u00fdz\u017ew]+ *){2,3}","title":"Name regex"},{"location":"Programming/Regex/#archive","text":"","title":"Archive"},{"location":"Programming/Regex/#vasdomovnik","text":"naj\u00edt [0-9]+[ ]*([^\\r\\n]*)[\\r\\n]+ nahradit '\\1' ,","title":"vasdomovnik"},{"location":"Programming/Regex/#sloupce-na-pole-php","text":"naj\u00edt ([^\\r\\n]*)([\\r\\n])+ nahradit '\\1' => ''\\2,","title":"sloupce na pole php"},{"location":"Programming/Regex/#pogamut-jen-logy-od-jednoho-bota","text":"\\(TeamCTF[^2][^\\r\\n]*\\r\\n nahradit za pr\u00e1zdno","title":"Pogamut - jen logy od jednoho bota"},{"location":"Programming/SQL%20Manual/","text":"Literals In SQL, there are three basic types of literals: - numeric : 1 , 1.2 , 1.2e3 - string : 'string' , \"string\" - boolean : true , false When we need a constant value for other types, we usually use either a constructor function or a specifically formatted string literal. String literals can also span multiple lines , we do not need any operator to split the line (unlike in Python and despite the JetBrains IDEs adds a string concatenation operator automatically on newline). WITH Statement for defining variables pointing to temporary data, that can be used in the related SELECT statement. Usage: WITH <var> AS (<sql that assigns data to var>) <sql that use the variable> Note that the variable has to appear in the FROM clause ! Multiple variables in the WITH statement ahould be delimited by a comma: WITH <var> AS (<sql that assigns data to var>), <var 2> AS (<sql that assigns data to var 2>) <sql that use the variables> SELECT Most common SQL statement, syntax: SELECT <EXPRESSION> [AS <ALIAS>][, <EXPRESSION 2> [AS <ALIAS 2> ...]] The most common expression is just a column name. Selecting row number We can select row numbers using the function ROW_SELECT() in the SELECT statement: SELECT ... ROW_NUMBER() OVER([<PARTITIONING AND NUMBERING ORDER>]) AS <RESULT COLUMN NAME>, ... Inside the OVER statement, we can specify the order of the row numbering. Note however, that this does not order the result, for that, we use the ORDER BY statement. If we want the rown numbering to correspond with the row order in the result, we can left the OVER statement empty. Count selecting rows The count() function can be used to count the selection. The standard syntax is: SELECT count(1) FROM ... Count distinct To count distinct values in a selection we can use: SELECT count(DISTINCT <column name>) FROM... Select from another column if the specified column is NULL We can use a replacement column using the coalesce function: SELECT coalesce (<primary column>, <secondary column>) The secondary column value will be used if the primary column value in the row is NULL . UNION UNION and UNION ALL are important keywords that enables to merge query results verticaly, i.e., appending rows of one query to the results set of another. The difference between them is that UNION discards duplicate rows, while UNION ALL keeps them The UNUION statement appends one SELECT statement to another, but some statements that appears to be part of the SELECT needs to stay outside (i.e., be specified just once for the whole union), namely ORDER BY , and LIMIT . In contrast, the GROUP BY and HAVING statement stays inside each individual select. JOIN Classical syntax: JOIN <table name> [[AS] <ALIAS>] ON <CONDITION> The alias is obligatory if there are duplicate names (e.g., we are joining one table twice) Types INNER (default): when there is nothing to join, the row is discarded [LEFT/RIGHT/FULL] OUTER : when there is nothing to join, the missing values are set to null CROSS : creates cartesian product between tables OUTER JOIN The OUTER JOIN has three subtypes - LEFT : joins right table to left, fills the missing rows on right with null - RIGHT : joins left to right, fills the missing rows on left with null - FULL : both LEFT and RIGHT JOIN is performend Properties When there are multiple matching rows, all of them are matched (i.e., it creates duplicit rows) If you want to filter the tables before join, you need to specify the condition inside ON caluse Join Only One Row Joining a specific row Sometimes, we want to join only one row from many fulfilling some condition. One way to do that is to use a subquery: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY column_in_b DESC LIMIT 1 ) This query joins table b to table a , using the row with the highest column_in_b . Note however, that all rows from a will be joined to the same row from b . To use a different row from b depending on a , we need to look outside the subquery to filter out b according to a . The folowing query, which should do exactly that, is invalid : SELECT * FROM a JOIN ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 ) The problem here is that the subquery cannot refer the tables outside in, in the preceeeding FROM clause. Luckily, in the most use db systems, there is a magical keyword LATERAL that enables exacly that: SELECT * FROM a JOIN LATERAL ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 ) Join random row To join a random row, we can use the RANDOM function in ordering, e.g.: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY RANDOM() LIMIT 1 ) However, this subquery is evaluated just once, hence we have the same problem as with the first example in this section: to every row in a , we are joining the same random row from b . To force the db system to execute the subquery for each row in a , we need to refer a in the subquery, even with some useless filter (and of course we need a LATERAL join for that): SELECT * FROM a JOIN LATERAL( SELECT * FROM b WHERE a.column_in_a IS NOT NULL ORDER BY RANDOM() LIMIT 1 ) Joining any row find active nodes Sometimes, we need to join any matching row from B to find a set of active (referenced) rows in A. For example, we need to find a set of custommers with pending orders. IF the average number of matches in B is low, we proceed with normal join and then aggregate the results or use DISTINCT . However, sometimes, the average number of matching rows in B can be very high (e.g., finding all countries with at least one MC Donald). For those cases, the LATERAL join is again the solution: SELECT * FROM zones JOIN LATERAL ( SELECT id AS request_id FROM demand WHERE zones.id = demand.destination LIMIT 1 ) demand ON TRUE For more information about the trade offs of this solution, check the SO answer Getting All Combinations of Rows This can be done using the CROSS JOIN , e.g.: SELECT * FROM table_a CROSS JOIN table_b The proble arises when you want to filter one of the tables before joining, because CROSS JOIN does not support the ON clause (see more in the Oracle docs ). Then you can use the equivalent INNER JOIN : SELECT * FROM table_a INNER JOIN table_b ON true Here you replace the true value with your intended condition. Inverse JOIN Sometimes, it is useful to find all rows in table A that has no match in table B. The usual approach is to use LEFT JOIN and filter out non null rows after the join: SELECT ... FROM tableA LEFT JOIN tableB WHERE tableB.id is NULL Joining a table on multiple options There are situations, where we want to join a single table on multiple possible matches. For example, we want to match all people having birthday or name day the same day as some promotion is happanning. The foollowing query is a straighrorward solution: SELECT * FROM promo JOIN people ON promo.date = people.birthaday OR promo.date = people.name_day However, as we explain in the performance chapter, using OR in SQL is almost never a good solution. The usual way of gatting rid of OR is to use IN : SELECT * FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day) Nevertheless, this query can still lead to problems, despite being more compact. When we use any complex condition while joining tables, we risk poor performance. In general, it is better to use just simple column-to-column matches, even when there are more joins as a result. If you have performance problems, consult the \"Replacing OR \" section in the \"Performance Optimization\" chapter. GROUP BY wiki GROUP BY is used to aggregate data. Usual syntax: GROUP BY <LIST OF COLUMNS> Note that we need to use some aggreagte function for all columns in the SELECT clause that are not present in the GROUP BY list. On the other hand, we can use columns not present in the SELECT in the GROUP BY statement. Using aggregate functions for the whole result set while using GROUP BY If a query contains a GROUP BY statement, all aggregate functions (e.g., count , avg ) are applied to groups, i.e., for each row of the result set. If we need to apply an aggregate function to the whole result set while using GROUP BY , we need to specify it using the OVER statement: SELECT count(1) OVER () as result_count ... This will add a coulumn with a total count to each row of the result. If we do not need the actual groups, but only the distinct count, we can use a LIMIT statement. Window functions Sometimes, we would need an aggregate function that somehow use two different columns (e.g., value of col A for the row where col B is largest). For that, we cannot use the classical aggregation, but we rather have to use a window function . A window function is a function that for each row returns a value computed from one or multiple rows. Syntactically, we recognize a window function by the OVER clause that determines the rows used as an input for the function. Functions with the same name can exist as aggregate and window functions. Window functions are evaluated after the GROUP BY clause and aggregate functions. Specifiing the range () : the whole result set (PARTITION BY <column set>) : the rows with the same values for that set of columns We can also order the result for the selected range using ORDER BY inside the parantheses. In some SQL dialects (e.g., PostgreSQL), there are even more sophisticated ways how to specify the range for the window functions. ORDER BY By default, there is no guarantee that the result will be in any particular order. To sort the result, we need to add an ORDER BY statement at the end of the query: ORDER BY <LIST OF COLUMNS> INSERT Standard syntax for the INSERT statement is INSERT INTO <table> (<col_1>, <col_2>,...) VALUES (<val_1>, <val_2>,...) If we fill all columns and we are confident with the column ordering, we can omit columns: INSERT INTO <table> VALUES (<val_1>, <val_2>,...) Sometimes, we need to handle the eroro cases, e.g., the case when the record already exists. The solutions for these cases are, however, database specific. INSERT SELECT If we want to duplicate the records, we can use: INSERT INTO <table> SELECT * FROM <table> [WHERE <condition>] If we need to fill some columns ourselfs: INSERT INTO <table> (<col_1>, <col_2>,...) SELECT <col_1>, <any expression> FROM <anything suported by select> [WHERE <condition>] UPDATE UPDATE query has the following structure: UPDATE table_name SET column1 = value1, column2 = value2...., columnN = valueN WHERE <condition> Unfortunately, the statement is ready to update N records with one set of values, but not to update N records with N set of values. To do that, we have only an option to select from another table: UPDATE table_name SET column1 = other_table.column1 FROM other_table WHERE other_table.id = table_name.id Don't forget the WHERE clause here, otherwise, you are matching the whole result set returned by the FROM clause to each row of the table. Use the table for updating itself If we are abou to update the table using date stored in int, we need to use aliases: UPDATE nodes_ways new SET way_id = ways.osm_id FROM nodes_ways old JOIN ways ON old.way_id = ways.id AND old.area = ways.area WHERE new.way_id = old.way_id AND new.area = old.area AND new.position = old.position DELETE To delete records from a table, we use the DELETE statement: DELETE FROM <table_name> WHERE <condition> If some data from another table are required for the selection of the records to be deleted, the syntax varies depending on the database engine. EXPLAIN Sources - official documentation - official cosumentation: usage - https://docs.gitlab.com/ee/development/understanding_explain_plans.html Remarks: to show the actual run times, we need to run EXPLAIN ANALYZE Nodes Sources - Plan nodes source code - PG documentation with nodes described Node example: -> Parallel Seq Scan on records_mon (cost=0.00..4053077.22 rows=2074111 width=6) (actual time=398.243..74465.389 rows=7221902 loops=2) Filter: ((trip_length_0_1_mi = '0'::double precision) AND (trip_length_1_2_mi = '0'::double precision) AND (trip_length_2_3_mi = '0'::double precision) AND (trip_length_3_4_mi = '0'::double precision) AND (trip_length_4_5_mi = '0'::double precision)) Rows Removed by Filter: 8639817 Buffers: shared hit=157989 read=3805864 Description: In first parantheses, there are expected values: - cost the estimated cost in arbitrary units. The format is startup cost..total cost , where startup cost is a flat cost of the node, an init cost, while total cost is the estimated cost of the node. Averege per loop. - rows : expected number of rows produced by this node. Averege per loop. - width the width of each row in bytes In the second parantheses, there are measured results: - actual time : The measured execution time in miliseconds. The format is startup time..total time . - rows The real number of rows returned. The Rows Removed by the Filter indicates the number of rows that were filtered out. The Buffers statistic shows the number of buffers used. Each buffer consists of 8 KB of data. Keys Keys serves as a way of identifying table rows, they are unique . There are many type of keys, see databastar article for the terminology overview. Primary key Most important keys in ORM are primary keys. Each table should have a single primary key. A primary key has to be non null. When choosing primary key, we can either - use a uniqu combination of database colums: a natural key - use and extra column: surogate key If we use a natural key and it is composed from multiple columns, we call it a composite key The following table summarize the adventages and disadvantages of each of the solutions: Area | Property | Natural key | Composite key | Surrogate key | |-|-|-|-|-| | usage | SQL joins | easy | hard | easy | || changing natural key columns | hard | hard | easy | | Performance | extra space | none | A lot if there are reference tables, otherwise none | one extra column | || space for indexes | normal | extra | normal || extra insertion time | no | A lot if there are reference tables, otherwise none | yes | || join performance | suboptimal due to sparse values | even more sub-optimal due to sparse values | optimal From the table above, we can see that using natural keys should be considered only if rows can be identified by a single column and we have a strong confidence in that natural id, specifically in its uniquness and timelessnes. Non-primary (UNIQUE) keys Sometimes, we need to enforse a uniqueness of a set of columns that does not compose a primary key (e.g., we use a surogate key). We can use a non primary key for that. One of the differences between primary and non-primary keys is that non-primary keys can be null, and each of the null values is considered unique. Indices Indices are essential for speeding queries containing conditions (including conditional joins). The basic syntax for creating an index is: CREATE INDEX <index name> ON <table name>(<column name>); Show Table Indices MySQL: SHOW INDEX FROM <tablename>; PostgreSQL SELECT * FROM pg_indexes WHERE tablename = '<tablename >'; Show Indices From All Tables MySQL: SELECT DISTINCT TABLE_NAME, INDEX_NAME FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = '<schemaname>'; CREATE TABLE The syntax is: CREATE TABLE <TABLE NAME> ( <COLUNM 1 NAME> <COLUNM 1 TYPE>, ... <COLUNM N NAME> <COLUNM N TYPE> [, PRIMARY KEY (<LIST OF KEY COLUMNS>)] ) The primary key is optional, but usually, any table should have one. Each row has to have a unique primary key. An index is created automatically for the list of primary key columns. ALTER TABLE Add column generated from other columns SQL: ALTER TABLE <table name> ADD <column name> AS (<generator expression>); In MySQL we have to add the data type: ALTER TABLE <tablename> ADD <columnname> <datatype> AS (<generator expression>); In PostgreSQL, the syntax is quite different: ALTER TABLE <tablename> ADD <columnname> <datatype> GENERATED ALWAYS AS (<generator expression>) STORED The advantage of the Postgres approach is that the column is set as generated, therefore it is generated for the new rows automatically. Procedures and functions In SQL there are two constructs that are able to encapsulate SQL statements to increase resusability and readability: functions and procedues. The main difference between these two is that functions are intended to be called inline from SLQ statements, while procedures cannot be used in SQL statements and instead, they are used as a wrapper of a set of SQL statement to be called repeatedly with different arguments. The extensive summary of the different capabilities of functions and procedures is on SO . Calling a procedure The keyword for calling a procedure differs between database system, refer to the documentation for your system for the right keyword. Creating a procedure The syntax for calling a procedure differs between database system, refer to the documentation for your system for the right syntax. Parameters Procedures and functions can have parameters similar to parameters in programming languages. We can use those parameters in the body of a function/procedure equally as a column or constant. Parameters can have default values , suplied after the = sign. We can test whether a default argument was supplied by testing the parameter for the default value. Views Views are basically named SQL queries stored in database. The queries are run on each view invocation, unles the view is materialized. The syntax is: CREATE VIEW <VIEW NAME> AS <QUERY> Modifying the view The view can be modified with CREATE OR REPLACE VIEW , however, existing columns cannot be changed . If you need to change existing columns, drop the view first. Performace Optimization When the query is slow, first inspect the following checklist: - Do not use OR or IN for a set of columns (see replacing OR below). - Check that all column and combination of columns used in conditions ( WHERE ) are indexed. - Check that all foreign keys are indexed. - Check that all joins are simple joins (column to column, or set of columns to a matching set of columns). If nothing from the above works, try to start with a simple query and add more complex pars to find where the problem is. If decomposing the query also does not bing light into the problem, refer to either one of the subsections below, or to the external sources. Also, note that some IDEs limits the number of returned rows automatically, which can hide serious problems and confuse you. Try to remove the limit when testing the performance this way. Replacing OR We can slow down the query significantly using OR or IN statements if the set of available options is not constant (e.g., IN(1, 2) is okish, while IN(origin, destination) can have drastic performance impact). To get rid of these disjunctioncs, we can use the UNION statement, basically duplicating the query. The resulting query will be double in size, but much faster: SELECT people.id FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day); -- can be revritten as SELECT * FROM promo JOIN people ON promo.date = people.birthaday UNION SELECT * FROM promo JOIN people ON promo.date = people.name_day GROUP BY people.id A specific join makes the query slow If a single join makes the query slow, there is a great chance that the index is not used for the join. Even if the table has an index on the referenced column(s), the join can still not use it if we are joining not to the table itself but to: - a subquery, - a variable created with a WITH statement, - a view, - or temborary table created from the indexed table. You can solve the situation by creating a materialized view or temporary table instead, and adding inedices to the table manualy. Specifically, you need to split the query into multiple queries: 1. delete the materialized view/table if exists 1. create the materialized view/table 1. create the required indices 1. perform the actual query that utilizes th view/table Of course we can skip the first three steps if the materialized view is constant for all queries, which is common during the testing phase. Slow DELETE When the delete is slow, the cause can be the missing index on the child table that refers to the table we are deleting from. Other possible causes are listed in the SO answer . Sources Database development mistakes made by application developers","title":"Literals"},{"location":"Programming/SQL%20Manual/#literals","text":"In SQL, there are three basic types of literals: - numeric : 1 , 1.2 , 1.2e3 - string : 'string' , \"string\" - boolean : true , false When we need a constant value for other types, we usually use either a constructor function or a specifically formatted string literal. String literals can also span multiple lines , we do not need any operator to split the line (unlike in Python and despite the JetBrains IDEs adds a string concatenation operator automatically on newline).","title":"Literals"},{"location":"Programming/SQL%20Manual/#with","text":"Statement for defining variables pointing to temporary data, that can be used in the related SELECT statement. Usage: WITH <var> AS (<sql that assigns data to var>) <sql that use the variable> Note that the variable has to appear in the FROM clause ! Multiple variables in the WITH statement ahould be delimited by a comma: WITH <var> AS (<sql that assigns data to var>), <var 2> AS (<sql that assigns data to var 2>) <sql that use the variables>","title":"WITH"},{"location":"Programming/SQL%20Manual/#select","text":"Most common SQL statement, syntax: SELECT <EXPRESSION> [AS <ALIAS>][, <EXPRESSION 2> [AS <ALIAS 2> ...]] The most common expression is just a column name.","title":"SELECT"},{"location":"Programming/SQL%20Manual/#selecting-row-number","text":"We can select row numbers using the function ROW_SELECT() in the SELECT statement: SELECT ... ROW_NUMBER() OVER([<PARTITIONING AND NUMBERING ORDER>]) AS <RESULT COLUMN NAME>, ... Inside the OVER statement, we can specify the order of the row numbering. Note however, that this does not order the result, for that, we use the ORDER BY statement. If we want the rown numbering to correspond with the row order in the result, we can left the OVER statement empty.","title":"Selecting row number"},{"location":"Programming/SQL%20Manual/#count-selecting-rows","text":"The count() function can be used to count the selection. The standard syntax is: SELECT count(1) FROM ...","title":"Count selecting rows"},{"location":"Programming/SQL%20Manual/#count-distinct","text":"To count distinct values in a selection we can use: SELECT count(DISTINCT <column name>) FROM...","title":"Count distinct"},{"location":"Programming/SQL%20Manual/#select-from-another-column-if-the-specified-column-is-null","text":"We can use a replacement column using the coalesce function: SELECT coalesce (<primary column>, <secondary column>) The secondary column value will be used if the primary column value in the row is NULL .","title":"Select from another column if the specified column is NULL"},{"location":"Programming/SQL%20Manual/#union","text":"UNION and UNION ALL are important keywords that enables to merge query results verticaly, i.e., appending rows of one query to the results set of another. The difference between them is that UNION discards duplicate rows, while UNION ALL keeps them The UNUION statement appends one SELECT statement to another, but some statements that appears to be part of the SELECT needs to stay outside (i.e., be specified just once for the whole union), namely ORDER BY , and LIMIT . In contrast, the GROUP BY and HAVING statement stays inside each individual select.","title":"UNION"},{"location":"Programming/SQL%20Manual/#join","text":"Classical syntax: JOIN <table name> [[AS] <ALIAS>] ON <CONDITION> The alias is obligatory if there are duplicate names (e.g., we are joining one table twice)","title":"JOIN"},{"location":"Programming/SQL%20Manual/#types","text":"INNER (default): when there is nothing to join, the row is discarded [LEFT/RIGHT/FULL] OUTER : when there is nothing to join, the missing values are set to null CROSS : creates cartesian product between tables","title":"Types"},{"location":"Programming/SQL%20Manual/#outer-join","text":"The OUTER JOIN has three subtypes - LEFT : joins right table to left, fills the missing rows on right with null - RIGHT : joins left to right, fills the missing rows on left with null - FULL : both LEFT and RIGHT JOIN is performend","title":"OUTER JOIN"},{"location":"Programming/SQL%20Manual/#properties","text":"When there are multiple matching rows, all of them are matched (i.e., it creates duplicit rows) If you want to filter the tables before join, you need to specify the condition inside ON caluse","title":"Properties"},{"location":"Programming/SQL%20Manual/#join-only-one-row","text":"","title":"Join Only One Row"},{"location":"Programming/SQL%20Manual/#joining-a-specific-row","text":"Sometimes, we want to join only one row from many fulfilling some condition. One way to do that is to use a subquery: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY column_in_b DESC LIMIT 1 ) This query joins table b to table a , using the row with the highest column_in_b . Note however, that all rows from a will be joined to the same row from b . To use a different row from b depending on a , we need to look outside the subquery to filter out b according to a . The folowing query, which should do exactly that, is invalid : SELECT * FROM a JOIN ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 ) The problem here is that the subquery cannot refer the tables outside in, in the preceeeding FROM clause. Luckily, in the most use db systems, there is a magical keyword LATERAL that enables exacly that: SELECT * FROM a JOIN LATERAL ( SELECT * FROM b WHERE column_in_b <= a.column_in_a ORDER BY column_in_b DESC LIMIT 1 )","title":"Joining a specific row"},{"location":"Programming/SQL%20Manual/#join-random-row","text":"To join a random row, we can use the RANDOM function in ordering, e.g.: SELECT * FROM a JOIN ( SELECT * FROM b ORDER BY RANDOM() LIMIT 1 ) However, this subquery is evaluated just once, hence we have the same problem as with the first example in this section: to every row in a , we are joining the same random row from b . To force the db system to execute the subquery for each row in a , we need to refer a in the subquery, even with some useless filter (and of course we need a LATERAL join for that): SELECT * FROM a JOIN LATERAL( SELECT * FROM b WHERE a.column_in_a IS NOT NULL ORDER BY RANDOM() LIMIT 1 )","title":"Join random row"},{"location":"Programming/SQL%20Manual/#joining-any-row-find-active-nodes","text":"Sometimes, we need to join any matching row from B to find a set of active (referenced) rows in A. For example, we need to find a set of custommers with pending orders. IF the average number of matches in B is low, we proceed with normal join and then aggregate the results or use DISTINCT . However, sometimes, the average number of matching rows in B can be very high (e.g., finding all countries with at least one MC Donald). For those cases, the LATERAL join is again the solution: SELECT * FROM zones JOIN LATERAL ( SELECT id AS request_id FROM demand WHERE zones.id = demand.destination LIMIT 1 ) demand ON TRUE For more information about the trade offs of this solution, check the SO answer","title":"Joining any row find active nodes"},{"location":"Programming/SQL%20Manual/#getting-all-combinations-of-rows","text":"This can be done using the CROSS JOIN , e.g.: SELECT * FROM table_a CROSS JOIN table_b The proble arises when you want to filter one of the tables before joining, because CROSS JOIN does not support the ON clause (see more in the Oracle docs ). Then you can use the equivalent INNER JOIN : SELECT * FROM table_a INNER JOIN table_b ON true Here you replace the true value with your intended condition.","title":"Getting All Combinations of Rows"},{"location":"Programming/SQL%20Manual/#inverse-join","text":"Sometimes, it is useful to find all rows in table A that has no match in table B. The usual approach is to use LEFT JOIN and filter out non null rows after the join: SELECT ... FROM tableA LEFT JOIN tableB WHERE tableB.id is NULL","title":"Inverse JOIN"},{"location":"Programming/SQL%20Manual/#joining-a-table-on-multiple-options","text":"There are situations, where we want to join a single table on multiple possible matches. For example, we want to match all people having birthday or name day the same day as some promotion is happanning. The foollowing query is a straighrorward solution: SELECT * FROM promo JOIN people ON promo.date = people.birthaday OR promo.date = people.name_day However, as we explain in the performance chapter, using OR in SQL is almost never a good solution. The usual way of gatting rid of OR is to use IN : SELECT * FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day) Nevertheless, this query can still lead to problems, despite being more compact. When we use any complex condition while joining tables, we risk poor performance. In general, it is better to use just simple column-to-column matches, even when there are more joins as a result. If you have performance problems, consult the \"Replacing OR \" section in the \"Performance Optimization\" chapter.","title":"Joining a table on multiple options"},{"location":"Programming/SQL%20Manual/#group-by","text":"wiki GROUP BY is used to aggregate data. Usual syntax: GROUP BY <LIST OF COLUMNS> Note that we need to use some aggreagte function for all columns in the SELECT clause that are not present in the GROUP BY list. On the other hand, we can use columns not present in the SELECT in the GROUP BY statement.","title":"GROUP BY"},{"location":"Programming/SQL%20Manual/#using-aggregate-functions-for-the-whole-result-set-while-using-group-by","text":"If a query contains a GROUP BY statement, all aggregate functions (e.g., count , avg ) are applied to groups, i.e., for each row of the result set. If we need to apply an aggregate function to the whole result set while using GROUP BY , we need to specify it using the OVER statement: SELECT count(1) OVER () as result_count ... This will add a coulumn with a total count to each row of the result. If we do not need the actual groups, but only the distinct count, we can use a LIMIT statement.","title":"Using aggregate functions for the whole result set while using GROUP BY"},{"location":"Programming/SQL%20Manual/#window-functions","text":"Sometimes, we would need an aggregate function that somehow use two different columns (e.g., value of col A for the row where col B is largest). For that, we cannot use the classical aggregation, but we rather have to use a window function . A window function is a function that for each row returns a value computed from one or multiple rows. Syntactically, we recognize a window function by the OVER clause that determines the rows used as an input for the function. Functions with the same name can exist as aggregate and window functions. Window functions are evaluated after the GROUP BY clause and aggregate functions.","title":"Window functions"},{"location":"Programming/SQL%20Manual/#specifiing-the-range","text":"() : the whole result set (PARTITION BY <column set>) : the rows with the same values for that set of columns We can also order the result for the selected range using ORDER BY inside the parantheses. In some SQL dialects (e.g., PostgreSQL), there are even more sophisticated ways how to specify the range for the window functions.","title":"Specifiing the range"},{"location":"Programming/SQL%20Manual/#order-by","text":"By default, there is no guarantee that the result will be in any particular order. To sort the result, we need to add an ORDER BY statement at the end of the query: ORDER BY <LIST OF COLUMNS>","title":"ORDER BY"},{"location":"Programming/SQL%20Manual/#insert","text":"Standard syntax for the INSERT statement is INSERT INTO <table> (<col_1>, <col_2>,...) VALUES (<val_1>, <val_2>,...) If we fill all columns and we are confident with the column ordering, we can omit columns: INSERT INTO <table> VALUES (<val_1>, <val_2>,...) Sometimes, we need to handle the eroro cases, e.g., the case when the record already exists. The solutions for these cases are, however, database specific.","title":"INSERT"},{"location":"Programming/SQL%20Manual/#insert-select","text":"If we want to duplicate the records, we can use: INSERT INTO <table> SELECT * FROM <table> [WHERE <condition>] If we need to fill some columns ourselfs: INSERT INTO <table> (<col_1>, <col_2>,...) SELECT <col_1>, <any expression> FROM <anything suported by select> [WHERE <condition>]","title":"INSERT SELECT"},{"location":"Programming/SQL%20Manual/#update","text":"UPDATE query has the following structure: UPDATE table_name SET column1 = value1, column2 = value2...., columnN = valueN WHERE <condition> Unfortunately, the statement is ready to update N records with one set of values, but not to update N records with N set of values. To do that, we have only an option to select from another table: UPDATE table_name SET column1 = other_table.column1 FROM other_table WHERE other_table.id = table_name.id Don't forget the WHERE clause here, otherwise, you are matching the whole result set returned by the FROM clause to each row of the table.","title":"UPDATE"},{"location":"Programming/SQL%20Manual/#use-the-table-for-updating-itself","text":"If we are abou to update the table using date stored in int, we need to use aliases: UPDATE nodes_ways new SET way_id = ways.osm_id FROM nodes_ways old JOIN ways ON old.way_id = ways.id AND old.area = ways.area WHERE new.way_id = old.way_id AND new.area = old.area AND new.position = old.position","title":"Use the table for updating itself"},{"location":"Programming/SQL%20Manual/#delete","text":"To delete records from a table, we use the DELETE statement: DELETE FROM <table_name> WHERE <condition> If some data from another table are required for the selection of the records to be deleted, the syntax varies depending on the database engine.","title":"DELETE"},{"location":"Programming/SQL%20Manual/#explain","text":"Sources - official documentation - official cosumentation: usage - https://docs.gitlab.com/ee/development/understanding_explain_plans.html","title":"EXPLAIN"},{"location":"Programming/SQL%20Manual/#remarks","text":"to show the actual run times, we need to run EXPLAIN ANALYZE","title":"Remarks:"},{"location":"Programming/SQL%20Manual/#nodes","text":"Sources - Plan nodes source code - PG documentation with nodes described Node example: -> Parallel Seq Scan on records_mon (cost=0.00..4053077.22 rows=2074111 width=6) (actual time=398.243..74465.389 rows=7221902 loops=2) Filter: ((trip_length_0_1_mi = '0'::double precision) AND (trip_length_1_2_mi = '0'::double precision) AND (trip_length_2_3_mi = '0'::double precision) AND (trip_length_3_4_mi = '0'::double precision) AND (trip_length_4_5_mi = '0'::double precision)) Rows Removed by Filter: 8639817 Buffers: shared hit=157989 read=3805864 Description: In first parantheses, there are expected values: - cost the estimated cost in arbitrary units. The format is startup cost..total cost , where startup cost is a flat cost of the node, an init cost, while total cost is the estimated cost of the node. Averege per loop. - rows : expected number of rows produced by this node. Averege per loop. - width the width of each row in bytes In the second parantheses, there are measured results: - actual time : The measured execution time in miliseconds. The format is startup time..total time . - rows The real number of rows returned. The Rows Removed by the Filter indicates the number of rows that were filtered out. The Buffers statistic shows the number of buffers used. Each buffer consists of 8 KB of data.","title":"Nodes"},{"location":"Programming/SQL%20Manual/#keys","text":"Keys serves as a way of identifying table rows, they are unique . There are many type of keys, see databastar article for the terminology overview.","title":"Keys"},{"location":"Programming/SQL%20Manual/#primary-key","text":"Most important keys in ORM are primary keys. Each table should have a single primary key. A primary key has to be non null. When choosing primary key, we can either - use a uniqu combination of database colums: a natural key - use and extra column: surogate key If we use a natural key and it is composed from multiple columns, we call it a composite key The following table summarize the adventages and disadvantages of each of the solutions: Area | Property | Natural key | Composite key | Surrogate key | |-|-|-|-|-| | usage | SQL joins | easy | hard | easy | || changing natural key columns | hard | hard | easy | | Performance | extra space | none | A lot if there are reference tables, otherwise none | one extra column | || space for indexes | normal | extra | normal || extra insertion time | no | A lot if there are reference tables, otherwise none | yes | || join performance | suboptimal due to sparse values | even more sub-optimal due to sparse values | optimal From the table above, we can see that using natural keys should be considered only if rows can be identified by a single column and we have a strong confidence in that natural id, specifically in its uniquness and timelessnes.","title":"Primary key"},{"location":"Programming/SQL%20Manual/#non-primary-unique-keys","text":"Sometimes, we need to enforse a uniqueness of a set of columns that does not compose a primary key (e.g., we use a surogate key). We can use a non primary key for that. One of the differences between primary and non-primary keys is that non-primary keys can be null, and each of the null values is considered unique.","title":"Non-primary (UNIQUE) keys"},{"location":"Programming/SQL%20Manual/#indices","text":"Indices are essential for speeding queries containing conditions (including conditional joins). The basic syntax for creating an index is: CREATE INDEX <index name> ON <table name>(<column name>);","title":"Indices"},{"location":"Programming/SQL%20Manual/#show-table-indices","text":"MySQL: SHOW INDEX FROM <tablename>; PostgreSQL SELECT * FROM pg_indexes WHERE tablename = '<tablename >';","title":"Show Table Indices"},{"location":"Programming/SQL%20Manual/#show-indices-from-all-tables","text":"MySQL: SELECT DISTINCT TABLE_NAME, INDEX_NAME FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = '<schemaname>';","title":"Show Indices From All Tables"},{"location":"Programming/SQL%20Manual/#create-table","text":"The syntax is: CREATE TABLE <TABLE NAME> ( <COLUNM 1 NAME> <COLUNM 1 TYPE>, ... <COLUNM N NAME> <COLUNM N TYPE> [, PRIMARY KEY (<LIST OF KEY COLUMNS>)] ) The primary key is optional, but usually, any table should have one. Each row has to have a unique primary key. An index is created automatically for the list of primary key columns.","title":"CREATE TABLE"},{"location":"Programming/SQL%20Manual/#alter-table","text":"","title":"ALTER TABLE"},{"location":"Programming/SQL%20Manual/#add-column-generated-from-other-columns","text":"SQL: ALTER TABLE <table name> ADD <column name> AS (<generator expression>); In MySQL we have to add the data type: ALTER TABLE <tablename> ADD <columnname> <datatype> AS (<generator expression>); In PostgreSQL, the syntax is quite different: ALTER TABLE <tablename> ADD <columnname> <datatype> GENERATED ALWAYS AS (<generator expression>) STORED The advantage of the Postgres approach is that the column is set as generated, therefore it is generated for the new rows automatically.","title":"Add column generated from other columns"},{"location":"Programming/SQL%20Manual/#procedures-and-functions","text":"In SQL there are two constructs that are able to encapsulate SQL statements to increase resusability and readability: functions and procedues. The main difference between these two is that functions are intended to be called inline from SLQ statements, while procedures cannot be used in SQL statements and instead, they are used as a wrapper of a set of SQL statement to be called repeatedly with different arguments. The extensive summary of the different capabilities of functions and procedures is on SO .","title":"Procedures and functions"},{"location":"Programming/SQL%20Manual/#calling-a-procedure","text":"The keyword for calling a procedure differs between database system, refer to the documentation for your system for the right keyword.","title":"Calling a procedure"},{"location":"Programming/SQL%20Manual/#creating-a-procedure","text":"The syntax for calling a procedure differs between database system, refer to the documentation for your system for the right syntax.","title":"Creating a procedure"},{"location":"Programming/SQL%20Manual/#parameters","text":"Procedures and functions can have parameters similar to parameters in programming languages. We can use those parameters in the body of a function/procedure equally as a column or constant. Parameters can have default values , suplied after the = sign. We can test whether a default argument was supplied by testing the parameter for the default value.","title":"Parameters"},{"location":"Programming/SQL%20Manual/#views","text":"Views are basically named SQL queries stored in database. The queries are run on each view invocation, unles the view is materialized. The syntax is: CREATE VIEW <VIEW NAME> AS <QUERY>","title":"Views"},{"location":"Programming/SQL%20Manual/#modifying-the-view","text":"The view can be modified with CREATE OR REPLACE VIEW , however, existing columns cannot be changed . If you need to change existing columns, drop the view first.","title":"Modifying the view"},{"location":"Programming/SQL%20Manual/#performace-optimization","text":"When the query is slow, first inspect the following checklist: - Do not use OR or IN for a set of columns (see replacing OR below). - Check that all column and combination of columns used in conditions ( WHERE ) are indexed. - Check that all foreign keys are indexed. - Check that all joins are simple joins (column to column, or set of columns to a matching set of columns). If nothing from the above works, try to start with a simple query and add more complex pars to find where the problem is. If decomposing the query also does not bing light into the problem, refer to either one of the subsections below, or to the external sources. Also, note that some IDEs limits the number of returned rows automatically, which can hide serious problems and confuse you. Try to remove the limit when testing the performance this way.","title":"Performace Optimization"},{"location":"Programming/SQL%20Manual/#replacing-or","text":"We can slow down the query significantly using OR or IN statements if the set of available options is not constant (e.g., IN(1, 2) is okish, while IN(origin, destination) can have drastic performance impact). To get rid of these disjunctioncs, we can use the UNION statement, basically duplicating the query. The resulting query will be double in size, but much faster: SELECT people.id FROM promo JOIN people ON promo.date IN(people.birthaday, people.name_day); -- can be revritten as SELECT * FROM promo JOIN people ON promo.date = people.birthaday UNION SELECT * FROM promo JOIN people ON promo.date = people.name_day GROUP BY people.id","title":"Replacing OR"},{"location":"Programming/SQL%20Manual/#a-specific-join-makes-the-query-slow","text":"If a single join makes the query slow, there is a great chance that the index is not used for the join. Even if the table has an index on the referenced column(s), the join can still not use it if we are joining not to the table itself but to: - a subquery, - a variable created with a WITH statement, - a view, - or temborary table created from the indexed table. You can solve the situation by creating a materialized view or temporary table instead, and adding inedices to the table manualy. Specifically, you need to split the query into multiple queries: 1. delete the materialized view/table if exists 1. create the materialized view/table 1. create the required indices 1. perform the actual query that utilizes th view/table Of course we can skip the first three steps if the materialized view is constant for all queries, which is common during the testing phase.","title":"A specific join makes the query slow"},{"location":"Programming/SQL%20Manual/#slow-delete","text":"When the delete is slow, the cause can be the missing index on the child table that refers to the table we are deleting from. Other possible causes are listed in the SO answer .","title":"Slow DELETE"},{"location":"Programming/SQL%20Manual/#sources","text":"Database development mistakes made by application developers","title":"Sources"},{"location":"Programming/Slurm/","text":"Commands srun Run the task in the current shell in blocking mode, i.e., the console will be blocked till the task finishes. This command is only useful if we expect that the resources will be available immediatelly and the task will finish quickly. Otherwise, we should use sbatch . Params: - --pty runs the in terminal mode. Output and error streams are closed for everything except the first task. - -i Input setting. If followed by no param, it indicates that the input stream is closed. sbatch Request the execution of a task, with the required resources specified as sbatch parameters. The plain call with all resources defaulted is: sbatch <bash script> Note that the <bash script> here realy needs to be a bash script, it cannot be an arbitrary command or executable. Important parameters: - -n, --ntasks : maximum number of tasks/threads that will be allocated by the job - default is one task per node - -N, --nodes : number of allocated nodes. - default: minimum nodes that are needed to allocate resources according to other parameters (e.g., --ntasks , --mem ). - --mem maximum memory that will be allocated by the job. The suffix G stands for gigabytes, by default, it uses megabytes. Example: --mem=40G . - -t, --time : time limit. possible formats are <minutes> , <minutes:seconds> , <hours:minutes:seconds> , <days-hours> , <days-hours:minutes> , and <days-hours:minutes:seconds> . - default: partition time limit - -p , --partition= : partition name - -o , --output= : job's output file name. The default name is slurm-<JOB ID>.out squeue -u <username> filter just a specific user --start print the expected start time and the nodes planed to run the task -w --nodelist filter jobs running (but not planned) on specific nodes. The format for nodelist is <name>[<range>] , e.g., n[05-06] . sinfo Prints information about the computer cluster.","title":"Commands"},{"location":"Programming/Slurm/#commands","text":"","title":"Commands"},{"location":"Programming/Slurm/#srun","text":"Run the task in the current shell in blocking mode, i.e., the console will be blocked till the task finishes. This command is only useful if we expect that the resources will be available immediatelly and the task will finish quickly. Otherwise, we should use sbatch . Params: - --pty runs the in terminal mode. Output and error streams are closed for everything except the first task. - -i Input setting. If followed by no param, it indicates that the input stream is closed.","title":"srun"},{"location":"Programming/Slurm/#sbatch","text":"Request the execution of a task, with the required resources specified as sbatch parameters. The plain call with all resources defaulted is: sbatch <bash script> Note that the <bash script> here realy needs to be a bash script, it cannot be an arbitrary command or executable. Important parameters: - -n, --ntasks : maximum number of tasks/threads that will be allocated by the job - default is one task per node - -N, --nodes : number of allocated nodes. - default: minimum nodes that are needed to allocate resources according to other parameters (e.g., --ntasks , --mem ). - --mem maximum memory that will be allocated by the job. The suffix G stands for gigabytes, by default, it uses megabytes. Example: --mem=40G . - -t, --time : time limit. possible formats are <minutes> , <minutes:seconds> , <hours:minutes:seconds> , <days-hours> , <days-hours:minutes> , and <days-hours:minutes:seconds> . - default: partition time limit - -p , --partition= : partition name - -o , --output= : job's output file name. The default name is slurm-<JOB ID>.out","title":"sbatch"},{"location":"Programming/Slurm/#squeue","text":"-u <username> filter just a specific user --start print the expected start time and the nodes planed to run the task -w --nodelist filter jobs running (but not planned) on specific nodes. The format for nodelist is <name>[<range>] , e.g., n[05-06] .","title":"squeue"},{"location":"Programming/Slurm/#sinfo","text":"Prints information about the computer cluster.","title":"sinfo"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/","text":"This guide presents how to prepare the following working environment: One or more from the following toolchains MinGW MSVC GCC Cmake Clion and/or Visual studio vcpkg Toolchain There are various toolchains available on Windows and Linux, but we limit this guide for only some of them, specifically those which are frequently updated and works great with Clion. MSYS2 (Windows only) download follow the installation guide on the homepage install MinGW64 using: pacman -S mingw-w64-x86_64-gcc MSCV (Windows only) install Visual Studio 2019 Comunity Edition Common Compiler Flags /MD , /MT , and similar : these determines the version of the standard run-time library. The /MD flag is the default and also prefered. /nologo : do not print the copyright banner and information messages /EH : exception handeling flags GCC (Linux/WSL) If on Windows, Install the WSL first (preferably WSL 2) sudo apt-get update sudo apt-get upgrade sudo apt-get install build-essential rsync zip ninja-build make 3 For using gcc 10: - sudo apt-get install gcc-10 g++-10 Cmake Windows: Install CMake from https://cmake.org/download/ if your CMake is too old (e.g. error: \u201cCMake 3.15 or higher is required\u201d), update CMake (same as new install) Linux: If cmake is installed already, uninstall it! Do not use the cmake from linux repositories!! Download CMake sh installer from https://cmake.org/download/ install: sudo chmod +x <INSTALLER> sudo <INSTALLER> sudo rm <INSTALLER> add cmake executable to path Other details about CMake can be found in the CMake Manual. vcpkg follow the installation guide , including the user and PowerShell/bash integration add the vcpkg directory to PATH , so the program can be run from anywhere Beware that to run it with sudo on linux, it is not that easy . add a new system variable VCPKG_DEFAULT_TRIPLET , so your default library version installed with vcpkg will be x64 (like our builds), set it to: x64-linux for Linux Compilers x64-windows for MSVC x64-MinGW for MinGW CMake Integration By default, CMake does not see the vcpkg. To set up the appropriate enviroment variables, paths, etc., we need to run cmake commands with path to cmake toolchain file: vcpkg/scripts/buildsystems/vcpkg.cmake . See the IDE and command line section for the detailed instructions how to execute cmake with the path to the vcpkg toolchain file. The toolchain file is executed early on, so it is safe to assume that the environment will be correctly set up before the commands in yor cmake script. Update git pull bootstrap vcpkg again Windows: bootstrap-vcpkg.bat Linux: bootstrap-vcpkg.sh Update package Update vcpkg vcpkg update to get a list of available updates vcpkg upgrade --no-dry-run to actually perform the upgrade you can supply the name of the package (e.g., zlib:x64-windows) as an argument to upgrade just one package Package Features Some libraries have optional features, which are not installed by default, but we can install them explicitely. For example llvm . After vcpkg install llvm and typing vcpkg search llvm : llvm 10.0.0#6 The LLVM Compiler Infrastructure llvm[clang] Build C Language Family Front-end. llvm[clang-tools-extra] Build Clang tools. ... llvm[target-all] Build with all backends. llvm[target-amdgpu] Build with AMDGPU backend. llvm[target-arm] Build with ARM backend. Above, we can see that there are a lot of optional targets. To install the the arm target, for example, we can use vcpkg install llvm[target-arm] . Sometimes, a new build of the main package is required, in that case, we need to type vcpkg install llvm[target-arm] --recurse . Directory Structure Module Installation Scripts They are located in the ports directory. There is no special way how to update just the port dir, so update the whole vcpkg by git pull in case you need to update the list of available packages. Modules Vcpkg has it s own find_package macro in the toolchain file. It executes the script: vcpkg/installed/<tripplet>/share/<package name>/vcpkg-cmake-wrapper.cmake , if exists. Then, it executes the cmake scripts in that directory using the standard find_package , like a cmake config package. IDE Clion Toolchain configuration Go to settings -> Build, Execution, Deployment -> toolchain , add new toolchain and set: - Name to whatever you want - The environment should point to your toolchain: - MSVC: C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community - MSYS: C:\\MSYS2 - WSL: From the drop-down list, choose the environment you configured for using with CLion in the previous steps - Credentials (WSL) click to the setting button next to the credentials and fill - host: localhost - port: 2222 - user and password according to your WSL system credentials - Architecture (non WSL): amd64 - CMake: C:\\Program Files\\CMake\\bin\\cmake.exe , for WSL, leave it as it is - other fields should be filled automatically Project configuration Most project settings resides (hereinafter Project settings ) in settings -> Build, Execution, Deployment -> CMake . For each build configuration, add a new template and set: - Name to whatever you want - Build type to debug - To Cmake options , add: - path to vcpkg toolchain file: - Linux: -DCMAKE_TOOLCHAIN_FILE=/opt/vcpkg/scripts/buildsystems/vcpkg.cmake - Windows: -DCMAKE_TOOLCHAIN_FILE=C:/vcpkg/scripts/buildsystems/vcpkg.cmake - Set the correct vcpkg triplet - MSVC: -DVCPKG_TARGET_TRIPLET=x64-windows - MinGW: -DVCPKG_TARGET_TRIPLET=x64-MinGW - Linux: -DVCPKG_TARGET_TRIPLET=x64-linux WSL extra configuration The CLion does not see the WSL's environment variables (as of 2023-03, see here ). To fix it, go to Project settings and set add the necessary environment variables to Environment field. WSL configuration - Deprecated Clion connects to WSL through SSH. Therefore, you need to configure SSH in WSL. To do it, run the following script: wget https://raw.githubusercontent.com/JetBrains/clion-wsl/master/ubuntu_setup_env.sh && bash ubuntu_setup_env.sh Next, It\u2019s necessary to modify the WSL/create the WSL initialization script to fix a CMake issue when connecting from CLion. Download the wsl.conf file, and put it in /etc/. The restart the WSL (wsl.exe -t Ubuntu-20.04) Visual Studio Installation Install Visual Studio Open/Create a CMake project Install ReSharper C++ Setting Synchronization Sign-in in Visual Studio using a Mictosoft account. A lot of settings should be synchronized automatically . Apply the layout: Window -> Apply Window Layout -> <Layout Name> Sync ReSharper settings: you can share the file: %APPDATA%\\JetBrains\\Shared\\vAny\\ ( ~\\AppData\\Roaming\\JetBrains\\Shared\\vAny\\ ). This does not work good though as the files are changed on both sides constantly. unfortunately, as of 01/2023, there is no good way how to share resharper settings Install roamed plugins Basic Configuration Add 120 char guideline install the extension add the guideline in command window: Edit.AddGuideline 120 if there is an error extension ... did not load properly , you need to install the developer analytic tools package to the Visual Studio: Visual Studio Installer -> modify Go to the Individual Components tab search for the extension and select it proceed with the Visual Studio Modification If you need to use the system CMake, configure it now (described below) If you use *.tpp file, configure a support for them (described below). installation Enable template implementation files ( .*tpp ) syntax highlighting: Go to Tools -> Options -> Text Editor -> File Extension select Microsoft Visual C++ write tpp to the field and click add (reopen the file to see changes) To Change the Build Verbosity Go to Tools -> Options -> Projects and Solutions -> Build and Run Change the value of the MSBuild project build output verbosity. Project Setting Configure Visual Studio to use system CMake: Go to Project -> CMake Settings it should open the CMakeSettings.json file Scroll to the bottom and click on show advanced settings Set the CMake executable to point to the cmake.exe file of your system CMake Build Setting and Enviromental Variables The build configuration is in the file CMakePresets.json , located in the root of the project. The file can be also opened by right clicking on CMakeLists.txt ad selecting Edit CMake presets . Set the CMake Toolchain File To set the vcpkg toolchain file add the following value to the base configuration cacheVariables dictionary: \"CMAKE_TOOLCHAIN_FILE\": { \"value\": \"C:/vcpkg/scripts/buildsystems/vcpkg.cmake\", \"type\": \"FILEPATH\" } Set the Compiler The MSVC toolchain has two compiler executables, default one, and clang. The default compiler configuration looks like this: \"cacheVariables\": { ... \"CMAKE_C_COMPILER\": \"cl.exe\", \"CMAKE_CXX_COMPILER\": \"cl.exe\" ... }, To change the compiler to clang, replace cl.exe by clang-cl.exe in both rows. Old Method Using CMakeSettings.json We can open the build setting by right click on CMakeList.txt -> Cmake Settings To configure configure vcpkg toolchain file: Under General , fill to the Cmake toolchain file the following: C:/vcpkg/scripts/buildsystems/vcpkg.cmake To configure the enviromental variable, edit the CmakeSettings.json file directly. The global variables can be set in the environments array, the per configuration ones in <config object>/environments ( exmaple ). Launch Setting The launch settings determins the launch configuration, most importantly, the run arguments. To modify the run arguments: 1. open the launch.vs.json file: - use the context menu: - Right-click on CMakeLists.txt -> Add Debug Configuration - select default - or open the file directly, it is stored in <PROJECT DIR>/.vs/ 2. in launch.vs.json configure: - type : default for MSVC or cppgdb for WSL - projectTarget : the name of the target (executable) - name : the display name in Visual Studio - args : json array with arguments as strings - arguments with spaces have to be quoted with escaped quotes - cwd : the working directory 3. Select the launch configuration in the drop-down menu next to the play button launch.vs.json reference Other launch.vs.json options cwd : the working directory Microsoft reference for launch.vs.json WSL Configuration For using GCC 10: - go to CmakeSettings.json -> CMake variables and cache - select show advanced variables checkbox - set CMAKE_CXX_COMPILER variable to /usr/bin/g++-10 Other Configuration show white spaces: Edit -> Advanced -> View White Space . configure indentation: described here Determine Visual Studio version At total, there are 5 different versionigs related to Visual Studio . The version which the compiler support table refers to is the version of the compiler ( cl.exe ). we can find it be examining the compiler executable stored in C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.34.31933\\bin\\Hostx64\\x64 . Problems & solutions Cannot regenerate Cmak cache go to ./vs and look for file named CmakeWorkspaceSettings . It most likelz contains a line with disable = true . Just delete the file, or the specific line. Installing Library Dependencies Vcpkg Libraries type vcpkg list , if the library you need is not listed, continue to the next steps type vcpkg search <library simple name> and inspect the result to determine the exact name of the package you need if the library is not listed, check the presence in vcpkg repo if the library is in repo, but search does not find it, update vcpkg type vcpkg install <exact name> to install the package at the end of the installation log, there will be a cmake command needed to integrate the library, put it to the appropriate place to your CMakeList.txt file Boost With boost, we should install only the necessary components. Then to include boost, we need: - find_package(Boost REQUIRED) - with all compiled components listed - target_include_directories(<YOUR TARGET NAME> PUBLIC ${Boost_INCLUDE_DIRS}) JNI for JNI, a JAVA_HOME system property needs to be set to the absolute path to the JDK, e.g., C:\\Program Files\\Java\\jdk-15.0.1 Gurobi If you don\u2019t have Gurobi installed, do it now, and check that the installation is working Windows: just install as usual Linux: download the archive to /opt sudo tar xvfz <gurobi archive> add the the file that introduce environment variables needed for gurobi to etc/profile.d Linux only: it is necessary to build the C++ library for your version of the compiler. Steps: 1. cd <GUROBI DIR>/linux64/src/build/ 2. make 3. mv libgurobi_c++.a ../../lib/libgurobi_c++_<some id for you, like version>.a 4. cd ../../lib/ 5. ln -sf ./libgurobi_c++<some id for you, like version>.a libgurobi_c++.a Follow this guide , specifically: 1. put the attached our custom FindGUROBI script to: - Windows: C:\\Program Files\\CMake\\share\\cmake-<your cmake version>\\Modules/ - Linux: /opt/<CMAKNAME>/share/cmake-<VERSION>/Modules 2. to your CMakeLists.txt , add: - find_package(GUROBI REQUIRED) - target_include_directories(<your executable> PRIVATE ${GUROBI_INCLUDE_DIRS}) - target_link_libraries(<your executable> PRIVATE ${GUROBI_LIBRARY}) - target_link_libraries(<your executable> PRIVATE optimized ${GUROBI_CXX_LIBRARY} debug ${GUROBI_CXX_DEBUG_LIBRARY}) 3. try to load the cmake projects (i.e., generate the build scripts using cmake). 4. if the C++ library is not found ( Gurobi c++ library not found ), check whether the correct C++ library is in the gurobi home, the file <library name>.lib has to be in the lib directory of the gurobi installation. If the file is not there, it is possible that your gurobi version is too old Update Gurobi Updating is done by installing the new version and generating and using new licence key . after update, you need to delete your build dir in order to prevent using of cached path to old Gurobi install Also, you need to update the library name on line 10 of the FindGUROBI.cmake script. Other Libraries Not Available in vcpkg Test Library linking/inclusion For testing purposes, we can follow this simple pattern: 1. build the library 2. include the library: target_include_directories(<target name> PUBLIC <path to include dir>) , where include dir is the directory with the main header file of the library. 3. if the library is not the header only library, we need to: 3.1 link the library: target_link_libraries(<target name> PUBLIC <path to lib file>) , where path to lib file is the path to the dynamic library file used for linking ( .so on Linux, .lib on Windows). 3.2. add the dynamic library to some path visible for the executable - here the library file is .so on Linux and .dll on Windows - there are plenty options for the visible path, the most common being the system PATH variable, or the directory with the executable. Dependencies with WSL and CLion In WSL, when combined with CLion, some find scripts does not work, because they depend on system variables, that are not correctly passed from CLIon SSH connection to CMake. Therefore, it is necessary to add hints with absolute path to these scripts. Some of them can be downloaded here . Package that require these hints: - JNI - Gurobi Handling Case Insensitivity Windows builds are, in line with the OS, case insensitive. Moreover, the Visual Studio does some magic with names internally, so the build is case insensitive even on VS WSL builds. The case insensitivity can bring inconsistencies that later breake Unix builds. Therefore, it is desirable to have the build case sensitive even on Windows. Fortunatelly, we can toggle the case sensitivity at the OS level using this PowerShell command: Get-ChildItem <PROJECT ROOT PATH> -Recurse -Directory | ForEach-Object { fsutil.exe file setCaseSensitiveInfo $_.FullName enable } Note that this can break the git commits, so it is necessary to also configure git in your case-sensitive repo: git config core.ignorecase false # Compilation for a specific CPU ## MSVC MSVC cannot compile for a specific CPU or CPU series. It can, however, use new instructions sets more efficiently if it compiles the code without the support for CPUs thad does not support these instruction sets. The command for the compiler is: `/arch: (see MSVC documentation for details). ## GCC In GCC, the march option enables compilation for a specific hardware. ml) option enables compilation for a specific hardware.","title":"C++ Workflow"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#toolchain","text":"There are various toolchains available on Windows and Linux, but we limit this guide for only some of them, specifically those which are frequently updated and works great with Clion.","title":"Toolchain"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#msys2-windows-only","text":"download follow the installation guide on the homepage install MinGW64 using: pacman -S mingw-w64-x86_64-gcc","title":"MSYS2 (Windows only)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#mscv-windows-only","text":"install Visual Studio 2019 Comunity Edition","title":"MSCV (Windows only)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#common-compiler-flags","text":"/MD , /MT , and similar : these determines the version of the standard run-time library. The /MD flag is the default and also prefered. /nologo : do not print the copyright banner and information messages /EH : exception handeling flags","title":"Common Compiler Flags"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#gcc-linuxwsl","text":"If on Windows, Install the WSL first (preferably WSL 2) sudo apt-get update sudo apt-get upgrade sudo apt-get install build-essential rsync zip ninja-build make 3 For using gcc 10: - sudo apt-get install gcc-10 g++-10","title":"GCC (Linux/WSL)"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#cmake","text":"Windows: Install CMake from https://cmake.org/download/ if your CMake is too old (e.g. error: \u201cCMake 3.15 or higher is required\u201d), update CMake (same as new install) Linux: If cmake is installed already, uninstall it! Do not use the cmake from linux repositories!! Download CMake sh installer from https://cmake.org/download/ install: sudo chmod +x <INSTALLER> sudo <INSTALLER> sudo rm <INSTALLER> add cmake executable to path Other details about CMake can be found in the CMake Manual.","title":"Cmake"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#vcpkg","text":"follow the installation guide , including the user and PowerShell/bash integration add the vcpkg directory to PATH , so the program can be run from anywhere Beware that to run it with sudo on linux, it is not that easy . add a new system variable VCPKG_DEFAULT_TRIPLET , so your default library version installed with vcpkg will be x64 (like our builds), set it to: x64-linux for Linux Compilers x64-windows for MSVC x64-MinGW for MinGW","title":"vcpkg"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#cmake-integration","text":"By default, CMake does not see the vcpkg. To set up the appropriate enviroment variables, paths, etc., we need to run cmake commands with path to cmake toolchain file: vcpkg/scripts/buildsystems/vcpkg.cmake . See the IDE and command line section for the detailed instructions how to execute cmake with the path to the vcpkg toolchain file. The toolchain file is executed early on, so it is safe to assume that the environment will be correctly set up before the commands in yor cmake script.","title":"CMake Integration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#update","text":"git pull bootstrap vcpkg again Windows: bootstrap-vcpkg.bat Linux: bootstrap-vcpkg.sh","title":"Update"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#update-package","text":"Update vcpkg vcpkg update to get a list of available updates vcpkg upgrade --no-dry-run to actually perform the upgrade you can supply the name of the package (e.g., zlib:x64-windows) as an argument to upgrade just one package","title":"Update package"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#package-features","text":"Some libraries have optional features, which are not installed by default, but we can install them explicitely. For example llvm . After vcpkg install llvm and typing vcpkg search llvm : llvm 10.0.0#6 The LLVM Compiler Infrastructure llvm[clang] Build C Language Family Front-end. llvm[clang-tools-extra] Build Clang tools. ... llvm[target-all] Build with all backends. llvm[target-amdgpu] Build with AMDGPU backend. llvm[target-arm] Build with ARM backend. Above, we can see that there are a lot of optional targets. To install the the arm target, for example, we can use vcpkg install llvm[target-arm] . Sometimes, a new build of the main package is required, in that case, we need to type vcpkg install llvm[target-arm] --recurse .","title":"Package Features"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#directory-structure","text":"","title":"Directory Structure"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#module-installation-scripts","text":"They are located in the ports directory. There is no special way how to update just the port dir, so update the whole vcpkg by git pull in case you need to update the list of available packages.","title":"Module Installation Scripts"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#modules","text":"Vcpkg has it s own find_package macro in the toolchain file. It executes the script: vcpkg/installed/<tripplet>/share/<package name>/vcpkg-cmake-wrapper.cmake , if exists. Then, it executes the cmake scripts in that directory using the standard find_package , like a cmake config package.","title":"Modules"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#ide","text":"","title":"IDE"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#clion","text":"","title":"Clion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#toolchain-configuration","text":"Go to settings -> Build, Execution, Deployment -> toolchain , add new toolchain and set: - Name to whatever you want - The environment should point to your toolchain: - MSVC: C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community - MSYS: C:\\MSYS2 - WSL: From the drop-down list, choose the environment you configured for using with CLion in the previous steps - Credentials (WSL) click to the setting button next to the credentials and fill - host: localhost - port: 2222 - user and password according to your WSL system credentials - Architecture (non WSL): amd64 - CMake: C:\\Program Files\\CMake\\bin\\cmake.exe , for WSL, leave it as it is - other fields should be filled automatically","title":"Toolchain configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#project-configuration","text":"Most project settings resides (hereinafter Project settings ) in settings -> Build, Execution, Deployment -> CMake . For each build configuration, add a new template and set: - Name to whatever you want - Build type to debug - To Cmake options , add: - path to vcpkg toolchain file: - Linux: -DCMAKE_TOOLCHAIN_FILE=/opt/vcpkg/scripts/buildsystems/vcpkg.cmake - Windows: -DCMAKE_TOOLCHAIN_FILE=C:/vcpkg/scripts/buildsystems/vcpkg.cmake - Set the correct vcpkg triplet - MSVC: -DVCPKG_TARGET_TRIPLET=x64-windows - MinGW: -DVCPKG_TARGET_TRIPLET=x64-MinGW - Linux: -DVCPKG_TARGET_TRIPLET=x64-linux","title":"Project configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#wsl-extra-configuration","text":"The CLion does not see the WSL's environment variables (as of 2023-03, see here ). To fix it, go to Project settings and set add the necessary environment variables to Environment field.","title":"WSL extra configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#wsl-configuration-deprecated","text":"Clion connects to WSL through SSH. Therefore, you need to configure SSH in WSL. To do it, run the following script: wget https://raw.githubusercontent.com/JetBrains/clion-wsl/master/ubuntu_setup_env.sh && bash ubuntu_setup_env.sh Next, It\u2019s necessary to modify the WSL/create the WSL initialization script to fix a CMake issue when connecting from CLion. Download the wsl.conf file, and put it in /etc/. The restart the WSL (wsl.exe -t Ubuntu-20.04)","title":"WSL configuration - Deprecated"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#visual-studio","text":"","title":"Visual Studio"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#installation","text":"Install Visual Studio Open/Create a CMake project Install ReSharper C++","title":"Installation"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#setting-synchronization","text":"Sign-in in Visual Studio using a Mictosoft account. A lot of settings should be synchronized automatically . Apply the layout: Window -> Apply Window Layout -> <Layout Name> Sync ReSharper settings: you can share the file: %APPDATA%\\JetBrains\\Shared\\vAny\\ ( ~\\AppData\\Roaming\\JetBrains\\Shared\\vAny\\ ). This does not work good though as the files are changed on both sides constantly. unfortunately, as of 01/2023, there is no good way how to share resharper settings Install roamed plugins","title":"Setting Synchronization"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#basic-configuration","text":"Add 120 char guideline install the extension add the guideline in command window: Edit.AddGuideline 120 if there is an error extension ... did not load properly , you need to install the developer analytic tools package to the Visual Studio: Visual Studio Installer -> modify Go to the Individual Components tab search for the extension and select it proceed with the Visual Studio Modification If you need to use the system CMake, configure it now (described below) If you use *.tpp file, configure a support for them (described below). installation","title":"Basic Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#enable-template-implementation-files-tpp-syntax-highlighting","text":"Go to Tools -> Options -> Text Editor -> File Extension select Microsoft Visual C++ write tpp to the field and click add (reopen the file to see changes)","title":"Enable template implementation files (.*tpp) syntax highlighting:"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#to-change-the-build-verbosity","text":"Go to Tools -> Options -> Projects and Solutions -> Build and Run Change the value of the MSBuild project build output verbosity.","title":"To Change the Build Verbosity"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#project-setting","text":"","title":"Project Setting"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#configure-visual-studio-to-use-system-cmake","text":"Go to Project -> CMake Settings it should open the CMakeSettings.json file Scroll to the bottom and click on show advanced settings Set the CMake executable to point to the cmake.exe file of your system CMake","title":"Configure Visual Studio to use system CMake:"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#build-setting-and-enviromental-variables","text":"The build configuration is in the file CMakePresets.json , located in the root of the project. The file can be also opened by right clicking on CMakeLists.txt ad selecting Edit CMake presets .","title":"Build Setting and Enviromental Variables"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#set-the-cmake-toolchain-file","text":"To set the vcpkg toolchain file add the following value to the base configuration cacheVariables dictionary: \"CMAKE_TOOLCHAIN_FILE\": { \"value\": \"C:/vcpkg/scripts/buildsystems/vcpkg.cmake\", \"type\": \"FILEPATH\" }","title":"Set the CMake Toolchain File"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#set-the-compiler","text":"The MSVC toolchain has two compiler executables, default one, and clang. The default compiler configuration looks like this: \"cacheVariables\": { ... \"CMAKE_C_COMPILER\": \"cl.exe\", \"CMAKE_CXX_COMPILER\": \"cl.exe\" ... }, To change the compiler to clang, replace cl.exe by clang-cl.exe in both rows.","title":"Set the Compiler"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#old-method-using-cmakesettingsjson","text":"We can open the build setting by right click on CMakeList.txt -> Cmake Settings To configure configure vcpkg toolchain file: Under General , fill to the Cmake toolchain file the following: C:/vcpkg/scripts/buildsystems/vcpkg.cmake To configure the enviromental variable, edit the CmakeSettings.json file directly. The global variables can be set in the environments array, the per configuration ones in <config object>/environments ( exmaple ).","title":"Old Method Using CMakeSettings.json"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#launch-setting","text":"The launch settings determins the launch configuration, most importantly, the run arguments. To modify the run arguments: 1. open the launch.vs.json file: - use the context menu: - Right-click on CMakeLists.txt -> Add Debug Configuration - select default - or open the file directly, it is stored in <PROJECT DIR>/.vs/ 2. in launch.vs.json configure: - type : default for MSVC or cppgdb for WSL - projectTarget : the name of the target (executable) - name : the display name in Visual Studio - args : json array with arguments as strings - arguments with spaces have to be quoted with escaped quotes - cwd : the working directory 3. Select the launch configuration in the drop-down menu next to the play button launch.vs.json reference","title":"Launch Setting"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#other-launchvsjson-options","text":"cwd : the working directory Microsoft reference for launch.vs.json","title":"Other launch.vs.json options"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#wsl-configuration","text":"For using GCC 10: - go to CmakeSettings.json -> CMake variables and cache - select show advanced variables checkbox - set CMAKE_CXX_COMPILER variable to /usr/bin/g++-10","title":"WSL Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#other-configuration","text":"show white spaces: Edit -> Advanced -> View White Space . configure indentation: described here","title":"Other Configuration"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#determine-visual-studio-version","text":"At total, there are 5 different versionigs related to Visual Studio . The version which the compiler support table refers to is the version of the compiler ( cl.exe ). we can find it be examining the compiler executable stored in C:\\Program Files\\Microsoft Visual Studio\\2022\\Community\\VC\\Tools\\MSVC\\14.34.31933\\bin\\Hostx64\\x64 .","title":"Determine Visual Studio version"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#problems-solutions","text":"","title":"Problems &amp; solutions"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#cannot-regenerate-cmak-cache","text":"go to ./vs and look for file named CmakeWorkspaceSettings . It most likelz contains a line with disable = true . Just delete the file, or the specific line.","title":"Cannot regenerate Cmak cache"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#installing-library-dependencies","text":"","title":"Installing Library Dependencies"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#vcpkg-libraries","text":"type vcpkg list , if the library you need is not listed, continue to the next steps type vcpkg search <library simple name> and inspect the result to determine the exact name of the package you need if the library is not listed, check the presence in vcpkg repo if the library is in repo, but search does not find it, update vcpkg type vcpkg install <exact name> to install the package at the end of the installation log, there will be a cmake command needed to integrate the library, put it to the appropriate place to your CMakeList.txt file","title":"Vcpkg Libraries"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#boost","text":"With boost, we should install only the necessary components. Then to include boost, we need: - find_package(Boost REQUIRED) - with all compiled components listed - target_include_directories(<YOUR TARGET NAME> PUBLIC ${Boost_INCLUDE_DIRS})","title":"Boost"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#jni","text":"for JNI, a JAVA_HOME system property needs to be set to the absolute path to the JDK, e.g., C:\\Program Files\\Java\\jdk-15.0.1","title":"JNI"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#gurobi","text":"If you don\u2019t have Gurobi installed, do it now, and check that the installation is working Windows: just install as usual Linux: download the archive to /opt sudo tar xvfz <gurobi archive> add the the file that introduce environment variables needed for gurobi to etc/profile.d Linux only: it is necessary to build the C++ library for your version of the compiler. Steps: 1. cd <GUROBI DIR>/linux64/src/build/ 2. make 3. mv libgurobi_c++.a ../../lib/libgurobi_c++_<some id for you, like version>.a 4. cd ../../lib/ 5. ln -sf ./libgurobi_c++<some id for you, like version>.a libgurobi_c++.a Follow this guide , specifically: 1. put the attached our custom FindGUROBI script to: - Windows: C:\\Program Files\\CMake\\share\\cmake-<your cmake version>\\Modules/ - Linux: /opt/<CMAKNAME>/share/cmake-<VERSION>/Modules 2. to your CMakeLists.txt , add: - find_package(GUROBI REQUIRED) - target_include_directories(<your executable> PRIVATE ${GUROBI_INCLUDE_DIRS}) - target_link_libraries(<your executable> PRIVATE ${GUROBI_LIBRARY}) - target_link_libraries(<your executable> PRIVATE optimized ${GUROBI_CXX_LIBRARY} debug ${GUROBI_CXX_DEBUG_LIBRARY}) 3. try to load the cmake projects (i.e., generate the build scripts using cmake). 4. if the C++ library is not found ( Gurobi c++ library not found ), check whether the correct C++ library is in the gurobi home, the file <library name>.lib has to be in the lib directory of the gurobi installation. If the file is not there, it is possible that your gurobi version is too old","title":"Gurobi"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#update-gurobi","text":"Updating is done by installing the new version and generating and using new licence key . after update, you need to delete your build dir in order to prevent using of cached path to old Gurobi install Also, you need to update the library name on line 10 of the FindGUROBI.cmake script.","title":"Update Gurobi"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#other-libraries-not-available-in-vcpkg","text":"","title":"Other Libraries Not Available in vcpkg"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#test-library-linkinginclusion","text":"For testing purposes, we can follow this simple pattern: 1. build the library 2. include the library: target_include_directories(<target name> PUBLIC <path to include dir>) , where include dir is the directory with the main header file of the library. 3. if the library is not the header only library, we need to: 3.1 link the library: target_link_libraries(<target name> PUBLIC <path to lib file>) , where path to lib file is the path to the dynamic library file used for linking ( .so on Linux, .lib on Windows). 3.2. add the dynamic library to some path visible for the executable - here the library file is .so on Linux and .dll on Windows - there are plenty options for the visible path, the most common being the system PATH variable, or the directory with the executable.","title":"Test Library linking/inclusion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#dependencies-with-wsl-and-clion","text":"In WSL, when combined with CLion, some find scripts does not work, because they depend on system variables, that are not correctly passed from CLIon SSH connection to CMake. Therefore, it is necessary to add hints with absolute path to these scripts. Some of them can be downloaded here . Package that require these hints: - JNI - Gurobi","title":"Dependencies with WSL and CLion"},{"location":"Programming/C%2B%2B/C%2B%2B%20Workflow/#handling-case-insensitivity","text":"Windows builds are, in line with the OS, case insensitive. Moreover, the Visual Studio does some magic with names internally, so the build is case insensitive even on VS WSL builds. The case insensitivity can bring inconsistencies that later breake Unix builds. Therefore, it is desirable to have the build case sensitive even on Windows. Fortunatelly, we can toggle the case sensitivity at the OS level using this PowerShell command: Get-ChildItem <PROJECT ROOT PATH> -Recurse -Directory | ForEach-Object { fsutil.exe file setCaseSensitiveInfo $_.FullName enable } Note that this can break the git commits, so it is necessary to also configure git in your case-sensitive repo: git config core.ignorecase false # Compilation for a specific CPU ## MSVC MSVC cannot compile for a specific CPU or CPU series. It can, however, use new instructions sets more efficiently if it compiles the code without the support for CPUs thad does not support these instruction sets. The command for the compiler is: `/arch: (see MSVC documentation for details). ## GCC In GCC, the march option enables compilation for a specific hardware. ml) option enables compilation for a specific hardware.","title":"Handling Case Insensitivity"},{"location":"Programming/C%2B%2B/CMake%20Manual/","text":"Commands Generating Build scripts General syntax is: cmake <dir> Here <dir> is the CMakeLists.txt directory. The build scripts are build in current directory. Toolchain file To work with package managers, a link to toolchain file has to be provided as an argument -DCMAKE_TOOLCHAIN_FILE . For vcpkg , the argument is as follows: -DCMAKE_TOOLCHAIN_FILE=<vcpkg location>/scripts/buildsystems/vcpkg.cmake Note that the toolchain is only loaded at the beginnning of the generation process. Once you forgot it, you need to delete the build scripts diectory content to make this argument work for subsequent cmake commands. Usefull arguments -LH to see cmake nonadvanced variables together with the description. -LHA to see also the advanced variables. Note that this prints only cached variables, to print all variables, we have to edit the CmakeLists.txt. To change build options ( option in CMakeLists.txt ), run cmake with -D <option name>=<option value> <build dir> . Example: cmake -D BUILD_TESTING=OFF . Building For building, use: cmake --build <build dir> where build dir is the directory containing the build scripts ( CmakeFiles folder). To list the build options: cmake -L Specify the target We can use the --target parameter for that: cmake --build . --target <TARGET NAME> Specify the build type (Debug, Release) In CMake, we use a specific build type string instead of compiler and linker flags: - Debug - Debug build - Release - Release build - RelWithDebInfo - Release build with debug information - MinSizeRel - Release build with minimal size Unfortunately, the way how the build type should be specified depends on the build system: - Single-configuration systems (GCC, Clang, MinGW) - Multi-configuration systems (MSVC) Single-configuration systems Single configuration systems have the compiler flags hardcoded in the build scripts. Therefore, we need to specify the build type for CMake when we generate the build scripts: cmake ../ -DCMAKE_BUILD_TYPE=Release By default, the build type is an empty string . This means that no extra flags are added to the compiler and linker so the compiler and linker run with their default settings. Interesting info can be found in this SO question . Multi-configuration systems In multi-configuration systems, the -DCMAKE_BUILD_TYPE parameter is ignored, because the build configuration is supposed to be determined when building the code (i.e., same build scripts for debug and for release). Therefore, we omit it, and instead specify the --config parameter when building the code: cmake --build . --config Release Clean the source files Run: cmake --build . --target clean Syntax Variables We can use the variable using their name: if(DEFINED <name>) ... In string, we need to wrap the variable in ${} : message(STATUS \"dir=${dir}\") Enviromental variables We can use environmental variables using the ENV variable: if(DEFINED ENV{<name>}) ... Be aware that in string, we use only one pair of curly braces (see variable references manual ): message(STATUS \"dir=$ENV{dir}\") Print all variables To print all variables, the following function can be used: function(dump_cmake_variables) if (ARGV0) message(STATUS \"Printing variables matching '${ARGV0}'\") else() message(STATUS \"Printing all variables\") endif() get_cmake_property(_variableNames VARIABLES) list (SORT _variableNames) foreach (_variableName ${_variableNames}) if (ARGV0) unset(MATCHED) string(REGEX MATCH ${ARGV0} MATCHED ${_variableName}) if (NOT MATCHED) continue() endif() endif() message(STATUS \"${_variableName}=${${_variableName}}\") endforeach() message(STATUS \"Printing variables - END\") endfunction() To print all variables related to HDF5 lib, call dump_cmake_variables(HDF) after the find_package call. Control structures if The if command has the following syntax: if(<condition>) ... elseif(<condition>) ... else() ... endif() Generator expressions Manual Generator expressions are a very useful tool to control the build process based on the build type, compiler type, or similar properties. CMake use them to generate mutliple build scripts from a single CMakeLists.txt file. The syntax for a basic condition expression is: \"$<$<condition>:<this will be printed if condition is satisfied>>\" CMakeLists.txt The typical structure of the CMakeLists.txt file is as follows: 1. Top section contains project wide setting like name, minimum cmake version, and the language specification. Targets sections containing: the target definition together with sources used target includes target linking Typical Top section content The typical content of the top section is: - minimum cmake version: cmake_minimum_required(VERSION <version>) - project name: project(<name>) - language specification: enable_language(<language>) - language standard, e.g.: set(CMAKE_CXX_STANDARD <version>) - compile options: add_compile_options(<option 1> <option 2> ...) Compile options Most of the compile options are now sets automatically based on the declarations in the CMakeLists.txt file. However, some notable exceptions exists. To set such options, we have to use the add_compile_options command: add_compile_options(<option 1> <option 2> ...) MSVC /permissive- to enable the strictest mode of the compiler GCC -pedantic-errors to report all cases where non-standard GCC extension is used and treat them as errors Linker Options Linker options can be set with add_link_options command. Example: add_link_options(\"/STACK: 10000000\") Searching for libraries Although it is possible to hard-code the paths for includes and linking, it is usually better to initialize the paths automatically using a rich set of commands cmake offers. It has the following advatages: - Hardcoding the paths is error-prone, while cmake commands usually deliver correct paths - It boost the productivity as we do not have to investigate where each library is installed - The resulting CMakeLists.txt file is more portable - And most importantly, potential errors concerning missing libraries are reported prior to the compilation/linking . Most of the libraries have CMake support, so their CMake variables can be initialized simply by calling the find_package command described below. These packages have either: - their own cmake config (cmake-aware libs usually installed through the package manager like vcpkg ) - or they have a Find<package name> script created by someone else that heuristically search for the packege (The default location for these scripts is CMake/share/cmake-<version>/Modules ). For packages without the CMake support, we have to use lower-level cmake commands like find_path or find_libraries . For convinience, we can put these command to our own Find<name> script taht can be used by multiple project or even shared. find_package The find_package command is the primary way of obtaining correct variables for a library including: - include paths - linking paths - platform/toolchain specific enviromental variables There are two types of package (library) info search: - module , which uses cmake scripts provided by CMake or OS. The modules are typically provided only for the most used libraries (e.g. boost). All modules provided by CMake are listed in the documentation . - config which uses CMake scripts provided by the developers of the package. They are typically distributed with the source code and downloaded by the package manager. Unless specified, the module mode is used. To force a speciic mode, we can use the MODULE / CONFIG parameters. Conig packages Config packages are CMake modules that were created as cmake projects by their developers. They are therefore naturally integrated into Cmake. The configuration files are executed as follows: 1. Package version file: <package name>-config-version.cmake or <package name>ConfigVersion.cmake . This file handles the version compatibility, i.e., it ensures that the installed version of the package is compatible with the version requested in the find_package command. 1. Package configuration file: <package name>-config.cmake or <package name>Config.cmake . Module Packages Module packages are packages that are not cmake projects themselves, but are hooked into cmake using custom find module scrips. These scripts are automatically executed by find_package . They are located in e.g.: CMake/share/cmake-3.22/Modules/Find<package name>.cmake . find_path The find_path command is intended to find the path (e.g., an include directory). A simple syntax is: find_path( <var name> NAMES <file names> PATHS <paths> ) Here: - <var name> is the name of the resulting variable - <file names> are all possible file names split by space. At least one of the files needs to be present in a path for it to be considered to be the found path. - <paths> are candidate paths split by space find_library The find_library command is used to populate a variable with a result of a specific file search optimized for libraries. The search algorithm works as follows: 1. ? 2. Search package paths - order: 1. <CurrentPackage>_ROOT , 2. ENV{<CurrentPackage>_ROOT} , 3. <ParentPackage>_ROOT , 4. ENV{<ParentPackage>_ROOT} - this only happens if the find_library command is called from within a find_<module> or find_package - this step can be skipped using the NO_PACKAGE_ROOT_PATH parameter 3. Search path from cmake cache. During a clean cmake generation, these can be only supplied by command line. - Considered variables: - CMAKE_LIBRARY_ARCHITECTURE - CMAKE_PREFIX_PATH - CMAKE_LIBRARY_PATH - CMAKE_FRAMEWORK_PATH - this step can be skipped using the NO_CMAKE_PATH parameter 4. Same as step 3, but the variables are searched among system environmental variables instead - this step can be skipped using the NO_CMAKE_ENVIRONMENT_PATH parameter 5. Search paths specified by the HINTS option 6. Search the standard system environmental paths - variables considered are LIB and PATH - this step can be skipped using the NO_SYSTEM_ENVIRONMENT_PATH parameter 7. Search in system paths - Considered variables: - CMAKE_LIBRARY_ARCHITECTURE - CMAKE_SYSTEM_PREFIX_PATH - CMAKE_SYSTEM_LIBRARY_PATH - CMAKE_SYSTEM_FRAMEWORK_PATH - this step can be skipped using the NO_CMAKE_SYSTEM_PATH parameter 8. Search the paths specified by the PATHS option Searching for libraries in the project dir Note that the project dir is not searched by default. To include in the search, use: HINTS ${PROJECT_SOURCE_DIR} . Full example on the Gurobi lib stored in <CMAKE LISTS DIR/lib/gurobi_c++.a> : find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED ) Creating a custom find script The structure of a simple find scripts is described in the documentation . We can either put the find script to the default location, so it will be available for all projects, or we can put it in the project directory and add that directory to the CMAKE_MODULE_PATH : list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\") The usual structure of the find script is: 1. the comment section describing the file: #[=======================================================================[.rst: FindMOSEK ------- Finds the MOSEK library. Result Variables ^^^^^^^^^^^^^^^^ This will define the following variables: ``MOESK_FOUND`` True if the system has the MOSEK library. ``MOSEK_INCLUDE_DIRS`` Include directories needed to use MOSEK. ``MOSEK_LIBRARIES`` Libraries needed to link to MOSEK. #]=======================================================================] find commands that fils some temp variables: find_path( MOSEK_INCLUDE_DIR NAMES mosek.h PATHS \"$ENV{MOSEK_HOME}/h\" ) find_library( MOSEK_LIBRARY NAMES libmosek64.so.10.0 libfusion64.so.10.0 PATHS \"$ENV{MOSEK_HOME}/bin\" ) handeling of the result of the file commands. The standard approach is: include(FindPackageHandleStandardArgs) find_package_handle_standard_args(MOSEK FOUND_VAR MOSEK_FOUND REQUIRED_VARS MOSEK_LIBRARY MOSEK_INCLUDE_DIR ) setting the final variables: if(MOSEK_FOUND) set(MOSEK_LIBRARIES ${MOSEK_LIBRARY}) set(MOSEK_INCLUDE_DIRS ${MOSEK_INCLUDE_DIR}) endif() Setting include directories To inlude the headers, we need to use a inlude_directories (global), or better target_include_directories command. Linking configuration For linkink, use the target_link_libraries command. Include directories All the global include directories are stored in the INCLUDE_DIRECTORIES property, to print them, add this to the CMakeLists.txt file: get_property(dirs DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} PROPERTY INCLUDE_DIRECTORIES) foreach(dir ${dirs}) message(STATUS \"dir='${dir}'\") endforeach() CMake Directory Structure System Find_XXX.cmake files The system find scripts are located in the CMake/share/cmake-<version>/Modules/ directory.","title":"CMake Manual"},{"location":"Programming/C%2B%2B/CMake%20Manual/#commands","text":"","title":"Commands"},{"location":"Programming/C%2B%2B/CMake%20Manual/#generating-build-scripts","text":"General syntax is: cmake <dir> Here <dir> is the CMakeLists.txt directory. The build scripts are build in current directory.","title":"Generating Build scripts"},{"location":"Programming/C%2B%2B/CMake%20Manual/#toolchain-file","text":"To work with package managers, a link to toolchain file has to be provided as an argument -DCMAKE_TOOLCHAIN_FILE . For vcpkg , the argument is as follows: -DCMAKE_TOOLCHAIN_FILE=<vcpkg location>/scripts/buildsystems/vcpkg.cmake Note that the toolchain is only loaded at the beginnning of the generation process. Once you forgot it, you need to delete the build scripts diectory content to make this argument work for subsequent cmake commands.","title":"Toolchain file"},{"location":"Programming/C%2B%2B/CMake%20Manual/#usefull-arguments","text":"-LH to see cmake nonadvanced variables together with the description. -LHA to see also the advanced variables. Note that this prints only cached variables, to print all variables, we have to edit the CmakeLists.txt. To change build options ( option in CMakeLists.txt ), run cmake with -D <option name>=<option value> <build dir> . Example: cmake -D BUILD_TESTING=OFF .","title":"Usefull arguments"},{"location":"Programming/C%2B%2B/CMake%20Manual/#building","text":"For building, use: cmake --build <build dir> where build dir is the directory containing the build scripts ( CmakeFiles folder). To list the build options: cmake -L","title":"Building"},{"location":"Programming/C%2B%2B/CMake%20Manual/#specify-the-target","text":"We can use the --target parameter for that: cmake --build . --target <TARGET NAME>","title":"Specify the target"},{"location":"Programming/C%2B%2B/CMake%20Manual/#specify-the-build-type-debug-release","text":"In CMake, we use a specific build type string instead of compiler and linker flags: - Debug - Debug build - Release - Release build - RelWithDebInfo - Release build with debug information - MinSizeRel - Release build with minimal size Unfortunately, the way how the build type should be specified depends on the build system: - Single-configuration systems (GCC, Clang, MinGW) - Multi-configuration systems (MSVC)","title":"Specify the build type (Debug, Release)"},{"location":"Programming/C%2B%2B/CMake%20Manual/#single-configuration-systems","text":"Single configuration systems have the compiler flags hardcoded in the build scripts. Therefore, we need to specify the build type for CMake when we generate the build scripts: cmake ../ -DCMAKE_BUILD_TYPE=Release By default, the build type is an empty string . This means that no extra flags are added to the compiler and linker so the compiler and linker run with their default settings. Interesting info can be found in this SO question .","title":"Single-configuration systems"},{"location":"Programming/C%2B%2B/CMake%20Manual/#multi-configuration-systems","text":"In multi-configuration systems, the -DCMAKE_BUILD_TYPE parameter is ignored, because the build configuration is supposed to be determined when building the code (i.e., same build scripts for debug and for release). Therefore, we omit it, and instead specify the --config parameter when building the code: cmake --build . --config Release","title":"Multi-configuration systems"},{"location":"Programming/C%2B%2B/CMake%20Manual/#clean-the-source-files","text":"Run: cmake --build . --target clean","title":"Clean the source files"},{"location":"Programming/C%2B%2B/CMake%20Manual/#syntax","text":"","title":"Syntax"},{"location":"Programming/C%2B%2B/CMake%20Manual/#variables","text":"We can use the variable using their name: if(DEFINED <name>) ... In string, we need to wrap the variable in ${} : message(STATUS \"dir=${dir}\")","title":"Variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#enviromental-variables","text":"We can use environmental variables using the ENV variable: if(DEFINED ENV{<name>}) ... Be aware that in string, we use only one pair of curly braces (see variable references manual ): message(STATUS \"dir=$ENV{dir}\")","title":"Enviromental variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#print-all-variables","text":"To print all variables, the following function can be used: function(dump_cmake_variables) if (ARGV0) message(STATUS \"Printing variables matching '${ARGV0}'\") else() message(STATUS \"Printing all variables\") endif() get_cmake_property(_variableNames VARIABLES) list (SORT _variableNames) foreach (_variableName ${_variableNames}) if (ARGV0) unset(MATCHED) string(REGEX MATCH ${ARGV0} MATCHED ${_variableName}) if (NOT MATCHED) continue() endif() endif() message(STATUS \"${_variableName}=${${_variableName}}\") endforeach() message(STATUS \"Printing variables - END\") endfunction() To print all variables related to HDF5 lib, call dump_cmake_variables(HDF) after the find_package call.","title":"Print all variables"},{"location":"Programming/C%2B%2B/CMake%20Manual/#control-structures","text":"","title":"Control structures"},{"location":"Programming/C%2B%2B/CMake%20Manual/#if","text":"The if command has the following syntax: if(<condition>) ... elseif(<condition>) ... else() ... endif()","title":"if"},{"location":"Programming/C%2B%2B/CMake%20Manual/#generator-expressions","text":"Manual Generator expressions are a very useful tool to control the build process based on the build type, compiler type, or similar properties. CMake use them to generate mutliple build scripts from a single CMakeLists.txt file. The syntax for a basic condition expression is: \"$<$<condition>:<this will be printed if condition is satisfied>>\"","title":"Generator expressions"},{"location":"Programming/C%2B%2B/CMake%20Manual/#cmakeliststxt","text":"The typical structure of the CMakeLists.txt file is as follows: 1. Top section contains project wide setting like name, minimum cmake version, and the language specification. Targets sections containing: the target definition together with sources used target includes target linking","title":"CMakeLists.txt"},{"location":"Programming/C%2B%2B/CMake%20Manual/#typical-top-section-content","text":"The typical content of the top section is: - minimum cmake version: cmake_minimum_required(VERSION <version>) - project name: project(<name>) - language specification: enable_language(<language>) - language standard, e.g.: set(CMAKE_CXX_STANDARD <version>) - compile options: add_compile_options(<option 1> <option 2> ...)","title":"Typical Top section content"},{"location":"Programming/C%2B%2B/CMake%20Manual/#compile-options","text":"Most of the compile options are now sets automatically based on the declarations in the CMakeLists.txt file. However, some notable exceptions exists. To set such options, we have to use the add_compile_options command: add_compile_options(<option 1> <option 2> ...)","title":"Compile options"},{"location":"Programming/C%2B%2B/CMake%20Manual/#msvc","text":"/permissive- to enable the strictest mode of the compiler","title":"MSVC"},{"location":"Programming/C%2B%2B/CMake%20Manual/#gcc","text":"-pedantic-errors to report all cases where non-standard GCC extension is used and treat them as errors","title":"GCC"},{"location":"Programming/C%2B%2B/CMake%20Manual/#linker-options","text":"Linker options can be set with add_link_options command. Example: add_link_options(\"/STACK: 10000000\")","title":"Linker Options"},{"location":"Programming/C%2B%2B/CMake%20Manual/#searching-for-libraries","text":"Although it is possible to hard-code the paths for includes and linking, it is usually better to initialize the paths automatically using a rich set of commands cmake offers. It has the following advatages: - Hardcoding the paths is error-prone, while cmake commands usually deliver correct paths - It boost the productivity as we do not have to investigate where each library is installed - The resulting CMakeLists.txt file is more portable - And most importantly, potential errors concerning missing libraries are reported prior to the compilation/linking . Most of the libraries have CMake support, so their CMake variables can be initialized simply by calling the find_package command described below. These packages have either: - their own cmake config (cmake-aware libs usually installed through the package manager like vcpkg ) - or they have a Find<package name> script created by someone else that heuristically search for the packege (The default location for these scripts is CMake/share/cmake-<version>/Modules ). For packages without the CMake support, we have to use lower-level cmake commands like find_path or find_libraries . For convinience, we can put these command to our own Find<name> script taht can be used by multiple project or even shared.","title":"Searching for libraries"},{"location":"Programming/C%2B%2B/CMake%20Manual/#find_package","text":"The find_package command is the primary way of obtaining correct variables for a library including: - include paths - linking paths - platform/toolchain specific enviromental variables There are two types of package (library) info search: - module , which uses cmake scripts provided by CMake or OS. The modules are typically provided only for the most used libraries (e.g. boost). All modules provided by CMake are listed in the documentation . - config which uses CMake scripts provided by the developers of the package. They are typically distributed with the source code and downloaded by the package manager. Unless specified, the module mode is used. To force a speciic mode, we can use the MODULE / CONFIG parameters.","title":"find_package"},{"location":"Programming/C%2B%2B/CMake%20Manual/#conig-packages","text":"Config packages are CMake modules that were created as cmake projects by their developers. They are therefore naturally integrated into Cmake. The configuration files are executed as follows: 1. Package version file: <package name>-config-version.cmake or <package name>ConfigVersion.cmake . This file handles the version compatibility, i.e., it ensures that the installed version of the package is compatible with the version requested in the find_package command. 1. Package configuration file: <package name>-config.cmake or <package name>Config.cmake .","title":"Conig packages"},{"location":"Programming/C%2B%2B/CMake%20Manual/#module-packages","text":"Module packages are packages that are not cmake projects themselves, but are hooked into cmake using custom find module scrips. These scripts are automatically executed by find_package . They are located in e.g.: CMake/share/cmake-3.22/Modules/Find<package name>.cmake .","title":"Module Packages"},{"location":"Programming/C%2B%2B/CMake%20Manual/#find_path","text":"The find_path command is intended to find the path (e.g., an include directory). A simple syntax is: find_path( <var name> NAMES <file names> PATHS <paths> ) Here: - <var name> is the name of the resulting variable - <file names> are all possible file names split by space. At least one of the files needs to be present in a path for it to be considered to be the found path. - <paths> are candidate paths split by space","title":"find_path"},{"location":"Programming/C%2B%2B/CMake%20Manual/#find_library","text":"The find_library command is used to populate a variable with a result of a specific file search optimized for libraries. The search algorithm works as follows: 1. ? 2. Search package paths - order: 1. <CurrentPackage>_ROOT , 2. ENV{<CurrentPackage>_ROOT} , 3. <ParentPackage>_ROOT , 4. ENV{<ParentPackage>_ROOT} - this only happens if the find_library command is called from within a find_<module> or find_package - this step can be skipped using the NO_PACKAGE_ROOT_PATH parameter 3. Search path from cmake cache. During a clean cmake generation, these can be only supplied by command line. - Considered variables: - CMAKE_LIBRARY_ARCHITECTURE - CMAKE_PREFIX_PATH - CMAKE_LIBRARY_PATH - CMAKE_FRAMEWORK_PATH - this step can be skipped using the NO_CMAKE_PATH parameter 4. Same as step 3, but the variables are searched among system environmental variables instead - this step can be skipped using the NO_CMAKE_ENVIRONMENT_PATH parameter 5. Search paths specified by the HINTS option 6. Search the standard system environmental paths - variables considered are LIB and PATH - this step can be skipped using the NO_SYSTEM_ENVIRONMENT_PATH parameter 7. Search in system paths - Considered variables: - CMAKE_LIBRARY_ARCHITECTURE - CMAKE_SYSTEM_PREFIX_PATH - CMAKE_SYSTEM_LIBRARY_PATH - CMAKE_SYSTEM_FRAMEWORK_PATH - this step can be skipped using the NO_CMAKE_SYSTEM_PATH parameter 8. Search the paths specified by the PATHS option","title":"find_library"},{"location":"Programming/C%2B%2B/CMake%20Manual/#searching-for-libraries-in-the-project-dir","text":"Note that the project dir is not searched by default. To include in the search, use: HINTS ${PROJECT_SOURCE_DIR} . Full example on the Gurobi lib stored in <CMAKE LISTS DIR/lib/gurobi_c++.a> : find_library(GUROBI_CXX_LIBRARY NAMES gurobi_c++ HINTS ${PROJECT_SOURCE_DIR} PATH_SUFFIXES lib NO_CMAKE_ENVIRONMENT_PATH REQUIRED )","title":"Searching for libraries in the project dir"},{"location":"Programming/C%2B%2B/CMake%20Manual/#creating-a-custom-find-script","text":"The structure of a simple find scripts is described in the documentation . We can either put the find script to the default location, so it will be available for all projects, or we can put it in the project directory and add that directory to the CMAKE_MODULE_PATH : list(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_LIST_DIR}/cmake\") The usual structure of the find script is: 1. the comment section describing the file: #[=======================================================================[.rst: FindMOSEK ------- Finds the MOSEK library. Result Variables ^^^^^^^^^^^^^^^^ This will define the following variables: ``MOESK_FOUND`` True if the system has the MOSEK library. ``MOSEK_INCLUDE_DIRS`` Include directories needed to use MOSEK. ``MOSEK_LIBRARIES`` Libraries needed to link to MOSEK. #]=======================================================================] find commands that fils some temp variables: find_path( MOSEK_INCLUDE_DIR NAMES mosek.h PATHS \"$ENV{MOSEK_HOME}/h\" ) find_library( MOSEK_LIBRARY NAMES libmosek64.so.10.0 libfusion64.so.10.0 PATHS \"$ENV{MOSEK_HOME}/bin\" ) handeling of the result of the file commands. The standard approach is: include(FindPackageHandleStandardArgs) find_package_handle_standard_args(MOSEK FOUND_VAR MOSEK_FOUND REQUIRED_VARS MOSEK_LIBRARY MOSEK_INCLUDE_DIR ) setting the final variables: if(MOSEK_FOUND) set(MOSEK_LIBRARIES ${MOSEK_LIBRARY}) set(MOSEK_INCLUDE_DIRS ${MOSEK_INCLUDE_DIR}) endif()","title":"Creating a custom find script"},{"location":"Programming/C%2B%2B/CMake%20Manual/#setting-include-directories","text":"To inlude the headers, we need to use a inlude_directories (global), or better target_include_directories command.","title":"Setting include directories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#linking-configuration","text":"For linkink, use the target_link_libraries command.","title":"Linking configuration"},{"location":"Programming/C%2B%2B/CMake%20Manual/#include-directories","text":"All the global include directories are stored in the INCLUDE_DIRECTORIES property, to print them, add this to the CMakeLists.txt file: get_property(dirs DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR} PROPERTY INCLUDE_DIRECTORIES) foreach(dir ${dirs}) message(STATUS \"dir='${dir}'\") endforeach()","title":"Include directories"},{"location":"Programming/C%2B%2B/CMake%20Manual/#cmake-directory-structure","text":"","title":"CMake Directory Structure"},{"location":"Programming/C%2B%2B/CMake%20Manual/#system-find_xxxcmake-files","text":"The system find scripts are located in the CMake/share/cmake-<version>/Modules/ directory.","title":"System Find_XXX.cmake files"},{"location":"Programming/Java/Debugging%20Java/","text":"General rules Warnings can help you to spot the problems. Check that they are enabled ( Xlint ). Sometimes, warnings may help to understand the problem. However, they are not emmited due to compilation error. Try to comment out the errorneous code and compile the code to see all warnings. Missing Resources First, check if the resources are where you expected in the jar or in the target folder. The structure is described here on SO . If the resources are not there, try to rebuild the project.","title":"General rules"},{"location":"Programming/Java/Debugging%20Java/#general-rules","text":"Warnings can help you to spot the problems. Check that they are enabled ( Xlint ). Sometimes, warnings may help to understand the problem. However, they are not emmited due to compilation error. Try to comment out the errorneous code and compile the code to see all warnings.","title":"General rules"},{"location":"Programming/Java/Debugging%20Java/#missing-resources","text":"First, check if the resources are where you expected in the jar or in the target folder. The structure is described here on SO . If the resources are not there, try to rebuild the project.","title":"Missing Resources"},{"location":"Programming/Java/Java%20Programming%20Guide/","text":"Strings Classical string literals has to be enclosed in quotes ( \"my string\" ). They cannot contain some characters (newline, \" ). The backslash ( \\ ) character is reserved for Java escape sequences and need to be ecaped as \\\\ . In addition, since Java 15, there are text blocks, enclosed in three quotes: \"\"\" My long \"text block\" \"\"\" The properties of textt blocks: - can be used as standard string literals - can contain newlines, quotes, and some other symbols that have to be escaped in standard string literals. The new line after opening triple quoute is obligatory. Note that backslash ( \\ ) is still interpreted as Java escape character, so we need to escape it using \\\\ . Overloading When calling an oveladed method with a null argument, we receive a compilation error: reference to <METHOD> is ambiguous . A solution to this problem is to cast the null pointer to the desired type: public set(string s){ ... } public set(MyClass c){ .... } set(null) // doesn't work set((MyClass) null) // works set((String) null) // works Regular expressions First thing we need to know about regexes in java is that we need to escape all backslashes (refer to the Strings chapter), there are no raw strings in Java. We can work with regexes either by using specialized high level methods that accepts regex as string, or by using the tools from the java.util.regex package. Testing if string matches a regex We can do it simply by: String MyString = \"tests\"; String MyRegex = \"t\\\\p+\" boolean matches = myString.matches(myRegex); Exceptions The exception mechanism in Java is similar to other languages. The big difference is, however, that in Java, all exception not derived from the RuntimeException class are checked , which means that their handeling is enforced by the compiler. The following code does not compile, because the java.langException class is a checked exception class. private testMethod{ ... throw new Exception(); } Therefore, we need to handle the exception using the try cache block or add throws <EXCEPTION TYPE> to the method declaration. When deciding between try/catch and throws , the rule of thumb is to use the try/cache if we can handle the exception and throws if we want to leave it for the caller. The problem arises when the method we are in implements an interface that does not have the throws declaration we need. Then the only solution is to use try/cache with a cache that does not handle the exception, and indicate the problem to the caller differently, e.g, by returning a null pointer. Genericity Oracle official tutorial for Java 8 Like many other languages, Java offers generic types with the classical syntax: class GenericClass<T>{ ... }; An object is then created as: GenericClass<OtherClass> gc = new GenericClass<OtherClass>(); Diamond operator At all places where the type can be inferred, we can use the dianmond operator ( <> ) without specifiyng a type: GenericClass<OtherClass> gc = new GenericClass<>(); Raw types specification For the backward compatibility of Java library classes that were converted to generic classes a concept of raw types was introduces. However, in addition, this concept brings a lot of very subtle compile errors, which are caused by a nonintentional creation of raw type instead of a specific type. GenericClass gc = new GenericClass(); // raw type The parameterized type can be assigned to the raw type, but when we do it the other way arround, the code emmits warning and is not runtime safe: GenericClass<OtherClass> generic = new GenericClass<>(); GenericClass raw = generic // OK GenericClass raw = new GenericClass(); GenericClass<OtherClass> generic = raw // unsafe Calling the generic methods on raw types has a simmilar consequences. But the worst property of raw types is that all generic non-static members of raw types that are not inherited and are also eraw types. Therefore, the genericity is lost even for the completely unrelated type parameters, even specific types are erased to Object . The consequence can be seen on the following example: class GenericClass<T>{ public List<String> getListofStrings() }; GenericClass raw = new GenericClass(); raw.getListofStrings(); // returns List<Object> !!! Generic method A generic method in Java is written as: public <<GENERIC PARAM NAME>> <RESULT TYPE> <METHOD NAME>(<PARAMS>){ ... } example: public static <T> ProcedureParameterValidationResult<T> error(String msg){ return new ProcedureParameterValidationResult<>(false, msg, null); } if the generic parameter cannot be infered from the method arguments, we can call the method with explicit generic argument like this: <<GENERIC PARAM VALUE>><METHOD NAME>(<ARGUMENTS>); example: ProcedureParameterValidationResult.<String>error(\"Value cannot be empty.\"); Retrieving the type of a generic argument Because the genericity in Java is resolved at runtime, it is not possible to use the generic parameter as a type. What does it mean? Whille it is possible to use Myclass.class it is not possible to use T.class . The same applies for the class methods. The usual solution is to pass the class to the generic method as a method parameter: void processGeneric(Class<T> classType){ ... } Default generic argument It is not possible to set the default value of a generic parameter. If we want to achive such behavior, we need to create a new class: class GeneriClass<T>{ ... } class StringClass extends GenericClass<String>{}; Wildcards The wildcard symbol ( ? ) represents a concrete but unspecified type. It can be bounded by superclass/interface ( ? exctends MyInterface ), or by sublcass ( ? super MyClass ). The typical use cases: - single non-generic method accepting generic types of multiple generic parameter values: myMethod(Wrapper<? extends MyInterface> wrapper){ MyInterface mi = wrapper.get(); ... } method that can process generic class but does not need to work with the generic parameters at all: getLength(List<?> list){ return list.size(); } Difference between MyClass , MyClass<Object> , and MyClass<?> class MyClass<T>{ private T content; private set(T content){ this.content = content; } private List<String> getList; } // raw type processMyClass(MyClass myClass){...}; // My class of object types processMyClass(MyClass<Object> myClass){...}; // My class of any specific type processMyClass(MyClass<?> myClass){...}; MyClass (raw type) MyClass<Object> MyClass<?> type safe (Check types at compile time) no yes yes set can be called with any Object any Object null only getList() returns List<Object> List<String> List<String> processMyClass can be called with any List List<Object> only any List Collections Set From java 9, sets can be simply initialized as: Set<int> nums = Set.of(1,2,3); Note that this method returns an immutable set. In the earlier versions of Java, the Collections.singleton method can be used. Enums Java enumerations are declared using the enum keyword: public enum Semaphore{ RED, ORAGNGE, GREEN } Iterating over enum items we can iterate over enum items using the values method: for(Semaphore s: Semaphore.values()){ ... } The above code iterates over values of a specific enum. If we need an iteration over any enum (generic), we need to use a class method: Class<E> enumClass; for(E item: enumClass.getEnumConstants()){ } Converting a string to enum We can convert a String to enum using the valueOf method that is generated for each enum class> Semaphore sem = Semaphore.valueOf(\"RED\"); Note that the string has to match the enum value exactly, including the case. Extra whitespaces are not permited. There is also a generic static valueOf method in the Enum class, which we can use for generic enums: Class<E> enumClass; E genEnumInst = Enum.valueOf(enumClass, \"RED\"); Note that here E has to be a properly typed enum ( Enum<E> ), not the raw enum ( Enum ). Lambda expressions wiki An important thing about lambda expressions in Java is that we can only use them to create types satisfying some functional interface. This means that: - They can be used only in a context where a functional interface is expected - They need to be convertible to that interface. The example that demonstrate this behavior is bellow. Usually, we use some standard interface, but here we create a new one for clarity: interface OurFunctionalInterface(){ int operation(int a) } ... public void process(int num, OurFunctionalInterface op){ ... } With the above interface and method that uses it, we can call process(0, a -> a + 5) Which is an equivalent of writing OurFunctionalInterface addFive = a -> a + 5; process(0, addFive); Syntax A full syntax for lambda expressions in Java is: (<PARAMS>) -> { <EXPRESSIONS> } If there is only one expression, we can ommit the code block: (<PARAMS>) -> <EXPRESSION> And also, we can ommit the return statement from that expression. The two lambda epressions below are equivalent: (a, b) -> {return a + b;} (a, b) -> a + b If there is only one parameter, we can ommit the parantheses: <PARAM> -> <EXPRESSION> By default, the parameter types are infered from the functional interface. If we need more specific parameters for our function, we can specify the parameter type, we have to specify all of them however: (a, b) -> a + b // valid (int a, int b) -> a + b // valid (int a, b) -> a + b // invalid Also, it is necesary to use the parantheses, if we use the param type. Method references We can also create lambda functions from existing mehods (if they satisfy the desired interface) using a mechanism called method reference . For example, we can use the Integer.sum(int a, int b) method in conext where the IntBinaryOperator interface is required. Instead of IntBinaryOperator op = (a, b) -> a + b; // new lambda body we can write: IntBinaryOperator op = Integer::sum; // lambda from existing function Exceptions in lambda expressions Beware that the checked exceptions thrown inside lambda expressions has to be caught inside the lambda expression. The following code does not compile: try{ process(0, a -> a.methodThatThrows()) catch(exception e){ ... } Instead, we have to write: process(0, a -> { try{ return a.methodThatThrows()); catch(exception e){ ... } } Iteration Java for each loop has the following syntax: for(<TYPE> <VARNAME>: <ITERABLE>){ ... } Here, the <ITERABLE> can be either a Java Iterable , or an array. Enumerated iteration There is no enumerate equivalent in Java. One can use a stream API range method , however, it is less readable than standard for loop because the code execuded in loop has to be in a separate function. Iterating using an iterator The easiest way how to iterate if we have a given iterator is to use its forEachRemaining method. It takes a Consumer object as an argument, iterates using the iterator, and calls Consumer.accept method on each iteration. The example below uses a lambda function that satisfies the consumer interface: class Our{ void process(){ ... } ... } Iterator<Our> objectsIterator = ...; objectsIterator.forEachRemaining(o -> o.process()); Functional Programming Turning array to stream Arrays.stream(<ARRAY>); Creating stream from iterator The easiest way is to first create an Iterable from the iterator and then use the StreamSupport.stream method: Iterable<JsonNode> iterable = () -> iterator; var stream = StreamSupport.stream(iterable.spliterator(), false); Filtering A lambda function can be supplied as a filter: stream.filter(number -> number > 5) returns a stream with numbers greater then five. Transformation We can transform the stream with the map function: transformed = stream.map(object -> doSomething(object)) Materialize stream We can materrialize stream with the collect metod: List<String> result = stringStream.collect(Collectors.toList()); Which can be, in case of List shorten to: List<String> result = stringStream.toList(); var openjdk document Using var is similar to auto in C++. Unlike in C++, it can be used only for local variables. The var can be tricky when using together with diamond operator or generic methods. The compilation works fine, however, the raw type will be created. Type cast Java use a tradidional casting syntax: (<TARGET TYPE>) <VAR> There are two different types of cast: - value cast , which is used for value types (only primitive types in Java) and change the data - reference cast , which is used for Java objects and does not change it, it just change the objects interface Reference cast The upcasting (casting to a supertype) is done implicitely by the compiler, so instead of Object o = (Object) new MyClass(); we can do Object o = new MyClass(); Typically, we need to use the explicit cast for downcasting : Object o = ...; MyClass c = (MyClass) o; // when we are sure that o is an instance of MyClass... Downcasting to an incorrect type leads to a runtime ClassCastException . Casting to an unrelated type is not allowed by the compiler. Casting generic types Casting with generic types is notoriously dangerous due to type erasure. The real types of generic arguments are not known at runtime, therefore, an incorrect cast does not raise an error: Object o = new Object(); String s = (String) o; // ClassCatsException List<Object> lo = new ArrayList<>(); List<String> ls = (List<String>) lo; // OK at runtime, therefore, it emmits warning at compile time. Casting case expression Often, when we work with an interface, we have to treat each implementation differently. The best way to handle that is to use the polymorphism, i.e., add a dedicated method for treating the object to the interface and handle the differences in each implementation. However, sometimes, we cannot modify the interface as it comes from the outside of our codebase, or we do not want to put the code to the interface because it belongs to a completely different part of the application (e.g., GUI vs database connection). A typicall solution for this is to use branching on instanceof and a consecutive cast: if(obj instance of Apple){ Apple apple = (Apple) obj; ... } else if(obj instance of Peach){ Peach peach = (Peach) obj; ... } ... Casting switch This approach works, it is safe, but it is also error prone. A new preview swich statement is ready to replace this technique: switch(obj) case Apple apple: ... break; case Peach peach: ... break; ... } Note that unsafe type opperations are not allowed in these new switch statements, and an error is emitted instead of warning: List list = ... List<String> ls = (List<String>) list; // warning: unchecked switch(list){ case List<String> ls: // error: cannot be safely cast ... } Random numbers Generate random integer To get an infinite iterator of random integers, we can use the Random.ints method: var it = new Random().ints(0, map.nodesFromAllGraphs.size()).iterator();\\ int a = it.nextInt(); Reflection Reflection is a toolset for working with Java types. Its methods can be accessed from a class object associated with all classes. The class object represents the type and it can be accessed either by MyClass.class or by calling the MyClass.getClass method. The type of the class object is Class<> Test if a class is a superclass or interface of another class It can be tested simply as: ClassA.isAssignableFrom(ClassB) Get field by name Field field = MyClass().getDeclaredField(\"fieldName\"); SQL Java has a build in support for SQL in package java.sql . The typical operation: 1. 1. 1. create a PreparedStatement from the cursor using the SQL string as an input 1. Fill the parameters of the PreparedStatement 1. Execute the query Filling the query parameters This process consist of safe replacement of ? marks in the SQL string with real values. The PreparedStatement class has dedicated methods for that: - setString for strings - setObject for complex objects Each method has the index of the ? to be replaced as the first parameter. The index start from 1 . Note that these methods can be used to supply arguments for data part of the SQL statement. If the ? is used for table names or SQL keywords, the query execution will fail. Therefore, if you need to dynamically set the non-data part of an SQL query (e.g., table name), you need to sanitize the argument manually and add it to the SQl querry. Processing the result The java sql framework returns the result of the SQL query in form of a ResultSet . To process the result set, we need to iterate the rows using the next method and for each row, we can access the column values using one of the methods (depending on the dtat type in the column), each of which accepts either column index or label as a parameter. Example: var result = statement.executeQuery(); while(result.next()){ var str = result.getString(\"column_name\")); ... } Jackson Documentation Jackson is a (de)serialization library primarily focused to the JSON format. It supports annotations for automatic serialization and deserialization of Java objects. Ignore specific fields If you want to ignore specific fields during serialization or deserialization, you can use the @JsonIgnoreProperties annotation . Use it as a class or type annotation. Example: @JsonIgnoreProperties({ \"par1\", \"par2\" }) public class ConfigModel { // \"par1\" and \"par2\" will be ignored } This annotation can also prevent the \"Unrecognized field\" error during deserialization, as the ignored fields does not have to be present as Java class members. Represent a class by a single member If we want the java class to be represented by a single value in the serialized file, we can achieve that by adding the @JsonValue annotation above the member or method that should represent the class. Note, however, that this only works for simple values, because the member serializers are not called, the members is serialized as a simple value instead. If you want to represent a class by a single but complex member, use a custom serializer instead . An equivalent annotation for deserialization is the @JsonCreator annotation which should be placed above a constructor or factory method. Deserialization The standard usage is: ObjectMapper mapper = new ObjectMapper(); // ... configure mapper File file = new File(<PATH TO FILE>); Instance instance = mapper.readValue(file, Instance.class); By default, new objects are created for all members in the object hierarchy that are either present in the serialized file. New objects are created using the setter, if exists, otherwise, the costructor is called. Multiple Setters If there are multiple setters, we need to specify the one that should be used for deserialization by marking it with the @JsonSetter annotation. Update existing instance You can update an existing instance using the readerForUpdating method: ObjectReader or = mapper.readerForUpdating(instance); // special reader or.readValue(file) // we can use the method we already know on the object reader Note that by default, the update is shalow . Only the instance object itself is updated, but its members are brand new objects. If you want to keep all objects from an existing object hierarchy, you need to use the @JsonMerge annotation. You should put this annotation above any member of the root object you want to update instead of replacing it. The @JsonUpgrade annotation is recursive : the members of the member annotated with @JsonUpgrade are updated as well and so on. Updating polymorphic types For updating polymorpic type, the rule is that the exact type has to match. Also, you need jackson-databind version 2.14.0 or greater. Read just part of the file For reading just part of the file, use the at selector taht is available in the ObjectReader class. We need to first obtain the reader from a mapper, and then use the selector: ObjectReader reader = mapper.readerFor(Instance.class); Instance instance = reader.at(\"data\").readValue(file) Note that if the path parameter of the at method is incorrect, the method throws an exception with the message: \"no content to map due to end-of-input\". Check that some node is present To check for presence of a node, we should use the JsonPointer class: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); We can also use the JsonPointer in the at method: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); if(found){ Instance instance = reader.at(jsonPointer).readValue(file, Instance.class) } Interface or abstract class When serializing interface or abstract class, it is important to include the implementation type into serialization. Otherwise, the deserialization fails, because it cannot determine the concreate type. To serialize the concrete type, we can use the @JsonRypeInfo and JsonSubTypes annotations: @JsonTypeInfo( use = JsonTypeInfo.Id.CLASS, // what value should we store, here the class name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) public interface Interface { ... } Te above code will work, the full class name will be serialized in the file, however. If we want to use a shorter syntax, i.e., some codename for the class, we need to specify a mapping between this codename and the conreate class: @JsonTypeInfo( use = JsonTypeInfo.Id.NAME, // what value should we store, here a custom name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) @JsonSubTypes({ @JsonSubTypes.Type(value = Implementation.class, name = \"Implementation name\") }) public interface Interface { ... } Custom deserializer An alternative to @JsonTypeInfo is to use a custom deserializer: https://stackoverflow.com/questions/44122782/jackson-deserialize-based-on-type Custom deserializer If our architecture is so complex or specific that none of the Jackson annotations can help us to achieve the desired behavior, we can use a custom deserializer. For that we need to: 1. Implement a custom deserializer by extending the JsonDeserializer class 1. Registering the deserializer in the ObjectMapper Creating a custom deserializer for class The only method we need to implement is the: T deserialize(JsonParser jsonParser, DeserializationContext deserializationContext) where T is the type of the object we are deserializing. To get the Jaksons representation of the JSON tree, we can call: JsonNode node = jsonParser.getCodec().readTree(jsonParser); We can get all the fields by calling using the node.fields() method. For arrays, there is a method node.elements() ; Registering the deserializer ObjectMapper mapper = new ObjectMapper(); SimpleModule module = new SimpleModule(); module.addDeserializer(OurClass.class, new OurClassDeserializer()); mapper.registerModule(module); Custom deserializer with generic types When we need a custom deserializer for a generic class, we need to use a wildcard to cover multiple values of generic argument: public class OurDeserializer extends JsonDeserializer<OurGenericClass<?>>{ ... } If we also need to get the generic argument type from JSON, we need to implement the ContextualDeserializer interface. This is discribed in a SO answer . Custom deserializer with inheritance We can have a common deserializer for both parent and child class, or multiple child classes. However, it is necessary to - make the deserializer generic and - register the deserializer for all classes, not just for the parent. Example: public class PolymorphicDeserializer<T extends Parent> extends JsonDeserializer<T> @Override public T deserialize(JsonParser p, DeserializationContext ctxt) throws IOException, JsonProcessingException { ... } } module.addDeserializer(ChildA.class, new PolymorphicDeserializer<>()); module.addDeserializer(ChildB.class, new ProcedureParameterDeserializer<>()); Serialization Standard serialization: // compressed mapper.writer().writeValue(file, object); // or with indents, etc. mapper.writerWithDefaultPrettyPrinter().writeValue(file, object); By default, new objects are created for all members in the object hierarchy that are either: - public value mebers (fields) - public getters: public methods with name get<NAME> , where <NAME> is a name of some value member Other getters with different name are called only if there is an annotation above them. A special annotation dedicated for this is @JsonProperty . Appending to an existing file To append to an existing file, we need to create an output stream in the append mode and then use it in jackson: ObjectMapper mapper = ... JsonGenerator generator = mapper.getFactory().createGenerator(new FileOutputStream(new File(<FILENAME>), true)); mapper.writeValue(generator, output); Complex member filters Insted of adding annotations to each member we want to ignore, we can also apply some more compex filters, to do that, we need to: 1. add a @JsonFilter(\"<FILTER NAME>\") annotation to all classes for which we want to use the filter 2. create the filter 3. pass a set of all filters we want to use to the writer we are using for serialization The example below keeps only members inherited from MyClass : // object on which we apply the filter @JsonFilter(\"myFilter\") class targetClass{ ... } // filter PropertyFilter filter = new SimpleBeanPropertyFilter() { @Override public void serializeAsField( Object pojo, JsonGenerator jgen, SerializerProvider provider, PropertyWriter writer ) throws Exception { if(writer.getType().isTypeOrSubTypeOf(MyClass.class)){ writer.serializeAsField(pojo, jgen, provider); } } }; FilterProvider filters = new SimpleFilterProvider().addFilter(\"myFilter\", filter); // use a writer created with filters mapper.writer(filters).writeValue(generator, output); ... Flatting the hierarchy When we desire to simplify the object hierarchy, we can use the @JsonUnwrapped annotation above a member of a class. With this annotation, the annotated member object will be skipped while all its members will be serialized into its parent. Custom serializer If the serialization requirements are too complex to be expressed using Jackson annotations, we can use a custom serialzier: public class MyCustomSerializer extends JsonSerializer<MyClass> { @Override public void serialize(MyClass myClass, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException { ... } } Analogously to custom deserializer, we can register custom serializer either in the object mapper: SimpleModule module = new SimpleModule(); module.addSerializer(MyClass.class, new MyCustomSerializer()); mapper.registerModule(module); or by annotating the class: @JsonSerialize(using = MyCustomSerializer.class) public class MyClass{ ... } You can call standard serializers from custom serializers using the SerializerProvider and JsonGenerator instances supplied as a parameters of the serialize method. For example the standard serialized value of som inner/member object class can be obtained using: serializerProvider.defaultSerializeValue(myInnerInstance, jsonGenerator); Annotations Multiple objects If there are multiple objects involved in the (de)serializetion, we can use the @JsonCreator and @JsonProperty annotations to split the work: @JsonCreator public ConfigModel( @JsonProperty(\"first\") ClassA instanceA, @JsonProperty(\"second\") ClassB instanceB ) { ... } in the above examples, the first and second are keys mapping the objects in the serialized file from which instanceA and instanceB should be created.","title":"Installation"},{"location":"Programming/Java/Java%20Programming%20Guide/#strings","text":"Classical string literals has to be enclosed in quotes ( \"my string\" ). They cannot contain some characters (newline, \" ). The backslash ( \\ ) character is reserved for Java escape sequences and need to be ecaped as \\\\ . In addition, since Java 15, there are text blocks, enclosed in three quotes: \"\"\" My long \"text block\" \"\"\" The properties of textt blocks: - can be used as standard string literals - can contain newlines, quotes, and some other symbols that have to be escaped in standard string literals. The new line after opening triple quoute is obligatory. Note that backslash ( \\ ) is still interpreted as Java escape character, so we need to escape it using \\\\ .","title":"Strings"},{"location":"Programming/Java/Java%20Programming%20Guide/#overloading","text":"When calling an oveladed method with a null argument, we receive a compilation error: reference to <METHOD> is ambiguous . A solution to this problem is to cast the null pointer to the desired type: public set(string s){ ... } public set(MyClass c){ .... } set(null) // doesn't work set((MyClass) null) // works set((String) null) // works","title":"Overloading"},{"location":"Programming/Java/Java%20Programming%20Guide/#regular-expressions","text":"First thing we need to know about regexes in java is that we need to escape all backslashes (refer to the Strings chapter), there are no raw strings in Java. We can work with regexes either by using specialized high level methods that accepts regex as string, or by using the tools from the java.util.regex package.","title":"Regular expressions"},{"location":"Programming/Java/Java%20Programming%20Guide/#testing-if-string-matches-a-regex","text":"We can do it simply by: String MyString = \"tests\"; String MyRegex = \"t\\\\p+\" boolean matches = myString.matches(myRegex);","title":"Testing if string matches a regex"},{"location":"Programming/Java/Java%20Programming%20Guide/#exceptions","text":"The exception mechanism in Java is similar to other languages. The big difference is, however, that in Java, all exception not derived from the RuntimeException class are checked , which means that their handeling is enforced by the compiler. The following code does not compile, because the java.langException class is a checked exception class. private testMethod{ ... throw new Exception(); } Therefore, we need to handle the exception using the try cache block or add throws <EXCEPTION TYPE> to the method declaration. When deciding between try/catch and throws , the rule of thumb is to use the try/cache if we can handle the exception and throws if we want to leave it for the caller. The problem arises when the method we are in implements an interface that does not have the throws declaration we need. Then the only solution is to use try/cache with a cache that does not handle the exception, and indicate the problem to the caller differently, e.g, by returning a null pointer.","title":"Exceptions"},{"location":"Programming/Java/Java%20Programming%20Guide/#genericity","text":"Oracle official tutorial for Java 8 Like many other languages, Java offers generic types with the classical syntax: class GenericClass<T>{ ... }; An object is then created as: GenericClass<OtherClass> gc = new GenericClass<OtherClass>();","title":"Genericity"},{"location":"Programming/Java/Java%20Programming%20Guide/#diamond-operator","text":"At all places where the type can be inferred, we can use the dianmond operator ( <> ) without specifiyng a type: GenericClass<OtherClass> gc = new GenericClass<>();","title":"Diamond operator"},{"location":"Programming/Java/Java%20Programming%20Guide/#raw-types","text":"specification For the backward compatibility of Java library classes that were converted to generic classes a concept of raw types was introduces. However, in addition, this concept brings a lot of very subtle compile errors, which are caused by a nonintentional creation of raw type instead of a specific type. GenericClass gc = new GenericClass(); // raw type The parameterized type can be assigned to the raw type, but when we do it the other way arround, the code emmits warning and is not runtime safe: GenericClass<OtherClass> generic = new GenericClass<>(); GenericClass raw = generic // OK GenericClass raw = new GenericClass(); GenericClass<OtherClass> generic = raw // unsafe Calling the generic methods on raw types has a simmilar consequences. But the worst property of raw types is that all generic non-static members of raw types that are not inherited and are also eraw types. Therefore, the genericity is lost even for the completely unrelated type parameters, even specific types are erased to Object . The consequence can be seen on the following example: class GenericClass<T>{ public List<String> getListofStrings() }; GenericClass raw = new GenericClass(); raw.getListofStrings(); // returns List<Object> !!!","title":"Raw types"},{"location":"Programming/Java/Java%20Programming%20Guide/#generic-method","text":"A generic method in Java is written as: public <<GENERIC PARAM NAME>> <RESULT TYPE> <METHOD NAME>(<PARAMS>){ ... } example: public static <T> ProcedureParameterValidationResult<T> error(String msg){ return new ProcedureParameterValidationResult<>(false, msg, null); } if the generic parameter cannot be infered from the method arguments, we can call the method with explicit generic argument like this: <<GENERIC PARAM VALUE>><METHOD NAME>(<ARGUMENTS>); example: ProcedureParameterValidationResult.<String>error(\"Value cannot be empty.\");","title":"Generic method"},{"location":"Programming/Java/Java%20Programming%20Guide/#retrieving-the-type-of-a-generic-argument","text":"Because the genericity in Java is resolved at runtime, it is not possible to use the generic parameter as a type. What does it mean? Whille it is possible to use Myclass.class it is not possible to use T.class . The same applies for the class methods. The usual solution is to pass the class to the generic method as a method parameter: void processGeneric(Class<T> classType){ ... }","title":"Retrieving the type of a generic argument"},{"location":"Programming/Java/Java%20Programming%20Guide/#default-generic-argument","text":"It is not possible to set the default value of a generic parameter. If we want to achive such behavior, we need to create a new class: class GeneriClass<T>{ ... } class StringClass extends GenericClass<String>{};","title":"Default generic argument"},{"location":"Programming/Java/Java%20Programming%20Guide/#wildcards","text":"The wildcard symbol ( ? ) represents a concrete but unspecified type. It can be bounded by superclass/interface ( ? exctends MyInterface ), or by sublcass ( ? super MyClass ). The typical use cases: - single non-generic method accepting generic types of multiple generic parameter values: myMethod(Wrapper<? extends MyInterface> wrapper){ MyInterface mi = wrapper.get(); ... } method that can process generic class but does not need to work with the generic parameters at all: getLength(List<?> list){ return list.size(); }","title":"Wildcards"},{"location":"Programming/Java/Java%20Programming%20Guide/#difference-between-myclass-myclassobject-and-myclass","text":"class MyClass<T>{ private T content; private set(T content){ this.content = content; } private List<String> getList; } // raw type processMyClass(MyClass myClass){...}; // My class of object types processMyClass(MyClass<Object> myClass){...}; // My class of any specific type processMyClass(MyClass<?> myClass){...}; MyClass (raw type) MyClass<Object> MyClass<?> type safe (Check types at compile time) no yes yes set can be called with any Object any Object null only getList() returns List<Object> List<String> List<String> processMyClass can be called with any List List<Object> only any List","title":"Difference between MyClass, MyClass&lt;Object&gt;, and MyClass&lt;?&gt;"},{"location":"Programming/Java/Java%20Programming%20Guide/#collections","text":"","title":"Collections"},{"location":"Programming/Java/Java%20Programming%20Guide/#set","text":"From java 9, sets can be simply initialized as: Set<int> nums = Set.of(1,2,3); Note that this method returns an immutable set. In the earlier versions of Java, the Collections.singleton method can be used.","title":"Set"},{"location":"Programming/Java/Java%20Programming%20Guide/#enums","text":"Java enumerations are declared using the enum keyword: public enum Semaphore{ RED, ORAGNGE, GREEN }","title":"Enums"},{"location":"Programming/Java/Java%20Programming%20Guide/#iterating-over-enum-items","text":"we can iterate over enum items using the values method: for(Semaphore s: Semaphore.values()){ ... } The above code iterates over values of a specific enum. If we need an iteration over any enum (generic), we need to use a class method: Class<E> enumClass; for(E item: enumClass.getEnumConstants()){ }","title":"Iterating over enum items"},{"location":"Programming/Java/Java%20Programming%20Guide/#converting-a-string-to-enum","text":"We can convert a String to enum using the valueOf method that is generated for each enum class> Semaphore sem = Semaphore.valueOf(\"RED\"); Note that the string has to match the enum value exactly, including the case. Extra whitespaces are not permited. There is also a generic static valueOf method in the Enum class, which we can use for generic enums: Class<E> enumClass; E genEnumInst = Enum.valueOf(enumClass, \"RED\"); Note that here E has to be a properly typed enum ( Enum<E> ), not the raw enum ( Enum ).","title":"Converting a string to enum"},{"location":"Programming/Java/Java%20Programming%20Guide/#lambda-expressions","text":"wiki An important thing about lambda expressions in Java is that we can only use them to create types satisfying some functional interface. This means that: - They can be used only in a context where a functional interface is expected - They need to be convertible to that interface. The example that demonstrate this behavior is bellow. Usually, we use some standard interface, but here we create a new one for clarity: interface OurFunctionalInterface(){ int operation(int a) } ... public void process(int num, OurFunctionalInterface op){ ... } With the above interface and method that uses it, we can call process(0, a -> a + 5) Which is an equivalent of writing OurFunctionalInterface addFive = a -> a + 5; process(0, addFive);","title":"Lambda expressions"},{"location":"Programming/Java/Java%20Programming%20Guide/#syntax","text":"A full syntax for lambda expressions in Java is: (<PARAMS>) -> { <EXPRESSIONS> } If there is only one expression, we can ommit the code block: (<PARAMS>) -> <EXPRESSION> And also, we can ommit the return statement from that expression. The two lambda epressions below are equivalent: (a, b) -> {return a + b;} (a, b) -> a + b If there is only one parameter, we can ommit the parantheses: <PARAM> -> <EXPRESSION> By default, the parameter types are infered from the functional interface. If we need more specific parameters for our function, we can specify the parameter type, we have to specify all of them however: (a, b) -> a + b // valid (int a, int b) -> a + b // valid (int a, b) -> a + b // invalid Also, it is necesary to use the parantheses, if we use the param type.","title":"Syntax"},{"location":"Programming/Java/Java%20Programming%20Guide/#method-references","text":"We can also create lambda functions from existing mehods (if they satisfy the desired interface) using a mechanism called method reference . For example, we can use the Integer.sum(int a, int b) method in conext where the IntBinaryOperator interface is required. Instead of IntBinaryOperator op = (a, b) -> a + b; // new lambda body we can write: IntBinaryOperator op = Integer::sum; // lambda from existing function","title":"Method references"},{"location":"Programming/Java/Java%20Programming%20Guide/#exceptions-in-lambda-expressions","text":"Beware that the checked exceptions thrown inside lambda expressions has to be caught inside the lambda expression. The following code does not compile: try{ process(0, a -> a.methodThatThrows()) catch(exception e){ ... } Instead, we have to write: process(0, a -> { try{ return a.methodThatThrows()); catch(exception e){ ... } }","title":"Exceptions in lambda expressions"},{"location":"Programming/Java/Java%20Programming%20Guide/#iteration","text":"Java for each loop has the following syntax: for(<TYPE> <VARNAME>: <ITERABLE>){ ... } Here, the <ITERABLE> can be either a Java Iterable , or an array.","title":"Iteration"},{"location":"Programming/Java/Java%20Programming%20Guide/#enumerated-iteration","text":"There is no enumerate equivalent in Java. One can use a stream API range method , however, it is less readable than standard for loop because the code execuded in loop has to be in a separate function.","title":"Enumerated iteration"},{"location":"Programming/Java/Java%20Programming%20Guide/#iterating-using-an-iterator","text":"The easiest way how to iterate if we have a given iterator is to use its forEachRemaining method. It takes a Consumer object as an argument, iterates using the iterator, and calls Consumer.accept method on each iteration. The example below uses a lambda function that satisfies the consumer interface: class Our{ void process(){ ... } ... } Iterator<Our> objectsIterator = ...; objectsIterator.forEachRemaining(o -> o.process());","title":"Iterating using an iterator"},{"location":"Programming/Java/Java%20Programming%20Guide/#functional-programming","text":"","title":"Functional Programming"},{"location":"Programming/Java/Java%20Programming%20Guide/#turning-array-to-stream","text":"Arrays.stream(<ARRAY>);","title":"Turning array to stream"},{"location":"Programming/Java/Java%20Programming%20Guide/#creating-stream-from-iterator","text":"The easiest way is to first create an Iterable from the iterator and then use the StreamSupport.stream method: Iterable<JsonNode> iterable = () -> iterator; var stream = StreamSupport.stream(iterable.spliterator(), false);","title":"Creating stream from iterator"},{"location":"Programming/Java/Java%20Programming%20Guide/#filtering","text":"A lambda function can be supplied as a filter: stream.filter(number -> number > 5) returns a stream with numbers greater then five.","title":"Filtering"},{"location":"Programming/Java/Java%20Programming%20Guide/#transformation","text":"We can transform the stream with the map function: transformed = stream.map(object -> doSomething(object))","title":"Transformation"},{"location":"Programming/Java/Java%20Programming%20Guide/#materialize-stream","text":"We can materrialize stream with the collect metod: List<String> result = stringStream.collect(Collectors.toList()); Which can be, in case of List shorten to: List<String> result = stringStream.toList();","title":"Materialize stream"},{"location":"Programming/Java/Java%20Programming%20Guide/#var","text":"openjdk document Using var is similar to auto in C++. Unlike in C++, it can be used only for local variables. The var can be tricky when using together with diamond operator or generic methods. The compilation works fine, however, the raw type will be created.","title":"var"},{"location":"Programming/Java/Java%20Programming%20Guide/#type-cast","text":"Java use a tradidional casting syntax: (<TARGET TYPE>) <VAR> There are two different types of cast: - value cast , which is used for value types (only primitive types in Java) and change the data - reference cast , which is used for Java objects and does not change it, it just change the objects interface","title":"Type cast"},{"location":"Programming/Java/Java%20Programming%20Guide/#reference-cast","text":"The upcasting (casting to a supertype) is done implicitely by the compiler, so instead of Object o = (Object) new MyClass(); we can do Object o = new MyClass(); Typically, we need to use the explicit cast for downcasting : Object o = ...; MyClass c = (MyClass) o; // when we are sure that o is an instance of MyClass... Downcasting to an incorrect type leads to a runtime ClassCastException . Casting to an unrelated type is not allowed by the compiler.","title":"Reference cast"},{"location":"Programming/Java/Java%20Programming%20Guide/#casting-generic-types","text":"Casting with generic types is notoriously dangerous due to type erasure. The real types of generic arguments are not known at runtime, therefore, an incorrect cast does not raise an error: Object o = new Object(); String s = (String) o; // ClassCatsException List<Object> lo = new ArrayList<>(); List<String> ls = (List<String>) lo; // OK at runtime, therefore, it emmits warning at compile time.","title":"Casting generic types"},{"location":"Programming/Java/Java%20Programming%20Guide/#casting-case-expression","text":"Often, when we work with an interface, we have to treat each implementation differently. The best way to handle that is to use the polymorphism, i.e., add a dedicated method for treating the object to the interface and handle the differences in each implementation. However, sometimes, we cannot modify the interface as it comes from the outside of our codebase, or we do not want to put the code to the interface because it belongs to a completely different part of the application (e.g., GUI vs database connection). A typicall solution for this is to use branching on instanceof and a consecutive cast: if(obj instance of Apple){ Apple apple = (Apple) obj; ... } else if(obj instance of Peach){ Peach peach = (Peach) obj; ... } ...","title":"Casting case expression"},{"location":"Programming/Java/Java%20Programming%20Guide/#casting-switch","text":"This approach works, it is safe, but it is also error prone. A new preview swich statement is ready to replace this technique: switch(obj) case Apple apple: ... break; case Peach peach: ... break; ... } Note that unsafe type opperations are not allowed in these new switch statements, and an error is emitted instead of warning: List list = ... List<String> ls = (List<String>) list; // warning: unchecked switch(list){ case List<String> ls: // error: cannot be safely cast ... }","title":"Casting switch"},{"location":"Programming/Java/Java%20Programming%20Guide/#random-numbers","text":"","title":"Random numbers"},{"location":"Programming/Java/Java%20Programming%20Guide/#generate-random-integer","text":"To get an infinite iterator of random integers, we can use the Random.ints method: var it = new Random().ints(0, map.nodesFromAllGraphs.size()).iterator();\\ int a = it.nextInt();","title":"Generate random integer"},{"location":"Programming/Java/Java%20Programming%20Guide/#reflection","text":"Reflection is a toolset for working with Java types. Its methods can be accessed from a class object associated with all classes. The class object represents the type and it can be accessed either by MyClass.class or by calling the MyClass.getClass method. The type of the class object is Class<>","title":"Reflection"},{"location":"Programming/Java/Java%20Programming%20Guide/#test-if-a-class-is-a-superclass-or-interface-of-another-class","text":"It can be tested simply as: ClassA.isAssignableFrom(ClassB)","title":"Test if a class is a superclass or interface of another class"},{"location":"Programming/Java/Java%20Programming%20Guide/#get-field-by-name","text":"Field field = MyClass().getDeclaredField(\"fieldName\");","title":"Get field by name"},{"location":"Programming/Java/Java%20Programming%20Guide/#sql","text":"Java has a build in support for SQL in package java.sql . The typical operation: 1. 1. 1. create a PreparedStatement from the cursor using the SQL string as an input 1. Fill the parameters of the PreparedStatement 1. Execute the query","title":"SQL"},{"location":"Programming/Java/Java%20Programming%20Guide/#filling-the-query-parameters","text":"This process consist of safe replacement of ? marks in the SQL string with real values. The PreparedStatement class has dedicated methods for that: - setString for strings - setObject for complex objects Each method has the index of the ? to be replaced as the first parameter. The index start from 1 . Note that these methods can be used to supply arguments for data part of the SQL statement. If the ? is used for table names or SQL keywords, the query execution will fail. Therefore, if you need to dynamically set the non-data part of an SQL query (e.g., table name), you need to sanitize the argument manually and add it to the SQl querry.","title":"Filling the query parameters"},{"location":"Programming/Java/Java%20Programming%20Guide/#processing-the-result","text":"The java sql framework returns the result of the SQL query in form of a ResultSet . To process the result set, we need to iterate the rows using the next method and for each row, we can access the column values using one of the methods (depending on the dtat type in the column), each of which accepts either column index or label as a parameter. Example: var result = statement.executeQuery(); while(result.next()){ var str = result.getString(\"column_name\")); ... }","title":"Processing the result"},{"location":"Programming/Java/Java%20Programming%20Guide/#jackson","text":"Documentation Jackson is a (de)serialization library primarily focused to the JSON format. It supports annotations for automatic serialization and deserialization of Java objects.","title":"Jackson"},{"location":"Programming/Java/Java%20Programming%20Guide/#ignore-specific-fields","text":"If you want to ignore specific fields during serialization or deserialization, you can use the @JsonIgnoreProperties annotation . Use it as a class or type annotation. Example: @JsonIgnoreProperties({ \"par1\", \"par2\" }) public class ConfigModel { // \"par1\" and \"par2\" will be ignored } This annotation can also prevent the \"Unrecognized field\" error during deserialization, as the ignored fields does not have to be present as Java class members.","title":"Ignore specific fields"},{"location":"Programming/Java/Java%20Programming%20Guide/#represent-a-class-by-a-single-member","text":"If we want the java class to be represented by a single value in the serialized file, we can achieve that by adding the @JsonValue annotation above the member or method that should represent the class. Note, however, that this only works for simple values, because the member serializers are not called, the members is serialized as a simple value instead. If you want to represent a class by a single but complex member, use a custom serializer instead . An equivalent annotation for deserialization is the @JsonCreator annotation which should be placed above a constructor or factory method.","title":"Represent a class by a single member"},{"location":"Programming/Java/Java%20Programming%20Guide/#deserialization","text":"The standard usage is: ObjectMapper mapper = new ObjectMapper(); // ... configure mapper File file = new File(<PATH TO FILE>); Instance instance = mapper.readValue(file, Instance.class); By default, new objects are created for all members in the object hierarchy that are either present in the serialized file. New objects are created using the setter, if exists, otherwise, the costructor is called.","title":"Deserialization"},{"location":"Programming/Java/Java%20Programming%20Guide/#multiple-setters","text":"If there are multiple setters, we need to specify the one that should be used for deserialization by marking it with the @JsonSetter annotation.","title":"Multiple Setters"},{"location":"Programming/Java/Java%20Programming%20Guide/#update-existing-instance","text":"You can update an existing instance using the readerForUpdating method: ObjectReader or = mapper.readerForUpdating(instance); // special reader or.readValue(file) // we can use the method we already know on the object reader Note that by default, the update is shalow . Only the instance object itself is updated, but its members are brand new objects. If you want to keep all objects from an existing object hierarchy, you need to use the @JsonMerge annotation. You should put this annotation above any member of the root object you want to update instead of replacing it. The @JsonUpgrade annotation is recursive : the members of the member annotated with @JsonUpgrade are updated as well and so on.","title":"Update existing instance"},{"location":"Programming/Java/Java%20Programming%20Guide/#updating-polymorphic-types","text":"For updating polymorpic type, the rule is that the exact type has to match. Also, you need jackson-databind version 2.14.0 or greater.","title":"Updating polymorphic types"},{"location":"Programming/Java/Java%20Programming%20Guide/#read-just-part-of-the-file","text":"For reading just part of the file, use the at selector taht is available in the ObjectReader class. We need to first obtain the reader from a mapper, and then use the selector: ObjectReader reader = mapper.readerFor(Instance.class); Instance instance = reader.at(\"data\").readValue(file) Note that if the path parameter of the at method is incorrect, the method throws an exception with the message: \"no content to map due to end-of-input\".","title":"Read just part of the file"},{"location":"Programming/Java/Java%20Programming%20Guide/#check-that-some-node-is-present","text":"To check for presence of a node, we should use the JsonPointer class: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); We can also use the JsonPointer in the at method: JsonPointer jsonPointer = Json.createPointer(\"<JSON PATH>\"); boolean found = jsonPointer.containsValue(jsonStructure); if(found){ Instance instance = reader.at(jsonPointer).readValue(file, Instance.class) }","title":"Check that some node is present"},{"location":"Programming/Java/Java%20Programming%20Guide/#interface-or-abstract-class","text":"When serializing interface or abstract class, it is important to include the implementation type into serialization. Otherwise, the deserialization fails, because it cannot determine the concreate type. To serialize the concrete type, we can use the @JsonRypeInfo and JsonSubTypes annotations: @JsonTypeInfo( use = JsonTypeInfo.Id.CLASS, // what value should we store, here the class name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) public interface Interface { ... } Te above code will work, the full class name will be serialized in the file, however. If we want to use a shorter syntax, i.e., some codename for the class, we need to specify a mapping between this codename and the conreate class: @JsonTypeInfo( use = JsonTypeInfo.Id.NAME, // what value should we store, here a custom name include = JsonTypeInfo.As.PROPERTY, // serialize as a property of the JSON object property = \"type\" // property name ) @JsonSubTypes({ @JsonSubTypes.Type(value = Implementation.class, name = \"Implementation name\") }) public interface Interface { ... }","title":"Interface or abstract class"},{"location":"Programming/Java/Java%20Programming%20Guide/#custom-deserializer","text":"An alternative to @JsonTypeInfo is to use a custom deserializer: https://stackoverflow.com/questions/44122782/jackson-deserialize-based-on-type","title":"Custom deserializer"},{"location":"Programming/Java/Java%20Programming%20Guide/#custom-deserializer_1","text":"If our architecture is so complex or specific that none of the Jackson annotations can help us to achieve the desired behavior, we can use a custom deserializer. For that we need to: 1. Implement a custom deserializer by extending the JsonDeserializer class 1. Registering the deserializer in the ObjectMapper","title":"Custom deserializer"},{"location":"Programming/Java/Java%20Programming%20Guide/#creating-a-custom-deserializer-for-class","text":"The only method we need to implement is the: T deserialize(JsonParser jsonParser, DeserializationContext deserializationContext) where T is the type of the object we are deserializing. To get the Jaksons representation of the JSON tree, we can call: JsonNode node = jsonParser.getCodec().readTree(jsonParser); We can get all the fields by calling using the node.fields() method. For arrays, there is a method node.elements() ;","title":"Creating a custom deserializer for class"},{"location":"Programming/Java/Java%20Programming%20Guide/#registering-the-deserializer","text":"ObjectMapper mapper = new ObjectMapper(); SimpleModule module = new SimpleModule(); module.addDeserializer(OurClass.class, new OurClassDeserializer()); mapper.registerModule(module);","title":"Registering the deserializer"},{"location":"Programming/Java/Java%20Programming%20Guide/#custom-deserializer-with-generic-types","text":"When we need a custom deserializer for a generic class, we need to use a wildcard to cover multiple values of generic argument: public class OurDeserializer extends JsonDeserializer<OurGenericClass<?>>{ ... } If we also need to get the generic argument type from JSON, we need to implement the ContextualDeserializer interface. This is discribed in a SO answer .","title":"Custom deserializer with generic types"},{"location":"Programming/Java/Java%20Programming%20Guide/#custom-deserializer-with-inheritance","text":"We can have a common deserializer for both parent and child class, or multiple child classes. However, it is necessary to - make the deserializer generic and - register the deserializer for all classes, not just for the parent. Example: public class PolymorphicDeserializer<T extends Parent> extends JsonDeserializer<T> @Override public T deserialize(JsonParser p, DeserializationContext ctxt) throws IOException, JsonProcessingException { ... } } module.addDeserializer(ChildA.class, new PolymorphicDeserializer<>()); module.addDeserializer(ChildB.class, new ProcedureParameterDeserializer<>());","title":"Custom deserializer with inheritance"},{"location":"Programming/Java/Java%20Programming%20Guide/#serialization","text":"Standard serialization: // compressed mapper.writer().writeValue(file, object); // or with indents, etc. mapper.writerWithDefaultPrettyPrinter().writeValue(file, object); By default, new objects are created for all members in the object hierarchy that are either: - public value mebers (fields) - public getters: public methods with name get<NAME> , where <NAME> is a name of some value member Other getters with different name are called only if there is an annotation above them. A special annotation dedicated for this is @JsonProperty .","title":"Serialization"},{"location":"Programming/Java/Java%20Programming%20Guide/#appending-to-an-existing-file","text":"To append to an existing file, we need to create an output stream in the append mode and then use it in jackson: ObjectMapper mapper = ... JsonGenerator generator = mapper.getFactory().createGenerator(new FileOutputStream(new File(<FILENAME>), true)); mapper.writeValue(generator, output);","title":"Appending to an existing file"},{"location":"Programming/Java/Java%20Programming%20Guide/#complex-member-filters","text":"Insted of adding annotations to each member we want to ignore, we can also apply some more compex filters, to do that, we need to: 1. add a @JsonFilter(\"<FILTER NAME>\") annotation to all classes for which we want to use the filter 2. create the filter 3. pass a set of all filters we want to use to the writer we are using for serialization The example below keeps only members inherited from MyClass : // object on which we apply the filter @JsonFilter(\"myFilter\") class targetClass{ ... } // filter PropertyFilter filter = new SimpleBeanPropertyFilter() { @Override public void serializeAsField( Object pojo, JsonGenerator jgen, SerializerProvider provider, PropertyWriter writer ) throws Exception { if(writer.getType().isTypeOrSubTypeOf(MyClass.class)){ writer.serializeAsField(pojo, jgen, provider); } } }; FilterProvider filters = new SimpleFilterProvider().addFilter(\"myFilter\", filter); // use a writer created with filters mapper.writer(filters).writeValue(generator, output); ...","title":"Complex member filters"},{"location":"Programming/Java/Java%20Programming%20Guide/#flatting-the-hierarchy","text":"When we desire to simplify the object hierarchy, we can use the @JsonUnwrapped annotation above a member of a class. With this annotation, the annotated member object will be skipped while all its members will be serialized into its parent.","title":"Flatting the hierarchy"},{"location":"Programming/Java/Java%20Programming%20Guide/#custom-serializer","text":"If the serialization requirements are too complex to be expressed using Jackson annotations, we can use a custom serialzier: public class MyCustomSerializer extends JsonSerializer<MyClass> { @Override public void serialize(MyClass myClass, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException { ... } } Analogously to custom deserializer, we can register custom serializer either in the object mapper: SimpleModule module = new SimpleModule(); module.addSerializer(MyClass.class, new MyCustomSerializer()); mapper.registerModule(module); or by annotating the class: @JsonSerialize(using = MyCustomSerializer.class) public class MyClass{ ... } You can call standard serializers from custom serializers using the SerializerProvider and JsonGenerator instances supplied as a parameters of the serialize method. For example the standard serialized value of som inner/member object class can be obtained using: serializerProvider.defaultSerializeValue(myInnerInstance, jsonGenerator);","title":"Custom serializer"},{"location":"Programming/Java/Java%20Programming%20Guide/#annotations","text":"","title":"Annotations"},{"location":"Programming/Java/Java%20Programming%20Guide/#multiple-objects","text":"If there are multiple objects involved in the (de)serializetion, we can use the @JsonCreator and @JsonProperty annotations to split the work: @JsonCreator public ConfigModel( @JsonProperty(\"first\") ClassA instanceA, @JsonProperty(\"second\") ClassB instanceB ) { ... } in the above examples, the first and second are keys mapping the objects in the serialized file from which instanceA and instanceB should be created.","title":"Multiple objects"},{"location":"Programming/Java/Java%20Workflow/","text":"Java developement stack We use this stack: - Toolchain: JDK - Package manager: standalone Maven - IDE: Netbeans, Idea JDK Java developem kit is the standard Java toolchain. Most comon tools are: - javac for compilation - java for execution javac The syntax is: javac [options] [sourcefiles] Warning control By default, all warnings are disabled and only a summary of the warnings is displayed in the output (i.e., all warning types encountered, without specific lines where they occur). To enable all warnings use the -Xlint argument. To enable/disable specific warnings, use -Xlint:<warning name> and -Xlint:-<warning name> , respectively. Maven Download maven from the official website and extract the archive somewhere (e.g. C:/ ) Add absolute path to <mavendir>/bin to PATH Netbeans Install the latest version of Apache Netbeans Configuration: Autosaving: Editor -> Autosave Tab size, tabs instead of spaces, 120 char marker line: Editor -> Formatting -> All Languages Multi-row tabs: Appearance -> DocumentTabs Git labels on projects: Team -> Versioning -> Show Versioning Labels Internet Browser: General -> web browser Javadoc config: if the javadoc for java SE does not work out of the box, maybe there is a wrong URL. Go to Tools -> Java Platforms -> Javadoc and enter there the path where the Javadoc is accessible online Install the basic plugins Markdown Project configuration Configure Maven Goals These can be configured in Project properties -> Actions Troubleshooting If there is a serious problem, one way to solve it can be to delete the Netbeans cache located in ~\\AppData\\Local\\NetBeans\\Cache . Idea Configuration Settings synchronization Log in into JetBrains Toolbox or to the App Click on the gear icon on the top-right and choose Sync Check all categories and click on pull settings from the cloud Resatart Ida to update all the settings More on Jetbrains Project configuration The correct JDK has to be set up in various places: - compielr has to be at least target jdk, set it in: File -> Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Project bytecode version and Per-module bytecode version - language level should be the same as target jdk: File -> Project Structure... -> Modules -> Language Level Compilation Everything is compiled in the background automatically. However, if we need to compile manually using maven, e.g., to activate certain pluginns, we need to: Set Java version for project There are two options to set: - source version determines the least version of java that supports all language tools used in the source code. It is the least version that can be used to compile the source code. - target version determines the least version of java that can be used to run your java application. The target version can be higher than the source version, but not he other way around. most of the time, however, we use the same version for source and for target. Usually, these versions needs to be set: 1. in the project compilation tool (maven) to configure the project compilation 2. in the IDE project properties, to configure the compilation of individual files, which is executed in the background to report the compilation errors at real time. Sometimes, the Java version used for running Maven has to be also set because it cannot be lower then the Java version used for project compilation using Maven. SO explanation Setting Java version in Maven Since Java 9, both Java versions can be set at once with the release option: <properties> <maven.compiler.release>10</maven.compiler.release> </properties> or equivalently <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> <release>10</release> </configuration> </plugin> The old way is to use separate properties: <properties> <maven.compiler.target>10</maven.compiler.target> <maven.compiler.source>10</maven.compiler.source> </properties> Note that the Java version configured in the pom file cannot be greater then the Java version used to run the Maven. Setting Java version in Netbeans Note that for maven projects, this is automatically set to match the properties in the pom file. For the Netbeans real-time compiler, the cross compilation does not make sense, so both source and target Java version is set in one place: 1. Right click on project -> Properties -> Sources 2. In the bottom, change the Source/Binary Format Setting Java version used for executing Maven If the Maven is executed from command line, edit the JAVA_HOME system property. If the Maven is executed from Netbeans, edit Project Properties -> Build -> Compile -> Java Platform Enabling preview features The preview features can be enabled using the --enable-preview argument. In Maven, this has to be passed to the compiler plugin: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <enablePreview>true</enablePreview> </configuration> </plugin> </plugins> </build> For Maven compiler plugin older then version 3.10.1, use: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <compilerArgs> <arg>--enable-preview</arg> </compilerArgs> </configuration> </plugin> </plugins> </build> Unfortunatelly, it is also necessary to enable preview features in the IDE configuration: - In Netbeans, add --enable-preview to Project Properties -> Run -> VM options . - In IDEA: 1. add --enable-preview to Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Override compiler parameters per-module 2. add --enable-preview to run configuration -> Build and run vm options Gurobi Gurobi is a commercial project not contained in any public maven repositories. It is necessary to install the gurobi maven artifact manualy. Potential problems unsatisfied linker error : Check if the gurobi version in the error log matches the gurobi version installed. Archive AIC maven repo access To Access the AIC maven repo, copy maven settings from another computer (located in ~/.m2) Written with StackEdit .","title":"Java developement stack"},{"location":"Programming/Java/Java%20Workflow/#java-developement-stack","text":"We use this stack: - Toolchain: JDK - Package manager: standalone Maven - IDE: Netbeans, Idea","title":"Java developement stack"},{"location":"Programming/Java/Java%20Workflow/#jdk","text":"Java developem kit is the standard Java toolchain. Most comon tools are: - javac for compilation - java for execution","title":"JDK"},{"location":"Programming/Java/Java%20Workflow/#javac","text":"The syntax is: javac [options] [sourcefiles]","title":"javac"},{"location":"Programming/Java/Java%20Workflow/#warning-control","text":"By default, all warnings are disabled and only a summary of the warnings is displayed in the output (i.e., all warning types encountered, without specific lines where they occur). To enable all warnings use the -Xlint argument. To enable/disable specific warnings, use -Xlint:<warning name> and -Xlint:-<warning name> , respectively.","title":"Warning control"},{"location":"Programming/Java/Java%20Workflow/#maven","text":"Download maven from the official website and extract the archive somewhere (e.g. C:/ ) Add absolute path to <mavendir>/bin to PATH","title":"Maven"},{"location":"Programming/Java/Java%20Workflow/#netbeans","text":"Install the latest version of Apache Netbeans Configuration: Autosaving: Editor -> Autosave Tab size, tabs instead of spaces, 120 char marker line: Editor -> Formatting -> All Languages Multi-row tabs: Appearance -> DocumentTabs Git labels on projects: Team -> Versioning -> Show Versioning Labels Internet Browser: General -> web browser Javadoc config: if the javadoc for java SE does not work out of the box, maybe there is a wrong URL. Go to Tools -> Java Platforms -> Javadoc and enter there the path where the Javadoc is accessible online Install the basic plugins Markdown","title":"Netbeans"},{"location":"Programming/Java/Java%20Workflow/#project-configuration","text":"","title":"Project configuration"},{"location":"Programming/Java/Java%20Workflow/#configure-maven-goals","text":"These can be configured in Project properties -> Actions","title":"Configure Maven Goals"},{"location":"Programming/Java/Java%20Workflow/#troubleshooting","text":"If there is a serious problem, one way to solve it can be to delete the Netbeans cache located in ~\\AppData\\Local\\NetBeans\\Cache .","title":"Troubleshooting"},{"location":"Programming/Java/Java%20Workflow/#idea","text":"","title":"Idea"},{"location":"Programming/Java/Java%20Workflow/#configuration","text":"","title":"Configuration"},{"location":"Programming/Java/Java%20Workflow/#settings-synchronization","text":"Log in into JetBrains Toolbox or to the App Click on the gear icon on the top-right and choose Sync Check all categories and click on pull settings from the cloud Resatart Ida to update all the settings More on Jetbrains","title":"Settings synchronization"},{"location":"Programming/Java/Java%20Workflow/#project-configuration_1","text":"The correct JDK has to be set up in various places: - compielr has to be at least target jdk, set it in: File -> Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Project bytecode version and Per-module bytecode version - language level should be the same as target jdk: File -> Project Structure... -> Modules -> Language Level","title":"Project configuration"},{"location":"Programming/Java/Java%20Workflow/#compilation","text":"Everything is compiled in the background automatically. However, if we need to compile manually using maven, e.g., to activate certain pluginns, we need to:","title":"Compilation"},{"location":"Programming/Java/Java%20Workflow/#set-java-version-for-project","text":"There are two options to set: - source version determines the least version of java that supports all language tools used in the source code. It is the least version that can be used to compile the source code. - target version determines the least version of java that can be used to run your java application. The target version can be higher than the source version, but not he other way around. most of the time, however, we use the same version for source and for target. Usually, these versions needs to be set: 1. in the project compilation tool (maven) to configure the project compilation 2. in the IDE project properties, to configure the compilation of individual files, which is executed in the background to report the compilation errors at real time. Sometimes, the Java version used for running Maven has to be also set because it cannot be lower then the Java version used for project compilation using Maven. SO explanation","title":"Set Java version for project"},{"location":"Programming/Java/Java%20Workflow/#setting-java-version-in-maven","text":"Since Java 9, both Java versions can be set at once with the release option: <properties> <maven.compiler.release>10</maven.compiler.release> </properties> or equivalently <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> <release>10</release> </configuration> </plugin> The old way is to use separate properties: <properties> <maven.compiler.target>10</maven.compiler.target> <maven.compiler.source>10</maven.compiler.source> </properties> Note that the Java version configured in the pom file cannot be greater then the Java version used to run the Maven.","title":"Setting Java version in Maven"},{"location":"Programming/Java/Java%20Workflow/#setting-java-version-in-netbeans","text":"Note that for maven projects, this is automatically set to match the properties in the pom file. For the Netbeans real-time compiler, the cross compilation does not make sense, so both source and target Java version is set in one place: 1. Right click on project -> Properties -> Sources 2. In the bottom, change the Source/Binary Format","title":"Setting Java version in Netbeans"},{"location":"Programming/Java/Java%20Workflow/#setting-java-version-used-for-executing-maven","text":"If the Maven is executed from command line, edit the JAVA_HOME system property. If the Maven is executed from Netbeans, edit Project Properties -> Build -> Compile -> Java Platform","title":"Setting Java version used for executing Maven"},{"location":"Programming/Java/Java%20Workflow/#enabling-preview-features","text":"The preview features can be enabled using the --enable-preview argument. In Maven, this has to be passed to the compiler plugin: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <enablePreview>true</enablePreview> </configuration> </plugin> </plugins> </build> For Maven compiler plugin older then version 3.10.1, use: <build> <plugins> <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.8.0</version> <configuration> <compilerArgs> <arg>--enable-preview</arg> </compilerArgs> </configuration> </plugin> </plugins> </build> Unfortunatelly, it is also necessary to enable preview features in the IDE configuration: - In Netbeans, add --enable-preview to Project Properties -> Run -> VM options . - In IDEA: 1. add --enable-preview to Settings -> Build, Execution, Deployment -> Compiler -> Java Compiler -> Override compiler parameters per-module 2. add --enable-preview to run configuration -> Build and run vm options","title":"Enabling preview features"},{"location":"Programming/Java/Java%20Workflow/#gurobi","text":"Gurobi is a commercial project not contained in any public maven repositories. It is necessary to install the gurobi maven artifact manualy.","title":"Gurobi"},{"location":"Programming/Java/Java%20Workflow/#potential-problems","text":"unsatisfied linker error : Check if the gurobi version in the error log matches the gurobi version installed.","title":"Potential problems"},{"location":"Programming/Java/Java%20Workflow/#archive","text":"","title":"Archive"},{"location":"Programming/Java/Java%20Workflow/#aic-maven-repo-access","text":"To Access the AIC maven repo, copy maven settings from another computer (located in ~/.m2) Written with StackEdit .","title":"AIC maven repo access"},{"location":"Programming/Java/Maven/","text":"Dependencies All Maven dependencies should work out of the box. If some dependencies cannot be resolved: - check that the dependencies are on the maven central. - if not, check that they are in some special repo and check that the repo is present in the pom of the project that requires the dependency Compilation reference Compilation is handeled by the Maven compiler plugin. Usually, typing mvn compile is enough to compile the project. If you need any customization, add it to the compiler plugin configuration <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> .... </configuration> </plugin> Passing arguments to javac A single argument can be passed using the <compilerArgument> property. For more compiler arguments, use <compilerArgs> : <compilerArgs> <arg>-Xmaxerrs=1000</arg> <arg>-Xlint</arg> </compilerArgs> Tests Tests are usually executed with the Maven Surefire plugin using the test goal. Run a single tests file To run a single test file, use: mvn test -Dtest=\"<TEST CLASS NAME>\" The name should be just a class name without the package and without file extension: mvn test -Dtest=\"OSMFileTest\" We can also use a fully qualified name, if there are more test classes with the same name: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.OSMFileTest\" Run tests using a pattern More test can be run with a pattern, for example: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.**\" runs all tests within the input package. Execute Programs from Maven For executing programs, Maven has the exec target. Mostly, you need to exacute java programs from maven. Basic example: mvn exec:java -Dexec.mainClass=test.Main Other usefull arguments: - -Dexec.args=\"arg1 arg2 arg3\" If you want to pass the arguments like Xmx to jvm, you cannot use the java subgoal of the exec command. That is because with the java subgoal, Maven uses the same jvm it is running in to execute the program. To conigure jvm, we need to start a new instance. That is possible with the exec subgoal: mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"Xmx30g -classpath %classpath test.Main\" Note that here, the -Dexec.args parameter is used both for vm and runtime arguments. We can also use -Dexec.mainClass with exec:exec , but we need to refer it in the -classpath argument. The following three maven commands run the same Java program: mvn exec:java -Dexec.mainClass=test.Main mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"-classpath %classpath test.Main\" mvn exec:exec -Dexec.executable=\"java\" -Dexec.mainClass=test.Main -Dexec.args=\"-classpath %classpath ${exec.mainClass}\" HTTPS certificates Sometimes, it can happen that maven cannot connect to a repository with this error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target This error signals that the server SSL certificate of the maven repo (when using HTTPS) is not present in the local SSL certificate keystore. This can have two reasons, to disintiguish between them, try to access the repo from your browser over https: - If you can access the repo from your browser, it means that the server has a valid SSL certificate, but it is not in zour local keystore (just in the browser keystore). You can solve this problem by adding the certificate to your java SSL keystore (see below). - if you cannot access the server from your browser, it is likely that the server does not have a valid SSL certificate, and you have to solve it on the serer side. Adding a new SSL certificate to your keystore Open the repo URL in your browser Export the certificate: Chrome: click on the padlock icon left to address in address bar, select Certificate -> Details -> Copy to File and save in format \"Der-encoded binary, single certificate\". Firefox: click on HTTPS certificate chain (the lock icon right next to URL address). Click more info -> security -> show certificate -> details -> export.. . Pickup the name and choose file type *.cer Determine the keystore location: 3.1 Using maven --version , find out the location of the java used by Maven 3.2 The keystore is saved in file: <JAVA LOCATION>/lib/security/cacerts Open console as administrator and add the certificate to the keystore using: keytool -import -keystore \"<PATH TO cacerts>\" -file \"PATH TO TH EXPORTED *.cer FILE\" You can check that the operation was sucessful by listing all certificates: keytool -keystore \"<PATH TO cacerts>\" -list Debugging maven First, try to look at the versions of related dependencies and plugins. Old versions of these can cause many problems. No tests found using the Dtest argument of the test goal Check the class name/path/pattern If the name works, but pattern does not, it can be caused by an old version of the surefire plugin that use a different patten syntax. uncompilable source code Try to clean and compile again","title":"Dependencies"},{"location":"Programming/Java/Maven/#dependencies","text":"All Maven dependencies should work out of the box. If some dependencies cannot be resolved: - check that the dependencies are on the maven central. - if not, check that they are in some special repo and check that the repo is present in the pom of the project that requires the dependency","title":"Dependencies"},{"location":"Programming/Java/Maven/#compilation","text":"reference Compilation is handeled by the Maven compiler plugin. Usually, typing mvn compile is enough to compile the project. If you need any customization, add it to the compiler plugin configuration <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-compiler-plugin</artifactId> <version>3.10.1</version> <configuration> .... </configuration> </plugin>","title":"Compilation"},{"location":"Programming/Java/Maven/#passing-arguments-to-javac","text":"A single argument can be passed using the <compilerArgument> property. For more compiler arguments, use <compilerArgs> : <compilerArgs> <arg>-Xmaxerrs=1000</arg> <arg>-Xlint</arg> </compilerArgs>","title":"Passing arguments to javac"},{"location":"Programming/Java/Maven/#tests","text":"Tests are usually executed with the Maven Surefire plugin using the test goal.","title":"Tests"},{"location":"Programming/Java/Maven/#run-a-single-tests-file","text":"To run a single test file, use: mvn test -Dtest=\"<TEST CLASS NAME>\" The name should be just a class name without the package and without file extension: mvn test -Dtest=\"OSMFileTest\" We can also use a fully qualified name, if there are more test classes with the same name: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.OSMFileTest\"","title":"Run a single tests file"},{"location":"Programming/Java/Maven/#run-tests-using-a-pattern","text":"More test can be run with a pattern, for example: mvn test -Dtest=\"cz.cvut.fel.aic.roadgraphtool.procedures.input.**\" runs all tests within the input package.","title":"Run tests using a pattern"},{"location":"Programming/Java/Maven/#execute-programs-from-maven","text":"For executing programs, Maven has the exec target. Mostly, you need to exacute java programs from maven. Basic example: mvn exec:java -Dexec.mainClass=test.Main Other usefull arguments: - -Dexec.args=\"arg1 arg2 arg3\" If you want to pass the arguments like Xmx to jvm, you cannot use the java subgoal of the exec command. That is because with the java subgoal, Maven uses the same jvm it is running in to execute the program. To conigure jvm, we need to start a new instance. That is possible with the exec subgoal: mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"Xmx30g -classpath %classpath test.Main\" Note that here, the -Dexec.args parameter is used both for vm and runtime arguments. We can also use -Dexec.mainClass with exec:exec , but we need to refer it in the -classpath argument. The following three maven commands run the same Java program: mvn exec:java -Dexec.mainClass=test.Main mvn exec:exec -Dexec.executable=\"java\" -Dexec.args=\"-classpath %classpath test.Main\" mvn exec:exec -Dexec.executable=\"java\" -Dexec.mainClass=test.Main -Dexec.args=\"-classpath %classpath ${exec.mainClass}\"","title":"Execute Programs from Maven"},{"location":"Programming/Java/Maven/#https-certificates","text":"Sometimes, it can happen that maven cannot connect to a repository with this error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target This error signals that the server SSL certificate of the maven repo (when using HTTPS) is not present in the local SSL certificate keystore. This can have two reasons, to disintiguish between them, try to access the repo from your browser over https: - If you can access the repo from your browser, it means that the server has a valid SSL certificate, but it is not in zour local keystore (just in the browser keystore). You can solve this problem by adding the certificate to your java SSL keystore (see below). - if you cannot access the server from your browser, it is likely that the server does not have a valid SSL certificate, and you have to solve it on the serer side.","title":"HTTPS certificates"},{"location":"Programming/Java/Maven/#adding-a-new-ssl-certificate-to-your-keystore","text":"Open the repo URL in your browser Export the certificate: Chrome: click on the padlock icon left to address in address bar, select Certificate -> Details -> Copy to File and save in format \"Der-encoded binary, single certificate\". Firefox: click on HTTPS certificate chain (the lock icon right next to URL address). Click more info -> security -> show certificate -> details -> export.. . Pickup the name and choose file type *.cer Determine the keystore location: 3.1 Using maven --version , find out the location of the java used by Maven 3.2 The keystore is saved in file: <JAVA LOCATION>/lib/security/cacerts Open console as administrator and add the certificate to the keystore using: keytool -import -keystore \"<PATH TO cacerts>\" -file \"PATH TO TH EXPORTED *.cer FILE\" You can check that the operation was sucessful by listing all certificates: keytool -keystore \"<PATH TO cacerts>\" -list","title":"Adding a new SSL certificate to your keystore"},{"location":"Programming/Java/Maven/#debugging-maven","text":"First, try to look at the versions of related dependencies and plugins. Old versions of these can cause many problems.","title":"Debugging maven"},{"location":"Programming/Java/Maven/#no-tests-found-using-the-dtest-argument-of-the-test-goal","text":"Check the class name/path/pattern If the name works, but pattern does not, it can be caused by an old version of the surefire plugin that use a different patten syntax.","title":"No tests found using the Dtest argument of the test goal"},{"location":"Programming/Java/Maven/#uncompilable-source-code","text":"Try to clean and compile again","title":"uncompilable  source code"},{"location":"Programming/Python/Matplotlib%20Manual/","text":"Written with StackEdit .","title":"Matplotlib Manual"},{"location":"Programming/Python/Pandas%20Manual/","text":"Creating a DataFrame The DataFrame class has a constructor that supports multiple formats of input data as well as many configuration parameters. Therefore , for most formats of input data, we can create a dataframe using the constructor. However, we can also crete a dataframe using the from_* functions, and for some formats, these functions are the only way to create a dataframe. From a dictionary When having a dictionary, we can choose between two options the constructor and the from_dict function. The required syntax depend on the shape of the dictionary with respect to the required dataframe. Keys are column names, values are list of column values df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) # or equivalently df.DataFrame.from_dict({'col1': [1, 2], 'col2': [3, 4]}) Note that the values of the dictionary have to be lists. If we have a dictionary with values that are not lists (i.e., only one row), we have to use the orient parameter to specify the orientation of the data and then transpose the dataframe: d = {'col1': 1, 'col2': 2} df = pd.DataFrame.from_dict(d, orient='index').T # or equivalently df = pd.DataFrame([d], columns=d.keys()) Keys are indices, values are values of a single column df = pd.DataFrame.from_dict({'row1': 1, 'row2': 2}, orient='index', columns=['Values']) Keys are indices, values are values of single row df = pd.DataFrame.from_dict({'row1': [1, 2], 'row2': [3, 4]}, orient='index') Keys are one column, values are another column d = {'row1 col1': 'row1 col2', 'row2 col1': 'row2 col2' df = pd.DataFrame.from_dict(d.items()) # or equivalently df = pd.DataFrame({'col1': d.keys(), 'col2': d.values()}) From a list of dictionaries df = pd.DataFrame([{'col1': 1, 'col2': 3}, {'col1': 2, 'col2': 4}]) From a list of lists df = pd.DataFrame([[1, 3], [2, 4]], columns=['col1', 'col2']) From Obtaining info about dataset For a DataFrame df : - column names: df.columns - column types: df.dtypes - number of rows: len(df) Iteration Standard Iteration https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas Iteration without modifying the dataframe. From the fastest to the slowest. Vector operations List Comprehensions Apply itertuples() https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.htm Returns dataframe rows as pandas named tuples with index as the first member of the tuple. iterrows() https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html returns a tuple (index, data) it does not preserve the dtype Written with StackEdit . items() https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.items.html Iterates over columns Iteration with modification When doing some modifications, we need to copy the dataframe and do the modifications on the copy. Filtration filtered = df[df['max_delay'] == x] or equivalently: filtered = df[df.max_delay == x] Filtration by Multiple Columns Example: filtered = df[(df['max_delay'] == x) & (df['exp_length'] == y)] Using the query function The query function can be used for more complicated filters. It is more flexible and the syntax is less verbose. The above filter can be rewriten as: filtered = df.query('max_delay == x and exp_length == y'] Filtering Series A seris can be filtered even simpler then the dataframe: s = df['col'] sf = s[s <= 10] # now we have a Series with values from df['col'] less than 10 Useful filter functions non null/nan values: <column selection>.notnull() diltring using the string value: <column selection>.str.<string function> Selection If we want to select a part of the dataframe (a set of rows and columns) independently of the values of the dataframe (for that, see filtration ), we can use these methods: - loc : select by label, works for both rows and columns - iloc : select by position, works for both rows and columns - [] : select by label, works only for columns There are also other methods that works for selection but does not work for setting values, such as: - xs : select by label, works for both rows and columns loc The operator loc has has many possible input parameters, the most common syntax is df.loc[<row selection>, <column selection>] each selection has the form of <start label>:<end label> . For the whole column, we therefore use: df.loc[:, <column name>] Difference between array operator on dataframe and on loc Both methods can be used both for getting and setting the column: a = df['col'] # or equivalently a = df.loc[:, 'col'] df2['col'] = a # or equivalently df2.loc[:, 'col'] = a The difference between these two methods is apparent when we want to use a chained selection, i.e., selecting from a selection. While the loc selects the appropriate columns in one step, so we know that we still refer to the original dataframe, the array operator operations are separate, and therefore, the result value can refer to a temporary: dfmi.loc[:, ('one', 'second')] = value # we set a value of a part of dfmi dfmi['one']['second'] = value # can be dangerous, we can set value to a temporary This problem is indicated by a SettingWithCopy warning. Sometimes it is not obvious that we use a chain of array operator selections, e.g.: sel = df[['a', 'b']] ..... sel['a'] = ... # we possibly edit a temporary! For more, see the dovumentation . iloc The iloc method works similarly to the loc method, but it uses the position instead of the label. Be aware that if the iloc operator selects by single value (e.g.: df.iloc[3] ), it returns the single row as series . To get a dataframe slice, we need to use a list of values (e.g.: df.iloc[[3]] ). Selecting all columns but one If we do not mind copying the dataframe, we can use the drop function. Otherwise, we can use the loc method and supply the filtered column lables obtained using the columns property: df.loc[:, df.columns != '<column to skip>'] Multi-index selection documentation When selecting from a dataframe with a multi-index, things get a bit more complicated. We can specify index levels using the level argument. Example: df.loc[<row selection>, <column selection>, level=<level number>] If we want to specify more than one level, we can use a tuple: df.loc[(<row index level 1>, <row index level 2>, ...), (<col index level 1>, <col index level 2>, ...)] If we select an upper level only, all lover level values are selected. For more complex cases where we wanto to select all from upper level but limit the lower level, we can use the slice function: df.loc[(slice(None), slice('15', '30')), ...] We can obtain the same result with a more readable syntax using the IndexSlice object: idx = pd.IndexSlice dft.loc[idx[:, '15':'30'], ...] Also, note that for multi-index slicing, the index needs to be sorted. If it is not, we can use the sort_index function. Sorting for sorting the dataframe, we can use the sort_values function. The first argument is the list of columns to sort by, starting with the most important column. Example: df.sort_values(['col1', 'col2']) If we want to use a custom sorting function, we can use the key argument. The key function should satisfy the classical python sorting interface (see Python manual) and additionaly, it should be a vector function, i.e., instead of returning a single position for a given value, it should return a vector of positions for a given vector of values. Example key function: def key_fn(l: list): return [len(x) for x in l] Working with columns Adding a column The preferable way is to use the assign function: # adds a column named 'instance_length' with constant value result_df_5_nyc_mv.assign(instance_length = 5) Multiple columns can be added at once: trips = trips.assign(dropoff_datetime = 0, dropoff_GPS_lon = 0, dropoff_GPS_lat = 0, pickup_GPS_lon = 0, pickup_GPS_lat = 0) Rename a column To rename a column, we can use the pandas rename function: df.rename(columns={'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}) # or equivalently df.rename({'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}, axis='columns') Working with the index Index of a dataframe df can be accessed by df.index . Standard range operation can be applied to index. Changing the index For that, we can use the set_index function. Renaming the index The Index.rename function can be used for that. Aggregation Analogously to SQL, pandas has a groupby function for aggreagting rows. Depending on the aggregate function we choose, we get a different statistic, for example: df.groupby('col').sum() Sums the results for each group (column by column) To get a count, we need to call the size function: df.groupby('col').size() Joins Appending one dataframe to another We can use the concat function for that: pd.concat([df1, df2]) Exporting to CSV We can use the to_csv method for that: df.to_csv(<file name> [, <other params>]) Insert dataframe into db We can use the to_sql method for that: df.to_sql(<table name>, <sql alchemy engine> [, <other params>]) Important params: - to append, not replace existing records: if_exists='append' - do not import dataframe index: index=False For larger datasets, it is important to not insert everything at once, while also tracking the progress. The following code does exactly that def chunker(seq, size): return (seq[pos:pos + size] for pos in range(0, len(seq), size)) chunksize = int(len(data) / 1000) # 0.1% with tqdm(total=len(data)) as pbar: for i, cdf in enumerate(chunker(data, chunksize)): cdf.to_sql(<table name>, <sqlalchemy_engine>) pbar.update(chunksize) If the speed is slow, it can be caused by a low upload speed of your internet connection. Note that due to the SQL syntax, the size of the SQL strings may be much larger than the size of the dataframe. Latex export Currently, the to_latex function is deprecated. The Styler class should be used for latex exports instead. You can get the Styler from the DataFrame using the style property. The usual workfolow is: 1. Create a Styler object from the dataframe using the style property. 1. Apply the desired formatting to the styler object. 1. Export DataFrame to latex using the to_latex method. Keep in mind that the Styler object is immutable, so you need to assign the result of each formatting operation to a new variable or chain the calls . Example: # wrong, the format is not apllied df.style.format(...) df.style.to_latex(...) # correct: temp var s = df.style.format(...) s.to_latex(...) # correct: chain calls df.style.format(...).to_latex(...) Formatting the index: columns and row labels The columns' and row labels' format is configures by the format_index function. Important parameters: - axis : 0 for rows, 1 for columns (cannot be both) - escape : by default, the index is not escaped, to do so, we need to set escape to 'latex' . Formatting the values The values are formated by the format function. Important parameters: - escape : by default, the values are not escaped, to do so, we need to set escape to 'latex' . - na_rep : the string to use for missing values - precision : the number of decimal places to use for floats Hihglighting min/max values For highlighting the min/max values, we can use the highlight_min and highlight_max functions. Important parameters: - subset : the columns in which the highlighting should be applied - props : the css properties to apply to the highlighted cells Hiding some columns, rows, or indices For hiding some columns, rows, or indices, we can use the hide function. Example: df.style.hide(level=<index name>) # hide the index with the given name Exporting to latex For the export, we use the to_latex function. Important parameters: Displaying the dataframe in console We can display the dataframe in the conslo print or int the log just by supplying the dataframe as an argument because it implements the __repr__ method. Sometimes, however, the default display parameters are not sufficient. In that case, we can use the set_option function to change the display parameters: pd.set_option('display.max_rows', 1000) Important parameters: - display.max_rows : the maximum number of rows to display - display.max_columns : the maximum number of columns to display - display.max_colwidth : the maximum width of a column Other useful functions drop_duplicates to quickly drop duplicate rows based on a subset of columns. factorize to encode a series values as a categorical variable, i.e., assigns a different number to each unique value in series. pivot_table : function that can aggragate and transform a dataframe in one step. with this function, one can create a pivot table, but also a lot more. pivot_table The pivot table (mega)function do a lot of things at once: - it aggregates the data - it transforms the data - it sorts the data due to reindexing Although this function is very powerfall there are also many pitfalls. The most important ones are: - column data type change for columns with missing values Column data type change for columns with missing values The tranformation often creates row-column combinations that do not exist in the original data. These are filled with NaN values. But some data types does not support NaN values, and in conclusion, the data type of the columns with missing values is changed to float . Possible solutions: - we can use the fill_value parameter to fill the missing values with some value that is supported by the data type (e.g. -1 for integers) - we can use the dropna parameter to drop the rows with missing values - we can change the data type of the columns with missing values prior to calling the pivot_table function. For example, the pandas integer data types support NaN values. Geopandas Geopandas is a GIS addon to pandas, an equivalent to PostGIS. Unfortunately, it currently supports only one geometry column per table. Do not ever copy paste the geometries from jupyter notebook as the coordinates are rounded! Use the to_wkt function instead. Create a geodataframe from CSV Geopandas has it's own read_csv function, however, it requires a very specific csv format, so it is usually easier to first import csv to pandas and then create geopandas dataframe from pandas dataframe. Converting pandas Dataframe to geopandas Dataframe The geopandas dataframe constructor accepts pandas dataframes, we just need to specify the geometry column and the coordinate system: gdf = gpd.GeoDataFrame( <PANDAS DATAFRAME> geometry=gpd.points_from_xy(<X COLUMN>, <Y COLUMN>), crs=<SRID> ) Create geodataframe from shapely To load data from shapely, execute gdf = gpd.read_file(<PATH TO FOLDER WITH SHAPEFILES>) Working with the geometry The geometry can be accessed using the geometry property of the geodataframe. Spliting multi-geometry columns If the geometry column contains multi-geometries, we can split them into separate rows using the explode function: gdf = gdf.explode() Insert geodataframe into db preprocesssing Before inserting a geodataframe into the database, we need to process it a little bit: 1. set the SRID: gdf.set_crs(epsg=<SRID>, allow_override=True, inplace=True) 2. set the geometry: gdf.set_geometry('geom', inplace=True) 3. select, rename, or add columns so that the resulting geodataframe match the corresponding database table. This process is same as when working with pandas Simple insertion When the data are in the correct format and we don|t need any customization for the db query, we can use the to_postgis method: gdf.to_postgis(<TABLE NAME>, <SQL ALCHEMY CONNECTION>, if_exists='append') Customized Insertion: geoalchemy If we need some special insert statement, we cannot rely on the geodataframe.to_postgis function, as it is not flexible enough. The pandas dataframe.to_sql function is more flexible, however, it has trouble when working with geodata. The easiest options is therefore to use geoalchemy , the database wraper used in geopandas (extension of sqlalchemy , which is a database wrapper for pandas ). First, we need to create the insert statement. The example here uses a modification for handeling duplicite elements. meta = sqlalchemy.MetaData() # create a collection for geoalchemy database # objects table = geoalchemy2.Table( '<TABLE NAME>', meta, autoload_with=<SQL ALCHEMY CONNECTION>) insert_statement = sqlalchemy.dialects.postgresql.insert(table).on_conflict_do_nothing() In the above example, we create a geoalchemy representation of a table and then we use this representation to create a customized insert statement (the on_conflict_do_nothing is the speciality here.). Note that we use a speciatl PostgreSQL insert statement instead of the standard SQLAlchemy insert statement. Second, we need to prepare the data as a list of dictionary entries: list_to_insert = [ {'id': 0, 'geom': <GEOM>, ...}, {'id': 0, 'geom': <GEOM>, ...}, .... ] Note that the geometry in the geodataframe is in the shapely format. Therefore, we need to convert it to string using the geoalchemy from_shape function: geoalchemy2.shape.from_shape(<GEOMETRY>, srid=<SRID>) Finally, we can execute the query using an sqlalchemy connection: sqlalchemy_connection.execute(insert_statement, list_to_insert)","title":"Creating a DataFrame"},{"location":"Programming/Python/Pandas%20Manual/#creating-a-dataframe","text":"The DataFrame class has a constructor that supports multiple formats of input data as well as many configuration parameters. Therefore , for most formats of input data, we can create a dataframe using the constructor. However, we can also crete a dataframe using the from_* functions, and for some formats, these functions are the only way to create a dataframe.","title":"Creating a DataFrame"},{"location":"Programming/Python/Pandas%20Manual/#from-a-dictionary","text":"When having a dictionary, we can choose between two options the constructor and the from_dict function. The required syntax depend on the shape of the dictionary with respect to the required dataframe.","title":"From a dictionary"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-column-names-values-are-list-of-column-values","text":"df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) # or equivalently df.DataFrame.from_dict({'col1': [1, 2], 'col2': [3, 4]}) Note that the values of the dictionary have to be lists. If we have a dictionary with values that are not lists (i.e., only one row), we have to use the orient parameter to specify the orientation of the data and then transpose the dataframe: d = {'col1': 1, 'col2': 2} df = pd.DataFrame.from_dict(d, orient='index').T # or equivalently df = pd.DataFrame([d], columns=d.keys())","title":"Keys are column names, values are list of column values"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-indices-values-are-values-of-a-single-column","text":"df = pd.DataFrame.from_dict({'row1': 1, 'row2': 2}, orient='index', columns=['Values'])","title":"Keys are indices, values are values of a single column"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-indices-values-are-values-of-single-row","text":"df = pd.DataFrame.from_dict({'row1': [1, 2], 'row2': [3, 4]}, orient='index')","title":"Keys are indices, values are values of single row"},{"location":"Programming/Python/Pandas%20Manual/#keys-are-one-column-values-are-another-column","text":"d = {'row1 col1': 'row1 col2', 'row2 col1': 'row2 col2' df = pd.DataFrame.from_dict(d.items()) # or equivalently df = pd.DataFrame({'col1': d.keys(), 'col2': d.values()})","title":"Keys are one column, values are another column"},{"location":"Programming/Python/Pandas%20Manual/#from-a-list-of-dictionaries","text":"df = pd.DataFrame([{'col1': 1, 'col2': 3}, {'col1': 2, 'col2': 4}])","title":"From a list of dictionaries"},{"location":"Programming/Python/Pandas%20Manual/#from-a-list-of-lists","text":"df = pd.DataFrame([[1, 3], [2, 4]], columns=['col1', 'col2'])","title":"From a list of lists"},{"location":"Programming/Python/Pandas%20Manual/#from","text":"","title":"From"},{"location":"Programming/Python/Pandas%20Manual/#obtaining-info-about-dataset","text":"For a DataFrame df : - column names: df.columns - column types: df.dtypes - number of rows: len(df)","title":"Obtaining info about dataset"},{"location":"Programming/Python/Pandas%20Manual/#iteration","text":"","title":"Iteration"},{"location":"Programming/Python/Pandas%20Manual/#standard-iteration","text":"https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas Iteration without modifying the dataframe. From the fastest to the slowest.","title":"Standard Iteration"},{"location":"Programming/Python/Pandas%20Manual/#vector-operations","text":"","title":"Vector operations"},{"location":"Programming/Python/Pandas%20Manual/#list-comprehensions","text":"","title":"List Comprehensions"},{"location":"Programming/Python/Pandas%20Manual/#apply","text":"","title":"Apply"},{"location":"Programming/Python/Pandas%20Manual/#itertuples","text":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.htm Returns dataframe rows as pandas named tuples with index as the first member of the tuple.","title":"itertuples()"},{"location":"Programming/Python/Pandas%20Manual/#iterrows","text":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html returns a tuple (index, data) it does not preserve the dtype Written with StackEdit .","title":"iterrows()"},{"location":"Programming/Python/Pandas%20Manual/#items","text":"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.items.html Iterates over columns","title":"items()"},{"location":"Programming/Python/Pandas%20Manual/#iteration-with-modification","text":"When doing some modifications, we need to copy the dataframe and do the modifications on the copy.","title":"Iteration with modification"},{"location":"Programming/Python/Pandas%20Manual/#filtration","text":"filtered = df[df['max_delay'] == x] or equivalently: filtered = df[df.max_delay == x]","title":"Filtration"},{"location":"Programming/Python/Pandas%20Manual/#filtration-by-multiple-columns","text":"Example: filtered = df[(df['max_delay'] == x) & (df['exp_length'] == y)]","title":"Filtration by Multiple Columns"},{"location":"Programming/Python/Pandas%20Manual/#using-the-query-function","text":"The query function can be used for more complicated filters. It is more flexible and the syntax is less verbose. The above filter can be rewriten as: filtered = df.query('max_delay == x and exp_length == y']","title":"Using the query function"},{"location":"Programming/Python/Pandas%20Manual/#filtering-series","text":"A seris can be filtered even simpler then the dataframe: s = df['col'] sf = s[s <= 10] # now we have a Series with values from df['col'] less than 10","title":"Filtering Series"},{"location":"Programming/Python/Pandas%20Manual/#useful-filter-functions","text":"non null/nan values: <column selection>.notnull() diltring using the string value: <column selection>.str.<string function>","title":"Useful filter functions"},{"location":"Programming/Python/Pandas%20Manual/#selection","text":"If we want to select a part of the dataframe (a set of rows and columns) independently of the values of the dataframe (for that, see filtration ), we can use these methods: - loc : select by label, works for both rows and columns - iloc : select by position, works for both rows and columns - [] : select by label, works only for columns There are also other methods that works for selection but does not work for setting values, such as: - xs : select by label, works for both rows and columns","title":"Selection"},{"location":"Programming/Python/Pandas%20Manual/#loc","text":"The operator loc has has many possible input parameters, the most common syntax is df.loc[<row selection>, <column selection>] each selection has the form of <start label>:<end label> . For the whole column, we therefore use: df.loc[:, <column name>]","title":"loc"},{"location":"Programming/Python/Pandas%20Manual/#difference-between-array-operator-on-dataframe-and-on-loc","text":"Both methods can be used both for getting and setting the column: a = df['col'] # or equivalently a = df.loc[:, 'col'] df2['col'] = a # or equivalently df2.loc[:, 'col'] = a The difference between these two methods is apparent when we want to use a chained selection, i.e., selecting from a selection. While the loc selects the appropriate columns in one step, so we know that we still refer to the original dataframe, the array operator operations are separate, and therefore, the result value can refer to a temporary: dfmi.loc[:, ('one', 'second')] = value # we set a value of a part of dfmi dfmi['one']['second'] = value # can be dangerous, we can set value to a temporary This problem is indicated by a SettingWithCopy warning. Sometimes it is not obvious that we use a chain of array operator selections, e.g.: sel = df[['a', 'b']] ..... sel['a'] = ... # we possibly edit a temporary! For more, see the dovumentation .","title":"Difference between array operator on dataframe and on loc"},{"location":"Programming/Python/Pandas%20Manual/#iloc","text":"The iloc method works similarly to the loc method, but it uses the position instead of the label. Be aware that if the iloc operator selects by single value (e.g.: df.iloc[3] ), it returns the single row as series . To get a dataframe slice, we need to use a list of values (e.g.: df.iloc[[3]] ).","title":"iloc"},{"location":"Programming/Python/Pandas%20Manual/#selecting-all-columns-but-one","text":"If we do not mind copying the dataframe, we can use the drop function. Otherwise, we can use the loc method and supply the filtered column lables obtained using the columns property: df.loc[:, df.columns != '<column to skip>']","title":"Selecting all columns but one"},{"location":"Programming/Python/Pandas%20Manual/#multi-index-selection","text":"documentation When selecting from a dataframe with a multi-index, things get a bit more complicated. We can specify index levels using the level argument. Example: df.loc[<row selection>, <column selection>, level=<level number>] If we want to specify more than one level, we can use a tuple: df.loc[(<row index level 1>, <row index level 2>, ...), (<col index level 1>, <col index level 2>, ...)] If we select an upper level only, all lover level values are selected. For more complex cases where we wanto to select all from upper level but limit the lower level, we can use the slice function: df.loc[(slice(None), slice('15', '30')), ...] We can obtain the same result with a more readable syntax using the IndexSlice object: idx = pd.IndexSlice dft.loc[idx[:, '15':'30'], ...] Also, note that for multi-index slicing, the index needs to be sorted. If it is not, we can use the sort_index function.","title":"Multi-index selection"},{"location":"Programming/Python/Pandas%20Manual/#sorting","text":"for sorting the dataframe, we can use the sort_values function. The first argument is the list of columns to sort by, starting with the most important column. Example: df.sort_values(['col1', 'col2']) If we want to use a custom sorting function, we can use the key argument. The key function should satisfy the classical python sorting interface (see Python manual) and additionaly, it should be a vector function, i.e., instead of returning a single position for a given value, it should return a vector of positions for a given vector of values. Example key function: def key_fn(l: list): return [len(x) for x in l]","title":"Sorting"},{"location":"Programming/Python/Pandas%20Manual/#working-with-columns","text":"","title":"Working with columns"},{"location":"Programming/Python/Pandas%20Manual/#adding-a-column","text":"The preferable way is to use the assign function: # adds a column named 'instance_length' with constant value result_df_5_nyc_mv.assign(instance_length = 5) Multiple columns can be added at once: trips = trips.assign(dropoff_datetime = 0, dropoff_GPS_lon = 0, dropoff_GPS_lat = 0, pickup_GPS_lon = 0, pickup_GPS_lat = 0)","title":"Adding a column"},{"location":"Programming/Python/Pandas%20Manual/#rename-a-column","text":"To rename a column, we can use the pandas rename function: df.rename(columns={'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}) # or equivalently df.rename({'<old name 1>': '<new name 1>', '<old name 2>': '<new name 2>'}, axis='columns')","title":"Rename a column"},{"location":"Programming/Python/Pandas%20Manual/#working-with-the-index","text":"Index of a dataframe df can be accessed by df.index . Standard range operation can be applied to index.","title":"Working with the index"},{"location":"Programming/Python/Pandas%20Manual/#changing-the-index","text":"For that, we can use the set_index function.","title":"Changing the index"},{"location":"Programming/Python/Pandas%20Manual/#renaming-the-index","text":"The Index.rename function can be used for that.","title":"Renaming the index"},{"location":"Programming/Python/Pandas%20Manual/#aggregation","text":"Analogously to SQL, pandas has a groupby function for aggreagting rows. Depending on the aggregate function we choose, we get a different statistic, for example: df.groupby('col').sum() Sums the results for each group (column by column) To get a count, we need to call the size function: df.groupby('col').size()","title":"Aggregation"},{"location":"Programming/Python/Pandas%20Manual/#joins","text":"","title":"Joins"},{"location":"Programming/Python/Pandas%20Manual/#appending-one-dataframe-to-another","text":"We can use the concat function for that: pd.concat([df1, df2])","title":"Appending one dataframe to another"},{"location":"Programming/Python/Pandas%20Manual/#exporting-to-csv","text":"We can use the to_csv method for that: df.to_csv(<file name> [, <other params>])","title":"Exporting to CSV"},{"location":"Programming/Python/Pandas%20Manual/#insert-dataframe-into-db","text":"We can use the to_sql method for that: df.to_sql(<table name>, <sql alchemy engine> [, <other params>]) Important params: - to append, not replace existing records: if_exists='append' - do not import dataframe index: index=False For larger datasets, it is important to not insert everything at once, while also tracking the progress. The following code does exactly that def chunker(seq, size): return (seq[pos:pos + size] for pos in range(0, len(seq), size)) chunksize = int(len(data) / 1000) # 0.1% with tqdm(total=len(data)) as pbar: for i, cdf in enumerate(chunker(data, chunksize)): cdf.to_sql(<table name>, <sqlalchemy_engine>) pbar.update(chunksize) If the speed is slow, it can be caused by a low upload speed of your internet connection. Note that due to the SQL syntax, the size of the SQL strings may be much larger than the size of the dataframe.","title":"Insert dataframe into db"},{"location":"Programming/Python/Pandas%20Manual/#latex-export","text":"Currently, the to_latex function is deprecated. The Styler class should be used for latex exports instead. You can get the Styler from the DataFrame using the style property. The usual workfolow is: 1. Create a Styler object from the dataframe using the style property. 1. Apply the desired formatting to the styler object. 1. Export DataFrame to latex using the to_latex method. Keep in mind that the Styler object is immutable, so you need to assign the result of each formatting operation to a new variable or chain the calls . Example: # wrong, the format is not apllied df.style.format(...) df.style.to_latex(...) # correct: temp var s = df.style.format(...) s.to_latex(...) # correct: chain calls df.style.format(...).to_latex(...)","title":"Latex export"},{"location":"Programming/Python/Pandas%20Manual/#formatting-the-index-columns-and-row-labels","text":"The columns' and row labels' format is configures by the format_index function. Important parameters: - axis : 0 for rows, 1 for columns (cannot be both) - escape : by default, the index is not escaped, to do so, we need to set escape to 'latex' .","title":"Formatting the index: columns and row labels"},{"location":"Programming/Python/Pandas%20Manual/#formatting-the-values","text":"The values are formated by the format function. Important parameters: - escape : by default, the values are not escaped, to do so, we need to set escape to 'latex' . - na_rep : the string to use for missing values - precision : the number of decimal places to use for floats","title":"Formatting the values"},{"location":"Programming/Python/Pandas%20Manual/#hihglighting-minmax-values","text":"For highlighting the min/max values, we can use the highlight_min and highlight_max functions. Important parameters: - subset : the columns in which the highlighting should be applied - props : the css properties to apply to the highlighted cells","title":"Hihglighting min/max values"},{"location":"Programming/Python/Pandas%20Manual/#hiding-some-columns-rows-or-indices","text":"For hiding some columns, rows, or indices, we can use the hide function. Example: df.style.hide(level=<index name>) # hide the index with the given name","title":"Hiding some columns, rows, or indices"},{"location":"Programming/Python/Pandas%20Manual/#exporting-to-latex","text":"For the export, we use the to_latex function. Important parameters:","title":"Exporting to latex"},{"location":"Programming/Python/Pandas%20Manual/#displaying-the-dataframe-in-console","text":"We can display the dataframe in the conslo print or int the log just by supplying the dataframe as an argument because it implements the __repr__ method. Sometimes, however, the default display parameters are not sufficient. In that case, we can use the set_option function to change the display parameters: pd.set_option('display.max_rows', 1000) Important parameters: - display.max_rows : the maximum number of rows to display - display.max_columns : the maximum number of columns to display - display.max_colwidth : the maximum width of a column","title":"Displaying the dataframe in console"},{"location":"Programming/Python/Pandas%20Manual/#other-useful-functions","text":"drop_duplicates to quickly drop duplicate rows based on a subset of columns. factorize to encode a series values as a categorical variable, i.e., assigns a different number to each unique value in series. pivot_table : function that can aggragate and transform a dataframe in one step. with this function, one can create a pivot table, but also a lot more.","title":"Other useful functions"},{"location":"Programming/Python/Pandas%20Manual/#pivot_table","text":"The pivot table (mega)function do a lot of things at once: - it aggregates the data - it transforms the data - it sorts the data due to reindexing Although this function is very powerfall there are also many pitfalls. The most important ones are: - column data type change for columns with missing values","title":"pivot_table"},{"location":"Programming/Python/Pandas%20Manual/#column-data-type-change-for-columns-with-missing-values","text":"The tranformation often creates row-column combinations that do not exist in the original data. These are filled with NaN values. But some data types does not support NaN values, and in conclusion, the data type of the columns with missing values is changed to float . Possible solutions: - we can use the fill_value parameter to fill the missing values with some value that is supported by the data type (e.g. -1 for integers) - we can use the dropna parameter to drop the rows with missing values - we can change the data type of the columns with missing values prior to calling the pivot_table function. For example, the pandas integer data types support NaN values.","title":"Column data type change for columns with missing values"},{"location":"Programming/Python/Pandas%20Manual/#geopandas","text":"Geopandas is a GIS addon to pandas, an equivalent to PostGIS. Unfortunately, it currently supports only one geometry column per table. Do not ever copy paste the geometries from jupyter notebook as the coordinates are rounded! Use the to_wkt function instead.","title":"Geopandas"},{"location":"Programming/Python/Pandas%20Manual/#create-a-geodataframe-from-csv","text":"Geopandas has it's own read_csv function, however, it requires a very specific csv format, so it is usually easier to first import csv to pandas and then create geopandas dataframe from pandas dataframe.","title":"Create a geodataframe from CSV"},{"location":"Programming/Python/Pandas%20Manual/#converting-pandas-dataframe-to-geopandas-dataframe","text":"The geopandas dataframe constructor accepts pandas dataframes, we just need to specify the geometry column and the coordinate system: gdf = gpd.GeoDataFrame( <PANDAS DATAFRAME> geometry=gpd.points_from_xy(<X COLUMN>, <Y COLUMN>), crs=<SRID> )","title":"Converting pandas Dataframe to geopandas Dataframe"},{"location":"Programming/Python/Pandas%20Manual/#create-geodataframe-from-shapely","text":"To load data from shapely, execute gdf = gpd.read_file(<PATH TO FOLDER WITH SHAPEFILES>)","title":"Create geodataframe from shapely"},{"location":"Programming/Python/Pandas%20Manual/#working-with-the-geometry","text":"The geometry can be accessed using the geometry property of the geodataframe.","title":"Working with the geometry"},{"location":"Programming/Python/Pandas%20Manual/#spliting-multi-geometry-columns","text":"If the geometry column contains multi-geometries, we can split them into separate rows using the explode function: gdf = gdf.explode()","title":"Spliting multi-geometry columns"},{"location":"Programming/Python/Pandas%20Manual/#insert-geodataframe-into-db","text":"","title":"Insert geodataframe into db"},{"location":"Programming/Python/Pandas%20Manual/#preprocesssing","text":"Before inserting a geodataframe into the database, we need to process it a little bit: 1. set the SRID: gdf.set_crs(epsg=<SRID>, allow_override=True, inplace=True) 2. set the geometry: gdf.set_geometry('geom', inplace=True) 3. select, rename, or add columns so that the resulting geodataframe match the corresponding database table. This process is same as when working with pandas","title":"preprocesssing"},{"location":"Programming/Python/Pandas%20Manual/#simple-insertion","text":"When the data are in the correct format and we don|t need any customization for the db query, we can use the to_postgis method: gdf.to_postgis(<TABLE NAME>, <SQL ALCHEMY CONNECTION>, if_exists='append')","title":"Simple insertion"},{"location":"Programming/Python/Pandas%20Manual/#customized-insertion-geoalchemy","text":"If we need some special insert statement, we cannot rely on the geodataframe.to_postgis function, as it is not flexible enough. The pandas dataframe.to_sql function is more flexible, however, it has trouble when working with geodata. The easiest options is therefore to use geoalchemy , the database wraper used in geopandas (extension of sqlalchemy , which is a database wrapper for pandas ). First, we need to create the insert statement. The example here uses a modification for handeling duplicite elements. meta = sqlalchemy.MetaData() # create a collection for geoalchemy database # objects table = geoalchemy2.Table( '<TABLE NAME>', meta, autoload_with=<SQL ALCHEMY CONNECTION>) insert_statement = sqlalchemy.dialects.postgresql.insert(table).on_conflict_do_nothing() In the above example, we create a geoalchemy representation of a table and then we use this representation to create a customized insert statement (the on_conflict_do_nothing is the speciality here.). Note that we use a speciatl PostgreSQL insert statement instead of the standard SQLAlchemy insert statement. Second, we need to prepare the data as a list of dictionary entries: list_to_insert = [ {'id': 0, 'geom': <GEOM>, ...}, {'id': 0, 'geom': <GEOM>, ...}, .... ] Note that the geometry in the geodataframe is in the shapely format. Therefore, we need to convert it to string using the geoalchemy from_shape function: geoalchemy2.shape.from_shape(<GEOMETRY>, srid=<SRID>) Finally, we can execute the query using an sqlalchemy connection: sqlalchemy_connection.execute(insert_statement, list_to_insert)","title":"Customized Insertion: geoalchemy"},{"location":"Programming/Python/Plotly%20Manual/","text":"In plotly, we have two options: - plot quickly with plotly express - full control using graph objects Note that thes options can hardly be mixed. For example, we cannot use plotly express to create a figure and then add a subplot to it. Similarly, we cannot use the make_subplots function to create a figure and then add a plotly express plot to it. In general, it is easier to use plotly express, so we should use it if we are not affected by its limitations. The plotly express cannot - create custom subplots . Howwever, automatic \"facet\" subplots (same plot divided between multiple plots using some data attribute) are possible. Plotly Express Histogram documentation Plotly express has histogram function for creating histograms. The basic syntax is: px.histogram(<dataframe>, <xcol name>) The y is then the number of occurences of each value in the x column. Important parameters: - nbins : number of bins. Bar Chart documentation For bar charts, we use the px.bar function. The basic syntax is: px.bar(<dataframe>, <xcol name>, <y col name>) Important parameters: - barmode : how to combine the bars in case of multiple traces. Can be group (default), stack (default in facet plots ), relative or overlay . Automatic Subplots: Facet plots Facet plots can be created using the same plot function as for normal plotly express plots and supplying the facet_col and/or facet_row parameters. Example: fig = px.histogram(df, x=\"column\", facet_row=\"<col 1>\", facet_col=\"<col 2>\") Here, the figure will be devided into subplotts. Each row will share the <column 1> values, and each column will share the <column 2> values. The number of rows and columns will be determined automatically as the number of unique values in <column 1> and <column 2> , respectively. Independent axes between rows and columns It can happen that each row or column should have its own x or y axis due to a different scale. We can accomplish this by calling the update_xaxes and update_yaxes functions on the figure. Example: fig.update_xaxes(matches=None) fig.update_yaxes(matches=None) Removing the column name from the row/column annotations For each row and column, a label is added to the subplot. This label has a format of <column name> = <value> . To remove the column name, we can use the for_each_annotation function. Example: fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1])) Sharing the axes titles between rows and columns Unfortunately, the axes titles cannot be easily shared between rows and columns. The only way is to delete the titles and add shared titles manually using the add_annotation function. Example: # shared y axis title fig.for_each_yaxis(lambda y: y.update(title = '')) fig.add_annotation(x=-0.05, y=0.5, text=\"Vehicle hours\", textangle=-90, xref=\"paper\", yref=\"paper\", showarrow=False) # shared x axis title fig.for_each_xaxis(lambda y: y.update(title = '')) fig.add_annotation(x=0.5, y=-0.12, text=\"Occupancy\", xref=\"paper\", yref=\"paper\", showarrow=False) Plotly Graph Objects Bar Chart The basic syntax is: go.Bar(x, y, ...) Some more complicated examples are in the documentation . Line Chart The line chart is created using the go.Scatter function. Example: go.Scatter(x, y, ...) documentation Create subplots documentation and examples To create a figure with multiple subplots, we use the make_subplots function. Example: fig = make_subplots(rows=2, cols=2, start_cell=\"bottom-left\") fig.add_trace(go.Scatter(x=[1, 2, 3], y=[4, 5, 6]), row=1, col=1) ... fig.show() Importanta parameters: - shared_xaxes and shared_yaxes : if True , the subplots will share the same x and y axes. - x_title and y_title : the titles of the x and y axes. - horizontal_spacing and vertical_spacing : the spacing between the subplots. Customizing the Figure The figure object can be custommized in many ways. Size and margins The size and margins can be set using the figure.update_layout function. The specific parameters are: - width : the width of the figure in pixels - height : the height of the figure in pixels - autosize : needs to be False if we want to set the width and height manually - margin : dictionary containing the margins. The format is: l , r , t , b : the left, right, top and bottom margins in pixels - pad : the padding between the plot and the margins in pixels documentation Customize axes For customizing the axes, we can use the figure.update_xaxes and figure.update_yaxes functions. The most important parameters are: - dtick : the distance between the ticks - tickvals : the exact values of the ticks. This overrides the dtick parameter. - title_text : the title of the axis Legend documentation The legend can be styled using the figure.update_layout function. The most important parameters are: - legend_title_text : the title of the legend - legend : dictionary containing many parameters - orientation : h or v for horizontal or vertical legend - x , y : the position of the legend from the bottom left corner of the figure - xanchor , yanchor : the position of the legend box relative to the x and y coordinates - title : if string, the title of the legend (equivalent to the legend_title_text parameter). If dictionary, multiple legent title parameters can be set. Adding Figure Annotations For adding annotations to the whole Figure, we can use the add_annotation function. Exporting the Figure documentation The static export is handeled using the figure's write_image function. Example: fig.write_image(\"figure.png\") The output format is determined by the extension.","title":"Plotly Manual"},{"location":"Programming/Python/Plotly%20Manual/#plotly-express","text":"","title":"Plotly Express"},{"location":"Programming/Python/Plotly%20Manual/#histogram","text":"documentation Plotly express has histogram function for creating histograms. The basic syntax is: px.histogram(<dataframe>, <xcol name>) The y is then the number of occurences of each value in the x column. Important parameters: - nbins : number of bins.","title":"Histogram"},{"location":"Programming/Python/Plotly%20Manual/#bar-chart","text":"documentation For bar charts, we use the px.bar function. The basic syntax is: px.bar(<dataframe>, <xcol name>, <y col name>) Important parameters: - barmode : how to combine the bars in case of multiple traces. Can be group (default), stack (default in facet plots ), relative or overlay .","title":"Bar Chart"},{"location":"Programming/Python/Plotly%20Manual/#automatic-subplots-facet-plots","text":"Facet plots can be created using the same plot function as for normal plotly express plots and supplying the facet_col and/or facet_row parameters. Example: fig = px.histogram(df, x=\"column\", facet_row=\"<col 1>\", facet_col=\"<col 2>\") Here, the figure will be devided into subplotts. Each row will share the <column 1> values, and each column will share the <column 2> values. The number of rows and columns will be determined automatically as the number of unique values in <column 1> and <column 2> , respectively.","title":"Automatic Subplots: Facet plots"},{"location":"Programming/Python/Plotly%20Manual/#independent-axes-between-rows-and-columns","text":"It can happen that each row or column should have its own x or y axis due to a different scale. We can accomplish this by calling the update_xaxes and update_yaxes functions on the figure. Example: fig.update_xaxes(matches=None) fig.update_yaxes(matches=None)","title":"Independent axes between rows and columns"},{"location":"Programming/Python/Plotly%20Manual/#removing-the-column-name-from-the-rowcolumn-annotations","text":"For each row and column, a label is added to the subplot. This label has a format of <column name> = <value> . To remove the column name, we can use the for_each_annotation function. Example: fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))","title":"Removing the column name from the row/column annotations"},{"location":"Programming/Python/Plotly%20Manual/#sharing-the-axes-titles-between-rows-and-columns","text":"Unfortunately, the axes titles cannot be easily shared between rows and columns. The only way is to delete the titles and add shared titles manually using the add_annotation function. Example: # shared y axis title fig.for_each_yaxis(lambda y: y.update(title = '')) fig.add_annotation(x=-0.05, y=0.5, text=\"Vehicle hours\", textangle=-90, xref=\"paper\", yref=\"paper\", showarrow=False) # shared x axis title fig.for_each_xaxis(lambda y: y.update(title = '')) fig.add_annotation(x=0.5, y=-0.12, text=\"Occupancy\", xref=\"paper\", yref=\"paper\", showarrow=False)","title":"Sharing the axes titles between rows and columns"},{"location":"Programming/Python/Plotly%20Manual/#plotly-graph-objects","text":"","title":"Plotly Graph Objects"},{"location":"Programming/Python/Plotly%20Manual/#bar-chart_1","text":"The basic syntax is: go.Bar(x, y, ...) Some more complicated examples are in the documentation .","title":"Bar Chart"},{"location":"Programming/Python/Plotly%20Manual/#line-chart","text":"The line chart is created using the go.Scatter function. Example: go.Scatter(x, y, ...) documentation","title":"Line Chart"},{"location":"Programming/Python/Plotly%20Manual/#create-subplots","text":"documentation and examples To create a figure with multiple subplots, we use the make_subplots function. Example: fig = make_subplots(rows=2, cols=2, start_cell=\"bottom-left\") fig.add_trace(go.Scatter(x=[1, 2, 3], y=[4, 5, 6]), row=1, col=1) ... fig.show() Importanta parameters: - shared_xaxes and shared_yaxes : if True , the subplots will share the same x and y axes. - x_title and y_title : the titles of the x and y axes. - horizontal_spacing and vertical_spacing : the spacing between the subplots.","title":"Create subplots"},{"location":"Programming/Python/Plotly%20Manual/#customizing-the-figure","text":"The figure object can be custommized in many ways.","title":"Customizing the Figure"},{"location":"Programming/Python/Plotly%20Manual/#size-and-margins","text":"The size and margins can be set using the figure.update_layout function. The specific parameters are: - width : the width of the figure in pixels - height : the height of the figure in pixels - autosize : needs to be False if we want to set the width and height manually - margin : dictionary containing the margins. The format is: l , r , t , b : the left, right, top and bottom margins in pixels - pad : the padding between the plot and the margins in pixels documentation","title":"Size and margins"},{"location":"Programming/Python/Plotly%20Manual/#customize-axes","text":"For customizing the axes, we can use the figure.update_xaxes and figure.update_yaxes functions. The most important parameters are: - dtick : the distance between the ticks - tickvals : the exact values of the ticks. This overrides the dtick parameter. - title_text : the title of the axis","title":"Customize axes"},{"location":"Programming/Python/Plotly%20Manual/#legend","text":"documentation The legend can be styled using the figure.update_layout function. The most important parameters are: - legend_title_text : the title of the legend - legend : dictionary containing many parameters - orientation : h or v for horizontal or vertical legend - x , y : the position of the legend from the bottom left corner of the figure - xanchor , yanchor : the position of the legend box relative to the x and y coordinates - title : if string, the title of the legend (equivalent to the legend_title_text parameter). If dictionary, multiple legent title parameters can be set.","title":"Legend"},{"location":"Programming/Python/Plotly%20Manual/#adding-figure-annotations","text":"For adding annotations to the whole Figure, we can use the add_annotation function.","title":"Adding Figure Annotations"},{"location":"Programming/Python/Plotly%20Manual/#exporting-the-figure","text":"documentation The static export is handeled using the figure's write_image function. Example: fig.write_image(\"figure.png\") The output format is determined by the extension.","title":"Exporting the Figure"},{"location":"Programming/Python/Python%20Debugging/","text":"Pycharm has a build in debugger, however, there are some tricky problems described below. Breaking on Exception Breaking on exception is one of the most important debugger tools. However, there are some problems with Pycharm exception debugging. Breake on Termination vs on Raise By default, the program breakes on termination (unhandled exception). This is usually a correct configuration. However, in jupyter, all exceptions are caught to not break the jupyter itself. Therefore, in jupyter, all exceptions are ignored by the debugger if the breakpoins are set to break on termination. To break on exceptions in jupyter, we have to breake on raise. By this setting, however, we stop even on expected/handeled exceptions, stoping potentially on hundereds breakpoints in library code. Another issue is with the setting itself. To propagate the change between breaking on raise/termination, we have to deactivate and then activate again the exception breakpoints, otherwise, the setting is ignored. Written with StackEdit .","title":"Python Debugging"},{"location":"Programming/Python/Python%20Debugging/#breaking-on-exception","text":"Breaking on exception is one of the most important debugger tools. However, there are some problems with Pycharm exception debugging.","title":"Breaking on Exception"},{"location":"Programming/Python/Python%20Debugging/#breake-on-termination-vs-on-raise","text":"By default, the program breakes on termination (unhandled exception). This is usually a correct configuration. However, in jupyter, all exceptions are caught to not break the jupyter itself. Therefore, in jupyter, all exceptions are ignored by the debugger if the breakpoins are set to break on termination. To break on exceptions in jupyter, we have to breake on raise. By this setting, however, we stop even on expected/handeled exceptions, stoping potentially on hundereds breakpoints in library code. Another issue is with the setting itself. To propagate the change between breaking on raise/termination, we have to deactivate and then activate again the exception breakpoints, otherwise, the setting is ignored.","title":"Breake on Termination vs on Raise"},{"location":"Programming/Python/Python%20Debugging/#_1","text":"Written with StackEdit .","title":""},{"location":"Programming/Python/Python%20Manual/","text":"Conditions and boolean context Comparison operators Python uses the standard set of comparison operators ( == , != , < , > , <= , >= ). They are functionally similar to C++ operators: they can be overloaded and the semantic meaning of == is equality, not identity (in contrast to Java). Automatic conversion to bool Unlike in other languages, any expression can be used in boolean context in python, as there are rules how to convert any type to bool . The following statement is valid, foor example: s = 'hello' if s: print(s) The code above prints 'hello', as the variable s evaluates to True . Any object in Python evaluates to True , with exeption of: - False - None - numerically zero values (e.g., 0 , 0.0 ) - standard library types that are empty (e.g., empty string, list , dict ) The automatic conversion to bool in boolean context has some couner intuitive consequences. The following conditions are not equal: s = 'hello' if s: # s evaluates to True if s == True: # the result of s == True is False, then False evaluete to False Functions Argument unpacking if we need to conditionaly execute function with a different set of parameters (supposed the function has optional/default parameters), we can avoid multiple function calls inside the branching tree by using argument unpacking. Suppose we have a function with three optional parameters: a , b , c . If we skip only last n parameters, we can use a list for parameters and unpack it using * : def call_me(a, b, c): ... l = ['param A', True] call_me(*l) # calls the function with a = 'param A' and b = True If we need to skip some parameters in the middle, we have to use a dict and unpack it using ** : d = {'c': 142} call_me(**d) # calls the function with c = 142 Checking the type To check the exact type: if type(<VAR>) is <TYPE>: # e.g. if type(o) is str: To check the type in the polymorphic way, including the subtypes: if isinstance(<VAR>, <TYPE>): # e.g. if isinstance(o, str): String formatting To format python strings we can use the format function of the string or the equivalen fstring: a = 'world' message = \"Hello {} world\".format(a) message = f\"Hello {a}\" # equivalent If we need to a special formatting for a variable, we can specify it behind : as we can see in the following example that padds the number from left: uid = '47' message = \"Hello user {:0>4d}\".format(a) # prints \"Hello user 0047\" message = f\"Hello {a:0>4d}\" # equivalent More formating optios can be found in the Python string formatting cookbook . Exceptions documentation Syntax: try: <code that can raise exception> except <ERROR TYPE> as <ERROR VAR>: <ERROR HANDELING> Date and time Python documentation The base object for date and time is datetime datetime construction The datetime object can be directly constructed from the parts: from daterime import datetime d = datetime(2022, 12, 20, 22, 30, 0) # 2022-12-20 22:30:00 The time part can be ommited. We can load the datetime from string using the strptime function: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') For all possible time formats, check the strftime cheatsheet Accessing the parts of datetime The datetime object has the following attributes: - year - month - day - hour - minute - second We can also query the day of the week using the weekday() method. The day of the week is represented as an integer, where Monday is 0 and Sunday is 6. Intervals There is also a dedicated object for time interval named timedelta . It can be constructed from parts (seconds to days), all parts are optional. We can obtain a timedelta by substracting a datetime from another datetime : d1 = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') d2 = datetime.strptime('2022-05-20 18:30', '%Y-%m-%d %H:%M') interval = d2 - d1 # 30 minutes We can also add or substract a timedelta object from the datetime object: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') interval = timedelta(hours=1) d2 = d + interval # '2022-05-20 19:00' Working with paths We can use strings to manipulate with pths in Python, however, it is easier to manipulate the Path or PurePath objects from pathlib for complicated situations. The folowing code compares both approaches for path concatenation: a = \"C:/workspace\" b = \"project/file.txt\" # string path concatenation c = f\"{a}/{b}\" # pathlib concatentation a = Path(\"C:/workspace\") b = Path(\"project/file.txt\") c = a / b Computing relative path To prevent misetakes, it is better to compute relative paths beteen directories than to hard-code them. Fortunately, there are methods we can use for that. If the desired relative path is a child of the start path, we can simply use the relative_to method of the Path object: a = Path(\"C:/workspace\") b = Path(\"C:/workspace/project/file.txt\") rel = b.relative_to(a) # rel = 'project/file.txt' However, if we need to go back in the filetree, we need a more sophisticated method from os.path : a = Path(\"C:/Users\") b = Path(\"C:/workspace/project/file.txt\") rel = os.path.relpath(a, b) # rel = '../Workspaces/project/file.txt' iterating over files There are different method available, depending what we need - standard iteration - recursive iteration: os.walk(<path>) , see example on SO Get parent directory We can use the parent property of the Path object: p = Path(\"C:/workspace/project/file.txt\") parent = p.parent # 'C:\\\\workspace\\\\project' Splitting paths and working with path parts We can split the path into parts using the parts property: p = Path(\"C:/workspace/project/file.txt\") parts = p.parts # ('C:\\\\', 'workspace', 'project', 'file.txt') To find the index of some specific part, we can use the index method: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 Later, we can use the index to manipulate the path: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 p = Path(*p.parts[:index]) # 'C:\\\\workspace' Built-in data structures Python has several built-in data structures, most notably list , tuple , dict , and set . These are less efficient then comparable structures in other languages, but they are very convenient to use. Dictionary Official Manual Disctionaries are initialized using curly braces ( {} ) and the : operator: d = { 'key1': 'value1', 'key2': 'value2', ... } Two dictionaries can be merged using the | operator: d3 = d1 | d2 Comprehensions In addition to literals, Python has a convinient way of creating basic data structures: the comprehensions. The basic syntax is: <struct var> = <op. brace> <member var expr.> for <member var> in <iterable><cl. brace> As for literals, we use square braces ( [] ) for lists, curly braces ( {} ) for sets, and curly braces with colons for dictionaries. In contrast, we get a generator expression when using round braces ( () ), not a tuple. We can also use the if keyword to filter the elements: a = [it for it in range(10) if it % 2 == 0] # [0, 2, 4, 6, 8] Sorting Official Manual For sorting, you can use the sorted function. Instead of using comparators, Python has a different concept of key functions for custom sorting. The key function is a function that is applied to each element before sorting. For any expected object, the key function should return a value that can be compared. I/O HDF5 HDF5 is a binary file format for storing large amounts of data. The h5py module provides a Python interface for working with HDF5 files. An example of reading a dataset from an HDF5 file on SO Command line arguments The sys module provides access to the command line arguments. They are stored in the argv list with the first element being the name of the script. Numpy Usefull array properties: size : number of array items unlike len, it counts all items in the mutli-dimensional array itemsize : memory (bytes) needed to store one item in the array nbytes : array size in bytes. Should be equal to size * itemsize . Usefull functions sort Regular expressions In Python, the regex patterns are not compiled by default. Therefore we can use strings to store them. The basic syntax for regex search is: result = re.search(<pattern>, <string>) if result: # pattern matches group = result.group(<group index>)) # print the first group The 0th group is the whole match, as usual. Jupyter Memory Most of the time, when the memory allocated by the notebook is larger than expected, it is caused by some library objects (plots, tables...]). However sometimes, it can be forgotten user objects. To list all user objects, from the largest: # These are the usual ipython objects, including this one you are creating ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars'] # Get a sorted list of the objects and their sizes sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True) Docstrings For documenting Python code, we use docstrings, special comments soroudned by three quotation marks: \"\"\" docstring \"\"\" Unlike in other languages, there are multiple styles for docstring content. PostgreSQL When working with PostgreSQL databases, we usually use either - the psycopg2 adapter or, - the sqlalchemy . SQLAlchemy Simple query: sqlalchemy_engine.execute(\"<sql>\") Working with GIS when working with gis data, we usually change the pandas library for its GIS extension called geopandas . For more,, see the pandas manual.","title":"Conditions and boolean context"},{"location":"Programming/Python/Python%20Manual/#conditions-and-boolean-context","text":"","title":"Conditions and boolean context"},{"location":"Programming/Python/Python%20Manual/#comparison-operators","text":"Python uses the standard set of comparison operators ( == , != , < , > , <= , >= ). They are functionally similar to C++ operators: they can be overloaded and the semantic meaning of == is equality, not identity (in contrast to Java).","title":"Comparison operators"},{"location":"Programming/Python/Python%20Manual/#automatic-conversion-to-bool","text":"Unlike in other languages, any expression can be used in boolean context in python, as there are rules how to convert any type to bool . The following statement is valid, foor example: s = 'hello' if s: print(s) The code above prints 'hello', as the variable s evaluates to True . Any object in Python evaluates to True , with exeption of: - False - None - numerically zero values (e.g., 0 , 0.0 ) - standard library types that are empty (e.g., empty string, list , dict ) The automatic conversion to bool in boolean context has some couner intuitive consequences. The following conditions are not equal: s = 'hello' if s: # s evaluates to True if s == True: # the result of s == True is False, then False evaluete to False","title":"Automatic conversion to bool"},{"location":"Programming/Python/Python%20Manual/#functions","text":"","title":"Functions"},{"location":"Programming/Python/Python%20Manual/#argument-unpacking","text":"if we need to conditionaly execute function with a different set of parameters (supposed the function has optional/default parameters), we can avoid multiple function calls inside the branching tree by using argument unpacking. Suppose we have a function with three optional parameters: a , b , c . If we skip only last n parameters, we can use a list for parameters and unpack it using * : def call_me(a, b, c): ... l = ['param A', True] call_me(*l) # calls the function with a = 'param A' and b = True If we need to skip some parameters in the middle, we have to use a dict and unpack it using ** : d = {'c': 142} call_me(**d) # calls the function with c = 142","title":"Argument unpacking"},{"location":"Programming/Python/Python%20Manual/#checking-the-type","text":"To check the exact type: if type(<VAR>) is <TYPE>: # e.g. if type(o) is str: To check the type in the polymorphic way, including the subtypes: if isinstance(<VAR>, <TYPE>): # e.g. if isinstance(o, str):","title":"Checking the type"},{"location":"Programming/Python/Python%20Manual/#string-formatting","text":"To format python strings we can use the format function of the string or the equivalen fstring: a = 'world' message = \"Hello {} world\".format(a) message = f\"Hello {a}\" # equivalent If we need to a special formatting for a variable, we can specify it behind : as we can see in the following example that padds the number from left: uid = '47' message = \"Hello user {:0>4d}\".format(a) # prints \"Hello user 0047\" message = f\"Hello {a:0>4d}\" # equivalent More formating optios can be found in the Python string formatting cookbook .","title":"String formatting"},{"location":"Programming/Python/Python%20Manual/#exceptions","text":"documentation Syntax: try: <code that can raise exception> except <ERROR TYPE> as <ERROR VAR>: <ERROR HANDELING>","title":"Exceptions"},{"location":"Programming/Python/Python%20Manual/#date-and-time","text":"Python documentation The base object for date and time is datetime","title":"Date and time"},{"location":"Programming/Python/Python%20Manual/#datetime-construction","text":"The datetime object can be directly constructed from the parts: from daterime import datetime d = datetime(2022, 12, 20, 22, 30, 0) # 2022-12-20 22:30:00 The time part can be ommited. We can load the datetime from string using the strptime function: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') For all possible time formats, check the strftime cheatsheet","title":"datetime construction"},{"location":"Programming/Python/Python%20Manual/#accessing-the-parts-of-datetime","text":"The datetime object has the following attributes: - year - month - day - hour - minute - second We can also query the day of the week using the weekday() method. The day of the week is represented as an integer, where Monday is 0 and Sunday is 6.","title":"Accessing the parts of datetime"},{"location":"Programming/Python/Python%20Manual/#intervals","text":"There is also a dedicated object for time interval named timedelta . It can be constructed from parts (seconds to days), all parts are optional. We can obtain a timedelta by substracting a datetime from another datetime : d1 = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') d2 = datetime.strptime('2022-05-20 18:30', '%Y-%m-%d %H:%M') interval = d2 - d1 # 30 minutes We can also add or substract a timedelta object from the datetime object: d = datetime.strptime('2022-05-20 18:00', '%Y-%m-%d %H:%M') interval = timedelta(hours=1) d2 = d + interval # '2022-05-20 19:00'","title":"Intervals"},{"location":"Programming/Python/Python%20Manual/#working-with-paths","text":"We can use strings to manipulate with pths in Python, however, it is easier to manipulate the Path or PurePath objects from pathlib for complicated situations. The folowing code compares both approaches for path concatenation: a = \"C:/workspace\" b = \"project/file.txt\" # string path concatenation c = f\"{a}/{b}\" # pathlib concatentation a = Path(\"C:/workspace\") b = Path(\"project/file.txt\") c = a / b","title":"Working with paths"},{"location":"Programming/Python/Python%20Manual/#computing-relative-path","text":"To prevent misetakes, it is better to compute relative paths beteen directories than to hard-code them. Fortunately, there are methods we can use for that. If the desired relative path is a child of the start path, we can simply use the relative_to method of the Path object: a = Path(\"C:/workspace\") b = Path(\"C:/workspace/project/file.txt\") rel = b.relative_to(a) # rel = 'project/file.txt' However, if we need to go back in the filetree, we need a more sophisticated method from os.path : a = Path(\"C:/Users\") b = Path(\"C:/workspace/project/file.txt\") rel = os.path.relpath(a, b) # rel = '../Workspaces/project/file.txt'","title":"Computing relative path"},{"location":"Programming/Python/Python%20Manual/#iterating-over-files","text":"There are different method available, depending what we need - standard iteration - recursive iteration: os.walk(<path>) , see example on SO","title":"iterating over files"},{"location":"Programming/Python/Python%20Manual/#get-parent-directory","text":"We can use the parent property of the Path object: p = Path(\"C:/workspace/project/file.txt\") parent = p.parent # 'C:\\\\workspace\\\\project'","title":"Get parent directory"},{"location":"Programming/Python/Python%20Manual/#splitting-paths-and-working-with-path-parts","text":"We can split the path into parts using the parts property: p = Path(\"C:/workspace/project/file.txt\") parts = p.parts # ('C:\\\\', 'workspace', 'project', 'file.txt') To find the index of some specific part, we can use the index method: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 Later, we can use the index to manipulate the path: p = Path(\"C:/workspace/project/file.txt\") index = p.parts.index('project') # 2 p = Path(*p.parts[:index]) # 'C:\\\\workspace'","title":"Splitting paths and working with path parts"},{"location":"Programming/Python/Python%20Manual/#built-in-data-structures","text":"Python has several built-in data structures, most notably list , tuple , dict , and set . These are less efficient then comparable structures in other languages, but they are very convenient to use.","title":"Built-in data structures"},{"location":"Programming/Python/Python%20Manual/#dictionary","text":"Official Manual Disctionaries are initialized using curly braces ( {} ) and the : operator: d = { 'key1': 'value1', 'key2': 'value2', ... } Two dictionaries can be merged using the | operator: d3 = d1 | d2","title":"Dictionary"},{"location":"Programming/Python/Python%20Manual/#comprehensions","text":"In addition to literals, Python has a convinient way of creating basic data structures: the comprehensions. The basic syntax is: <struct var> = <op. brace> <member var expr.> for <member var> in <iterable><cl. brace> As for literals, we use square braces ( [] ) for lists, curly braces ( {} ) for sets, and curly braces with colons for dictionaries. In contrast, we get a generator expression when using round braces ( () ), not a tuple. We can also use the if keyword to filter the elements: a = [it for it in range(10) if it % 2 == 0] # [0, 2, 4, 6, 8]","title":"Comprehensions"},{"location":"Programming/Python/Python%20Manual/#sorting","text":"Official Manual For sorting, you can use the sorted function. Instead of using comparators, Python has a different concept of key functions for custom sorting. The key function is a function that is applied to each element before sorting. For any expected object, the key function should return a value that can be compared.","title":"Sorting"},{"location":"Programming/Python/Python%20Manual/#io","text":"","title":"I/O"},{"location":"Programming/Python/Python%20Manual/#hdf5","text":"HDF5 is a binary file format for storing large amounts of data. The h5py module provides a Python interface for working with HDF5 files. An example of reading a dataset from an HDF5 file on SO","title":"HDF5"},{"location":"Programming/Python/Python%20Manual/#command-line-arguments","text":"The sys module provides access to the command line arguments. They are stored in the argv list with the first element being the name of the script.","title":"Command line arguments"},{"location":"Programming/Python/Python%20Manual/#numpy","text":"","title":"Numpy"},{"location":"Programming/Python/Python%20Manual/#usefull-array-properties","text":"size : number of array items unlike len, it counts all items in the mutli-dimensional array itemsize : memory (bytes) needed to store one item in the array nbytes : array size in bytes. Should be equal to size * itemsize .","title":"Usefull array properties:"},{"location":"Programming/Python/Python%20Manual/#usefull-functions","text":"sort","title":"Usefull functions"},{"location":"Programming/Python/Python%20Manual/#regular-expressions","text":"In Python, the regex patterns are not compiled by default. Therefore we can use strings to store them. The basic syntax for regex search is: result = re.search(<pattern>, <string>) if result: # pattern matches group = result.group(<group index>)) # print the first group The 0th group is the whole match, as usual.","title":"Regular expressions"},{"location":"Programming/Python/Python%20Manual/#jupyter","text":"","title":"Jupyter"},{"location":"Programming/Python/Python%20Manual/#memory","text":"Most of the time, when the memory allocated by the notebook is larger than expected, it is caused by some library objects (plots, tables...]). However sometimes, it can be forgotten user objects. To list all user objects, from the largest: # These are the usual ipython objects, including this one you are creating ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars'] # Get a sorted list of the objects and their sizes sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)","title":"Memory"},{"location":"Programming/Python/Python%20Manual/#docstrings","text":"For documenting Python code, we use docstrings, special comments soroudned by three quotation marks: \"\"\" docstring \"\"\" Unlike in other languages, there are multiple styles for docstring content.","title":"Docstrings"},{"location":"Programming/Python/Python%20Manual/#postgresql","text":"When working with PostgreSQL databases, we usually use either - the psycopg2 adapter or, - the sqlalchemy .","title":"PostgreSQL"},{"location":"Programming/Python/Python%20Manual/#sqlalchemy","text":"Simple query: sqlalchemy_engine.execute(\"<sql>\")","title":"SQLAlchemy"},{"location":"Programming/Python/Python%20Manual/#working-with-gis","text":"when working with gis data, we usually change the pandas library for its GIS extension called geopandas . For more,, see the pandas manual.","title":"Working with GIS"},{"location":"Programming/Python/Python%20Workflow/","text":"Dev Stack I use the following stack: - the latest Python, 64 bit - pip as the package manager - Pycharm IDE - pytest test suite - Visual Studio for deugging native code Python Python should be installed from the official web page , not using any package manager. Steps: 1. Dowload the 64-bit installer 2. Run the installer, choose advanced install - Include the GUI tools (Tkinter, TK) - Includ *.py launcher, but only if there is no newer python version installed. If this is checked and there is a newer vesrsion of Python installed, the setup will fail. - Include documentation - Check download debug symbols to enable native code debugging The source code for python can be inspected on GitHub Command line We execute python scripts from the command line as: python <path to the .py file> . Parameters: - -m executes a module as a script, e.g. python -m venv . This is useful for executing scripts whithout knowing the path to the script. Installing Packages Normal packages are installed using: pip install <package name> . However, if a package uses a C/C++ backend and does not contain the compiled wheel on PyPy, this approach will fail on Windows. Instead, you have to download the wheel from the Chris Gohlke page and install it: pip install <path to wheel> . Also, you have to install the dependencies mentioned on that page. Troubleshooting If the installation fails, check the following: 1. if you installed the package by name, check for the wheel on the Chris Golthke page. 2. if you installed the package from a wheel, check the notes/requirement info on Chris Golthke page 3. Check the log. Specifically, no building should appear there whatsoever. If a build starts, it means that some dependency that should be installed as a prebuild wheel is missing. Possible reasons: 1. you forget to install the dependency, go back to step 2 2. the dependency version does not correspond with the version required by the package you are installing. Check the log for the required version. Pycharm Configuration Settings synchronization Same as in IDEA: 1. Log in into JetBrains Toolbox or to the App 1. Click on the gear icon on the top-right and choose Sync 1. Check all categories and click on pull settings from the cloud Do Not Run scripts in Python Console by Default Run configuration select box -> Edit Configurations... -> Edit configuration templates -> Python -> uncheck the Run with Python Console Enable Progress Bars in output console Run configuration select box -> Edit Configurations... -> Select the configuration -> check the Emulate terminal in output console Project Configuration configure the correct test suite in File -> Settings -> Tools -> Python Integrated Tools -> testing Known problems & solutions Non deterministic output in the run window Problem: It can happen that the output printing/logging can be reordered randomly (not matching the order of calls in the source, neither the system console output). Solution: Edit Configurations... -> select configuration for the script -> check Emulate terminal in output console . Jupyter Jupyter can be used both in Pycharm and in a web browser. Jupyter in Pycharm The key for the effectivity of Jupyter in Pycharm is using the command mode . To enter the command mode, press Esc . To enter the edit mode, pres enter. Command mode shortcuts m : change cell type to markdown Ctrl + C : copy cell Ctrl + V : paste cell Ctrl + Shift + Up : move cell up Ctrl + Shift + Down : move cell down Ctrl + - : split cell Web Browser Configuration Install Extension Manager with basic extensions Best use the official installation guide . The extensions then can be toggled on in the Nbextensions tab in the jupyter homepage. Be sure to unselect the disable configuration for nbextensions without explicit compatibility (they may break your notebook environment, but can be useful to show for nbextension development) checkbox, otherwise, all extensions will be disabled. Debugging Pycharm contains a good debuger for python code. However, it cannot step into most standard library functions, as those are native, implemented in C/C++. For that, we need mixed python/native debugging. Testing To run pytest, simply go to the folder and run pytest . Arguments: - -x : stop on first failure Mixed Python-native debugging In theory, there are two ways how to debug python native code: - use a tool that can step from python to C++ (only Visual Studio offers this possibility AFAIK) - use two debuggers, start with a python debugger, and attach a native debugger to the python process. This way, the debuggers can be independent. However, one needs to put a breakpoint in the native code, and for that, we need to know the location of the code that will be executed from python (non-trivial for standard library) Python-native debugger in Visual Studio First check and install the requirements: - Python 3.5-3.9, including debugging symbols - Python 3.10 is not compatible yet - in Visual Studio, a Python development component needs to be installed, including Python native development tools Then, configure the Visual studio: - Go to Tools -> Options -> Python -> Debugging and check: Enable debugging of the Python standard library . Finally, create a new Python project (either clean, or from existing code) and configure it: - use the interpreter directly, do not create a virtual environment - enable native code debugging: 1. Go to project properties -> Debug 2. Check enable native code debugging - use the -i flag to always see the debug console, even if the program ends without breaking 1. Go to project properties -> Debug 2. Add the -i flag to the Interpreter Arguments field Known issues The reference assemblies for .NETFramework,Version=v4.7.2 were not found. -> Install this component using the visual studio installer Other Sources Microsoft official documentation Python tools for Visual Studio GitHub page","title":"Dev Stack"},{"location":"Programming/Python/Python%20Workflow/#dev-stack","text":"I use the following stack: - the latest Python, 64 bit - pip as the package manager - Pycharm IDE - pytest test suite - Visual Studio for deugging native code","title":"Dev Stack"},{"location":"Programming/Python/Python%20Workflow/#python","text":"Python should be installed from the official web page , not using any package manager. Steps: 1. Dowload the 64-bit installer 2. Run the installer, choose advanced install - Include the GUI tools (Tkinter, TK) - Includ *.py launcher, but only if there is no newer python version installed. If this is checked and there is a newer vesrsion of Python installed, the setup will fail. - Include documentation - Check download debug symbols to enable native code debugging The source code for python can be inspected on GitHub","title":"Python"},{"location":"Programming/Python/Python%20Workflow/#command-line","text":"We execute python scripts from the command line as: python <path to the .py file> . Parameters: - -m executes a module as a script, e.g. python -m venv . This is useful for executing scripts whithout knowing the path to the script.","title":"Command line"},{"location":"Programming/Python/Python%20Workflow/#installing-packages","text":"Normal packages are installed using: pip install <package name> . However, if a package uses a C/C++ backend and does not contain the compiled wheel on PyPy, this approach will fail on Windows. Instead, you have to download the wheel from the Chris Gohlke page and install it: pip install <path to wheel> . Also, you have to install the dependencies mentioned on that page.","title":"Installing Packages"},{"location":"Programming/Python/Python%20Workflow/#troubleshooting","text":"If the installation fails, check the following: 1. if you installed the package by name, check for the wheel on the Chris Golthke page. 2. if you installed the package from a wheel, check the notes/requirement info on Chris Golthke page 3. Check the log. Specifically, no building should appear there whatsoever. If a build starts, it means that some dependency that should be installed as a prebuild wheel is missing. Possible reasons: 1. you forget to install the dependency, go back to step 2 2. the dependency version does not correspond with the version required by the package you are installing. Check the log for the required version.","title":"Troubleshooting"},{"location":"Programming/Python/Python%20Workflow/#pycharm","text":"","title":"Pycharm"},{"location":"Programming/Python/Python%20Workflow/#configuration","text":"","title":"Configuration"},{"location":"Programming/Python/Python%20Workflow/#settings-synchronization","text":"Same as in IDEA: 1. Log in into JetBrains Toolbox or to the App 1. Click on the gear icon on the top-right and choose Sync 1. Check all categories and click on pull settings from the cloud","title":"Settings synchronization"},{"location":"Programming/Python/Python%20Workflow/#do-not-run-scripts-in-python-console-by-default","text":"Run configuration select box -> Edit Configurations... -> Edit configuration templates -> Python -> uncheck the Run with Python Console","title":"Do Not Run scripts in Python Console by Default"},{"location":"Programming/Python/Python%20Workflow/#enable-progress-bars-in-output-console","text":"Run configuration select box -> Edit Configurations... -> Select the configuration -> check the Emulate terminal in output console","title":"Enable Progress Bars in output console"},{"location":"Programming/Python/Python%20Workflow/#project-configuration","text":"configure the correct test suite in File -> Settings -> Tools -> Python Integrated Tools -> testing","title":"Project Configuration"},{"location":"Programming/Python/Python%20Workflow/#known-problems-solutions","text":"","title":"Known problems &amp; solutions"},{"location":"Programming/Python/Python%20Workflow/#non-deterministic-output-in-the-run-window","text":"Problem: It can happen that the output printing/logging can be reordered randomly (not matching the order of calls in the source, neither the system console output). Solution: Edit Configurations... -> select configuration for the script -> check Emulate terminal in output console .","title":"Non deterministic output in the run window"},{"location":"Programming/Python/Python%20Workflow/#jupyter","text":"Jupyter can be used both in Pycharm and in a web browser.","title":"Jupyter"},{"location":"Programming/Python/Python%20Workflow/#jupyter-in-pycharm","text":"The key for the effectivity of Jupyter in Pycharm is using the command mode . To enter the command mode, press Esc . To enter the edit mode, pres enter.","title":"Jupyter in Pycharm"},{"location":"Programming/Python/Python%20Workflow/#command-mode-shortcuts","text":"m : change cell type to markdown Ctrl + C : copy cell Ctrl + V : paste cell Ctrl + Shift + Up : move cell up Ctrl + Shift + Down : move cell down Ctrl + - : split cell","title":"Command mode shortcuts"},{"location":"Programming/Python/Python%20Workflow/#web-browser-configuration","text":"","title":"Web Browser Configuration"},{"location":"Programming/Python/Python%20Workflow/#install-extension-manager-with-basic-extensions","text":"Best use the official installation guide . The extensions then can be toggled on in the Nbextensions tab in the jupyter homepage. Be sure to unselect the disable configuration for nbextensions without explicit compatibility (they may break your notebook environment, but can be useful to show for nbextension development) checkbox, otherwise, all extensions will be disabled.","title":"Install Extension Manager with basic extensions"},{"location":"Programming/Python/Python%20Workflow/#debugging","text":"Pycharm contains a good debuger for python code. However, it cannot step into most standard library functions, as those are native, implemented in C/C++. For that, we need mixed python/native debugging.","title":"Debugging"},{"location":"Programming/Python/Python%20Workflow/#testing","text":"To run pytest, simply go to the folder and run pytest . Arguments: - -x : stop on first failure","title":"Testing"},{"location":"Programming/Python/Python%20Workflow/#mixed-python-native-debugging","text":"In theory, there are two ways how to debug python native code: - use a tool that can step from python to C++ (only Visual Studio offers this possibility AFAIK) - use two debuggers, start with a python debugger, and attach a native debugger to the python process. This way, the debuggers can be independent. However, one needs to put a breakpoint in the native code, and for that, we need to know the location of the code that will be executed from python (non-trivial for standard library)","title":"Mixed Python-native debugging"},{"location":"Programming/Python/Python%20Workflow/#python-native-debugger-in-visual-studio","text":"First check and install the requirements: - Python 3.5-3.9, including debugging symbols - Python 3.10 is not compatible yet - in Visual Studio, a Python development component needs to be installed, including Python native development tools Then, configure the Visual studio: - Go to Tools -> Options -> Python -> Debugging and check: Enable debugging of the Python standard library . Finally, create a new Python project (either clean, or from existing code) and configure it: - use the interpreter directly, do not create a virtual environment - enable native code debugging: 1. Go to project properties -> Debug 2. Check enable native code debugging - use the -i flag to always see the debug console, even if the program ends without breaking 1. Go to project properties -> Debug 2. Add the -i flag to the Interpreter Arguments field","title":"Python-native debugger in Visual Studio"},{"location":"Programming/Python/Python%20Workflow/#known-issues","text":"The reference assemblies for .NETFramework,Version=v4.7.2 were not found. -> Install this component using the visual studio installer","title":"Known issues"},{"location":"Programming/Python/Python%20Workflow/#other-sources","text":"Microsoft official documentation Python tools for Visual Studio GitHub page","title":"Other Sources"},{"location":"Programming/Web/CSS%20Manual/","text":"Layout options Grid Layout For complex pages. See the tutorial . Flex Layout For simpler pages. See the tutorial . Note that setting max_width: 100% for child elements of flex items does not work frequenly, so it's better to specify max_witdth ( SO ). Oldschool Layout Oldschool layout use floats. Very Oldschool Layout With tables... Practical Selectors Last Child of a Specific Parent #parent > :last-child Written with StackEdit .","title":"Layout options"},{"location":"Programming/Web/CSS%20Manual/#layout-options","text":"","title":"Layout options"},{"location":"Programming/Web/CSS%20Manual/#grid-layout","text":"For complex pages. See the tutorial .","title":"Grid Layout"},{"location":"Programming/Web/CSS%20Manual/#flex-layout","text":"For simpler pages. See the tutorial . Note that setting max_width: 100% for child elements of flex items does not work frequenly, so it's better to specify max_witdth ( SO ).","title":"Flex Layout"},{"location":"Programming/Web/CSS%20Manual/#oldschool-layout","text":"Oldschool layout use floats.","title":"Oldschool Layout"},{"location":"Programming/Web/CSS%20Manual/#very-oldschool-layout","text":"With tables...","title":"Very Oldschool Layout"},{"location":"Programming/Web/CSS%20Manual/#practical-selectors","text":"","title":"Practical Selectors"},{"location":"Programming/Web/CSS%20Manual/#last-child-of-a-specific-parent","text":"#parent > :last-child Written with StackEdit .","title":"Last Child of a Specific Parent"},{"location":"Programming/Web/JavaScript/","text":"","title":"JavaScript"},{"location":"Programming/Web/htaccess/","text":"kakpsatweb","title":"Htaccess"},{"location":"Ubuntu/Linux%20Manual/","text":"Usefull Comands Check Ubuntu Version lsb_release -a Printing used ports sudo lsof -i -P Checking GLIBC version ldd --version Exit codes Exit code of the last command: $? Common exit codes for C++ programs Show path to executable Use the which command: which <executable> Unpack file Foe unpacking, you can use the tar -f <file> command. The most used options are: - x : extract Environment Variables We will demonstrate it on the PATH example. If you have a program in a custom location, adding it to $PATH permanently and be able to run it under all circumstances is not an easy task on linux. Standard procedure to add a system variable: 1. Create a dedicated .sh file in /etc/profile.d. for your configuration (config for each app should be stored in a separate file). 2. the file should contain: export PATH=$PATH:YOURPATH 1. exit nano and save the file: ctrl+x and y 3. logout and login again to load the newly added varibales - for WSL close the console and reopen it, it is not necessary to restart the WSL ( click here for detailed description ) Enable Variable with sudo To enable the variable even if you use sudo , you need to edit sudo config using sudo visudo and: 1. exclude PATH from variable being reset when running sudo : Defaults env_keep += \"PATH\" 2. disable the safe path mechanism for sudo , i.e., comment the line: Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin 3. logout and login again to load the new added varibales Enable the Variable Outside Bash If you need the variable outside bash, the above-mentioned approach won\u2019t work. Currently, I do not know about any general solution for variables. The solution below, unfortunately, work only for PAM shells (see this SO answer why). Add the variable to /etc/environment . Note that it is not a script file, so you can use only simple variable assignments. Enable Variable on a Device Without Root Access Without root access, we can only edit the user config files. put the necessary config into: - ~/.bash_profile if it already exists - or to ~/.profile Note that the .profile file is ignored when the .bash_profile file exists. Remove Windows $PATH from WSL By default, the PATH environment variable from Windows is inluded in the PATH variable in Ubuntu running in WSL. This can lead to conflicts, when the same executables are used in both systems (e.g., vcpkg, cmake). To turn of the Windows PATH inclusion: 1. open /etc/wsl.conf 1. add the fllowing code: [interop] appendWindowsPath = false restart WSL File System copy file The cp command is used to copy files. The most used options are: - -r , -R : copy recursively - -v : verbose - -f : force - -p : preserve permissions and timestamps - -a : same as -p -R plus some other options Network netstat The netstat command is the basic command to monitor the networ. It displays the TCP connections. It is available both on Linux and on Windows, although the interface differs. Important parameters: - -n : do not translate IP and ports into human readable names - -a : show all connections. Without this, some connections can be skipped. SSH SSH Tunneling An SSH tunnel can be created by the ssh command. The usuall syntax is following: ssh -L <local port>:<remote machine>:<remote port> <ssh server username>@<ssh server address> The -L argument stands for local port forwarding , i.e., we forward a local port to some remote port. Example: ssh -L 1111:localhost:5432 fiedler@its.fel.cvut.cz The local port (here 1111 ) is arbitrary, we can use any free port. The aplications using the tunnel should then be configured as: - host= localhost - port:= 1111 The remote machine is the address of the machine we want to access relative to the ssh server . If the ssh server is running on the computer we want to access through the tunnel, we can use localhost . Analogously, the remote port is the port we wan to use on the remote machine (here 5432 is a PostgreSQL db port). The ssh server username and ssh server address are then the username/address of the remote machine. On top of that, we need password or priveta key to validate our identity. Note that the credential here are the credential for the server, not the credentials of the service we are accessing through the ssh connection. Those credentials has to be usually supplied by the application accessing the service through the ssh tunnel. The connection can be canceled any time byping exit to the console. More information Debugging a SSH tunnel This guide suppose that the tunnel creation comman run without any error message. 1. If the tunnel seems to not work, first use a command line tool to be sure: - web browser for HTTP tunnels (remote port 80) - psql for postgeSQL tunnels (remote port 5432) - telnet for telnet tunnels (reote port 23) 2. If the test is unsucessful, try to kill all ssh connections to the server by shutting down all applications with ssh connections tunnels, untill there will be only one connection at the server (the console). The number of ssh connections can be checked with: sudo netstat -nap | grep :22 Enabnling SSH Access to Server install openssh: sudo apt update sudo apt install openssh-server configure access password: open /etc/ssh/sshd_config set PasswordAuothentication yes after restrat you can log in with the user and password used in Ubuntu keys: TODO restart the ssh server: sudo service ssh restart WSL configuration port 22 can be used on Windows, so change the port to 2222 in sshd_config when loging from Windows use 127.0.0.1 as a host Debugging 1n from now e as a t euin. check the ssh status with: s th service ssh status 2. check the ssh porot with sudso netstat t-tpln Executing a long running process over SSH When the SSH connection to a server is disconnected (either manually, or by network failure or timeout), the process running in the console is canceled. To overcome this limitation, we can use the screen command, which is especially usefull for long running processes. A typical workflow can look like this: 1. execute screen to start the screen session 2. run the long running command 3. disconnect 4. connect to the server again 5. run screen -r to recconect to the session and see the results of the command. Sometimes, the server does not detect the connection failure and do not allow you to resume the session (step 5). In this way, we need to find the screen session ID and perform a detach and atach: 6. screen -ls 7. read the ID from the output and exec screen -rd <ID> WSL configuration port 22 can be used on Windows, so change port to 2222 in sshd_config when loging from Windows use 127.0.0.1 as a host Debugging check the ssh status with: service ssh status check the ssh port with sudo netstat -tpln Bash General Remarks It's important to use Linux newlines, otherwise, bash scripts will fail with unexpected character error Empty constructs are not allowed, i.e, empty function or loop results in an error brackets needs spaces around them, otherwise, there will be a syntax error Working with I/O <command A> | <command B> forward output of <command A> to th input of <command B> . <command> <<< $<variable> forward <variable> to the input of <command> <command 1> | tee >(<command 2>) | <command 3> : forward <command 1> to both <command 2> and <command 3> . Note that the content of the pipe cannot be examined, the process on the right hand side consume it. Therefore, it is not possible to simply branch on pipe content while use it in the subsequent process. Command Substitution When we need to use an output of a command instead of a constant or variable, we have to use command substtution: `<command>` # or equivalently $(<command>) e.g.: echo resut is: `cut -d, -f 7` # or equivalently echo resut is: $(cut -d, -f 7) Bash Script Arguments We refer the arguments of a bash script as - $0 - the name of the script - $1..$n - the arguments of the script - $@ - all the arguments of the script Conditions In general, condition in bash has the following syntax: if <condition> then <command> else <command> fi The condition above uses the exit code of a command to determine the logical value. When testing that the command succeeded, we can use it like: if <command> then ... fi Example: if grep -q \"$text\" $file then ... fi But if we want to use some arbitrary value (e. g. the return value of a command), or comparisons in the condition (simmilar to programming languages), we have to use the test command: if test <comparison> then ... fi The test command is also invoked if we use square brackets: if [ <comparison> ] then ... fi Example: if [ $var = 1 ] then ... fi The test command expects to compare to values. If we want to compare a result of some command, we need to usethe command substtitution. Loops while condition do <command1> <command2> ... done Forward to loop We can forward an input into while loop using | as usuall. Additionally, it is possible to read from file directly by adding < to the end like this: while condition do <command1> <command2> ... done < <input> The same goes for the output, i.e., we can forward th outut of a loop with | . Strings literals String literals can be easily defined as: str=\"string literal\" # or equivalently str='string literal' If we use double quotes, the variables are expanded, e.g., echo \"Hello $USER\" will print Hello <username> . The problem arises when we want to use double quotes in the string literal containing variables, e.g., normal string \"quotted string\" $myvar . In this case, we have to use quite cumbersome syntax: a = \"normal string \"\\\"\"quotted string\"\\\" # or equivalently a = \"normal string \"'\"'\"quotted string\"'\"' # Managing packages with `apt` and `dpkg` To **update the list** of possible updates: ```bash sudo apt update To perform the update : sudo apt upgrade To find the location of an installed package : dpkg -L <package> To search for a package: apt-cache search <package> Changing package repositories If the downolad speed is not satisfactory, we can change the repositories. To find the fastest repository from the list of nearby repositories, run: curl -s http://mirrors.ubuntu.com/mirrors.txt | xargs -n1 -I {} sh -c 'echo `curl -r 0-10240000 -s -w %{speed_download} -o /dev/null {}/ls-lR.gz` {}' | sort -g -r The number in the leftmost column indicates the bandwidth in bytes (larger number is better). To change the repositories to the best mirror, we need to replace the mirror in etc/apt/source.list . We can do it manually, however, to prevent the mistakes, it is better to use a dedicated python script: apt-mirror-updater . Steps: 1. install the python script: sudo pip install apt-mirror-updater 1. backup the old file: sudo cp sources.list sources.list.bak 1. change the mirror with the script: apt-mirror-updater -c <mirror URL> Note that the apt-mirror-updater script can also measure the bandwidth, however, the result does not seem to be reliable. String Modification sed Sed is a multi purpose command for string modification. It can search and replace in string. The syntax is folowing: sed s/<search>/<replace>/[<occurance>] Example: s/$/,3/ Any regex can be used as <search> . Some characters (e.g. | ) must be escaped with backslash and used together with the -E parameter. This replace the nd of the line with string \",3\" . Note that there is a slash at the end, despite we use the default option for occurance. Also, it can delete lines containing string using: sed /<pattern>/d cut Cut is a useful command for data with delimiters. Usage: cut -f <columns> Where columns are splited by comma, e.g., 2,4,7 . If we need to specify delimiters, we use the -d parameter: cut -d, -f 1,5 Trim string <command with string output> | xargs Installing Java Oracle JDK Go to the download page, the link to the dowload page for current version of java is on the main JDK page . Click on the debian package, accept the license, and download it. If installing on system without GUI, copy now (after accepting the license) the target link and dowload the debian package with wget : wget --header \"Cookie: oraclelicense=accept-securebackup-cookie\" <COPIED LINK> . More info on SO . Install the package with sudo apt-get install <PATH TO DOWNLOADED .deb> if there is a problem with the isntallation, check the file integritiy with: sha256 <PATH TO DOWNLOADED .deb> . It should match with the checksums refered on the download page. If not cheksums do not match, go back to download step. In case there is another version of Java alreadz install, we need to overwrite it using the update-alternatives command: sudo update-alternatives --install /usr/bin/java java <PATH TO JAVA> <PRIORITY> . Example: sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk-16/bin/java 2 To check the priorities, call update-alternatives --query java . The newly installed JDK should have the highest priority. Python Python has to be executed with python3 by default, instead of python . Other usefull commands Selecting lines from file The head and tail commands are for that, printing the top and bottom 10 lines, respectively. Skip the header tail -n +2 Print lines from to tail -n +<from line> | head -n <number of lines> Progress bar the progress bar can be printed using the pv command. pv <file> | <other comands> # or <other comands> | pv | <other comands> nabling SSH Access to Server 1. install openssh: - sudo apt update - sudo apt install openssh-server 2. configure access - password: 1. open /etc/ssh/sshd_config 2. set PasswordAuthentication yes 3. Now you can log in with the user and password you use in Ubuntu 3. keys: TODO Upgrade First run the update of the current version . Then optionaly backup the WSL run sudo do-release-upgrade WSL backup check the WSL distro name: wsl -l -v shutdown WSL: wsl --shutdown backup the distro: wsl --export <disto name> <backup folder path>/<backup name>.tar","title":"Usefull Comands"},{"location":"Ubuntu/Linux%20Manual/#usefull-comands","text":"","title":"Usefull Comands"},{"location":"Ubuntu/Linux%20Manual/#check-ubuntu-version","text":"lsb_release -a","title":"Check Ubuntu Version"},{"location":"Ubuntu/Linux%20Manual/#printing-used-ports","text":"sudo lsof -i -P","title":"Printing used ports"},{"location":"Ubuntu/Linux%20Manual/#checking-glibc-version","text":"ldd --version","title":"Checking GLIBC version"},{"location":"Ubuntu/Linux%20Manual/#exit-codes","text":"Exit code of the last command: $? Common exit codes for C++ programs","title":"Exit codes"},{"location":"Ubuntu/Linux%20Manual/#show-path-to-executable","text":"Use the which command: which <executable>","title":"Show path to executable"},{"location":"Ubuntu/Linux%20Manual/#unpack-file","text":"Foe unpacking, you can use the tar -f <file> command. The most used options are: - x : extract","title":"Unpack file"},{"location":"Ubuntu/Linux%20Manual/#environment-variables","text":"We will demonstrate it on the PATH example. If you have a program in a custom location, adding it to $PATH permanently and be able to run it under all circumstances is not an easy task on linux. Standard procedure to add a system variable: 1. Create a dedicated .sh file in /etc/profile.d. for your configuration (config for each app should be stored in a separate file). 2. the file should contain: export PATH=$PATH:YOURPATH 1. exit nano and save the file: ctrl+x and y 3. logout and login again to load the newly added varibales - for WSL close the console and reopen it, it is not necessary to restart the WSL ( click here for detailed description )","title":"Environment Variables"},{"location":"Ubuntu/Linux%20Manual/#enable-variable-with-sudo","text":"To enable the variable even if you use sudo , you need to edit sudo config using sudo visudo and: 1. exclude PATH from variable being reset when running sudo : Defaults env_keep += \"PATH\" 2. disable the safe path mechanism for sudo , i.e., comment the line: Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin 3. logout and login again to load the new added varibales","title":"Enable Variable with sudo"},{"location":"Ubuntu/Linux%20Manual/#enable-the-variable-outside-bash","text":"If you need the variable outside bash, the above-mentioned approach won\u2019t work. Currently, I do not know about any general solution for variables. The solution below, unfortunately, work only for PAM shells (see this SO answer why). Add the variable to /etc/environment . Note that it is not a script file, so you can use only simple variable assignments.","title":"Enable the Variable Outside Bash"},{"location":"Ubuntu/Linux%20Manual/#enable-variable-on-a-device-without-root-access","text":"Without root access, we can only edit the user config files. put the necessary config into: - ~/.bash_profile if it already exists - or to ~/.profile Note that the .profile file is ignored when the .bash_profile file exists.","title":"Enable Variable on a Device Without Root Access"},{"location":"Ubuntu/Linux%20Manual/#remove-windows-path-from-wsl","text":"By default, the PATH environment variable from Windows is inluded in the PATH variable in Ubuntu running in WSL. This can lead to conflicts, when the same executables are used in both systems (e.g., vcpkg, cmake). To turn of the Windows PATH inclusion: 1. open /etc/wsl.conf 1. add the fllowing code: [interop] appendWindowsPath = false restart WSL","title":"Remove Windows $PATH from WSL"},{"location":"Ubuntu/Linux%20Manual/#file-system","text":"","title":"File System"},{"location":"Ubuntu/Linux%20Manual/#copy-file","text":"The cp command is used to copy files. The most used options are: - -r , -R : copy recursively - -v : verbose - -f : force - -p : preserve permissions and timestamps - -a : same as -p -R plus some other options","title":"copy file"},{"location":"Ubuntu/Linux%20Manual/#network","text":"","title":"Network"},{"location":"Ubuntu/Linux%20Manual/#netstat","text":"The netstat command is the basic command to monitor the networ. It displays the TCP connections. It is available both on Linux and on Windows, although the interface differs. Important parameters: - -n : do not translate IP and ports into human readable names - -a : show all connections. Without this, some connections can be skipped.","title":"netstat"},{"location":"Ubuntu/Linux%20Manual/#ssh","text":"","title":"SSH"},{"location":"Ubuntu/Linux%20Manual/#ssh-tunneling","text":"An SSH tunnel can be created by the ssh command. The usuall syntax is following: ssh -L <local port>:<remote machine>:<remote port> <ssh server username>@<ssh server address> The -L argument stands for local port forwarding , i.e., we forward a local port to some remote port. Example: ssh -L 1111:localhost:5432 fiedler@its.fel.cvut.cz The local port (here 1111 ) is arbitrary, we can use any free port. The aplications using the tunnel should then be configured as: - host= localhost - port:= 1111 The remote machine is the address of the machine we want to access relative to the ssh server . If the ssh server is running on the computer we want to access through the tunnel, we can use localhost . Analogously, the remote port is the port we wan to use on the remote machine (here 5432 is a PostgreSQL db port). The ssh server username and ssh server address are then the username/address of the remote machine. On top of that, we need password or priveta key to validate our identity. Note that the credential here are the credential for the server, not the credentials of the service we are accessing through the ssh connection. Those credentials has to be usually supplied by the application accessing the service through the ssh tunnel. The connection can be canceled any time byping exit to the console. More information","title":"SSH Tunneling"},{"location":"Ubuntu/Linux%20Manual/#debugging-a-ssh-tunnel","text":"This guide suppose that the tunnel creation comman run without any error message. 1. If the tunnel seems to not work, first use a command line tool to be sure: - web browser for HTTP tunnels (remote port 80) - psql for postgeSQL tunnels (remote port 5432) - telnet for telnet tunnels (reote port 23) 2. If the test is unsucessful, try to kill all ssh connections to the server by shutting down all applications with ssh connections tunnels, untill there will be only one connection at the server (the console). The number of ssh connections can be checked with: sudo netstat -nap | grep :22","title":"Debugging a SSH tunnel"},{"location":"Ubuntu/Linux%20Manual/#enabnling-ssh-access-to-server","text":"install openssh: sudo apt update sudo apt install openssh-server configure access password: open /etc/ssh/sshd_config set PasswordAuothentication yes after restrat you can log in with the user and password used in Ubuntu keys: TODO restart the ssh server: sudo service ssh restart","title":"Enabnling SSH Access to Server"},{"location":"Ubuntu/Linux%20Manual/#wsl-configuration","text":"port 22 can be used on Windows, so change the port to 2222 in sshd_config when loging from Windows use 127.0.0.1 as a host","title":"WSL configuration"},{"location":"Ubuntu/Linux%20Manual/#debugging","text":"1n from now e as a t euin. check the ssh status with: s th service ssh status 2. check the ssh porot with sudso netstat t-tpln","title":"Debugging"},{"location":"Ubuntu/Linux%20Manual/#executing-a-long-running-process-over-ssh","text":"When the SSH connection to a server is disconnected (either manually, or by network failure or timeout), the process running in the console is canceled. To overcome this limitation, we can use the screen command, which is especially usefull for long running processes. A typical workflow can look like this: 1. execute screen to start the screen session 2. run the long running command 3. disconnect 4. connect to the server again 5. run screen -r to recconect to the session and see the results of the command. Sometimes, the server does not detect the connection failure and do not allow you to resume the session (step 5). In this way, we need to find the screen session ID and perform a detach and atach: 6. screen -ls 7. read the ID from the output and exec screen -rd <ID>","title":"Executing a long running process over SSH"},{"location":"Ubuntu/Linux%20Manual/#wsl-configuration_1","text":"port 22 can be used on Windows, so change port to 2222 in sshd_config when loging from Windows use 127.0.0.1 as a host","title":"WSL configuration"},{"location":"Ubuntu/Linux%20Manual/#debugging_1","text":"check the ssh status with: service ssh status check the ssh port with sudo netstat -tpln","title":"Debugging"},{"location":"Ubuntu/Linux%20Manual/#bash","text":"","title":"Bash"},{"location":"Ubuntu/Linux%20Manual/#general-remarks","text":"It's important to use Linux newlines, otherwise, bash scripts will fail with unexpected character error Empty constructs are not allowed, i.e, empty function or loop results in an error brackets needs spaces around them, otherwise, there will be a syntax error","title":"General Remarks"},{"location":"Ubuntu/Linux%20Manual/#working-with-io","text":"<command A> | <command B> forward output of <command A> to th input of <command B> . <command> <<< $<variable> forward <variable> to the input of <command> <command 1> | tee >(<command 2>) | <command 3> : forward <command 1> to both <command 2> and <command 3> . Note that the content of the pipe cannot be examined, the process on the right hand side consume it. Therefore, it is not possible to simply branch on pipe content while use it in the subsequent process.","title":"Working with I/O"},{"location":"Ubuntu/Linux%20Manual/#command-substitution","text":"When we need to use an output of a command instead of a constant or variable, we have to use command substtution: `<command>` # or equivalently $(<command>) e.g.: echo resut is: `cut -d, -f 7` # or equivalently echo resut is: $(cut -d, -f 7)","title":"Command Substitution"},{"location":"Ubuntu/Linux%20Manual/#bash-script-arguments","text":"We refer the arguments of a bash script as - $0 - the name of the script - $1..$n - the arguments of the script - $@ - all the arguments of the script","title":"Bash Script Arguments"},{"location":"Ubuntu/Linux%20Manual/#conditions","text":"In general, condition in bash has the following syntax: if <condition> then <command> else <command> fi The condition above uses the exit code of a command to determine the logical value. When testing that the command succeeded, we can use it like: if <command> then ... fi Example: if grep -q \"$text\" $file then ... fi But if we want to use some arbitrary value (e. g. the return value of a command), or comparisons in the condition (simmilar to programming languages), we have to use the test command: if test <comparison> then ... fi The test command is also invoked if we use square brackets: if [ <comparison> ] then ... fi Example: if [ $var = 1 ] then ... fi The test command expects to compare to values. If we want to compare a result of some command, we need to usethe command substtitution.","title":"Conditions"},{"location":"Ubuntu/Linux%20Manual/#loops","text":"while condition do <command1> <command2> ... done","title":"Loops"},{"location":"Ubuntu/Linux%20Manual/#forward-to-loop","text":"We can forward an input into while loop using | as usuall. Additionally, it is possible to read from file directly by adding < to the end like this: while condition do <command1> <command2> ... done < <input> The same goes for the output, i.e., we can forward th outut of a loop with | .","title":"Forward to loop"},{"location":"Ubuntu/Linux%20Manual/#strings-literals","text":"String literals can be easily defined as: str=\"string literal\" # or equivalently str='string literal' If we use double quotes, the variables are expanded, e.g., echo \"Hello $USER\" will print Hello <username> . The problem arises when we want to use double quotes in the string literal containing variables, e.g., normal string \"quotted string\" $myvar . In this case, we have to use quite cumbersome syntax: a = \"normal string \"\\\"\"quotted string\"\\\" # or equivalently a = \"normal string \"'\"'\"quotted string\"'\"' # Managing packages with `apt` and `dpkg` To **update the list** of possible updates: ```bash sudo apt update To perform the update : sudo apt upgrade To find the location of an installed package : dpkg -L <package> To search for a package: apt-cache search <package>","title":"Strings literals"},{"location":"Ubuntu/Linux%20Manual/#changing-package-repositories","text":"If the downolad speed is not satisfactory, we can change the repositories. To find the fastest repository from the list of nearby repositories, run: curl -s http://mirrors.ubuntu.com/mirrors.txt | xargs -n1 -I {} sh -c 'echo `curl -r 0-10240000 -s -w %{speed_download} -o /dev/null {}/ls-lR.gz` {}' | sort -g -r The number in the leftmost column indicates the bandwidth in bytes (larger number is better). To change the repositories to the best mirror, we need to replace the mirror in etc/apt/source.list . We can do it manually, however, to prevent the mistakes, it is better to use a dedicated python script: apt-mirror-updater . Steps: 1. install the python script: sudo pip install apt-mirror-updater 1. backup the old file: sudo cp sources.list sources.list.bak 1. change the mirror with the script: apt-mirror-updater -c <mirror URL> Note that the apt-mirror-updater script can also measure the bandwidth, however, the result does not seem to be reliable.","title":"Changing package repositories"},{"location":"Ubuntu/Linux%20Manual/#string-modification","text":"","title":"String Modification"},{"location":"Ubuntu/Linux%20Manual/#sed","text":"Sed is a multi purpose command for string modification. It can search and replace in string. The syntax is folowing: sed s/<search>/<replace>/[<occurance>] Example: s/$/,3/ Any regex can be used as <search> . Some characters (e.g. | ) must be escaped with backslash and used together with the -E parameter. This replace the nd of the line with string \",3\" . Note that there is a slash at the end, despite we use the default option for occurance. Also, it can delete lines containing string using: sed /<pattern>/d","title":"sed"},{"location":"Ubuntu/Linux%20Manual/#cut","text":"Cut is a useful command for data with delimiters. Usage: cut -f <columns> Where columns are splited by comma, e.g., 2,4,7 . If we need to specify delimiters, we use the -d parameter: cut -d, -f 1,5","title":"cut"},{"location":"Ubuntu/Linux%20Manual/#trim-string","text":"<command with string output> | xargs","title":"Trim string"},{"location":"Ubuntu/Linux%20Manual/#installing-java","text":"","title":"Installing Java"},{"location":"Ubuntu/Linux%20Manual/#oracle-jdk","text":"Go to the download page, the link to the dowload page for current version of java is on the main JDK page . Click on the debian package, accept the license, and download it. If installing on system without GUI, copy now (after accepting the license) the target link and dowload the debian package with wget : wget --header \"Cookie: oraclelicense=accept-securebackup-cookie\" <COPIED LINK> . More info on SO . Install the package with sudo apt-get install <PATH TO DOWNLOADED .deb> if there is a problem with the isntallation, check the file integritiy with: sha256 <PATH TO DOWNLOADED .deb> . It should match with the checksums refered on the download page. If not cheksums do not match, go back to download step. In case there is another version of Java alreadz install, we need to overwrite it using the update-alternatives command: sudo update-alternatives --install /usr/bin/java java <PATH TO JAVA> <PRIORITY> . Example: sudo update-alternatives --install /usr/bin/java java /usr/lib/jvm/jdk-16/bin/java 2 To check the priorities, call update-alternatives --query java . The newly installed JDK should have the highest priority.","title":"Oracle JDK"},{"location":"Ubuntu/Linux%20Manual/#python","text":"Python has to be executed with python3 by default, instead of python .","title":"Python"},{"location":"Ubuntu/Linux%20Manual/#other-usefull-commands","text":"","title":"Other usefull commands"},{"location":"Ubuntu/Linux%20Manual/#selecting-lines-from-file","text":"The head and tail commands are for that, printing the top and bottom 10 lines, respectively.","title":"Selecting lines from file"},{"location":"Ubuntu/Linux%20Manual/#skip-the-header","text":"tail -n +2","title":"Skip the header"},{"location":"Ubuntu/Linux%20Manual/#print-lines-from-to","text":"tail -n +<from line> | head -n <number of lines>","title":"Print lines from to"},{"location":"Ubuntu/Linux%20Manual/#progress-bar","text":"the progress bar can be printed using the pv command. pv <file> | <other comands> # or <other comands> | pv | <other comands> nabling SSH Access to Server 1. install openssh: - sudo apt update - sudo apt install openssh-server 2. configure access - password: 1. open /etc/ssh/sshd_config 2. set PasswordAuthentication yes 3. Now you can log in with the user and password you use in Ubuntu 3. keys: TODO","title":"Progress bar"},{"location":"Ubuntu/Linux%20Manual/#upgrade","text":"First run the update of the current version . Then optionaly backup the WSL run sudo do-release-upgrade","title":"Upgrade"},{"location":"Ubuntu/Linux%20Manual/#wsl-backup","text":"check the WSL distro name: wsl -l -v shutdown WSL: wsl --shutdown backup the distro: wsl --export <disto name> <backup folder path>/<backup name>.tar","title":"WSL backup"},{"location":"Windows/Powershell%20Manual/","text":"Important Aspects New PowerShell The PowerShell Integrated in Windows is version 5. You can recognize it by the iconical blue background color. This old version has some important limitations (e. g. it cannot pass arguments containing arguments with spaces ). Therefore, it is best to install the new PowerShell first. Quick Edit / Insert Mode PowerShell enables copy/pase of commands. The downside is that every time you click inside PowerShell, the execution (if PowerShell is currently executing somethig) stops. To resume the execution, hit enter . Arguments starting with - If a program argument starts with - , and contains . it needs to be wrapped by ' . Otherwise, the argument will be split on the dot. Example: In sommand prompt or on Linux: mvn exec:java -Dexec.mainClass=com.example.MyExample -Dfile.encoding=UTF-8 In Powershell, this needs to be converted to: mvn exec:java '-Dexec.mainClass=com.example.MyExample' '-Dfile.encoding=UTF-8' Escaping \" and ' in Arguments Double quotes \" contained in arguments can be preserved by escaping with backslash: \\\" . Example for that can be passing an argument list to some executable: 'args=\\\"arg1 arg2\\\"' Single quotes ' are esceped by duble single quote: '' . Example can be passing a list of args, where some of them contains space: 'args=\\\"''arg1 with space'' arg2\\\"' No Output for EXE file Some errors are unfortunatelly not reported by powershell (e.g. missing dll ). The solution is to run such program in cmd, which reports the error. SSH More details can be found in the Linux Manual - ssh should be ready in PowerShell, if not, add it in -> Apps & Features -> Optional features - specifying port by <addres>:<port> is not supported in PowerShell, it is necessary to use the -p parameter - if we need something else than the default shell just append the command at the end of the script Network netstat The netstat is the basic network monitoring program. It displays TCP connections. It originated on Linux, so the usage and parameters are described in the Linux manual. Below, we discuss the differencies in the command interface and behavior of the Windows version of the command. Winndows manual . Lot of kubernetes entries in the log By default, the netstat command translates IPs into names. Unfortunatelly, on Windows, it also uses the information from the hosts file ( C:\\Windows\\System32\\drivers\\etc\\hosts ). This is a problem, because some services, like Docker, can use the hosts file to redirect some adddress to localhost. Therefore, at the end, all localhost entries are named kubernetes. Solution options: - use the -n parameter for netstat or - remove the redirection to localhost in the hosts file Display executable for connection To display executable for all connection, just use the -b swith. For filtering out only some, you have to use the -Context parameter in the Select-String command, as the executable is printed one line below the connection: netstat -b | sls <search pattern> -Context 0,1 Select-String The Select-String is the grep equivalent for PowerShell. The alias for the comand is sls . Parameters: - -Content <before>[, <after>] : Select also <before> lines before the matched line and <after> lines after the matched line Usefull Commands Print system variable echo $env:PATH File Encoding Conversion gc [INPUT PATH] | Out-File -en [ENCODING] [OUTPUT PATH] example: gc \"C:\\AIC data\\Roadmaptools big data test/map-filtered.osm\" | Out-File -en utf8 \"C:\\AIC data\\Roadmaptools big data test/map-filtered-utf8.osm\" NOTE: it is not very fast :) Delete all files with a certain extension ls *.extension -Recurse | foreach {rm $_} to try it, add the -WhatIf parameter to rm Batch rename Example dir . | % { $newName = $_.Name -replace '^DSC_0(.*)', 'DSC_1$1'; rename-item -newname $newName -literalPath $_.Fullname -whatif} Count Lines in large file switch -File FILE { default { ++$count } }","title":"Important Aspects"},{"location":"Windows/Powershell%20Manual/#important-aspects","text":"","title":"Important Aspects"},{"location":"Windows/Powershell%20Manual/#new-powershell","text":"The PowerShell Integrated in Windows is version 5. You can recognize it by the iconical blue background color. This old version has some important limitations (e. g. it cannot pass arguments containing arguments with spaces ). Therefore, it is best to install the new PowerShell first.","title":"New PowerShell"},{"location":"Windows/Powershell%20Manual/#quick-edit-insert-mode","text":"PowerShell enables copy/pase of commands. The downside is that every time you click inside PowerShell, the execution (if PowerShell is currently executing somethig) stops. To resume the execution, hit enter .","title":"Quick Edit / Insert Mode"},{"location":"Windows/Powershell%20Manual/#arguments-starting-with-","text":"If a program argument starts with - , and contains . it needs to be wrapped by ' . Otherwise, the argument will be split on the dot. Example: In sommand prompt or on Linux: mvn exec:java -Dexec.mainClass=com.example.MyExample -Dfile.encoding=UTF-8 In Powershell, this needs to be converted to: mvn exec:java '-Dexec.mainClass=com.example.MyExample' '-Dfile.encoding=UTF-8'","title":"Arguments starting with -"},{"location":"Windows/Powershell%20Manual/#escaping-and-in-arguments","text":"Double quotes \" contained in arguments can be preserved by escaping with backslash: \\\" . Example for that can be passing an argument list to some executable: 'args=\\\"arg1 arg2\\\"' Single quotes ' are esceped by duble single quote: '' . Example can be passing a list of args, where some of them contains space: 'args=\\\"''arg1 with space'' arg2\\\"'","title":"Escaping \" and ' in Arguments"},{"location":"Windows/Powershell%20Manual/#no-output-for-exe-file","text":"Some errors are unfortunatelly not reported by powershell (e.g. missing dll ). The solution is to run such program in cmd, which reports the error.","title":"No Output for EXE file"},{"location":"Windows/Powershell%20Manual/#ssh","text":"More details can be found in the Linux Manual - ssh should be ready in PowerShell, if not, add it in -> Apps & Features -> Optional features - specifying port by <addres>:<port> is not supported in PowerShell, it is necessary to use the -p parameter - if we need something else than the default shell just append the command at the end of the script","title":"SSH"},{"location":"Windows/Powershell%20Manual/#network","text":"","title":"Network"},{"location":"Windows/Powershell%20Manual/#netstat","text":"The netstat is the basic network monitoring program. It displays TCP connections. It originated on Linux, so the usage and parameters are described in the Linux manual. Below, we discuss the differencies in the command interface and behavior of the Windows version of the command. Winndows manual .","title":"netstat"},{"location":"Windows/Powershell%20Manual/#lot-of-kubernetes-entries-in-the-log","text":"By default, the netstat command translates IPs into names. Unfortunatelly, on Windows, it also uses the information from the hosts file ( C:\\Windows\\System32\\drivers\\etc\\hosts ). This is a problem, because some services, like Docker, can use the hosts file to redirect some adddress to localhost. Therefore, at the end, all localhost entries are named kubernetes. Solution options: - use the -n parameter for netstat or - remove the redirection to localhost in the hosts file","title":"Lot of kubernetes entries in the log"},{"location":"Windows/Powershell%20Manual/#display-executable-for-connection","text":"To display executable for all connection, just use the -b swith. For filtering out only some, you have to use the -Context parameter in the Select-String command, as the executable is printed one line below the connection: netstat -b | sls <search pattern> -Context 0,1","title":"Display executable for connection"},{"location":"Windows/Powershell%20Manual/#select-string","text":"The Select-String is the grep equivalent for PowerShell. The alias for the comand is sls . Parameters: - -Content <before>[, <after>] : Select also <before> lines before the matched line and <after> lines after the matched line","title":"Select-String"},{"location":"Windows/Powershell%20Manual/#usefull-commands","text":"","title":"Usefull Commands"},{"location":"Windows/Powershell%20Manual/#print-system-variable","text":"echo $env:PATH","title":"Print system variable"},{"location":"Windows/Powershell%20Manual/#file-encoding-conversion","text":"gc [INPUT PATH] | Out-File -en [ENCODING] [OUTPUT PATH] example: gc \"C:\\AIC data\\Roadmaptools big data test/map-filtered.osm\" | Out-File -en utf8 \"C:\\AIC data\\Roadmaptools big data test/map-filtered-utf8.osm\" NOTE: it is not very fast :)","title":"File Encoding Conversion"},{"location":"Windows/Powershell%20Manual/#delete-all-files-with-a-certain-extension","text":"ls *.extension -Recurse | foreach {rm $_} to try it, add the -WhatIf parameter to rm","title":"Delete all files with a certain extension"},{"location":"Windows/Powershell%20Manual/#batch-rename","text":"Example dir . | % { $newName = $_.Name -replace '^DSC_0(.*)', 'DSC_1$1'; rename-item -newname $newName -literalPath $_.Fullname -whatif}","title":"Batch rename"},{"location":"Windows/Powershell%20Manual/#count-lines-in-large-file","text":"switch -File FILE { default { ++$count } }","title":"Count Lines in large file"},{"location":"Windows/Windows%20Manual/","text":"General Guides For PowerShell solutions/guides, check the PoweShell manual Keyboard Shortcuts Alt + Shift : change input language Win + Space : change keyboard input method Wireless Network Problem: Can't connect to this network Solution: Forget the connection and connect to the network manually Connect to a Network Manually Control Panel -> Network and Internet -> Network and Sharing Center Set up a new connection or network Manually connect to a wireless network Fill the credentials: Network name: SSID Security type: dpends, try WPA2 personal Security key: password Click next Close the dialog Click the wifi icon in the taskbar and connect to the network There are various usefull comands. For most of the commands, you need to open PowerShell as admin. Various Commands Related to the Wifi Show All Network Profiles This command show network configurations stored on the device. netsh wlan show profile Various Wifi Reports in HTML netsh wlan show wlanreport Changing the input method It is possible to let the system have a different input method for each app. It is not possible however, to remember the input method (after app/OS restart). Troubleshooting Nothing happens after clicking on the input method in the taskbar (windows 10) restrat the computer :) Bluetooth Troiubleshooting Cannot connect to the device Try to remove the device and pair it with the PC again If it does not help, proceeed to the next section (even if the pairing is successfull) Cannot pair with the device Turn off the device and unplug it from the electricity/remove batteries. Then plug it back after ~10 seconds, power it of, and try to pair with it again. Bluetooth Command Line Tools https://bluetoothinstaller.com/bluetooth-command-line-tools Bluetooth Command Line Tools is a set off tools that enables command line interaction with blootooth services. Basic usage: - discover and list available devices: btdiscovery -s Sugarsync Quick orientation in the desktop app: - for file changes, check left menu -> Activity - for deleted files, check left menu -> Deleted Items Solving sync problems check if the file is updated in cloud using web browser if not, check the activity log on the computer with the updated file if the change is not in the log, a simple hack can help: copy the file outside SugarSync folder and back. Useful Commands Get Motherboard Info wmic baseboard get product,Manufacturer,version,serialnumber Copy multiple files/dirs robocopy is the best command for that. Usefull params: - /e : Copy subdirectories, including empty ones - /b : Copy in the backup mode, so that even files with a different owner can be copied - /xc : Excludes changed files. - /xn : Excludes newer files. - /xo : Excludes older files. - /r:<n> : Specifies the number of retries on failed copies. The default value of n is 1,000,000 (one million retries). - /w:<n> : Specifies the wait time between retries, in seconds. The default value of n is 30 (wait time 30 seconds). Installation Windows 11 Windows 11 can be installed only as an update of Windows 10. Windows 10 Can be installed from bootable USB created by a tool downloaded from the official Miccosoft website. Single image for all Windows versions, a particular version is choosen based on the license key. Steps: 1. Download the install tool from Microsoft 2. Create a bootable USB 3. Start the installation 4. Fill in the licence key we couldn\u2019t create a partition or locate an existing one Ensure that the boot priority of the drive where the Windows should be installed is right behind the installation USB priority. Diskpart Diskpart is a useful command line tool for work with diska, partitions, etc. Find out wheteher a disk is MBR or GPT Open Command Promp from the Windows 10 USB Insert the USB stick Wait till the first installation screen shift + F10 Firewall Generate firewall logs Go to Windows firewall and select properties on the right At the top, choose the profile corresponding to the current network profile In the logging section, click to customizze set both DROP and ACCEPT to yes Do not forgot to turn of the logging after the investigation! SSH For ssh, we can use the standard ssh commannd available in PowerShell (check Linux manual for more info). However, for more features, we can use more sophisticated programs - KiTTY for credentials storage, automatic reconection, etc. - [WinSCP] for file manipulation KiTTY It is best to use the portable version, so that nothing is stored in the Windows registry. Configurtation: - copy the PuTTY credentials : .\\kitty_portable-0.76.1.3.exe -convert-dir - auto reconnect: Connection -> auto reconnect on connection failure and auto reconnect on system wakeup Problems Folder Sharing Problems Note that updated Windows 10 disabled anonymous sharing , so password protected sharing has to be turned on. To login, use the credentials for the computer with the shared folder . Below is a list of possible problems, together with solutions. The user name or password is incorrect Check whether the computer can be seen in the network. If not, resolve this issue first. quick check by running net view <IP address> Check that you are using the right username. You need to use the username and password of the computer you are connecting to . Check that the user name is correct by running net user on the target computer Check that the folder is shared with you in: right click on the folder -> Properties -> Sharing -> Advanced Sharing... -> Permisions . Note that your full name can be there instead of your username, which is OK. Check that you are using the right password. You have to use the password associated with your microsoft account. Note that it can differ from the password (PIN) you are using to log in to the computer! check it on the command line: net use * \\\\<IP address>\\<drive letter>$ /use:<username> <password> Folder right and ownership cannot be read Try to clear the windows filecache (CCcleaner or restart) Computer does not see itself in Network section in File Explorer Solution to this problem is to restart the service called Function Discovery Resource Publication . Either restart it in Computer Management -> Services, or by: net stop FDResPub net start FDResPub PC wakes up or cannot enter sleep 1 Find the source Using the Event viwer 1. open the event viewer 2. go to windows logs -> system 3. In case of wake up 1. inspect the logs when the wake up happened and search for the Information log with the message \"The system has returned from a low power state.\" 2. There is a wake up source in the end of the log message. If the soure is Unknown go to the next section 4. In case of not entering sleep 1. Search for the any kernel power event 2. If there is an event stating: The system is entering connected standby , it means that the modern fake sleep is present in the system, replacing the real sleep mode. Using command line (admin): 1. Try powercfg -lastwake 2. If the results are not know, try to call powercfg -devicequery wake_armed to get the list of devices that can wake the computer 2 Solve the problem Device waken up by network adapter Open device manager and search for the specific network adapter right click -> Properties -> Power Management Check Only allow a magic packet to wake up the computer The real sleep mode is not available on the system If this is the case, use the hibernate mode instead. To add it to the start menu: 1. go to Control panel -> Hardware and sound -> Power options 2. click on the left panel to Choose what the power buttons does 3. click on Change settings that are currently unavailable 4. check the hibernate checkbox below Camera problem Symptoms: the screen is blank, black, single color, in all apps and there are no problems reported in device manager Cause: it can be caused by some external cameras (now disconnected) that are still selected in the apps using the camera. Go Solution: Go to the app setting and select the correct camera Phone app cannot see the connected cell phone It can be due to the fucked up Windows N edition. Just install the normal edition. vmmem process uses a lot of CPU This process represents all virtual systems. One cultprit is therefore WSL. Try to shutdown the WSL using wsl --shutdown","title":"General Guides"},{"location":"Windows/Windows%20Manual/#general-guides","text":"For PowerShell solutions/guides, check the PoweShell manual","title":"General Guides"},{"location":"Windows/Windows%20Manual/#keyboard-shortcuts","text":"Alt + Shift : change input language Win + Space : change keyboard input method","title":"Keyboard Shortcuts"},{"location":"Windows/Windows%20Manual/#wireless-network","text":"","title":"Wireless Network"},{"location":"Windows/Windows%20Manual/#problem-cant-connect-to-this-network","text":"Solution: Forget the connection and connect to the network manually","title":"Problem: Can't connect to this network"},{"location":"Windows/Windows%20Manual/#connect-to-a-network-manually","text":"Control Panel -> Network and Internet -> Network and Sharing Center Set up a new connection or network Manually connect to a wireless network Fill the credentials: Network name: SSID Security type: dpends, try WPA2 personal Security key: password Click next Close the dialog Click the wifi icon in the taskbar and connect to the network There are various usefull comands. For most of the commands, you need to open PowerShell as admin.","title":"Connect to a Network Manually"},{"location":"Windows/Windows%20Manual/#various-commands-related-to-the-wifi","text":"","title":"Various Commands Related to the Wifi"},{"location":"Windows/Windows%20Manual/#show-all-network-profiles","text":"This command show network configurations stored on the device. netsh wlan show profile","title":"Show All Network Profiles"},{"location":"Windows/Windows%20Manual/#various-wifi-reports-in-html","text":"netsh wlan show wlanreport","title":"Various Wifi Reports in HTML"},{"location":"Windows/Windows%20Manual/#changing-the-input-method","text":"It is possible to let the system have a different input method for each app. It is not possible however, to remember the input method (after app/OS restart).","title":"Changing the input method"},{"location":"Windows/Windows%20Manual/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"Windows/Windows%20Manual/#nothing-happens-after-clicking-on-the-input-method-in-the-taskbar-windows-10","text":"restrat the computer :)","title":"Nothing happens after clicking on the input method in the taskbar (windows 10)"},{"location":"Windows/Windows%20Manual/#bluetooth","text":"","title":"Bluetooth"},{"location":"Windows/Windows%20Manual/#troiubleshooting","text":"","title":"Troiubleshooting"},{"location":"Windows/Windows%20Manual/#cannot-connect-to-the-device","text":"Try to remove the device and pair it with the PC again If it does not help, proceeed to the next section (even if the pairing is successfull)","title":"Cannot connect to the device"},{"location":"Windows/Windows%20Manual/#cannot-pair-with-the-device","text":"Turn off the device and unplug it from the electricity/remove batteries. Then plug it back after ~10 seconds, power it of, and try to pair with it again.","title":"Cannot pair with the device"},{"location":"Windows/Windows%20Manual/#bluetooth-command-line-tools","text":"https://bluetoothinstaller.com/bluetooth-command-line-tools Bluetooth Command Line Tools is a set off tools that enables command line interaction with blootooth services. Basic usage: - discover and list available devices: btdiscovery -s","title":"Bluetooth Command Line Tools"},{"location":"Windows/Windows%20Manual/#sugarsync","text":"Quick orientation in the desktop app: - for file changes, check left menu -> Activity - for deleted files, check left menu -> Deleted Items","title":"Sugarsync"},{"location":"Windows/Windows%20Manual/#solving-sync-problems","text":"check if the file is updated in cloud using web browser if not, check the activity log on the computer with the updated file if the change is not in the log, a simple hack can help: copy the file outside SugarSync folder and back.","title":"Solving sync problems"},{"location":"Windows/Windows%20Manual/#useful-commands","text":"","title":"Useful Commands"},{"location":"Windows/Windows%20Manual/#get-motherboard-info","text":"wmic baseboard get product,Manufacturer,version,serialnumber","title":"Get Motherboard Info"},{"location":"Windows/Windows%20Manual/#copy-multiple-filesdirs","text":"robocopy is the best command for that. Usefull params: - /e : Copy subdirectories, including empty ones - /b : Copy in the backup mode, so that even files with a different owner can be copied - /xc : Excludes changed files. - /xn : Excludes newer files. - /xo : Excludes older files. - /r:<n> : Specifies the number of retries on failed copies. The default value of n is 1,000,000 (one million retries). - /w:<n> : Specifies the wait time between retries, in seconds. The default value of n is 30 (wait time 30 seconds).","title":"Copy multiple files/dirs"},{"location":"Windows/Windows%20Manual/#installation","text":"","title":"Installation"},{"location":"Windows/Windows%20Manual/#windows-11","text":"Windows 11 can be installed only as an update of Windows 10.","title":"Windows 11"},{"location":"Windows/Windows%20Manual/#windows-10","text":"Can be installed from bootable USB created by a tool downloaded from the official Miccosoft website. Single image for all Windows versions, a particular version is choosen based on the license key. Steps: 1. Download the install tool from Microsoft 2. Create a bootable USB 3. Start the installation 4. Fill in the licence key","title":"Windows 10"},{"location":"Windows/Windows%20Manual/#we-couldnt-create-a-partition-or-locate-an-existing-one","text":"Ensure that the boot priority of the drive where the Windows should be installed is right behind the installation USB priority.","title":"we couldn\u2019t create a partition or locate an existing one"},{"location":"Windows/Windows%20Manual/#diskpart","text":"Diskpart is a useful command line tool for work with diska, partitions, etc.","title":"Diskpart"},{"location":"Windows/Windows%20Manual/#find-out-wheteher-a-disk-is-mbr-or-gpt","text":"","title":"Find out wheteher a disk is MBR or GPT"},{"location":"Windows/Windows%20Manual/#open-command-promp-from-the-windows-10-usb","text":"Insert the USB stick Wait till the first installation screen shift + F10","title":"Open Command Promp from the Windows 10 USB"},{"location":"Windows/Windows%20Manual/#firewall","text":"","title":"Firewall"},{"location":"Windows/Windows%20Manual/#generate-firewall-logs","text":"Go to Windows firewall and select properties on the right At the top, choose the profile corresponding to the current network profile In the logging section, click to customizze set both DROP and ACCEPT to yes Do not forgot to turn of the logging after the investigation!","title":"Generate firewall logs"},{"location":"Windows/Windows%20Manual/#ssh","text":"For ssh, we can use the standard ssh commannd available in PowerShell (check Linux manual for more info). However, for more features, we can use more sophisticated programs - KiTTY for credentials storage, automatic reconection, etc. - [WinSCP] for file manipulation","title":"SSH"},{"location":"Windows/Windows%20Manual/#kitty","text":"It is best to use the portable version, so that nothing is stored in the Windows registry. Configurtation: - copy the PuTTY credentials : .\\kitty_portable-0.76.1.3.exe -convert-dir - auto reconnect: Connection -> auto reconnect on connection failure and auto reconnect on system wakeup","title":"KiTTY"},{"location":"Windows/Windows%20Manual/#problems","text":"","title":"Problems"},{"location":"Windows/Windows%20Manual/#folder-sharing-problems","text":"Note that updated Windows 10 disabled anonymous sharing , so password protected sharing has to be turned on. To login, use the credentials for the computer with the shared folder . Below is a list of possible problems, together with solutions.","title":"Folder Sharing Problems"},{"location":"Windows/Windows%20Manual/#the-user-name-or-password-is-incorrect","text":"Check whether the computer can be seen in the network. If not, resolve this issue first. quick check by running net view <IP address> Check that you are using the right username. You need to use the username and password of the computer you are connecting to . Check that the user name is correct by running net user on the target computer Check that the folder is shared with you in: right click on the folder -> Properties -> Sharing -> Advanced Sharing... -> Permisions . Note that your full name can be there instead of your username, which is OK. Check that you are using the right password. You have to use the password associated with your microsoft account. Note that it can differ from the password (PIN) you are using to log in to the computer! check it on the command line: net use * \\\\<IP address>\\<drive letter>$ /use:<username> <password>","title":"The user name or password is incorrect"},{"location":"Windows/Windows%20Manual/#folder-right-and-ownership-cannot-be-read","text":"Try to clear the windows filecache (CCcleaner or restart)","title":"Folder right and ownership cannot be read"},{"location":"Windows/Windows%20Manual/#computer-does-not-see-itself-in-network-section-in-file-explorer","text":"Solution to this problem is to restart the service called Function Discovery Resource Publication . Either restart it in Computer Management -> Services, or by: net stop FDResPub net start FDResPub","title":"Computer does not see itself in Network section in File Explorer"},{"location":"Windows/Windows%20Manual/#pc-wakes-up-or-cannot-enter-sleep","text":"","title":"PC wakes up or cannot enter sleep"},{"location":"Windows/Windows%20Manual/#1-find-the-source","text":"Using the Event viwer 1. open the event viewer 2. go to windows logs -> system 3. In case of wake up 1. inspect the logs when the wake up happened and search for the Information log with the message \"The system has returned from a low power state.\" 2. There is a wake up source in the end of the log message. If the soure is Unknown go to the next section 4. In case of not entering sleep 1. Search for the any kernel power event 2. If there is an event stating: The system is entering connected standby , it means that the modern fake sleep is present in the system, replacing the real sleep mode. Using command line (admin): 1. Try powercfg -lastwake 2. If the results are not know, try to call powercfg -devicequery wake_armed to get the list of devices that can wake the computer","title":"1 Find the source"},{"location":"Windows/Windows%20Manual/#2-solve-the-problem","text":"","title":"2 Solve the problem"},{"location":"Windows/Windows%20Manual/#device-waken-up-by-network-adapter","text":"Open device manager and search for the specific network adapter right click -> Properties -> Power Management Check Only allow a magic packet to wake up the computer","title":"Device waken up by network adapter"},{"location":"Windows/Windows%20Manual/#the-real-sleep-mode-is-not-available-on-the-system","text":"If this is the case, use the hibernate mode instead. To add it to the start menu: 1. go to Control panel -> Hardware and sound -> Power options 2. click on the left panel to Choose what the power buttons does 3. click on Change settings that are currently unavailable 4. check the hibernate checkbox below","title":"The real sleep mode is not available on the system"},{"location":"Windows/Windows%20Manual/#camera-problem","text":"Symptoms: the screen is blank, black, single color, in all apps and there are no problems reported in device manager Cause: it can be caused by some external cameras (now disconnected) that are still selected in the apps using the camera. Go Solution: Go to the app setting and select the correct camera","title":"Camera problem"},{"location":"Windows/Windows%20Manual/#phone-app-cannot-see-the-connected-cell-phone","text":"It can be due to the fucked up Windows N edition. Just install the normal edition.","title":"Phone app cannot see the connected cell phone"},{"location":"Windows/Windows%20Manual/#vmmem-process-uses-a-lot-of-cpu","text":"This process represents all virtual systems. One cultprit is therefore WSL. Try to shutdown the WSL using wsl --shutdown","title":"vmmem process uses a lot of CPU"}]}